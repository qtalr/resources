[
  {
    "objectID": "guides/guide-06/index.html",
    "href": "guides/guide-06/index.html",
    "title": "06. Identifying data and data sources",
    "section": "",
    "text": "Outcomes\n\nRecognize the difference between various sources of data and datasets.\nIdentify data and/or datasets that are relevant to your research project.\nLocate and access data and/datasets used in the textbook and resources."
  },
  {
    "objectID": "guides/guide-06/index.html#introduction",
    "href": "guides/guide-06/index.html#introduction",
    "title": "06. Identifying data and data sources",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "guides/guide-06/index.html#data-sharing-platforms",
    "href": "guides/guide-06/index.html#data-sharing-platforms",
    "title": "06. Identifying data and data sources",
    "section": "Data sharing platforms",
    "text": "Data sharing platforms\nThere are many data sharing platforms that include various types of research materials, often including datasets.\n\n\n\n\n\n\n\n\n\nPlatform\nDescription\nURL\n\n\n\n\nDataverse\nDataverse is an open-source web application to share, preserve, cite, explore, and analyze research data.\nhttps://dataverse.org/\n\n\nFigshare\nFigshare is a repository where users can make all of their research outputs available in a citable, shareable, and discoverable manner.\nhttps://figshare.com/\n\n\nZenodo\nZenodo is a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN.\nhttps://zenodo.org/\n\n\nDryad\nDryad is a curated general-purpose repository that makes the data underlying scientific publications discoverable, freely reusable, and citable.\nhttps://datadryad.org/\n\n\nOpen Science Framework\nThe Open Science Framework (OSF) is a free, open-source web application built to help researchers manage their workflows.\nhttps://osf.io/"
  },
  {
    "objectID": "guides/guide-06/index.html#language-data-repositories",
    "href": "guides/guide-06/index.html#language-data-repositories",
    "title": "06. Identifying data and data sources",
    "section": "Language data repositories",
    "text": "Language data repositories\n\n\n\n\n\n\n\n\n\nPlatform\nDescription\nURL\n\n\n\n\nLinguistic Data Consortium\nThe Linguistic Data Consortium is an open consortium of universities, companies, and government research laboratories.\nhttps://www.ldc.upenn.edu/\n\n\nOpen Language Archives Community\nThe Open Language Archives Community (OLAC) is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources.\nhttp://www.language-archives.org/\n\n\nThe Language Archive\nThe Language Archive is a digital repository for language resources.\nhttps://tla.mpi.nl/\n\n\nThe Language Bank\nThe Language Bank is a digital repository for language resources.\nhttps://www.sprakbanken.se/\n\n\nTalkBank\nTalkBank is a system for sharing and studying conversational interactions.\nhttps://talkbank.org/\n\n\nOxford Text Archive\nThe Oxford Text Archive develops, collects, catalogues, and preserves electronic literary and linguistic resources.\nhttps://ota.bodleian.ox.ac.uk/"
  },
  {
    "objectID": "guides/guide-06/index.html#developed-corpora",
    "href": "guides/guide-06/index.html#developed-corpora",
    "title": "06. Identifying data and data sources",
    "section": "Developed corpora",
    "text": "Developed corpora\n\n\n\n\n\n\n\n\n\nPlatform\nDescription\nURL\n\n\n\n\nBritish National Corpus\nThe British National Corpus (BNC) is a 100-million-word text corpus of samples of written and spoken language from a wide range of sources.\nhttps://www.english-corpora.org/bnc/\n\n\nAmerican National Corpus\nThe American National Corpus (ANC) is a text corpus of American English.\nhttps://anc.org/\n\n\nCorpus of Contemporary American English\nThe Corpus of Contemporary American English (COCA) is the largest freely-available corpus of English, and the only large and balanced corpus of American English.\nhttps://www.english-corpora.org/coca/"
  },
  {
    "objectID": "guides/guide-06/index.html#data-sources-in-the-textbook-and-resources",
    "href": "guides/guide-06/index.html#data-sources-in-the-textbook-and-resources",
    "title": "06. Identifying data and data sources",
    "section": "Data sources in the textbook and resources",
    "text": "Data sources in the textbook and resources\n\nTextbook\n\n\n\n\n\n\n\n\n\n\nDataset\nLocation(s)\nDescription\nURL\n\n\n\n\nmasc\nCh. 2 and 8\nThe masc dataset is drawn from the Manually Annotated Sub-Corpus (MASC) of the American National Corpus.\nhttps://anc.org/data/masc/\n\n\nbelc\nCh. 3\nThe belc dataset is acquired from the TalkBank repository. It is a dataset that contains the results of a study on the use of English as a second language. On the written portion is used.\nhttps://talkbank.org/\n\n\ncedel2\nCh. 5 and 9\nA corpus of Spanish as a second language. This dataset appears in chapter 5.\nhttp://cedel2.learnercorpora.com/\n\n\nswda\nCh. 5\nThe Switchboard Dialog Act Corpus (SWDA) is a corpus of telephone conversations.\nhttps://catalog.ldc.upenn.edu/docs/LDC97S62/\n\n\ncabnc\nCh. 5 and 6\nThe spoken portion of the British National Corpus. It is available through Talkbank.\nhttps://ca.talkbank.org/access/CABNC.html\n\n\neuroparl\nCh. 6 and 7\nThe Europarl Parallel Corpus is a parallel corpus of the European Parliament proceedings.\nhttps://www.statmt.org/europarl/\n\n\nenntt\nCh. 6 and 7\nThe Europarl Corpus of Native and Non-Native and Translated Texts (ENNTT) is a parallel corpus of the European Parliament proceedings.\nhttps://github.com/senisioi/enntt-release\n\n\ndative\nCh. 10\nThe dative from the {languageR} package is a dataset that contains the results of a study on the use of dative constructions in English.\nhttps://cran.r-project.org/web/packages/languageR/languageR.pdf"
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "01. Setting up an R environment\n\n\n\n\n\nIn this guide, we will explore options for setting up an R environment. We will discuss local, remote, and virtual environments. Each have their own advantages and shortcomings. The best option for you will depend on your needs and preferences. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n02. Installing and managing R packages\n\n\n\n\n\nIn this guide, we will cover how to install and manage R packages. We will discuss two primary methods for installing packages: using the RStudio IDE interface and using the R console. We will also cover how to attach and detach packages in an R session, and how to manage packages by listing, updating, and removing them. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03. Working with the interactive R programming lessons\n\n\n\n\n\nIn this guide, we provide an overview of the interactive R programming lessons, explain how to access the lessons, use the lessons, and remove the lessons. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n04. Setting up Git and GitHub\n\n\n\n\n\nIn this guide, we will cover the basics of setting up Git and GitHub. We will also cover the basics of using Git and GitHub to manage a project. This guide is intended for beginners who are new to Git and GitHub. It is also intended for those who are new to using Git and GitHub with R. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n05. Creating reproducible examples\n\n\nHelping you help yourself\n\n\nIn this guide, we will explore how to create reproducible examples using {reprex}. Reproducible examples are essential for effective communication and collaboration among data scientists and statisticians. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n06. Identifying data and data sources\n\n\n\n\n\nThis guide will outline some key data and dataset resources that will be usefull for your own text analysis projects. This guide is not exhaustive, but it will provide you with a good starting point for you to start to explore different types of data and data sources. \n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/guide-03/index.html",
    "href": "guides/guide-03/index.html",
    "title": "03. Working with the interactive R programming lessons",
    "section": "",
    "text": "Outcomes\n\n\nRecall the purpose of the interactive R programming lessons.\nSet up the interactive R programming lessons.\nUnderstand how to use the interactive R programming lessons."
  },
  {
    "objectID": "guides/guide-03/index.html#purpose",
    "href": "guides/guide-03/index.html#purpose",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Purpose",
    "text": "Purpose\nThe interactive R programming lessons are designed to help you learn R programming by doing. The lessons are interactive, meaning you can complete them directly in R and feedback is provided to ensure that you understand the concepts covered and can complete the lessons. The lessons cover a range of topics. In the early chapters, the lessons cover R fundamentals, such as data types, data structures, and functions. In the later chapters, the lessons prepare you for working with the programming concepts connected to the textbook, recipes, and labs.\nYou can preview the topics covered in the lessons by visiting the Lessons repository on GitHub. To access and to complete the lessons read below."
  },
  {
    "objectID": "guides/guide-03/index.html#access-the-lessons",
    "href": "guides/guide-03/index.html#access-the-lessons",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Access the lessons",
    "text": "Access the lessons\nTo access the interactive R programming lessons, you need to install the {swirl} package and download the lessons.\ninstall.packages(\"swirl\")\nlibrary(swirl)\ninstall_course_github(\"qtalr/Lessons\")\nThis code only needs to be run once. After you have installed the {swirl} package and downloaded the lessons, you can use the lessons whenever you want."
  },
  {
    "objectID": "guides/guide-03/index.html#use-the-lessons",
    "href": "guides/guide-03/index.html#use-the-lessons",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Use the lessons",
    "text": "Use the lessons\nTo use the interactive R programming lessons, you need to load the {swirl} package and run the swirl() function.\nlibrary(swirl)\nswirl()\n\n\n\n\n\n\nFigure 1: Run swirl(), select a course and a lesson to start\n\n\n\nWhen you run the swirl() function, you will be prompted to select a course. Select the course you want to complete and follow the prompts to complete the lessons.\nEach lesson will include a series of questions and exercises that you need to complete. You will be typing code directly into the R console to complete the exercises which will help you get accustomed to working with R in the R console.\n\n\n\n\n\n\n Tip\nQuick tip for working in the R console, you can use the up and down arrow keys on your keyboard to navigate through your command history. This can be helpful if you want to reuse a command that you previously ran."
  },
  {
    "objectID": "guides/guide-03/index.html#uninstall-the-lessons-optional",
    "href": "guides/guide-03/index.html#uninstall-the-lessons-optional",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Uninstall the lessons (optional)",
    "text": "Uninstall the lessons (optional)\nIf you want to uninstall the interactive R programming lessons, you can run the following code.\nlibrary(swirl)\nuninstall_course(\"Lessons\")\nThis code will remove the current lessons from your computer. If you want to reinstall the lessons, you can run the code above again."
  },
  {
    "objectID": "guides/guide-05/index.html",
    "href": "guides/guide-05/index.html",
    "title": "05. Creating reproducible examples",
    "section": "",
    "text": "Outcomes\n\nUnderstand the importance of reproducible examples\nCreate a reproducible example using {reprex} and other tools\nShare your reproducible example with others"
  },
  {
    "objectID": "guides/guide-05/index.html#introduction",
    "href": "guides/guide-05/index.html#introduction",
    "title": "05. Creating reproducible examples",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is a reproducible example?\nReproducible examples are crucial for effectively communicating problems, solutions, and ideas in the world of data science. In most cases, a simple description of an issue or concept is not enough to convey the full context of the problem. A reproducible example provides a minimal, self-contained piece of code (and other relevant resources) that demonstrates a specific issue or concept. It includes:\n\nA brief description of the problem or question and the expected output\nThe necessary (and only the necessary) data to reproduce the issue\nThe R code used to generate the output\nThe actual output, including any error messages or warnings\n\n\n\nWhy are reproducible examples important?\nYou may very well understand the problem you are facing, but others likely will not. By providing sufficient context to understand the problem, you can increase the likelihood of receiving a helpful response. Another reason to create reproducible examples is to help you think through the problem more clearly. By creating a minimal example, you may discover the source of the problem yourself!\n\n\nCreate a reproducible example\nThe trickiest part of asking a question about R code is often not the question itself, but providing this information in a self-contained, reproducible example. Luckily, there are a few R packages that provide tools to help you create reproducible examples. {reprex}(Bryan et al. 2024), {datapasta}(McBain et al. 2020), and creative uses of {knitr} and base R functions can help you create reproducible examples.\n\n\n\nTable 1: Package options for creating reproducible examples\n\n\n\n\n\n\n\n\n\n\nPackage\nDescription\nUse case\n\n\n\n\n{reprex}\nCreates reproducible examples\nGeneral use\n\n\n{datapasta}\nCopy and paste data frames\nData manipulation\n\n\n{knitr}\nSwiss Army knife of rendering\nExtract code from literate programming documents (i.e. Quarto)\n\n\nBase R functions\ndput(), dump(), sessionInfo()\nRepresent data as text and report environment settings\n\n\n\n\n\n\nIn this guide, we will focus on using {reprex} to create reproducible examples. {reprex} is a powerful tool that captures R code, input data, and output in a formatted output that can be easily shared with others. Let’s dive in!"
  },
  {
    "objectID": "guides/guide-05/index.html#building-blocks",
    "href": "guides/guide-05/index.html#building-blocks",
    "title": "05. Creating reproducible examples",
    "section": "Building blocks",
    "text": "Building blocks\n\nFormatting code and code output\nLet’s run through the building blocks of producing a reproducible example. Let’s start with a simple example. We’ll start with the following R code:\n\n# Load packages\nlibrary(stringr)\n# Sentences to tokenize\nx &lt;- c(\"This is a sentence.\", \"This is another sentence.\")\n# Tokenize the sentences\nstringr::str_split(x, \" \")\n\n[[1]]\n[1] \"This\"      \"is\"        \"a\"         \"sentence.\"\n\n[[2]]\n[1] \"This\"      \"is\"        \"another\"   \"sentence.\"\n\n\nFirst, we need to describe the problem or question the code attempts to address. In this case, we are trying to tokenize the sentences in the vector x. We should also include the expected output as part of the description. Here, the code functions without an error, but it does not seem to produce the desired output. On the one hand, punctuation is not removed and the words are not lowercased. On the other hand, the output is returned in a data structure we may not be familiar with –we’d like to see a data frame with one word per row. Something like this:\nSo our description could be:\n\nI am trying to tokenize the sentences in the vector x. The expected output is a data frame with one word per row, where punctuation is removed and words are lowercased. The output should look like something like this:\n\n\n\ntoken\n\n\n\n\nthis\n\n\nis\n\n\na\n\n\nsentence\n\n\n\n\nNext, we need to include the necessary R code to reproduce the issue. This is where the {reprex} package comes in handy. We can use the reprex() function to create a reproducible example from the code. The reprex() function will capture the code, input data, and output in a formatted output that can be easily shared with others.\nTo capture our example code, we first need to load {reprex} in our R session:\n\nlibrary(reprex)\n\nNext, we need to select and copy the code we want to include in the reproducible example. We can then call the reprex() function to create the example:\nreprex()\nreprex() will find the code we copied to the clipboard, run the code, and will generate a formatted output that includes the code, input data, and results. The output will be displayed in either a browser or preview pane and copied to the clipboard for easy sharing.\nHere is the output of the code from the clipboard:\n```r\n# Load packages\n  library(stringr)\n    # Sentences to tokenize\n      x &lt;- c(\"This is a sentence.\", \"This is another sentence.\")\n        # Tokenize the sentences\n          stringr::str_split(x, \" \")\n#&gt; [[1]]\n#&gt; [1] \"This\"      \"is\"        \"a\"         \"sentence.\"\n#&gt;\n#&gt; [[2]]\n#&gt; [1] \"This\"      \"is\"        \"another\"   \"sentence.\"\n\n&lt;sup&gt;Created on 2024-06-23 with [reprex v2.1.0](https://reprex.tidyverse.org)&lt;/sup&gt;\n```\nThe default output of reprex() is a markdown document that can be shared on various platforms such as GitHub, Stack Overflow, or any other markdown-enabled site. The formatted output makes it easy for others to understand the problem and provide a solution. If you plan to share the output on a platform that does not support markdown, you can use the venue argument to specify a different output format. For example, to can get the reprex formatted as:\n\nr for plain text\nrtf for rich text format\nhtml for HTML\n\nSo for example, to create a reprex formatted as plain text, you can use:\nreprex(venue = \"r\")\nThis is a handy output if you want to share a code snippet in an email or a chat message!\n\n\nIncluding data\nIn the previous example, our ‘data’ was the vector x. In more complex examples, you may need to include data frames or other data structures. Let’s say we are working on some code that aims to read some data from a file which has two columns doc_id and text, and calculate the number of words per document. The code we’ve written so far is giving us an error, and we need help from the community to debug it.\nThe code we have so far is:\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Read the text file\ndata &lt;- read_csv(\"data/text.csv\")\n\n# Tokenize the text\ntokens_tbl &lt;-\n  data |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(word) |&gt;\n  group_by(doc_id) |&gt;\n  summarize(doc_words = n())\n\nThis code produces the following error:\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `doc_id` is not found.\nRun `rlang::last_trace()` to see where the error occurred.\nIn this case, we need to include a relevant dataset that can be used to reproduce the error. Now, the first thing we should do is to consider if there are any built-in datasets that can be used to reproduce the error. It is always easier use a dataset that is comes with R, as it is readily available to everyone. If there is no (easily accessible) built-in dataset that can be used, we can add our own data to the reprex. Ideally, we should include the smallest amount of data that is necessary to reproduce the error.\nTo get a better understanding how we might proceed, let’s take a quick look at the data we are working with:\n\ndata\n\n# A tibble: 10 × 2\n   doc_id text                                                                \n    &lt;dbl&gt; &lt;chr&gt;                                                               \n 1      1 The Sapir-Whorf hypothesis suggests language influences thought.    \n 2      2 Cognitive dissonance occurs when beliefs contradict behaviors.      \n 3      3 Plato's allegory of the cave explores perception vs. reality.       \n 4      4 Object-oriented programming focuses on creating reusable code.      \n 5      5 Chomsky's universal grammar theory proposes innate language ability.\n 6      6 The bystander effect explains reduced helping in crowds.            \n 7      7 Descartes' 'I think, therefore I am' establishes existence.         \n 8      8 Machine learning algorithms improve with more data.                 \n 9      9 Phonemes are the smallest units of sound in language.               \n10     10 The halting problem proves some computations are undecidable.       \n\n\nFrom the output, we can see that the data has two columns: doc_id and text. We can create a small data frame with this structure to include in the reprex. We can use the tribble() function from the {tibble} package to create the data frame:\n\n# Create a small data frame\ndata &lt;- tibble::tribble(\n  ~doc_id, ~text,\n  1, \"This is a sentence.\",\n  2, \"This is another sentence.\"\n)\n\ndata\n\n# A tibble: 2 × 2\n  doc_id text                     \n   &lt;dbl&gt; &lt;chr&gt;                    \n1      1 This is a sentence.      \n2      2 This is another sentence.\n\n\nNow that we have the code to create some sample data, we can replace the call to the read_csv() function with the code to create the data frame. Copy the new code to the clipboard and run reprex() again to create a new reproducible example:\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Create a small data frame\ndata &lt;- tibble::tribble(\n  ~doc_id, ~text,\n  1, \"This is a sentence.\",\n  2, \"This is another sentence.\"\n)\n\n# Tokenize the text\ntokens_tbl &lt;-\n  data |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(word) |&gt;\n  group_by(doc_id) |&gt;\n  summarize(doc_words = n())\n\nWe the default setting for markdown output, the reprex will look like this:\n\n```r\n# Load packages\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Create a small data frame\ndata &lt;- tibble::tribble(\n  ~doc_id, ~text,\n  1, \"This is a sentence.\",\n  2, \"This is another sentence.\"\n)\n\n# Tokenize the text\ntokens_tbl &lt;-\n  data |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(word) |&gt;\n  group_by(doc_id) |&gt;\n  summarize(doc_words = n())\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `doc_id` is not found.\n```\n\n&lt;sup&gt;Created on 2024-06-23 with [reprex v2.1.0](https://reprex.tidyverse.org)&lt;/sup&gt;\n\n\nIncluding session information\nAnother piece of information that can prove key to solving a problem is the R session information. This information describes some important details about your particular R environment. If others are not able to reproduce the error, the session information can help them understand the context in which the error occurred. It’s not always the case that the code itself is the problem, necessarily, but rather the mismatch between the code and the environment in which it is run.\nConviently, the reprex() function can also include the session information in the output. The argument session_info = TRUE will include the session information in the output. This can be a lot of information, but don’t worry, it is common practice to include this information in a reprex.\nHere is an example of how to include the session information in the reprex:\nreprex(session_info = TRUE)\nNow, the reprex will include the session information at the end of the output. As an example, I’ll include the session information in a (formatted) reprex:\n# Load packages\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Create a small data frame\ndata &lt;- tibble::tribble(\n  ~doc_id, ~text,\n  1, \"This is a sentence.\",\n  2, \"This is another sentence.\"\n)\n\n# Tokenize the text\ntokens_tbl &lt;-\n  data |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(word) |&gt;\n  group_by(doc_id) |&gt;\n  summarize(doc_words = n())\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `doc_id` is not found.\nCreated on 2024-06-23 with reprex v2.1.0\n\n\nSession info\n\nsessioninfo::session_info()\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.4.1 (2024-06-14)\n#&gt;  os       macOS Sonoma 14.5\n#&gt;  system   aarch64, darwin23.4.0\n#&gt;  ui       unknown\n#&gt;  language (EN)\n#&gt;  collate  en_US.UTF-8\n#&gt;  ctype    en_US.UTF-8\n#&gt;  tz       America/New_York\n#&gt;  date     2024-06-23\n#&gt;  pandoc   3.2 @ /opt/homebrew/bin/ (via rmarkdown)\n#&gt;\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package     * version date (UTC) lib source\n#&gt;  cli           3.6.2   2023-12-11 [1] CRAN (R 4.4.0)\n#&gt;  colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.4.0)\n#&gt;  digest        0.6.35  2024-03-11 [1] CRAN (R 4.4.0)\n#&gt;  dplyr       * 1.1.4   2023-11-17 [1] CRAN (R 4.4.0)\n#&gt;  evaluate      0.24.0  2024-06-10 [1] CRAN (R 4.4.0)\n#&gt;  fansi         1.0.6   2023-12-08 [1] CRAN (R 4.4.0)\n#&gt;  fastmap       1.2.0   2024-05-15 [1] CRAN (R 4.4.0)\n#&gt;  forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.4.0)\n#&gt;  fs            1.6.4   2024-04-25 [1] CRAN (R 4.4.0)\n#&gt;  generics      0.1.3   2022-07-05 [1] CRAN (R 4.4.0)\n#&gt;  ggplot2     * 3.5.1   2024-04-23 [1] CRAN (R 4.4.0)\n#&gt;  glue          1.7.0   2024-01-09 [1] CRAN (R 4.4.0)\n#&gt;  gtable        0.3.5   2024-04-22 [1] CRAN (R 4.4.0)\n#&gt;  hms           1.1.3   2023-03-21 [1] CRAN (R 4.4.0)\n#&gt;  htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.4.0)\n#&gt;  janeaustenr   1.0.0   2022-08-26 [1] CRAN (R 4.4.0)\n#&gt;  knitr         1.47    2024-05-29 [1] CRAN (R 4.4.0)\n#&gt;  lattice       0.22-6  2024-03-20 [3] CRAN (R 4.4.1)\n#&gt;  lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.4.0)\n#&gt;  lubridate   * 1.9.3   2023-09-27 [1] CRAN (R 4.4.0)\n#&gt;  magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.4.0)\n#&gt;  Matrix        1.7-0   2024-04-26 [3] CRAN (R 4.4.1)\n#&gt;  munsell       0.5.1   2024-04-01 [1] CRAN (R 4.4.0)\n#&gt;  pillar        1.9.0   2023-03-22 [1] CRAN (R 4.4.0)\n#&gt;  pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.4.0)\n#&gt;  purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.4.0)\n#&gt;  R.cache       0.16.0  2022-07-21 [1] CRAN (R 4.4.0)\n#&gt;  R.methodsS3   1.8.2   2022-06-13 [1] CRAN (R 4.4.0)\n#&gt;  R.oo          1.26.0  2024-01-24 [1] CRAN (R 4.4.0)\n#&gt;  R.utils       2.12.3  2023-11-18 [1] CRAN (R 4.4.0)\n#&gt;  R6            2.5.1   2021-08-19 [1] CRAN (R 4.4.0)\n#&gt;  Rcpp          1.0.12  2024-01-09 [1] CRAN (R 4.4.0)\n#&gt;  readr       * 2.1.5   2024-01-10 [1] CRAN (R 4.4.0)\n#&gt;  reprex        2.1.0   2024-01-11 [1] CRAN (R 4.4.0)\n#&gt;  rlang         1.1.4   2024-06-04 [1] CRAN (R 4.4.0)\n#&gt;  rmarkdown     2.27    2024-05-17 [1] CRAN (R 4.4.0)\n#&gt;  scales        1.3.0   2023-11-28 [1] CRAN (R 4.4.0)\n#&gt;  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.4.0)\n#&gt;  SnowballC     0.7.1   2023-04-25 [1] CRAN (R 4.4.0)\n#&gt;  stringi       1.8.4   2024-05-06 [1] CRAN (R 4.4.0)\n#&gt;  stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.4.0)\n#&gt;  styler        1.10.3  2024-04-07 [1] CRAN (R 4.4.0)\n#&gt;  tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.4.0)\n#&gt;  tidyr       * 1.3.1   2024-01-24 [1] CRAN (R 4.4.0)\n#&gt;  tidyselect    1.2.1   2024-03-11 [1] CRAN (R 4.4.0)\n#&gt;  tidytext    * 0.4.2   2024-04-10 [1] CRAN (R 4.4.0)\n#&gt;  tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.4.0)\n#&gt;  timechange    0.3.0   2024-01-18 [1] CRAN (R 4.4.0)\n#&gt;  tokenizers    0.3.0   2022-12-22 [1] CRAN (R 4.4.0)\n#&gt;  tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.4.0)\n#&gt;  utf8          1.2.4   2023-10-22 [1] CRAN (R 4.4.0)\n#&gt;  vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.4.0)\n#&gt;  withr         3.0.0   2024-01-16 [1] CRAN (R 4.4.0)\n#&gt;  xfun          0.44    2024-05-15 [1] CRAN (R 4.4.0)\n#&gt;  yaml          2.3.8   2023-12-11 [1] CRAN (R 4.4.0)\n#&gt;\n#&gt;  [1] /Users/francojc/R/Library\n#&gt;  [2] /opt/homebrew/lib/R/4.4/site-library\n#&gt;  [3] /opt/homebrew/Cellar/r/4.4.1/lib/R/library\n#&gt;\n#&gt; ──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "guides/guide-05/index.html#conclusion",
    "href": "guides/guide-05/index.html#conclusion",
    "title": "05. Creating reproducible examples",
    "section": "Conclusion",
    "text": "Conclusion\nIn this guide, we have discussed the importance of reproducible examples and demonstrated how to create them using {reprex} in R. By creating clear and concise reprexes, you can effectively communicate problems, solutions, and ideas with your peers and collaborators. Give {reprex} a try and see how it can improve your workflow!"
  },
  {
    "objectID": "recipes/recipe-11/index.html",
    "href": "recipes/recipe-11/index.html",
    "title": "11. Sharing research",
    "section": "",
    "text": "R (or Python) research projects that take advantage of Quarto websites have access to a wide range of tools for sharing research. First, the entire research tool chain can be published as a website, which is a great way to share the research process. Second, the website can be used to share the research findings in particular formats including articles and presentations. Let’s focus in on these later formats and discuss strategies for setting up and formatting research articles and presentations.\nWe will assume the project directory structure in Snippet 1.\n\n\n\nSnippet 1: Quarto website structure for reproducible research\n\n\n1project/\n2  ├── data/\n  │   ├── analysis/\n  │   ├── derived/\n  │   └── original/\n3  ├── process/\n  │   ├── 1_acquire.qmd\n  │   ├── 2_curate.qmd\n  │   ├── 3_transform.qmd\n  │   └── 4_analyze.qmd\n4  ├── reports/\n5  │   ├── figures/\n6  │   ├── slides/\n7  │   │   ├── workshop/\n8  │   │   │    └── index.qmd\n  │   │   └── conference/\n9  │   ├── tables/\n10  │   ├── article.qmd\n11  │   ├── citation-style.csl\n12  │   ├── presentations.qmd\n13  │   └── bibliography.bib\n  ├── renv/\n  │   └── ...\n14  ├── _quarto.yml\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n\n1\n\nProject root: The root directory for the project.\n\n2\n\nData directory: The directory for storing data files.\n\n3\n\nProcess directory: The directory for storing process files.\n\n4\n\nReports directory: The directory for storing reports.\n\n5\n\nFigures directory: The directory for storing figures.\n\n6\n\nSlides directory: The directory for storing presentations.\n\n7\n\nWorkshop directory: The directory for storing a “workshop” presentation.\n\n8\n\nWorkshop presentation file: The file for the “workshop” presentation.\n\n9\n\nTables directory: The directory for storing tables.\n\n10\n\nArticle file: The file for the article.\n\n11\n\nCitation style file: The file for the citation style.\n\n12\n\nPresentations listing file: The file for the presentations listing.\n\n13\n\nReferences file: The file for the references.\n\n14\n\nQuarto configuration file: The file for the Quarto configuration.\n\n\n\n\n\nI will also assume the following Quarto configuration file _quarto.yml in Snippet 2.\n\n\n\nSnippet 2: Quarto configuration file for reproducible research\n\n\nproject:\n  title: \"Web project\"\n  type: website\n1  execute-dir: project\n2  render:\n    - index.qmd\n    - process/1_acquire.qmd\n    - process/2_curate.qmd\n    - process/3_transform.qmd\n    - process/4_analyze.qmd\n    - reports/\n\nwebsite:\n  sidebar:\n3    style: \"docked\"\n4    contents:\n      - index.qmd\n      - section: \"Process\"\n5        contents: process/*\n      - section: \"Reports\"\n6        contents: reports/*\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nexecute:\n7  freeze: auto\n\n1\n\nExecution directory: The root directory for all execution.\n\n2\n\nRender order: The order in which files are rendered.\n\n3\n\nSidebar style: The style of the sidebar.\n\n4\n\nSidebar contents: The contents of the sidebar.\n\n5\n\nProcess contents: The contents of the process section.\n\n6\n\nReports contents: The contents of the reports section.\n\n7\n\nFreeze option: The option for rendering only changed files.\n\n\n\n\n\nLooking more closely at the directory structure in Snippet 1, let’s focus on the aspects that are shared between articles and presentations. You will notice that the reports/ directory contains a figures/ directory for saving figures, a tables/ directory for saving tables, and a references.bib file for saving references. These are shared resources that can be used in both articles and presentations. In the process directory, you can save tables, figures, and other resources that are generated during the research process that you believe will be useful in commmunicating the research findings. Then, when you create your presentations, you can include the same materials in either format with the same files. If changes are made to the figures or tables, they will be updated in both the article and the presentation(s).\nNext, it is worth pointing out some important features that appear in the Snippet 2 configuration file. The execute-dir specifies the root directory for all execution. That is the path to directories and files will be the same no matter from what file the code is executed. The render option specifies the order in which the files are rendered. This is important for ensuring that the research process is executed in the correct order. The files that are executed and rendered for display appear in the website and the style and contents options specify the style and contents of the sidebar, respectively. Another key option is the freeze option under execute. This option specifies that only changed files will be rendered. This helps avoid re-rendering files that have not changed, which can be time-consuming and computationally expensive.\n\n\n\nIn the reports/ directory a file named article.qmd appears. This file, which can be named anything, will be the document in which we will draft the research article. This file is a standard Quarto document. However, we can take advantage of some options that we have not seen so far that adds functionality to the document.\nIn Snippet 3, we see an example of the YAML frontmatter for a Quarto article.\n\n\n\nSnippet 3: Quarto article YAML frontmatter\n\n\ntitle: \"Article\"\ndate: 2024-02-20\n1author:\n  - name: \"Your name\"\n    email: youremail@school.edu\n    affiliation: \"Your affiliation\"\n2abstract: |\n  This is a sample article. It is a work in progress and will be updated as the research progresses.\n3keywords:\n  - article\n  - example\n4csl: citation-style.csl\n5bibliography: ../bibliography.bib\n6citation: true\n7format:\n8  html: default\n9  pdf:\n    number-sections: true\n\n1\n\nAuthor information: The author information for the article.\n\n2\n\nAbstract: The abstract for the article.\n\n3\n\nKeywords: The keywords for the article.\n\n4\n\nCitation style: The citation style for the article.\n\n5\n\nBibliography: The bibliography for the article.\n\n6\n\nCitation: The citation for the article itself.\n\n7\n\nFormat: The format for the article.\n\n8\n\nHTML format: The HTML format for the article (for web presentation)\n\n9\n\nPDF format: The PDF format for the article (for printing)\n\n\n\n\n\nIn addition to typical YAML frontmatter, we see a number of new times. Looking at the first three, we see that we can add author information, an abstract, and keywords. These are standard for articles and are used to provide information about the article to readers.\nWhen rendered, the article header information will now contain this new information, as seen in Figure 1.\n\n\n\n\n\n\nFigure 1: Appearance of the header format for a Quarto article.\n\n\n\nThe next two items are the citation style and bibliography. These are used to create and format citations in the article. The citation style is a CSL file that specifies the citation style. You can find a database of various citation styles at the Zotero Style Repository. You can search for a style or by field. Once you find a style you like, you can download the CSL file and add it to your project. The bibliography is a BibTeX file that contains the references for the article. You can create this file (as mentioned before) in a reference manager like Zotero or Mendeley.\nNow the citation option is not for references that we have gathered. Rather, it is for generating a citation for the current article. This is useful if someone else would like to cite your article. When the article is rendered, the citation will appear at the bottom of the article, as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Appearance of the citation attribution in a Quarto article.\n\n\n\nThere are two other features to mention. One is the format option. Since the article is a Quarto document, it can be rendered in multiple formats. The html option ensures that our article is rendered in HTML format as part of the website. However, in addition, we can add a pdf option that will render the article in PDF format. Note that in Figure 1, the pdf option has created an “Other formats” listing on the right side below the table of contents. Clicking this will open the PDF version of the article.\nAlthough not employed in this example, it is also possible to use more extensive format changes with Quarto extensions. Currently, there are various extensions for different journals and publishing houses. For more information and examples, consult the documentation above.\n\n\n\nIn the reports/ directory we can also include presentations and associated slide decks. A popular web-based presentation framework is reveal.js. This framework is used in Quarto to create presentations. In Snippet 1, the slides/ directory contains a directory for each presentation and an index.qmd file within. The index.qmd file contains the presentation content, which we will see soon. To provide a listings page each presentation, the presentations.qmd file contains special YAML instructions to be a listings page.\nLet’s first dive into the index.qmd file for a presentation and discuss some of the key features. In Snippet 4, we see a basic example of a Quarto presentation.\n\n\n\nSnippet 4: Quarto presentation YAML frontmatter index.qmd\n\n\ntitle: \"Examle presentation\"\ndate: 2024-02-20\nauthor: \"Jerid Francom\"\nformat: revealjs\n\n\n\nThe YAML frontmatter for a Quarto presentation is similar to that of most Quarto documents. The title, date, and author are all included. The format option specifies that the presentation will be rendered in reveal.js format. When rendered, the presentation the slide deck will be interactive and can be navigated by the user. The slide deck will also be responsive and can be viewed on any device.\nIn Figure 3, we see an example of a Quarto presentation rendered in reveal.js format. I will discuss some of the key features of the presentation, in the presentation itself.\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#concepts-and-strategies",
    "href": "recipes/recipe-11/index.html#concepts-and-strategies",
    "title": "11. Sharing research",
    "section": "",
    "text": "R (or Python) research projects that take advantage of Quarto websites have access to a wide range of tools for sharing research. First, the entire research tool chain can be published as a website, which is a great way to share the research process. Second, the website can be used to share the research findings in particular formats including articles and presentations. Let’s focus in on these later formats and discuss strategies for setting up and formatting research articles and presentations.\nWe will assume the project directory structure in Snippet 1.\n\n\n\nSnippet 1: Quarto website structure for reproducible research\n\n\n1project/\n2  ├── data/\n  │   ├── analysis/\n  │   ├── derived/\n  │   └── original/\n3  ├── process/\n  │   ├── 1_acquire.qmd\n  │   ├── 2_curate.qmd\n  │   ├── 3_transform.qmd\n  │   └── 4_analyze.qmd\n4  ├── reports/\n5  │   ├── figures/\n6  │   ├── slides/\n7  │   │   ├── workshop/\n8  │   │   │    └── index.qmd\n  │   │   └── conference/\n9  │   ├── tables/\n10  │   ├── article.qmd\n11  │   ├── citation-style.csl\n12  │   ├── presentations.qmd\n13  │   └── bibliography.bib\n  ├── renv/\n  │   └── ...\n14  ├── _quarto.yml\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n\n1\n\nProject root: The root directory for the project.\n\n2\n\nData directory: The directory for storing data files.\n\n3\n\nProcess directory: The directory for storing process files.\n\n4\n\nReports directory: The directory for storing reports.\n\n5\n\nFigures directory: The directory for storing figures.\n\n6\n\nSlides directory: The directory for storing presentations.\n\n7\n\nWorkshop directory: The directory for storing a “workshop” presentation.\n\n8\n\nWorkshop presentation file: The file for the “workshop” presentation.\n\n9\n\nTables directory: The directory for storing tables.\n\n10\n\nArticle file: The file for the article.\n\n11\n\nCitation style file: The file for the citation style.\n\n12\n\nPresentations listing file: The file for the presentations listing.\n\n13\n\nReferences file: The file for the references.\n\n14\n\nQuarto configuration file: The file for the Quarto configuration.\n\n\n\n\n\nI will also assume the following Quarto configuration file _quarto.yml in Snippet 2.\n\n\n\nSnippet 2: Quarto configuration file for reproducible research\n\n\nproject:\n  title: \"Web project\"\n  type: website\n1  execute-dir: project\n2  render:\n    - index.qmd\n    - process/1_acquire.qmd\n    - process/2_curate.qmd\n    - process/3_transform.qmd\n    - process/4_analyze.qmd\n    - reports/\n\nwebsite:\n  sidebar:\n3    style: \"docked\"\n4    contents:\n      - index.qmd\n      - section: \"Process\"\n5        contents: process/*\n      - section: \"Reports\"\n6        contents: reports/*\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nexecute:\n7  freeze: auto\n\n1\n\nExecution directory: The root directory for all execution.\n\n2\n\nRender order: The order in which files are rendered.\n\n3\n\nSidebar style: The style of the sidebar.\n\n4\n\nSidebar contents: The contents of the sidebar.\n\n5\n\nProcess contents: The contents of the process section.\n\n6\n\nReports contents: The contents of the reports section.\n\n7\n\nFreeze option: The option for rendering only changed files.\n\n\n\n\n\nLooking more closely at the directory structure in Snippet 1, let’s focus on the aspects that are shared between articles and presentations. You will notice that the reports/ directory contains a figures/ directory for saving figures, a tables/ directory for saving tables, and a references.bib file for saving references. These are shared resources that can be used in both articles and presentations. In the process directory, you can save tables, figures, and other resources that are generated during the research process that you believe will be useful in commmunicating the research findings. Then, when you create your presentations, you can include the same materials in either format with the same files. If changes are made to the figures or tables, they will be updated in both the article and the presentation(s).\nNext, it is worth pointing out some important features that appear in the Snippet 2 configuration file. The execute-dir specifies the root directory for all execution. That is the path to directories and files will be the same no matter from what file the code is executed. The render option specifies the order in which the files are rendered. This is important for ensuring that the research process is executed in the correct order. The files that are executed and rendered for display appear in the website and the style and contents options specify the style and contents of the sidebar, respectively. Another key option is the freeze option under execute. This option specifies that only changed files will be rendered. This helps avoid re-rendering files that have not changed, which can be time-consuming and computationally expensive.\n\n\n\nIn the reports/ directory a file named article.qmd appears. This file, which can be named anything, will be the document in which we will draft the research article. This file is a standard Quarto document. However, we can take advantage of some options that we have not seen so far that adds functionality to the document.\nIn Snippet 3, we see an example of the YAML frontmatter for a Quarto article.\n\n\n\nSnippet 3: Quarto article YAML frontmatter\n\n\ntitle: \"Article\"\ndate: 2024-02-20\n1author:\n  - name: \"Your name\"\n    email: youremail@school.edu\n    affiliation: \"Your affiliation\"\n2abstract: |\n  This is a sample article. It is a work in progress and will be updated as the research progresses.\n3keywords:\n  - article\n  - example\n4csl: citation-style.csl\n5bibliography: ../bibliography.bib\n6citation: true\n7format:\n8  html: default\n9  pdf:\n    number-sections: true\n\n1\n\nAuthor information: The author information for the article.\n\n2\n\nAbstract: The abstract for the article.\n\n3\n\nKeywords: The keywords for the article.\n\n4\n\nCitation style: The citation style for the article.\n\n5\n\nBibliography: The bibliography for the article.\n\n6\n\nCitation: The citation for the article itself.\n\n7\n\nFormat: The format for the article.\n\n8\n\nHTML format: The HTML format for the article (for web presentation)\n\n9\n\nPDF format: The PDF format for the article (for printing)\n\n\n\n\n\nIn addition to typical YAML frontmatter, we see a number of new times. Looking at the first three, we see that we can add author information, an abstract, and keywords. These are standard for articles and are used to provide information about the article to readers.\nWhen rendered, the article header information will now contain this new information, as seen in Figure 1.\n\n\n\n\n\n\nFigure 1: Appearance of the header format for a Quarto article.\n\n\n\nThe next two items are the citation style and bibliography. These are used to create and format citations in the article. The citation style is a CSL file that specifies the citation style. You can find a database of various citation styles at the Zotero Style Repository. You can search for a style or by field. Once you find a style you like, you can download the CSL file and add it to your project. The bibliography is a BibTeX file that contains the references for the article. You can create this file (as mentioned before) in a reference manager like Zotero or Mendeley.\nNow the citation option is not for references that we have gathered. Rather, it is for generating a citation for the current article. This is useful if someone else would like to cite your article. When the article is rendered, the citation will appear at the bottom of the article, as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Appearance of the citation attribution in a Quarto article.\n\n\n\nThere are two other features to mention. One is the format option. Since the article is a Quarto document, it can be rendered in multiple formats. The html option ensures that our article is rendered in HTML format as part of the website. However, in addition, we can add a pdf option that will render the article in PDF format. Note that in Figure 1, the pdf option has created an “Other formats” listing on the right side below the table of contents. Clicking this will open the PDF version of the article.\nAlthough not employed in this example, it is also possible to use more extensive format changes with Quarto extensions. Currently, there are various extensions for different journals and publishing houses. For more information and examples, consult the documentation above.\n\n\n\nIn the reports/ directory we can also include presentations and associated slide decks. A popular web-based presentation framework is reveal.js. This framework is used in Quarto to create presentations. In Snippet 1, the slides/ directory contains a directory for each presentation and an index.qmd file within. The index.qmd file contains the presentation content, which we will see soon. To provide a listings page each presentation, the presentations.qmd file contains special YAML instructions to be a listings page.\nLet’s first dive into the index.qmd file for a presentation and discuss some of the key features. In Snippet 4, we see a basic example of a Quarto presentation.\n\n\n\nSnippet 4: Quarto presentation YAML frontmatter index.qmd\n\n\ntitle: \"Examle presentation\"\ndate: 2024-02-20\nauthor: \"Jerid Francom\"\nformat: revealjs\n\n\n\nThe YAML frontmatter for a Quarto presentation is similar to that of most Quarto documents. The title, date, and author are all included. The format option specifies that the presentation will be rendered in reveal.js format. When rendered, the presentation the slide deck will be interactive and can be navigated by the user. The slide deck will also be responsive and can be viewed on any device.\nIn Figure 3, we see an example of a Quarto presentation rendered in reveal.js format. I will discuss some of the key features of the presentation, in the presentation itself.\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#check-your-understanding",
    "href": "recipes/recipe-11/index.html#check-your-understanding",
    "title": "11. Sharing research",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\nTRUEFALSE Using Quarto websites for sharing research findings is the only way to meet the requirements of a reproducible research project.\nUsing Snippet 2, what option specifies the order in which files are rendered? execute-dirrendersidebarcontentsfreeze\nWhat is the name of the framework that Quarto uses to create presentations? \nTBD\nTBD\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#lab-preparation",
    "href": "recipes/recipe-11/index.html#lab-preparation",
    "title": "11. Sharing research",
    "section": "Lab preparation",
    "text": "Lab preparation\nTBD"
  },
  {
    "objectID": "recipes/recipe-04/index.html",
    "href": "recipes/recipe-04/index.html",
    "title": "04. Understanding the computing environment",
    "section": "",
    "text": "Skills\n\nUnderstanding the components of a reproducible project\nConnecting the computing environment to reproducible project management\nUnderstanding the workflow and project structure\nUsing Git and GitHub to manage a project"
  },
  {
    "objectID": "recipes/recipe-04/index.html#concepts-and-strategies",
    "href": "recipes/recipe-04/index.html#concepts-and-strategies",
    "title": "04. Understanding the computing environment",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nProject components\nReproducible projects are composed of two main components: the computing environment and the project structure. The computing environment is the hardware, operating system, and software that we use to do our work and the project structure is the organization of the files and folders that make up our project. As we can see in Figure 1, each of these components and subcomponents are nested within each other.\n\n\n\n\n\n\nFigure 1: Compontents of a reproducible project\n\n\n\nIn the lesson on the computing environment, we learned about the importance of understanding the computing environment and how to find out information about our computing environment inspecting an R session, as we seen in Snippet 1.\n\n\n\nSnippet 1: Session information output\n\n\n─ Session info ──────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Ventura 13.6.1\n system   x86_64, darwin22.6.0\n ui       unknown\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-11-26\n pandoc   3.1.9 @ /usr/local/bin/pandoc\n\n─ Packages ──────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.2)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.2)\n rlang         1.1.2   2023-11-04 [1] CRAN (R 4.3.2)\n\n [1] /Users/francojc/R/Library\n [2] /usr/local/Cellar/r/4.3.2/lib/R/library\n\n\n\nAs we learned, when the computing environment that is used to create a project is different from the computing environment that is used to reproduce the project, there is a risk that the project will not be reproducible. We will not tackle all the components in Figure 1 at once, however, but instead we will focus here on the project structure. Later on, with more experience, we will address the other components.\n\n\nManaging reproducible projects\nThe project structure is the organization of the files and folders that make up our project. A minimal project structure includes separation for input, process, and output of research, documentation about the project, how to reproduce the project, and the project files themselves. The project structure is important because it helps us organize our work and it helps others understand it.\nThere is no concensus about what the best project structure is. However, the principles covered in Chapter 4 can guide us in developing a project structure that both organizes our work and makes that work understandable to others. With these principles met, we can add additional structure to our project to meet our project-specific needs.\nTo create a reproducible project structure, we need no more than to create a directory and set of files that meet the principles for a minimal reproducible framework. During or after the project, would could back up these directories and files and/ or share them with others in a number of ways.\nAlthough this approach is already a good step in the right direction, it is error prone and will likely lead to inconsistencies across projects. A better approach is to develop, or adopt, a project structure template that can be used for all projects, use version control to track changes to your project, and upload your project to a remote repository where it is backed up (including the version history) and can be shared with others efficiently.\nThis later approach is the one that we will use in subsequent lessons, recipes, and labs in this course.\n\n\nLeveraging Git and GitHub\nAt this point, you are somewhat familiar with Git and Github. You have likely used Git to copy and download a repository from GitHub, say for example the labs for this course. However, what is going on behind the scenes is likely still a bit of a mystery. In this section, we will demystify Git and GitHub, a bit, and learn how to use them together to manage a project in different scenarios.\n\n\n\n\n\n\n Tip\nVerify that you have a working version of Git on your computing environment and make sure that you have a GitHub account. You can refer to Guide 2 for more information on how to do this.\n\n\n\nSo let’s rewind a bit and review what Git and Github are and how they work together. Git is a version control system that allows us to track changes to our project files. It is a command line tool, much like R, that is installed as software. Also like R, we can interact with Git through a graphical user interface (GUI), such as RStudio.\nDirectory and file tracking with Git can be added to a project at any time. A tracked project is called a repository, or repo for short. When used on our computing environment, the repo is called a local repository. There are many benefits to using Git to track changes to local repository, including the ability to revert to previous versions of files, create and edit parallel copies of files and then selectively integrate them, and much more.\nBut the real power of Git is realized in combination with GitHub. Github is a cloud-based remote repository that allows us to store our git-tracked projects. It is a web-based platform which requires an account to use. Once you are signed up, you can connect Git and Github to create remote repositories and upload your local repositories to them. You can also download remote repositories to your computing environment. There are many, many features that Github offers, but for now we will focus a few key features that will help us manage our projects.\nIn Figure 2, I provide a schematic look at the relationship between Github and Git for three common scenarios.\n\n\n\n\n\n\nFigure 2: Key features of GitHub\n\n\n\nScenario A: Clone a remote repository\nIn this scenario, we locate a remote repository on Github that someone has made publically available. Then we clone (copy and download) the repository to our computing environment. Once the repository is cloned locally, we can edit the files as we see fit.\nIn essence, we are just downloading a group of files and folders to our computing environment from Github. This is the scenario that we have been using in this course to download the lab repositories.\nSteps:\n\nLocate a remote repository on Github that someone has made publically available and copy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\n\nScenario B: Fork and clone a remote repository\nThis scenario differs from A in two respects. First, we first fork (copy to) the remote repository to our Github account before cloning it to our computing environment. Second, we commit (log edits) changes to the git tracking system and then push (sync to) the changes to our remote repository.\nIn this case, we are doing more than just downloading a group of files and folders. We are setting up a link between the other person’s remote repository and our own remote repository. We do not have to use this link, but if we do want to, we can pull (sync from) any changes that are made to the other person’s remote repository to our remote repository. This can be useful if we want to keep our remote repository up to date with the other person’s remote repository. Furthermore, this link allows us to propose changes to the other person’s remote repository with a pull request –a request for the other person to pull our changes into their remote repository. This can be useful if we want to collaborate with the other person on the project. Pull and pull request are more advanced features that we will not address at this point.\nThe second difference is that we are using Git to track changes to our local repository and then push those changes to our remote repository. This is a key feature because it allows us to keep track of changes to our project files and folders and revert to previous versions if needed. It also allows us to share our project with others and collaborate with them.\nSteps:\n\nLocate a remote repository on Github that someone has made publically available and click the fork button.\nStill on Github, choose new account as the owner of the forked repository.\nIn the forked repository, copy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\nMake changes to the project files and folders.\nCommit the changes to the git tracking system.\nPush the changes to the remote repository.\n\nScenario C: Create/ Join and clone a remote repository\nThis scenario is similar to B, but instead of forking a remote repository, we create a new remote repository on Github and then clone it to our computing environment. We then commit and push changes to our remote repository. In this case, we are creating a new remote repository and then using Git to track changes to our local repository and push those changes to our remote repository.\nThis scenario is common when we work on our own projects or when we want to collaborate with others on a project. In the latter case, we would create a remote repository and then invite others to collaborate on it. Everyone with permissions to the remote repository can clone it to their computing environment, make changes, and then push those changes to the same remote repository. This allows everyone to work on the same project and keep track of changes to the project files and folders.\nWhen working with multiple people on a project, you can imagine that if I’m working on the project locally and you are working on the project locally, we might make changes to the same files and folders. If we both push our changes to the remote repository, there is a risk that the changes will conflict with each other. Git and Github have features that help us manage these conflicts (pull, fetch, merge, etc.), but we will not address them at this point either.\nSteps:\n\nCreate a new remote repository on Github or accept an invitation to collaborate on a remote repository.\nCopy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\nMake changes to the project files and folders.\nCommit the changes to the git tracking system.\nPush the changes to the remote repository."
  },
  {
    "objectID": "recipes/recipe-04/index.html#summary",
    "href": "recipes/recipe-04/index.html#summary",
    "title": "04. Understanding the computing environment",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we reviewed the components of reproducible research projects: the computing environment and the project structure. The computing environment is the hardware, operating system, and software that we use to do our work and the project structure is the organization of the files and folders that make up our project. Furthermore, we learned more about Git and Github and how they can be used together to manage a project in different scenarios."
  },
  {
    "objectID": "recipes/recipe-04/index.html#check-your-understanding",
    "href": "recipes/recipe-04/index.html#check-your-understanding",
    "title": "04. Understanding the computing environment",
    "section": "Check your understanding",
    "text": "Check your understanding\nSelect the component of the computing environment that of the following are related to:\n\nWindows 10 hardwareoperating systemsoftware\nR version 4.3.1 hardwareoperating systemsoftware\ndplyr_1.1.4 hardwareoperating systemsoftware\n\nSelect the Git name for the following actions:\n\nCopy and download a remote repository to your computing environment cloneforkcommitpush\nLog edits to the git tracking system cloneforkcommitpush\nSync changes to the remote repository cloneforkcommitpush"
  },
  {
    "objectID": "recipes/recipe-04/index.html#lab-preparation",
    "href": "recipes/recipe-04/index.html#lab-preparation",
    "title": "04. Understanding the computing environment",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn lab 4, we will apply what we have learned in this recipe to scaffold our own research project by forking and cloning a research project template repository on Github. We will then edit the project files and folders, commit the changes to the git tracking system, and push the changes to our remote repository on Github.\nBefore beginning Lab 4, make sure that you are comfortable with the following:\n\nCloning a remote repository to your computing environment\nCreating and editing files and folders, in particular Quarto documents.\n\nThe additional knowledge and skills that you will need to complete the lab are covered in this recipe which include:\n\nUnderstanding the components of a reproducible project\nUnderstanding the importance of project structure for reproducible project management\nUsing Git, GitHub, and RStudio to manage a project using:\n\nForking, cloning, editing, commiting, and pushing a repository"
  },
  {
    "objectID": "recipes/recipe-04/index.html#references",
    "href": "recipes/recipe-04/index.html#references",
    "title": "04. Understanding the computing environment",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "recipes/recipe-02/index.html",
    "href": "recipes/recipe-02/index.html",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "",
    "text": "Skills\n\nLoading packages into an R session\nReading datasets into R with read_*() functions\nInspecting datasets with {dplyr} functions\nWriting datasets to a file with write_*() functions"
  },
  {
    "objectID": "recipes/recipe-02/index.html#concepts-and-strategies",
    "href": "recipes/recipe-02/index.html#concepts-and-strategies",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nQuarto documents and code blocks\nAsk you will remember from Recipes 0 and 1, Quarto documents can combine prose and code. The prose is written in Markdown and the code is written in R1. The code is contained in code blocks, which are opened by three backticks (`), the name of the programming language, r, in curly braces {r} and three backticks (`) to close the block. For example, the following minimal Quarto document contains an R code block:\n---\ntitle: My Quarto Document\nformat: pdf\n---\n\n# Goals\n\nThis script ...\n\n```{r}\n#| label: code-block-name\n\n# R code goes here\n```\n\nAs you can see in the code block, the ...\nCode blocks have various options that can be added by using key-value pairs that are prefixed with #|. Some common key-value pairs we will use in this Recipe are:\n\nlabel: A unique name for the code block. This is used to reference the code block.\necho: A boolean value (true or false) that determines whether the code is displayed in the output document.\ninclude: A boolean value (true or false) that determines whether the output of the code is displayed in the output document.\nmessage: A boolean value (true or false) that determines whether the messages from the code are displayed in the output document.\n\n\n\nSetting up the environment\nBefore we can read, inspect, and write data, we need to load the packages that contain the functions we will use. We will use {readr} to read datasets into R and write datasets to disk and {dplyr} to inspect and transform (subset) the data.\nThere are a few ways to load packages into an R session. The most common way is to use the library() function. The library() function loads a package into the R session and stops the script if the package is not available on the current computing environment.\nFor example, the following code block loads {readr} and {dplyr} into the R session:\n\n```{r}\n#| label: load-packages\n\n# Load packages\nlibrary(readr) # for reading and writing data\nlibrary(dplyr) # for inspecting and transforming data\n```\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis code block assumes that {readr} and {dplyr} are installed on the current computing environment. If the packages are not installed, the code block will stop and display an error message, such as:\nError in library(readr) : there is no package called ‘readr’\nThis error can be addressed by installing the missing package with install.packages(\"readr\") and then re-running the code block. This is not ideal for reproducibility, however, because the code block will stop if the package is not installed. We will consider a more reproducible approach later in the course.\n\n\n\n\n\n\n Dive deeper\nIf you interested in learning about safeguarding package loading in a reproducible way, see {renv}. {renv} is a project-oriented workflow to create a reproducible environment for R projects. For more information, see the renv documentation and/ or Recipe 11\n\n\n\n\n\nUnderstanding the data\nNow that we have our environment set up, we can read the dataset into R. But before we do, we should make sure that we understand the data by looking at the data documentation.\nThe dataset that we will read into our R session based on the Brown Corpus (Francis and Kuçera 1961). I’ve created a data origin file that contains the data documentation for the Brown Corpus, as we can see in Table 1.\n\n# Read and display the data origin file\n\nread_csv(file = \"data/original/brown_passives_do.csv\") |&gt;\n  kable() |&gt;\n  kable_styling() |&gt;\n  column_spec(1, width = \"15em\")\n\n\n\nTable 1: Data origin file for the Brown Corpus.\n\n\n\n\n\n\nattribute\ndescription\n\n\n\n\nResource name\nBrown Corpus\n\n\nData source\nhttp://korpus.uib.no/icame/brown/bcm.html\n\n\nData sampling frame\nEdited American English prose from various genres, published in the United States during the calendar year 1961.\n\n\nData collection date(s)\nOriginally published in 1964, revised in 1971 and 1979.\n\n\nData format\nMultiple formats including Form A (original), Form B (stripped version), Form C (tagged version), Bergen Forms I and II, and Brown MARC Form.\n\n\nData schema\n500 samples of approximately 2000 words each, covering a wide range of genres and styles. Includes coding for major and minor headings, special types (italics, bold, etc.), abbreviations, symbols, and other textual features.\n\n\nLicense\nUse restricted for scholarly research in linguistics, stylistics, and other disciplines. Specific copyright restrictions detailed in the manual.\n\n\nAttribution\nW. Nelson Francis and Henry Kucera, Brown University, 1964, revised 1971 and 1979.\n\n\n\n\n\n\n\n\n\n\nThis data origin file provides an overview of the original data source. In this case, the dataset we will read into R is a subset of the Brown Corpus which is an aggregate of the use of passive voice. This dataset was developed by the authors of {corpora} (Evert 2023). I’ve exported the dataset to a CSV file, which we will read into R.\nThe data dictionary which describes the dataset we will read appears in Table 2.\n# Read and display the data documentation file\nread_csv(file = \"../data/derived/brown_passives_curated_dd.csv\") |&gt;\n  kable() |&gt;\n  kable_styling()\n\n\n\n\nTable 2: Data dictionary file for the Brown Corpus.\n\n\n\n\n\n\nvariable\nname\nvariable_type\ndescription\n\n\n\n\ncat\nCategory\ncategorical\nGenre categories represented by letters\n\n\npassive\nPassive\nnumeric\nNumber of passive verb phrases in the genre\n\n\nn_w\nNumber of words\nnumeric\nNumber of words in the genre\n\n\nn_s\nNumber of sentences\nnumeric\nNumber of sentences in the genre\n\n\nname\nGenre\ncategorical\nGenre name\n\n\n\n\n\n\n\n\n\n\nWith this information, we are now in a position to read and inspect the dataset.\n\n\nReading datasets into R with {readr}\nWe’ve now prepared our Quarto document by loading the packages we will use and and we have reviewed the dataset documentation so that we understand the dataset we will read into R. We are now ready to read the dataset into R.\nR provides a number of functions to read data of many types in R. We will explore many types of data and datasets in this course. For now, we will focus on reading rectangular data into R. Rectangular data is data that is organized in rows and columns, such as a spreadsheet.\nOne of the most common file formats for rectangular data is the comma-separated values (CSV) file. CSV files are text files in which lines represent rows and commas separate columns of data. For example, the sample CSV file snippet below contains three rows and three columns of data:\n\"word\",\"frequency\",\"part_of_speech\"\n\"the\",69971,\"article\"\n\"of\",36412,\"preposition\"\n\"and\",28853,\"conjunction\"\nA CSV file is a type of delimited file, which means that the data is separated by a delimiter. In the case of a CSV file, the delimiter is a comma. Other types of delimited files use different delimiters, such as tab-separated values (TSV) files which use a tab character as the delimiter, or even a pipe (|) or semicolon (;).\n{readr} provides functions to read rectangular dataset into R. The read_csv() function reads CSV files, the read_tsv() function reads TSV files, and the read_delim() function reads other types of delimited files.\nLet’s use the read_csv() function to read the brown_passives_curated.csv file into R. To do this we will use the file = argument to specify the path to the file. Now, the file “path” is the location of the file on the computer. We can specify this path in two ways:\n\nRelative path: The relative path is the path to the file relative to the current working directory. The current working directory is the directory in which the R session is running.\nAbsolute path: The absolute path is the path to the file from the root directory of the computer.\n\nFor most purpose, the relative path is the better option because it is more portable. For example, if you share your code with someone else, they may have a different absolute path to the file. However, they will likely have the same relative path to the file.\nLet’s say that the directory structure of our project is as follows:\nproject/\n├── data/\n│   ├── original/\n│   │   └── brown_passives_do.csv\n│   └── derived/\n│       └── brown_passives_curated.csv\n└── code/\n    └── reading-inspecting-writing.qmd\nIn this case, the relative path from reading-inspecting-writing.qmd to the brown_passives_curated.csv file is ../data/derived/brown_passives_curated.csv. The .. means “go up one directory” and the rest of the path is the path to the file from the project/ directory.\nWith this in mind, we can read the brown_passives_curated.csv file into R with the following code block:\n#| label: read-dataset-brown-passives-curated\n\n# Read the dataset\nbrown_passives_df &lt;-\n  read_csv(file = \"../data/derived/brown_passives_curated.csv\")\nRunning the above code chunk in our Quarto document will read the dataset into R and assign it to the brown_passives_df variable. It will also show the code used to read the dataset into R. Furthermore, so functions will display messages in the output. For example, the read_csv() function will display a message that various parsing options were used to read the dataset into R.\nThis information can be helpful in an interactive session, as read_csv() tells us the dimensions of the dataset and the data types of each column. But this output is not necessary, and is unnecessarily verbose in a reproducible document.\nWe can hide any messages produced by a function by using the message = false key-value pair in the code block. For example, the following code block will read the dataset into R and assign it to the brown_passives_df variable without displaying any messages:\n#| label: read-dataset-brown-passives-curated\n#| message: false\n\n# Read the dataset\nbrown_passives_df &lt;-\n  read_csv(file = \"../data/derived/brown_passives_curated.csv\")\nNo messages are displayed in the document output.\n\n\nInspecting datasets with {dplyr}\nThe objective of this section is to demonstrate how to inspect and transform (subset) datasets using {dplyr}. We will use {dplyr} to inspect the dataset we read into R in the previous section.\nReading a CSV file into R will create a data frame object. Thus, I assigned the result to brown_passives_df. The df suffix is a common naming convention for rectangular data frames. It is good practice to use a consistent naming convention for objects in your code. This makes it easier to understand the code and to avoid errors.\nLet’s do get an overview of the dataset by using the glimpse() function from {dplyr}. The glimpse() function displays the dimensions of the data frame and the data types of each column.\n\n# Preview\nglimpse(brown_passives_df)\n\nRows: 15\nColumns: 5\n$ cat     &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"J\", \"K\", \"L\", \"M\", \"N…\n$ passive &lt;dbl&gt; 892, 543, 283, 351, 853, 1034, 1460, 837, 2423, 352, 265, 104,…\n$ n_w     &lt;dbl&gt; 101196, 61535, 40749, 39029, 82010, 110363, 173017, 69446, 181…\n$ n_s     &lt;dbl&gt; 3684, 2399, 1459, 1372, 3286, 4387, 6537, 2012, 6311, 3983, 36…\n$ name    &lt;chr&gt; \"press reportage\", \"press editorial\", \"press reviews\", \"religi…\n\n\nIf we want a more, tabular-like view of the data, we can simply print the dataset frame to the console. It’s worth mentioning, that all {readr} functions return tibbles, so we gain the benefits of tibbles when we read dataset into R with {readr} functions, one of which is that we do not have to worry that printing a data frame to the console, or our document, will print all of the data.\n\n# Print the data frame\nbrown_passives_df\n\n# A tibble: 15 × 5\n   cat   passive    n_w   n_s name            \n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n 1 A         892 101196  3684 press reportage \n 2 B         543  61535  2399 press editorial \n 3 C         283  40749  1459 press reviews   \n 4 D         351  39029  1372 religion        \n 5 E         853  82010  3286 skills / hobbies\n 6 F        1034 110363  4387 popular lore    \n 7 G        1460 173017  6537 belles lettres  \n 8 H         837  69446  2012 miscellaneous   \n 9 J        2423 181426  6311 learned         \n10 K         352  68599  3983 general fiction \n11 L         265  57624  3673 detective       \n12 M         104  14433   873 science fiction \n13 N         290  69909  4438 adventure       \n14 P         290  70476  4187 romance         \n15 R         146  21757   975 humour          \n\n\nBy default, printing tibbles will return the first 10 rows and all columns, unless the columns are too numerous to display width-wise.\n{dplyr} also provides a set of slice_*() functions which allow us to display the data in a tabular fashion, with some additional options. There are three slice_*() functions we will cover here:\n\nslice_head(): Select the first n rows of the data frame.\nslice_tail(): Select the last n rows of the data frame.\nslice_sample(): Select a random sample of n rows from the data frame.\n\nFor example, the following code block will select the first 5 rows of the data frame:\n\n# Select the first 5 rows\nslice_head(brown_passives_df, n = 5)\n\n# A tibble: 5 × 5\n  cat   passive    n_w   n_s name            \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n1 A         892 101196  3684 press reportage \n2 B         543  61535  2399 press editorial \n3 C         283  40749  1459 press reviews   \n4 D         351  39029  1372 religion        \n5 E         853  82010  3286 skills / hobbies\n\n\nWe can also select the last 5 rows of the data frame with the slice_tail() function:\n\n# Select the last 5 rows\nslice_tail(brown_passives_df, n = 5)\n\n# A tibble: 5 × 5\n  cat   passive   n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 L         265 57624  3673 detective      \n2 M         104 14433   873 science fiction\n3 N         290 69909  4438 adventure      \n4 P         290 70476  4187 romance        \n5 R         146 21757   975 humour         \n\n\nFinally, we can select a random sample of 5 rows from the data frame with the slice_sample() function:\n\n# Select a random sample of 5 rows\nslice_sample(brown_passives_df, n = 5)\n\n# A tibble: 5 × 5\n  cat   passive   n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 B         543 61535  2399 press editorial\n2 D         351 39029  1372 religion       \n3 H         837 69446  2012 miscellaneous  \n4 R         146 21757   975 humour         \n5 P         290 70476  4187 romance        \n\n\nThese functions can be helpful to get a sense of the dataset in different ways. In combination with arrange() function, we can also sort the data frame by a column or columns and then select the first or last rows.\nFor example, the following code block will sort the data frame by the passive column in ascending order and then select the first 5 rows:\n\n# Sort by the `passive` column and select the first 5 rows\nslice_head(arrange(brown_passives_df, passive), n = 5)\n\n# A tibble: 5 × 5\n  cat   passive   n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 M         104 14433   873 science fiction\n2 R         146 21757   975 humour         \n3 L         265 57624  3673 detective      \n4 C         283 40749  1459 press reviews  \n5 N         290 69909  4438 adventure      \n\n\nIf we want to sort be descending order, we can surround the column name with desc(), arrange(desc(passive)).\nNow, the previous code block does what we want, but it is not very readable. Enter the pipe operator. The pipe operator |&gt; is an operator which allows us to chain the output of one function to the input of another function. This allows us to write more readable code.\n\nbrown_passives_df |&gt;\n  arrange(passive) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 5\n  cat   passive   n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 M         104 14433   873 science fiction\n2 R         146 21757   975 humour         \n3 L         265 57624  3673 detective      \n4 C         283 40749  1459 press reviews  \n5 N         290 69909  4438 adventure      \n\n\nThe result is the same but the code makes more sense. We can read the code from left to right, top to bottom, which is the order in which the functions are executed.\n\n\n\n\n\n\n Dive deeper\nThe native R pipe |&gt; was introduced in R 4.1.0. If you are using an earlier version of R, you can use {magrittr} to load the pipe operator %&gt;%.\nThere are certain advantages to using the {magrittr} pipe operator, including the ability to use the pipe operator to pass arguments to functions with placeholders. For more information, see the magrittr documentation.\n\n\n\nIn addition to being more legible, using the pipe with each function on its own line allows us to add comments to each line of code. For example, the following code block is the same as the previous code block, but with comments added.\n\n# Sort by the passive column and select the first 5 rows\nbrown_passives_df |&gt;\n  arrange(passive) |&gt;\n  slice_head(n = 5)\n\nIt is a good practice to add comments when writing code, as long as it makes the code more readable and easier to understand for others and for your future self! If the comments are too verbose, and only repeat what the code is ‘saying’, then don’t include them.\n\n\nSubsetting datasets with {dplyr}\nNow that we have a sense of the data, we can subset the dataset to create a variations of our original data frame. We can subset the data frame by selecting columns and/ or rows.\nIn the R lesson “Packages and Functions”, we saw that base R provides the bracket ([]) operator to subset data frames. {dplyr} provides functions to subset data frames which can be more readable and easier to use.\nLet’s first look a selecting columns. The select() function allows us to select columns by name. For example, the following code block will select the passive and n_w columns from the data frame:\n\n# Select the `passive` and `n_w` columns\nselect(brown_passives_df, passive, n_w)\n\n# A tibble: 15 × 2\n   passive    n_w\n     &lt;dbl&gt;  &lt;dbl&gt;\n 1     892 101196\n 2     543  61535\n 3     283  40749\n 4     351  39029\n 5     853  82010\n 6    1034 110363\n 7    1460 173017\n 8     837  69446\n 9    2423 181426\n10     352  68599\n11     265  57624\n12     104  14433\n13     290  69909\n14     290  70476\n15     146  21757\n\n\nBeyond selecting columns, we can also reorder columns and rename columns. For example, the following code block will select the passive and n_w columns, rename the n_w column to num_words, and reorder the columns so that num_words is the first column:\n\n# Select rename and reorder columns\nbrown_passives_df |&gt;\n  select(num_words = n_w, passive)\n\n# A tibble: 15 × 2\n   num_words passive\n       &lt;dbl&gt;   &lt;dbl&gt;\n 1    101196     892\n 2     61535     543\n 3     40749     283\n 4     39029     351\n 5     82010     853\n 6    110363    1034\n 7    173017    1460\n 8     69446     837\n 9    181426    2423\n10     68599     352\n11     57624     265\n12     14433     104\n13     69909     290\n14     70476     290\n15     21757     146\n\n\n\n\n\n\n\n\n Dive deeper\nselect() also provides a number of helper functions to select columns. For example, we can use the starts_with() function inside the select() call to select columns that start with a certain string. Or we can select columns by their vector type by using where(is.character).\nFor more information, see the select() documentation or use the ?select command in the R console.\n\n\n\nBy selecting some columns and not others, we have effectively dropped the columns we did not select. If it is more effective to drop columns by name, we can use the select() function with the - operator. For example, the following code block will drop the cat column from the data frame:\n\n# Drop the `n_w` column\nbrown_passives_df |&gt;\n  select(-cat)\n\n# A tibble: 15 × 4\n   passive    n_w   n_s name            \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n 1     892 101196  3684 press reportage \n 2     543  61535  2399 press editorial \n 3     283  40749  1459 press reviews   \n 4     351  39029  1372 religion        \n 5     853  82010  3286 skills / hobbies\n 6    1034 110363  4387 popular lore    \n 7    1460 173017  6537 belles lettres  \n 8     837  69446  2012 miscellaneous   \n 9    2423 181426  6311 learned         \n10     352  68599  3983 general fiction \n11     265  57624  3673 detective       \n12     104  14433   873 science fiction \n13     290  69909  4438 adventure       \n14     290  70476  4187 romance         \n15     146  21757   975 humour          \n\n\nLet’s now turn our attention to subsetting rows. The filter() function allows us to select rows by a logical condition. For example, the following code block will select rows where the values of the passive column are less than &lt; 1,000:\n\n# Select rows where `passive` is less than 1,000\nbrown_passives_df |&gt;\n  filter(passive &lt; 1000)\n\n# A tibble: 12 × 5\n   cat   passive    n_w   n_s name            \n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n 1 A         892 101196  3684 press reportage \n 2 B         543  61535  2399 press editorial \n 3 C         283  40749  1459 press reviews   \n 4 D         351  39029  1372 religion        \n 5 E         853  82010  3286 skills / hobbies\n 6 H         837  69446  2012 miscellaneous   \n 7 K         352  68599  3983 general fiction \n 8 L         265  57624  3673 detective       \n 9 M         104  14433   873 science fiction \n10 N         290  69909  4438 adventure       \n11 P         290  70476  4187 romance         \n12 R         146  21757   975 humour          \n\n\nWe can also use the filter() function to select rows by a character string. For example, the following code block will select rows where the values of the name column are equal to religion:\n\n# Select rows where `name` is equal to `religion`\nbrown_passives_df |&gt;\n  filter(name == \"religion\")\n\n# A tibble: 1 × 5\n  cat   passive   n_w   n_s name    \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 D         351 39029  1372 religion\n\n\nThe inequality operator != can be used for character strings as well. To include multiple values, we can use the %in% operator. In this case we can pass a vector of values to the filter() function. For example, the following code block will select rows where the values of the name column are equal to religion or learned:\n\n# Select multiple values\nbrown_passives_df |&gt;\n  filter(name %in% c(\"religion\", \"learned\", \"detective\"))\n\n# A tibble: 3 × 5\n  cat   passive    n_w   n_s name     \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1 D         351  39029  1372 religion \n2 J        2423 181426  6311 learned  \n3 L         265  57624  3673 detective\n\n\n\n\n\n\n\n\n Dive deeper\nFor more sophisticated subsetting, we can use the str_detect() function from {stringr} to select rows where the values of the name column contain a certain string. This approach will be enhanced later in the course when we learn about regular expressions.\n\n\n\n\n\nWriting datasets to a file with {readr}\nFinally, we can write data, including data frames, to a file with the write_*() functions from {readr}. The write_*() functions include:\n\nwrite_csv(): Write a data frame to a CSV file.\nwrite_tsv(): Write a data frame to a TSV file.\nwrite_delim(): Write a data frame to a delimited file with the specified delimiter (|, ;, etc).\n\nTo create a distinct data frame from the one we read into R, let’s subset our brown_passives_df data frame by columns and rows to create a new data frame that contains only the passive, n_w, and name columns and only the rows where the values of the passive column are greater than &gt; 1,000 and assign it to the brown_passives_subset_df.\n\n# Subset the data frame\nbrown_passives_subset_df &lt;-\n  brown_passives_df |&gt;\n  select(passive, n_w, name) |&gt;\n  filter(passive &gt; 1000)\n\nNow the following code block will write the brown_passives_subset_df data frame to a CSV file given the specified file path:\n\n# Write the data frame to a CSV file\nwrite_csv(\n  x = brown_passives_subset_df,\n  file = \"../data/derived/brown_passives_subset.csv\"\n)\n\nGiven the example directory structure we saw earlier, our new file appears in the data/derived/ directory.\nproject/\n├── data/\n│   ├── original/\n│   │   └── brown_passives_do.csv\n│   └── derived/\n│       ├── brown_passives_curated.csv\n│       ├── brown_passives_curated_dd.csv\n│       └── brown_passives_subset.csv\n└── code/\n    └── reading-inspecting-writing.qmd\nThere is much more to learn about reading, inspecting, and writing datasets in R. We will introduce more functions and techniques in the coming lessons. For now, we have learned how to read, inspect, and write datasets using R functions and Quarto code blocks!"
  },
  {
    "objectID": "recipes/recipe-02/index.html#check-your-understanding",
    "href": "recipes/recipe-02/index.html#check-your-understanding",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE {readr} provides functions to read rectangular data into R.\nThe echomessageinclude option in a code block determines whether the code is displayed in the output document.\nTRUEFALSE {dplyr} provides functions to create data dictionaries.\nread_csv()read_tsv()read_delim() is used to read tab-separated values (TSV) files.\nWhich function is in {dplyr} is used to select columns by name? select()filter()slice_head()\nTRUEFALSE The R pipe operator |&gt; allows us to chain the output of one function to the input of another function."
  },
  {
    "objectID": "recipes/recipe-02/index.html#lab-preparation",
    "href": "recipes/recipe-02/index.html#lab-preparation",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn Lab 2 you will have the opportunity to apply the skills you learned in this Recipe to create a Quarto document that reads, inspects, and writes data.\nIn addition to the knowledge and skills you have developed in Labs 0 and 1, to complete Lab 2, you will need to be able to:\n\nCreate code blocks in a Quarto document\nUnderstand the purpose of the label, echo, message, and include options in a code block\nLoad packages into an R session with library()\nUnderstand how to read and create file relative file paths\nRead datasets into R with the read_csv() function\nInspect data frames with {dplyr} functions such as glimpse(), slice_head(), slice_tail(), slice_sample(), and arrange().\nUse the |&gt; pipe operator to chain functions together.\nSubset data frames with {dplyr} functions such as select() and filter().\nWrite data frames to a file with the write_csv() function."
  },
  {
    "objectID": "recipes/recipe-02/index.html#footnotes",
    "href": "recipes/recipe-02/index.html#footnotes",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Code block can be written in other programming languages as well such as Python, Bash, etc.↩︎"
  },
  {
    "objectID": "recipes/index.html",
    "href": "recipes/index.html",
    "title": "Recipes",
    "section": "",
    "text": "00. Literate Programming\n\n\nAn introduction to Quarto\n\n\nIn this recipe, we will introduce the concept of Literate Programming and describe how to implement this concept through Quarto. I will provide a demonstration of some of the features of Quarto and describe the main structural characteristics of a Quarto document to help you get off and running writing your own documents that combine code and prose. \n\n\n\n\n\n8 min\n\n\n1,449 words\n\n\n\n\n\n\n\n\n\n\n\n\n01. Academic writing with Quarto\n\n\nKey Quarto features for academic writing\n\n\nThe implementation of literate programming we are using in this course is Quarto with R. As we have seen in previously, Quarto provides the ability to combine prose and code in a single document. This is a powerful strategy for creating reproducible documents that can be easily updated and shared. \n\n\n\n\n\n12 min\n\n\n2,312 words\n\n\n\n\n\n\n\n\n\n\n\n\n02. Reading, inspecting, and writing datasets\n\n\nBasics of working with datasets in R\n\n\nThis Recipe guides you through the process of reading, inspecting, and writing datasets using R packages and functions in a Quarto environment. You’ll learn how to effectively combine code and narrative to create a reproducible document that can be shared with others. \n\n\n\n\n\n16 min\n\n\n3,199 words\n\n\n\n\n\n\n\n\n\n\n\n\n03. Descriptive assessment of datasets\n\n\nSummarizing data with statistics, tables, and plots\n\n\nIn this Recipe we will explore appropriate methods for summarizing variables in datasets given the number and informational values of the variable(s). We will build on our understanding of how to summarize data using statistics, tables, and plots. \n\n\n\n\n\n17 min\n\n\n3,271 words\n\n\n\n\n\n\n\n\n\n\n\n\n04. Understanding the computing environment\n\n\nIdentify layers of computing environments and preview Git and GitHub workflows\n\n\nIn this recipe, we will learn how to scaffold a research project and how to use the tools and resources available to us to manage research projects. We will build on our understanding of the computing environment and the structure of reproducible projects and introduce new features of Git and GitHub. \n\n\n\n\n\n11 min\n\n\n2,136 words\n\n\n\n\n\n\n\n\n\n\n\n\n05. Collecting and documenting data\n\n\nAcquiring datasets with the Project Gutenberg API\n\n\nAt this point, we now have a strong undertanding of the foundations of programming in R and the data science workflow. Previous lessons, recipes, and labs focused on developing these skills while the chapters aimed to provide a conceptual framework for understanding the steps in the data science workflow. We now turn to applying our conceptual knowledge and our technical skills to accomplish the tasks of the data science workflow, starting with data acquisition. \n\n\n\n\n\n25 min\n\n\n4,822 words\n\n\n\n\n\n\n\n\n\n\n\n\n06. Organizing and documenting data\n\n\nCurating semi-structured data\n\n\nAfter acquiring data, the next step in process is to organize data that is not tabular into a curated dataset. A curated dataset is a tidy dataset that reflects the data without major modifications. This dataset serves as a more general starting point for further data transformation. In this recipe, we will focus on curating data from a semi-structured format. \n\n\n\n\n\n25 min\n\n\n4,849 words\n\n\n\n\n\n\n\n\n\n\n\n\n07. Transforming and documenting data\n\n\nPrepare and enrich datasets for analysis\n\n\nThe curated dataset reflects a tidy version of the original data. This data is relatively project-neutral. A such, project-specific changes are often made to bring the data more in line with the research goals. This may include modifying the unit of observation and/ or adding additional attributes to the data. This process may generate one or more new datasets that are used for analysis. In this recipe, we will explore a practical example of transforming data. \n\n\n\n\n\n19 min\n\n\n3,663 words\n\n\n\n\n\n\n\n\n\n\n\n\n08. Employing exploratory methods\n\n\nDescriptive analysis and unsupervised machine learning\n\n\nExploratory analysis is a wide-ranging term that encompasses many different methods. In this recipe, we will focus on the methods that are most commonly used in the analysis of textual data. These include frequency and distributional analysis, clustering, and word embedding models. \n\n\n\n\n\n29 min\n\n\n5,781 words\n\n\n\n\n\n\n\n\n\n\n\n\n09. Building predictive models\n\n\nSupervised machine learning\n\n\nThis recipe will cover the process of building a predictive model to classify text into one of three Spanish dialects: Argentinian, Mexican, or Spanish. We will take a step-by-step approach that includes data preparation, model training and evaluation, and result interpretation. We will see practical examples of how to apply the {tidymodels} framework to build and evaluate a predictive model. \n\n\n\n\n\n29 min\n\n\n5,752 words\n\n\n\n\n\n\n\n\n\n\n\n\n10. Building inference models\n\n\nSimulation-based Null Hypothesis Testing\n\n\nIn this recipe, we will explore statistical modeling and data analysis with using a practical research hypothesis in the area of Second Language Acquisition and Teaching. We’ll use {infer} to understand inference-based models. You’ll learn how to work with key variables, examine data distributions, and employ statistical methods to test hypotheses about their relationships. Our discussion will also involve improving our computing skills through practical exercises in data manipulation, visualization, and statistical analysis. This will provide you with the necessary tools to prepare, conduct, and interpret complex datasets and analyses. \n\n\n\n\n\n17 min\n\n\n3,271 words\n\n\n\n\n\n\n\n\n\n\n\n\n11. Sharing research\n\n\nCommunicating research findings\n\n\nIn this recipe, I cover the tools and strategies for sharing research findings with the public and peers. We will begin assuming we are using Quarto websites as the primary tool for sharing research findings in both forums. From there, we will enter into some of the details of articles, presentations, and publishing research code and data. \n\n\n\n\n\n13 min\n\n\n2,411 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recipes/recipe-00/index.html",
    "href": "recipes/recipe-00/index.html",
    "title": "00. Literate Programming",
    "section": "",
    "text": "Skills\n\nIdentify the main components of a Quarto document\nCreate and render a Quarto document\nModify front-matter and prose sections"
  },
  {
    "objectID": "recipes/recipe-00/index.html#concepts-and-strategies",
    "href": "recipes/recipe-00/index.html#concepts-and-strategies",
    "title": "00. Literate Programming",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nLiterate Programming\nFirst introduced by Donald Knuth (1984), the aim of Literate Programming is to be able to combine computer code and text prose in one document. This allows an analyst to run code, view the output of the code, view the code itself, and provide prose description all in one document. In this way, a literate programming document allows for presenting your analysis in a way that performs the computing steps desired and presents it in an easily readable format. Literate programming is now a key component of creating and sharing reproducible research (Gandrud 2015).\n\n\nQuarto\nQuarto is a specific implementation of the literate programming paradigm. In Figure 1 we see an example of Quarto in action. On the left we see the Quarto source code, which is a combination of text and code. On the right we see the output of the Quarto source code as an HTML document.\n\n\n\n\n\n\n\n\nFigure 1: Quarto source (left) and output (right) example.\n\n\n\n\n\nQuarto documents generate various types of output: web documents (HTML), PDFs, Word documents, and many other types of output formats all based on the same source code. While the interleaving of code and prose to create a variety of output documents is one of the most attractive aspects of literate programming and Quarto, it is also possible to create documents with no code at all. It is a very versatile technology as you will come to appreciate.\n\n\n\n\n\n\n Dive deeper\nTo see Quarto in action, please check out the Quarto Gallery for a variety of examples of Quarto documents and their output.\n\n\n\nA Quarto source document is a plain-text file with the extension .qmd that can be opened in any plain text reader. We will be using the RStudio IDE (henceforth RStudio) to create, open, and edit, and generate output from .qmd files but any plain-text reader, such as TextEdit (MacOS) or Notepad (PC) can open these files.\nWith this in mind, let’s now move on to the anatomy of a Quarto document.\n\nAnatomy of a Quarto Document\nAt the most basic level a Quarto document contains two components:\n\na front-matter section and\na prose section.\n\nA third component, a code block, can be interleaved within the prose section to add code to the document. Let’s look at each of these in turn.\n\nFront-matter\nThe front matter of a Quarto document appears, well, at the front of the document (or the top, rather). Referring back to Figure Figure 1, we see the front matter at the top.\n---\ntitle: \"Introduction to Quarto\"\nauthor: \"Jerid Francom\"\nformat: html\n---\nWhen creating a Quarto document with RStudio the default attribute keys are title, author, and format. The front matter is fenced by three dashes ---.\nThe values for the first two keys are pretty straightforward and can be edited as needed. The value for the format attribute can also be edited to tell the .qmd file to generate other output types. Can you guess what value we might use to generate a PDF document? Yep, it’s just pdf. As we work Quarto you will learn more about how to use the RStudio interface to change some of these key-value pairs and add others!\n\n\nProse\nAnywhere below the front matter and not contained within a code block (see below) is open for prose. The prose section(s) have an added functionality in that they are Markdown aware. What does that mean, you say? Well, Markdown refers to a set of plain-text formatting conventions to produce formatted text in the output document. To quote Wikipedia:\n\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber and Aaron Swartz created Markdown in 2004 as a markup language that is appealing to human readers in its source code form. Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n\nWhat this enables us to do is to add simple text conventions to signal how the output should be formatted. Say we want to make some text bold. We just add ** around the text we want to appear bold.\n**bold text**\nWe can also do:\n\nitalics *italics*\nlinks [links](http://wfu.edu)\nstrikethrough ~~strikethrough~~\netc.\n\nFollow this link find more information on basic Markdown syntax.\n\n\nCode blocks\nCode blocks are where the R magic happens. Again, referring to Figure 1, we see that there is the following code block.\n```{r}\n1 + 1\n```\nA code block is bound by three backticks ```. After the first backticks the curly brackets {} allow us to tell Quarto which programming language to use to evaluate (i.e. run) in the code block. In most cases this will be R, hence the the opening curly bracket `{r}`. But there are other languages that can be used in Quarto, such as Python, SQL, and Bash.\nIn the previous example, R is used as a simple calculator adding 1 + 1. Here’s what this code block produces.\n\n1 + 1\n\n[1] 2\n\n\n```{r}\n#| label: add\n1 + 1\n```\nWe have only mentioned selecting the coding language and labeling the code block, but code blocks have various other options that can be used to determine how the code block should be used. Some common code block options are:\n\nhiding the code: #| echo: false\n\n```{r}\n#| label: add\n#| echo: false\n1 + 1\n```\n\n\n[1] 2\n\n\n\nhiding the output #| include: false\n\n```{r}\n#| label: add\n#| include: false\n1 + 1\n```\n\netc.\n\n\n\n\nCreate and render a Quarto document\nThe easiest and most efficient way to create a Quarto source file is to use the RStudio point-and-click interface. Just use the toolbar to create a new file and select “Quarto Document…”, as seen in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Creating a new Quarto document in RStudio.\n\n\n\n\n\nThis will provide you a dialogue box asking you to add a title and author to the document and also allows you to select the type of document format to output, as seen in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Dialogue box for creating a new Quarto document in RStudio.\n\n\n\n\n\nEnter a title and author and leave the format set to HTML.\nOn clicking ‘Create’ you will get a Quarto document, as in Figure 4, with some default/ boilerplate prose and code blocks. The prose and code blocks can be deleted, and we can start our own document.\n\n\n\n\n\n\n\n\nFigure 4: Quarto source in RStudio.\n\n\n\n\n\nBut for now, let’s leave things as they are and see how to generate the output report from this document. Click “Render” in the RStudio toolbar. Before it will render, you will be asked to save the file and give it a name.\nOnce you have done that the .qmd file will render in the format you have specified and open in the ‘Viewer’ pane, as seen in Figure 5.\n\n\n\n\n\n\n\n\nFigure 5: Quarto source and HTML output side-by-side in RStudio.\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\nWatch Getting Started with Quarto for a guided tour of Quarto (Çetinkaya-Rundel 2023)."
  },
  {
    "objectID": "recipes/recipe-00/index.html#check-your-understanding",
    "href": "recipes/recipe-00/index.html#check-your-understanding",
    "title": "00. Literate Programming",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE Literate Programming, first introduced by Donald Knuth in 1984, allows the combination of computer code and text prose in one document.\nThe programming paradigm Literate Programming is implemented through QuartoRRStudioGitHub, a platform that facilitates the creation of a variety of output documents based on the same source code.\nWhich of the following components does a basic Quarto document not contain? Front-matter sectionProse sectionBack-matter sectionCode block\nTo generate a PDF document in Quarto, you can edit the format attribute value in the front-matter section to .\nTRUEFALSE The code block options echo and include can be used to hide the code and output, respectively.\nTRUEFALSE In Quarto, a code block, where the programming language code is entered, is bounded by three underscores (_)."
  },
  {
    "objectID": "recipes/recipe-00/index.html#lab-preparation",
    "href": "recipes/recipe-00/index.html#lab-preparation",
    "title": "00. Literate Programming",
    "section": "Lab preparation",
    "text": "Lab preparation\nThis concludes our introduction to literate programming using Quarto. We have covered the basics there but there is much more to explore.\nIn preparation for Lab 0, ensure that you have completed the following:\n\nSetup your computing environment with R and RStudio\nInstalled the necessary packages:\n\nquarto\ntinytex\n\n\nand that you are prepared to do the following:\n\nOpen RStudio and understand the basic interface\nCreate, edit, and render Quarto documents\nUse some basic Markdown syntax to format text\n\nWith this in mind, you are ready to move on to Lab 00."
  },
  {
    "objectID": "recipes/recipe-06/index.html",
    "href": "recipes/recipe-06/index.html",
    "title": "06. Organizing and documenting data",
    "section": "",
    "text": "Skills\n\nReading and parsing semi-structured data\nCreating a custom function and iterating over a collection of files\nCombining the results into a single dataset\nDocumenting the data curation process and resulting dataset\nIn this recipe, we will make use of {readr}, {dplyr}, {stringr}, and {purrr}, employ regular expressions to parse the semi-structured data, and use {qtalrkit} to document the dataset. Let’s load those packages now.\n# Load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(fs)\nlibrary(qtalrkit)\nIn Lab 6, we will apply what we learn in this recipe to curate and document acquired data."
  },
  {
    "objectID": "recipes/recipe-06/index.html#concepts-and-strategies",
    "href": "recipes/recipe-06/index.html#concepts-and-strategies",
    "title": "06. Organizing and documenting data",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nAssessing the data\nAcquired data can be in a variety of formats. This will range from unstructured data such as running text to structured data such as tabular data. Semi-structured data is somewhere in between. It has some structure, but it is not as well defined as structured data and requires some work to organize it into a tidy dataset.\nAs a semi-structured example we will work with the The Switchboard Dialog Act Corpus (SWDA) (University of Colorado Boulder 2008) which extends the Switchboard Corpus with speech act annotation.\n\n\n\n\n\n\n Tip\nIf you would like to download and decompress the data yourself, you can do so by running the following code:\n\nqtalrkit::get_compressed_data(\n  url = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz\",\n  target_dir = \"data/original/swda/\"\n)\n\n\n\nAs a starting point, let’s assume you have acquired the SWDA corpus and decompressed it into your project’s data/original/swda/ directory, as seen below.\ndata/\n├── analysis/\n├── derived/\n└── original/\n    └── swda/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\nThe first step is to inspect the data directory and file structure (and of course any documentation files).\nThe README file contains basic information about the resource, the doc/ directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with sw… contain individual conversation files.\nTaking a closer look at the first conversation file directory, sw00utt/ we can see that it contains files with the .utt extension.\n├── sw00utt\n│   ├── sw_0001_4325.utt\n│   ├── sw_0002_4330.utt\n│   ├── sw_0003_4103.utt\n│   ├── sw_0004_4327.utt\n│   ├── sw_0005_4646.utt\nLet’s take a look inside a conversation file (sw_0001_4325.utt) to see how it is structured internally. You can do this by opening the file in a text editor or by using the read_lines() function from the {readr} package.\n\n\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:   4325_1632_1519\nTOPIC#:     323\nDATE:       920323\nTRANSCRIBER:    glp\nUTT_CODER:  tc\nDIFFICULTY: 1\nTOPICALITY: 3\nNATURALNESS:    2\nECHO_FROM_B:    1\nECHO_FROM_A:    4\nSTATIC_ON_A:    1\nSTATIC_ON_B:    1\nBACKGROUND_A:   1\nBACKGROUND_B:   2\nREMARKS:        None.\n\n=========================================================================\n  \n\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }   \n\nqy^d          B.2 utt1: [ [ I guess, +   \n\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /  \n\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /  \n\nqy          A.5 utt1: Does it say something? /  \n\nsd          B.6 utt1: I think it usually does.  /\n\n\nThere are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of = characters. Second, the header contains meta-information of various types. Third, the conversation text is interleaved with an annotation scheme.\nSome of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let’s take a look at the README file.\nIn this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 ‘DAMSL’ dialog act labels. The README file refers us to the doc/manual.august1.html file for more information on this scheme.\nAt this point we open the the doc/manual.august1.html file in a browser and do some investigation. We find out that ‘DAMSL’ stands for ‘Discourse Annotation and Markup System of Labeling’ and that the first characters of each line of the conversation text correspond to one or a combination of labels for each utterance. So for our first utterances we have:\no = \"Other\"\nqw = \"Wh-Question\"\nqy^d = \"Declarative Yes-No-Question\"\n+ = \"Segment (multi-utterance)\"\nEach utterance is also labeled for speaker (‘A’ or ‘B’), speaker turn (‘1’, ‘2’, ‘3’, etc.), and each utterance within that turn (‘utt1’, ‘utt2’, etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.\nNow let’s turn to the meta-data in the header. We see here that there is information about the creation of the file: ‘FILENAME’, ‘TOPIC’, ‘DATE’, etc. The doc/manual.august1.html file doesn’t have much to say about this information so I returned to the LDC Documentation and found more information in the Online Documentation section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the caller_tab.csv file. This tabular file does not contain column names, but the caller_doc.txt does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the ‘FILENAME’ information contained three pieces of useful information delimited by underscores _.\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:   4325_1632_1519\nTOPIC#:     323\nDATE:       920323\nTRANSCRIBER:    glp\nThe first information is the document id (4325), the second and third correspond to the speaker number: the first being speaker A (1632) and the second speaker B (1519).\nIn sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of = characters. The header section contains a ‘FILENAME’ line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let’s set out to create a tidy dataset with the column structure as in Table 1.\n\n\n\n\nTable 1: Idealized curated dataset\n\n\n\n\n\n\nvariable\nname\ntype\ndescription\n\n\n\n\ndoc_id\nDocument ID\ncharacter\nThe unique identifier for the conversation\n\n\ndamsl_tag\nDAMSL Tag\ncharacter\nThe DAMSL tag for the utterance\n\n\nspeaker\nSpeaker\ncharacter\nThe speaker of the utterance\n\n\nturn_num\nTurn Number\ncharacter\nThe turn number of the utterance\n\n\nutterance_num\nUtterance Number\nnumeric\nThe utterance number of the utterance\n\n\nutterance_text\nUtterance Text\ncharacter\nThe text of the utterance\n\n\nspeaker_id\nSpeaker ID\ncharacter\nThe unique identifier for the speaker\n\n\n\n\n\n\n\n\n\n\n\n\nTidy the data\nThere are many ways to approach the task of tidying the data in general, and this semi-structured data in particular. In this recipe, we will take a step-by-step approach to parsing the semi-structured data in one file and then apply this process to all of the files in the corpus using a custom function.\nLet’s begin by reading one of the conversation files into R as a character vector using the read_lines() function from {readr}.\n\n# Read a single file as character vector\ndoc_chr &lt;-\n  read_lines(file = \"data/original/swda/sw00utt/sw_0001_4325.utt\")\n\nTo isolate the vector element that contains the document and speaker ids, we use str_subset() from {stringr}. This function takes two arguments, a string and a pattern, and returns any vector element that matches the pattern.\nIn this case we are looking for a pattern that matches three groups of digits separated by underscores. To test out a pattern, we can use the str_view() function. We will use the regular expression character class \\\\d for digits and the + operator to match 1 or more contiguous digits. We then separate three groups of \\\\d+ with underscores _. The result is \\\\d+_\\\\d+_\\\\d+.\n\n# Test out a pattern\ndoc_chr |&gt;\n  str_view(pattern = \"\\\\d+_\\\\d+_\\\\d+\")\n\n[15] │ FILENAME:{\\t}&lt;4325_1632_1519&gt;\n\n\nWe can see that this pattern matches the line we are looking for. Now we can use this pattern with str_subset() to return the vector element that contains this pattern.\n\n# Isolate the vector element that contains the document and speaker ids\nstr_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\")\n\n[1] \"FILENAME:\\t4325_1632_1519\"\n\n\n\n\n\n\n\n\n Tip\nRegular Expressions are a powerful pattern matching syntax. They are used extensively in text manipulation and we will see them again and again.\nTo develop regular expressions, it is helpful to have a tool that allows you to interactively test your pattern matching. {stringr} has a handy function str_view() which allows for interactive pattern matching. A good website to practice Regular Expressions is RegEx101. You can also install {regexplain} (Aden-Buie 2024) in R to get access to a useful RStudio Addin.\n\n\n\nThe next step is to extract the three digit sequences that correspond to the doc_id, speaker_a_id, and speaker_b_id. First we extract the pattern that we have identified with str_extract() and then we can break up the single character vector into multiple parts based on the underscore _. The str_split() function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors.\n\nstr_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |&gt; # isolate vector element\n  str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |&gt; # extract the pattern\n  str_split(\"_\") # split the character vector by underscore\n\n[[1]]\n[1] \"4325\" \"1632\" \"1519\"\n\n\nA list is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same –hence the tabular format).\nIn this case we have a list of length 1, whose sole element is a character vector of length 3 –one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our str_split() function we don’t want the results to be conflated as a single character vector blurring the distinction between the individual character vectors.\nIn this case, however, we want to extract the three elements of the character vector and assign them to meaningful variable names. To do this we will use the unlist() function which will convert the list into a single character vector. We will assign this result to speaker_info_chr.\n\nspeaker_info_chr &lt;-\n  str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n  str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n  str_split(\"_\") |&gt;\n  unlist() # convert the list to a character vector\n\n# Preview\nspeaker_info_chr\n\n[1] \"4325\" \"1632\" \"1519\"\n\n\nspeaker_info_chr is now a character vector of length three. Let’s subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.\n\ndoc_id &lt;- speaker_info_chr[1] # extract by index\nspeaker_a_id &lt;- speaker_info_chr[2] # extract by index\nspeaker_b_id &lt;- speaker_info_chr[3] # extract by index\n\nThe next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of = separates the header section from the text section. What we need to do is to index the point in our character vector doc_chr where that line occurs and then subset the doc_chr from that point until the end of the character vector.\nLet’s first find the point where the = sequence occurs. We will again use the str_view() to test out a pattern that matches a contiguous sequence of =.\n\nstr_view(doc_chr, \"=+\")\n\n[31] │ &lt;=========================================================================&gt;\n\n\nSo for this file we see there is one element that matches and that element’s index is 31.\nNow it is important to keep in mind that we are working with a single file from the swda/ data. Since our plan is to use this code to apply to other files, we need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the =+ pattern will match =, or ==, or ===, etc. it is not implausible to believe that there might be a = character on some other line in one of the other files.\nLet’s update our regular expression to avoid this potential scenario by only matching sequences of three or more =. In this case we will make use of the curly bracket operators {}.\n\nstr_view(doc_chr, \"={3,}\")\n\n[31] │ &lt;=========================================================================&gt;\n\n\nWe will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for ===, ====, etc.\nTo extract just the index of the match, we can use the str_which() function with the same pattern. This will return the index of the vector element that matches the pattern. However, consider what we are doing. We actually are using this index to subset the vector, so we need to increment the index by 1 to get the next vector element. Let’s do this and then assign the result to text_start_index.\n\n# Find where text starts\ntext_start_index &lt;- str_which(doc_chr, \"={3,}\") + 1\n\nThe index for the end of the text is simply the length of the doc_chr vector. We can use the length() function to get this index.\n\n# Find where text ends\ntext_end_index &lt;- length(doc_chr)\n\nWe now have the bookends, so to speak, for our text section. To extract the text we subset the doc_chr vector by these indices.\n\n# Extract text between indices\ntext &lt;- doc_chr[text_start_index:text_end_index]\n\n# Preview\nhead(text)\n\n[1] \"  \"                                       \n[2] \"\"                                         \n[3] \"o          A.1 utt1: Okay.  /\"            \n[4] \"qw          A.1 utt2: {D So, }   \"        \n[5] \"\"                                         \n[6] \"qy^d          B.2 utt1: [ [ I guess, +   \"\n\n\nThe text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the str_trim() function which by default will remove leading and trailing whitespace from each line.\n\n# Remove leading and trailing whitespace\ntext &lt;- str_trim(text)\n\n# Preview\nhead(text)\n\n[1] \"\"                                      \n[2] \"\"                                      \n[3] \"o          A.1 utt1: Okay.  /\"         \n[4] \"qw          A.1 utt2: {D So, }\"        \n[5] \"\"                                      \n[6] \"qy^d          B.2 utt1: [ [ I guess, +\"\n\n\nTo remove blank lines we will use str_subset() to subset the text vector. The .+ pattern will match elements that are not blank. We will assign the result to text overwriting the original text vector.\n\n# Remove blank lines\ntext &lt;- str_subset(text, \".+\")\n\n# Preview\nhead(text)\n\n[1] \"o          A.1 utt1: Okay.  /\"                                                                  \n[2] \"qw          A.1 utt2: {D So, }\"                                                                 \n[3] \"qy^d          B.2 utt1: [ [ I guess, +\"                                                         \n[4] \"+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\"\n[5] \"+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\"                        \n[6] \"qy          A.5 utt1: Does it say something? /\"                                                 \n\n\nOur first step towards a tidy dataset is to now combine the doc_id and each element of text in a data frame, leaving aside our speaker ids. We will use the tibble() function and pass the variables as named arguments.\n\n# Combine info and text into a data frame\nswda_df &lt;- tibble(doc_id, text)\n\n# Preview\nslice_head(swda_df, n = 5)\n\n# A tibble: 5 × 2\n  doc_id text                                                                   \n  &lt;chr&gt;  &lt;chr&gt;                                                                  \n1 4325   o          A.1 utt1: Okay.  /                                          \n2 4325   qw          A.1 utt2: {D So, }                                         \n3 4325   qy^d          B.2 utt1: [ [ I guess, +                                 \n4 4325   +          A.3 utt1: What kind of experience [ do you, + do you ] have…\n5 4325   +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n\n\nWith our data now in a data frame, it’s time to parse the text column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns.\nTo do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row.\nThe best way to learn regular expressions is to use them. To this end I’ve included a link to the interactive regular expression practice website regex101.\nOpen this site and copy the text below into the ‘TEST STRING’ field.\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }\nqy^d          B.2 utt1: [ [ I guess, +\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\nqy          A.5 utt1: Does it say something? /\nsd          B.6 utt1: I think it usually does.  /\nad          B.6 utt2: You might try, {F uh, }  /\nh          B.6 utt3: I don't know,  /\nad          B.6 utt4: hold it down a little longer,  /\n\n\n\n\n\nRegEx101\n\n\n\n\nNow manually type the following regular expressions into the ‘REGULAR EXPRESSION’ field one-by-one (each is on a separate line). Notice what is matched as you type and when you’ve finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.\n^.+?\\s\n[AB]\\.\\d+\nutt\\d+\n:.+$\nAs you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text.\nTo apply these expressions to our data and extract this information into separate columns we will make use of the mutate() and str_extract() functions. mutate() will take our data frame and create new columns with values we match and extract from each row in the data frame with str_extract().\n\n\n\n\n\n\n Tip\nNotice that str_extract() is different than str_extract_all(). When we work with mutate() each row will be evaluated in turn, therefore we only need to make one match per row.\n\n\n\nI’ve chained each of these steps in the code below, dropping the original text column with select(-text), and overwriting swda_df with the results.\n\n# Extract column information from `text`\nswda_df &lt;-\n  swda_df |&gt; # current dataset\n  mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |&gt; # damsl tags\n  mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |&gt; # speaker_turn pairs\n  mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |&gt; # utterance number\n  mutate(utterance_text = str_extract(text, \":.+$\")) |&gt; # utterance text\n  select(-text) # drop the `text` column\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 5\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n$ speaker_turn   &lt;chr&gt; \"A.1\", \"A.1\", \"B.2\", \"A.3\", \"B.4\", \"A.5\", \"B.6\", \"B.6\",…\n$ utterance_num  &lt;chr&gt; \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n$ utterance_text &lt;chr&gt; \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n\n\n\n\n\n\n\n\n Warning\nOne twist you will notice is that regular expressions in R require double backslashes (\\\\) where other programming environments use a single backslash (\\).\n\n\n\nThere are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the speaker_turn column into speaker and turn_num columns and second we need to remove unwanted characters from the damsl_tag, utterance_num, and utterance_text columns.\nTo separate the values of a column into two columns we use the separate_wider_delim() function. It takes a column to separate, a delimiter to use to separate the values, and a character vector of the names of the new columns to create.\n\n# Separate speaker_turn into distinct columns\nswda_df &lt;-\n  swda_df |&gt;\n  separate_wider_delim(\n    cols = speaker_turn,\n    delim = \".\",\n    names = c(\"speaker\", \"turn_num\")\n  )\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 6\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n$ utterance_text &lt;chr&gt; \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n\n\nTo remove unwanted leading or trailing whitespace we apply the str_trim() function. For removing other characters we matching the character(s) and replace them with an empty string (\"\") with the str_replace() function. Again, I’ve chained these functions together and overwritten data with the results.\n\n# Clean up column information\nswda_df &lt;-\n  swda_df |&gt; # current dataset\n  mutate(damsl_tag = str_trim(damsl_tag)) |&gt; # remove leading/ trailing whitespace\n  mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |&gt; # remove 'utt'\n  mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |&gt; # remove ': '\n  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 6\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n\n\nTo round out our tidy dataset for this single conversation file we will connect the speaker_a_id and speaker_b_id with speaker A and B in our current dataset adding a new column speaker_id. The case_when() function does exactly this: allows us to map rows of speaker with the value “A” to speaker_a_id and rows with value “B” to speaker_b_id.\n\n# Link speaker with speaker_id\nswda_df &lt;-\n  swda_df |&gt; # current dataset\n  mutate(speaker_id = case_when( # create speaker_id\n    speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n    speaker == \"B\" ~ speaker_b_id, # speaker_b_id value when B\n    TRUE ~ NA_character_ # NA otherwise\n  ))\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 7\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nWe now have the tidy dataset we set out to create. But this dataset only includes one conversation file! We want to apply this code to all 1,155 conversation files in the swda/ corpus.\nThe approach will be to create a custom function which groups the code we’ve done for this single file and then iteratively send each file from the corpus through this function and combine the results into one data frame.\nHere’s the custom function with some extra code to print a progress message for each file when it runs.\n\n\n# [ ] add to {qtalrkit}, note the convention of `extract_` prefix for curation functions. In combination with `get_compressed_data()` this corpus can be curated with few steps.\n\nextract_swda_data &lt;- function(file) {\n  # Progress message\n  file_basename &lt;- basename(file) # file name\n  message(\"Processing \", file_basename, \"\\n\")\n\n  # Read `file` by lines\n  doc_chr &lt;- read_lines(file)\n\n  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`\n  speaker_info_chr &lt;-\n    str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n    str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n    str_split(\"_\") |&gt;\n    unlist()\n\n  doc_id &lt;- speaker_info_chr[1]\n  speaker_a_id &lt;- speaker_info_chr[2]\n  speaker_b_id &lt;- speaker_info_chr[3]\n\n  # Extract `text`\n  text_start_index &lt;- str_which(doc_chr, \"={3,}\") + 1\n  text_end_index &lt;- length(doc_chr)\n\n  text &lt;-\n    doc_chr[text_start_index:text_end_index] |&gt;\n    str_trim() |&gt;\n    str_subset(\".+\")\n\n  swda_df &lt;- tibble(doc_id, text) # tidy format `doc_id` and `text`\n\n  # Extract column information from `text`\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |&gt; # damsl tags\n    mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |&gt; # speaker_turn pairs\n    mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |&gt; # utterance number\n    mutate(utterance_text = str_extract(text, \":.+$\")) |&gt; # utterance text\n    select(-text) # drop the `text` column\n\n  # Separate speaker_turn into distinct columns\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    separate_wider_delim(\n      cols = speaker_turn,\n      delim = \".\",\n      names = c(\"speaker\", \"turn_num\")\n    )\n\n  # Clean up column information\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    mutate(damsl_tag = str_trim(damsl_tag)) |&gt; # remove leading/ trailing whitespace\n    mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |&gt; # remove 'utt'\n    mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |&gt; # remove ': '\n    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\n  # Link speaker with speaker_id\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    mutate(speaker_id = case_when( # create speaker_id\n      speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n      speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B\n    ))\n\n  message(\"Processed \", file_basename, \"\\n\")\n  return(swda_df)\n}\n\nAs a sanity check we will run the extract_swda_data() function on a the conversation file we were just working on to make sure it works as expected.\n\n# Process a single file (test)\nextract_swda_data(\n  file = \"../data/original/swda/sw00utt/sw_0001_4325.utt\"\n) |&gt;\n  glimpse()\n\n\n\nRows: 159\nColumns: 7\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nLooks good!\nSo now it’s time to create a vector with the paths to all of the conversation files. The ls_dif() function from {fs} interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (regexp = \\\\.utt$) so we don’t accidentally include other files in the corpus. recurse set to TRUE means we will get the full path to each file.\n\n# List all conversation files\nswda_files_chr &lt;-\n  dir_ls(\n    path = \"../data/original/swda/\", # source directory\n    recurse = TRUE, # traverse all sub-directories\n    type = \"file\", # only return files\n    regexp = \"\\\\.utt$\"\n  ) # only return files ending in .utt\n\nhead(swda_files_chr) # preview file paths\n\n\n\ndata/original/swda/sw00utt/sw_0001_4325.utt\ndata/original/swda/sw00utt/sw_0002_4330.utt\ndata/original/swda/sw00utt/sw_0003_4103.utt\ndata/original/swda/sw00utt/sw_0004_4327.utt\ndata/original/swda/sw00utt/sw_0005_4646.utt\ndata/original/swda/sw00utt/sw_0006_4108.utt\n\n\nTo pass each conversation file in the vector of paths to our conversation files iteratively to the extract_swda_data() function we use map_dfr(). This will apply the function to each conversation file and return a data frame for each and then combine the results into a single data frame.\n\n# Process all conversation files\nswda_df &lt;-\n  swda_files_chr |&gt; # pass file names\n  map_dfr(extract_swda_data) # read and tidy iteratively\n\n# Preview\nglimpse(swda_df)\n\nRows: 223,606\nColumns: 7\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nWe now see that we have 223, 606 observations (individual utterances in this dataset). The structure of the data frame matches our idealized dataset in Table @ref(tab:swda-idealized-dataset).\nIt also is a good idea to inspect the data frame to ensure that the data is as expected. One is to check for missing values. We can use the skim() function from {skimr} to get a quick summary of the data frame. Another is to spot check the data frame to see if the values are as expected. As we are working with a fairly large dataset, we can use the slice_sample() function from {dplyr} to randomly sample a subset of rows from the data frame.\n\n\nDocumentation\nWe now have a tidy dataset, but we need to document the data curation process and the resulting dataset. The script used to curate the data should be cleaned up and well documented in prose and code comments.\nWe then need to write the dataset to disk and create a data dictionary. We will make sure to add the curated dataset to the derived/ directory and the data dictionary close to the dataset.\n\n# Write to disk\ndir_create(path = \"data/derived/swda/\") # create swda subdirectory\n\nwrite_csv(swda_df,\n  file = \"data/derived/swda/swda_curated.csv\"\n)\n\nThe directory structure now looks like this:\ndata/\n├── analysis/\n├── derived/\n│   └── swda/\n│       └── swda_curated.csv\n└── original/\n    └── swda/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\nThe data dictionary file will contain information about the dataset variables and their values. This file can be created manually and edited with a text editor or spreadsheet software. Or alternatively, the scaffolding for a CSV file can be generated with the create_data_dictionary() function from {qtalrkit}.\n\n# Create data dictionary\ncreate_data_dictionary(\n  data = swda,\n  file_path = \"data/derived/swda/swda_dd.csv\"\n)"
  },
  {
    "objectID": "recipes/recipe-06/index.html#summary",
    "href": "recipes/recipe-06/index.html#summary",
    "title": "06. Organizing and documenting data",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we learned how to read and parse semi-structured data, create a custom function and iterate over a collection of files, combine the results into a single dataset, and document the data curation process and resulting dataset.\nThe skills we used in this recipe include regular expressions, the {readr}, {dplyr}, {stringr}, and {purrr}, and {qtalrkit} for documenting the dataset."
  },
  {
    "objectID": "recipes/recipe-06/index.html#check-your-understanding",
    "href": "recipes/recipe-06/index.html#check-your-understanding",
    "title": "06. Organizing and documenting data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nThe first thing that should be done in the data curation process is to know what packages you are going to useexplore the data documentation and understand the resourceread the data into Rparse the data into a tidy dataset.\nThe read_lines() function from {readr} will read a file into R as a character vectordata framelistmatrix.\nTRUEFALSE The separate_wider_delim() function from {tidyr} will separate a column into two or more columns based on a delimiter (e.g. -, ., etc.).\nWhich of the following functions from {stringr} will return vector elements which contain a match for a pattern? str_subset()str_extract()str_replace()str_trim()\nThe map_dfr() function from {purrr} will apply a function to each element of a vector and return a listnested data framedata frame with rows combineddata frame with columns combined.\nA data dictionary is a document that describes the data curation processdata analysis processdataset variables and their valuesdata visualization process."
  },
  {
    "objectID": "recipes/recipe-06/index.html#lab-preparation",
    "href": "recipes/recipe-06/index.html#lab-preparation",
    "title": "06. Organizing and documenting data",
    "section": "Lab preparation",
    "text": "Lab preparation\nBefore beginning Lab 6, review and ensure that you are familiar with the following:\n\nVector, data frame, and list data structures\nSubsetting and indexing vectors, data frames, and lists\nBasic regular expressions such as character classes, quantifiers, and anchors\nReading, writing, and manipulating files\nCreating and employing custom functions\n\nIn this lab, we will practice these skills and expand our use of the {readr}, {dplyr}, {stringr}, and {purrr} to curate and document a dataset.\nYou will have a choice of data to curate. Before you start the lab, you should consider which data source you would like to use, what the idealized structure the curated dataset will take, and what strategies you will likely employ to curate the dataset. You should also consider the information you need to document the data curation process."
  },
  {
    "objectID": "recipes/recipe-08/index.html",
    "href": "recipes/recipe-08/index.html",
    "title": "08. Employing exploratory methods",
    "section": "",
    "text": "Skills\n\nTokenize and prepare features for analysis\nConduct frequency and dispersion analysis including measures and visualizations\nTrain word embeddings and create a Term-Document Matrix\nConduct a word embedding analysis\nThe approach to exploratory analysis is rarely linear, but rather an interative cycle of the steps in Table 1. This cycle is repeated until the research question(s) have been addressed.\nWe will model how to explore iteratively using the output of one method to inform the next and ultimately to address the research question. For this reason, the subsequent sections of this recipe are grouped by research question rather than by approach step or method.\nLet’s get started by loading some of the packages we will likely use.\nlibrary(dplyr)      # for data manipulation\nlibrary(stringr)    # for string manipulation\nlibrary(tidyr)      # for data tidying\nlibrary(tidytext)   # for text analysis\nlibrary(ggplot2)    # for data visualization"
  },
  {
    "objectID": "recipes/recipe-08/index.html#concepts-and-strategies",
    "href": "recipes/recipe-08/index.html#concepts-and-strategies",
    "title": "08. Employing exploratory methods",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\nWe will use the SOTU corpus to demonstrate the different methods. We will select a subset of the corpus (post-1945) and explore the question:\n\nHow has the language of the SOTU changed over time?\n\nThis will include methods such as frequency and distributional analysis, dimensionality reduction, and word embedding models.\nLet’s look at the first few rows of the data to get a sense of what we have.\n\nsotu_df\n\n# # A tibble: 73 × 4\n#    president   year party      address                                          \n#    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                                            \n#  1 Truman      1947 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  2 Truman      1948 Democratic \"Mr. President, Mr. Speaker, and Members of the …\n#  3 Truman      1949 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  4 Truman      1950 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  5 Truman      1951 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  6 Truman      1952 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  7 Eisenhower  1953 Republican \"Mr. President, Mr. Speaker, Members of the Eigh…\n#  8 Eisenhower  1954 Republican \"Mr. President, Mr. Speaker, Members of the Eigh…\n#  9 Eisenhower  1955 Republican \"Mr. President, Mr. Speaker, Members of the Cong…\n# 10 Eisenhower  1956 Republican \"My Fellow Citizens: This morning I sent to the …\n# # ℹ 63 more rows\n\n\nWe can see that the dataset contains the president, year, party, and address for each SOTU address.\nNow let’s view a statistical overview summary of the data by using the skim() function.\nskimr::skim(sotu_df)\n\n\n# ── Data Summary ────────────────────────\n#                            Values \n# Name                       sotu_df\n# Number of rows             73     \n# Number of columns          4      \n# _______________________           \n# Column type frequency:            \n#   character                3      \n#   numeric                  1      \n# ________________________          \n# Group variables            None   \n# \n# ── Variable type: character ────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate  min   max empty n_unique whitespace\n# 1 president             0             1    4    10     0       12          0\n# 2 party                 0             1   10    10     0        2          0\n# 3 address               0             1 6160 51947     0       73          0\n# \n# ── Variable type: numeric ──────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate  mean   sd   p0  p25  p50  p75 p100\n# 1 year                  0             1 1984. 21.6 1947 1965 1984 2002 2020\n#   hist \n# 1 ▇▇▇▇▇\n\n\nWe can see that the dataset contains 73 rows –corresponding to the number of SOTU addresses. Looking at missing values, we can see that there are no missing values for any of the variables. Taking a closer look at each variable, the president variable is a character vector with 12 unique presidents. There are two unique parties and 73 unique addresses. The year variable is numeric with a minimum value of 1947 and a maximum value of 2020, therefore the addresses include 73 years.\n\n\nIdentify\nWith a general sense of the data, we can now move on to the first step in the exploratory analysis workflow: identifying variables of interest.\n\nWhat linguistic variables might be of interest to address this question?\n\nWords might be the most obvious variable, but we might also be interested in other linguistic variables such as parts of speech, syntactic structures, or some combination of these.\nLet’s start with words. If we look at words, we might be interested in the frequency of words, the distribution of words, or the meaning of words. We might also be interested in the relationship between words. For example, we might be interested in the co-occurrence of words or the similarity of words. These, and more, are all possible approaches that we might consider.\nAnother consideration is if we want to do comparisons across time, across presidents, across parties, etc.. In our research question, we have already identified that we want to compare across time so that will be our focus. However, what we mean by “time” is not clear. Do we mean across years, across decades, across presidencies, etc.? We will need to make a decision about how we want to define time, but we can fold this into our exploratory analysis, as we will see below.\nLet’s posit the following sub-questions:\n\nWhat are the most frequent words across time periods?\nHow does the distribution of words change across time periods?\nHow does the meaning of words change across time periods?\n\nWe will use these sub-questions to guide our exploratory analysis.\n\n\nInspect\nThe next step is to inspect the data. We will transform the data as necessary to prepare it for analysis and then do some diagnostic checks to make sure that the data is ready for analysis.\nSince we will be working with words, let’s tokenize the addresses variable to extract the words and maintain a tidy dataset. We will use the unnest_tokens() function from {tidytext} to do this. Let’s apply the function and assign the result to a new variable called sotu_words_df.\n\n# Tokenize the words by year (with numbers removed)\nsotu_words_df &lt;-\n  sotu_df |&gt;\n  unnest_tokens(word, address, strip_numeric = TRUE)\n\nLet’s continue by looking at whether there is a relationship between the number of words and years. We can do this by using the count() function on the year variable. This will group and count the number of observations (words) per year as n.\nWe can then visualize this with a line plot where the x-axis is the year and the y-axis is the number of words n. I’ll add the plot to a variable called p so that we can add layers to it.\n\n# Get the number of words per year --------------------------------\np &lt;-\n  sotu_words_df |&gt;\n  count(year) |&gt;\n  ggplot(aes(x = year, y = n)) +\n  geom_line()\n\n# View\np\n\n\n\n\n\n\n\nFigure 1: Number of words per year\n\n\n\n\n\n\n\n\n\n\n\n Tip\nThe count() function is a wrapper for the summarize() function. It is a convenient way to count the number of observations in a dataset.\n\n# the output of\ndf |&gt;\n  count(var1)\n\n# is equivalent to\ndf |&gt;\n  group_by(var1) |&gt;\n  summarize(n = n()) |&gt;\n  ungroup()\nThe difference is that count() grouping is added and removed automatically. In other cases, we can mimic this behavior for other operations inside a summarize() or mutate() function by using the .by argument. For example:\ndf |&gt;\n  summarize(n = n(), .by = var1)\n\n\n\nWe can see from Figure 2 that the number of words per year varies, sometimes quite a bit. To get a sense of the relationship between the number of words and the year, we can add a linear trend line to the plot. We can do this by adding the geom_smooth() function to the plot. We will set the method argument to \"lm\" to use a linear model.\n\np + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nThis plot shows that there is a positive relationship between the number of words and the year –that is the number of words increases over time. But the line is not a great fit and furthermore the angle of the line is more horizontal than vertical. This suggests that the relationship is not a strong one. We can confirm this by calculating the correlation between the number of words and the year. We can do this by using the cor.test() function on the year and n variables inside a summarize() function.\n\n# Get the correlation between the number of words and the year ----\nsotu_words_df |&gt;\n  count(year) |&gt;\n  summarize(cor = cor.test(year, n)$estimate)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.342\n\n\nSo, even though we are working to inspect our data, we already have a finding. The number of words increases over time, despite the fact that the relationship is not a strong one.\nNow, let’s turn our attention to the frequency of individual words. Let’s start by looking at the most frequent words for the entire corpus. We can do this by grouping by the word variable and then summarizing the number of times each word occurs. We will then arrange the data in descending order by the number of times each word occurs.\n\n# Get the most frequent words ------------------------------------\n\nsotu_words_df |&gt;\n  count(word, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\n# A tibble: 10 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the   21565\n 2 and   14433\n 3 to    13679\n 4 of    13087\n 5 in     8116\n 6 we     7327\n 7 a      7137\n 8 our    6822\n 9 that   5391\n10 for    4513\n\n\nThe usual suspects are at the top of the list. This is not surprising, and will likely be the case for most corpora, and sizeable subcorpora –in our case the time periods. Let’s address this.\nWe could use a stopwords list to just eliminate many of these common words, but that might be a bit too agressive and we will likely lose some words that we want to keep. Considering a more nuanced approach, we will use the \\(tf\\)-\\(idf\\) transformation to attenuate the effect of words that are common across all time periods, and on the flip side, to promote the effect of words that are more distinctive to each time period.\nIn order to do this, we will need to calculate the \\(tf\\)-\\(idf\\) for each word in each time period. To keep things simple, we will calculate the \\(tf\\)-\\(idf\\) for each word in each decade. We will do this by creating a new variable called decade that is the year rounded down to the nearest decade. Then we can group by this decade variable and then count the number of times each word occurs. We will then calculate the \\(tf\\)-\\(idf\\) for each word in each decade using the bind_tf_idf() function from {tidytext}. We will then arrange the data in descending order by the \\(tf\\)-\\(idf\\) value.\n\n# Get the tf-idf of words by decade ------------------------------\n\nsotu_words_tfidf_df &lt;-\n  sotu_words_df |&gt;\n  mutate(decade = floor(year / 10) * 10) |&gt;\n  count(decade, word) |&gt;\n  bind_tf_idf(word, decade, n) |&gt;\n  arrange(decade, desc(tf_idf))\n\n# Preview\nsotu_words_tfidf_df\n\n# A tibble: 40,073 × 6\n   decade word               n       tf   idf   tf_idf\n    &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   1940 boycotts           5 0.000344 2.20  0.000756\n 2   1940 jurisdictional     7 0.000482 1.50  0.000725\n 3   1940 interpretation     4 0.000275 2.20  0.000605\n 4   1940 unjustified        4 0.000275 2.20  0.000605\n 5   1940 insecurity         5 0.000344 1.50  0.000518\n 6   1940 arbitration        3 0.000207 2.20  0.000454\n 7   1940 output             6 0.000413 1.10  0.000454\n 8   1940 unjustifiable      3 0.000207 2.20  0.000454\n 9   1940 management        25 0.00172  0.251 0.000433\n10   1940 rental             4 0.000275 1.50  0.000414\n# ℹ 40,063 more rows\n\n\nOK. Even the preview shows that we are getting a more interesting list of words.\nLet’s look at the top 10 words for each decade. We group by decade and then slice the top 10 words by \\(tf\\)-\\(idf\\) value with slice_max(). Then we will use the reorder_within() function from {tidytext} to reorder the words within each facet by the \\(tf\\)-\\(idf\\) value.\nWe will visualize this with a bar chart where word is on the x-axis and the height of the bar is the \\(tf\\)-\\(idf\\) value. We will also facet the plot by decade. I’ve flipped the coordinates so that the words are on the y-axis and the bars are horizontal. This is a personal preference, but I find it easier to read the words this way.\n\nsotu_words_tfidf_df |&gt;\n  group_by(decade) |&gt;\n  slice_max(n = 10, tf_idf) |&gt;\n  ungroup() |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  mutate(word = reorder_within(word, desc(tf_idf), decade)) |&gt;\n  ggplot(aes(word, tf_idf, fill = decade)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~decade, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 3: Visualize the top 10 words by decade\n\n\n\n\n\nScanning the top words for the decades we can see some words that signal contemporary issues. This is a hint that we are picking up on some of the changes in the language of the SOTU over time.\n\n\n\n\n\n\n Dive deeper\nThe plot above makes use of the reorder_within() and scale_x_reordered() functions from {tidytext}. These functions allow us to reorder the words within each facet by the \\(tf\\)-\\(idf\\) value. This is a nice way to visualize the most distinctive words for each decade. These are more advanced functions. If you are interested in learning more about them, you can read more about them in the help documentation ?tidytext::reorder_within\n\n\n\nTo my eye, however, the 1940s and the 2020s don’t seem to jump out at me in the same way. Let’s take a closer look at the 1940s and the 2020s in our original dataset.\n\nsotu_df |&gt;\n  filter(year &lt; 1950 | year &gt;= 2020)\n\n# A tibble: 4 × 4\n  president  year party      address                                            \n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                                              \n1 Truman     1947 Democratic \"Mr. President, Mr. Speaker, Members of the Congre…\n2 Truman     1948 Democratic \"Mr. President, Mr. Speaker, and Members of the 80…\n3 Truman     1949 Democratic \"Mr. President, Mr. Speaker, Members of the Congre…\n4 Trump      2020 Republican \"Madam Speaker, Mr. Vice President, Members of Con…\n\n\nWell, that explains it. There are only 3 addresses in the 1940s and 1 address in the 2020s. This is not enough data to get a good sense of the language of the SOTU for these decades. Let’s remove these decades from our original dataset as not being representative of the language of the SOTU.\nAnother consideration that catches my eye in looking at the top words by decade is that our words like “communist” and “communists” are being counted separately. That is fine, but what if we want to count these as the same word? We can do this by lemmatizing the words –that is reducing the words to their root form. We can do this using the lemmatize_words() function from {textstem}.\nSo consider this example:\n\n# Lemmatize the words --------------------------------------------\nwords_chr &lt;- c(\"freedom\", \"free\", \"frees\", \"freeing\", \"freed\")\ntextstem::lemmatize_words(words_chr)\n\n[1] \"freedom\" \"free\"    \"free\"    \"free\"    \"free\"   \n\n\n\n\n\n\n\n\n Dive deeper\nBy default, the lemmatize_words() function uses a lookup table for English to lemmatize words. This is a simple approach that works well for many cases. However, it is not perfect. For example, it will not lemmatize words that are not in the lookup table.\nIf you want to lemmatize words that are not in the lookup table, or you want to lemmatize words in another language, you can create or add to a lookup table. You can read more about this in the help documentation ?textstem::lemmatize_words().\nA resource for lemma lookup tables can be found here https://github.com/michmech/lemmatization-lists.\n\n\n\nWith these considerations in mind, let’s update our sotu_df dataset to remove the 1940s and 2020s, tokenize and lemmatize the words, and add a decade variable.\n\n# Update the dataset ----------------------------------------------\nsotu_terms_df &lt;-\n  sotu_df |&gt;\n  filter(year &gt;= 1950 & year &lt; 2020) |&gt; # Remove the 1940s and 2020s\n  unnest_tokens(word, address, strip_numeric = TRUE) |&gt; # Tokenize the words\n  mutate(lemma = textstem::lemmatize_words(word)) |&gt; # Lemmatize the words\n  mutate(decade = floor(year / 10) * 10) |&gt; # Add a decade variable\n  select(president, decade, year, party, word, lemma) #\n\n# Preview\nsotu_terms_df\n\n# A tibble: 368,586 × 6\n   president decade  year party      word      lemma    \n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n 1 Truman      1950  1950 Democratic mr        mr       \n 2 Truman      1950  1950 Democratic president president\n 3 Truman      1950  1950 Democratic mr        mr       \n 4 Truman      1950  1950 Democratic speaker   speaker  \n 5 Truman      1950  1950 Democratic members   member   \n 6 Truman      1950  1950 Democratic of        of       \n 7 Truman      1950  1950 Democratic the       the      \n 8 Truman      1950  1950 Democratic congress  congress \n 9 Truman      1950  1950 Democratic a         a        \n10 Truman      1950  1950 Democratic year      year     \n# ℹ 368,576 more rows\n\n\nThis inspection process could go on for a while. We could continue to inspect the data and make changes to the dataset but it is often the case that in the process of analysis we will often run into issues that require us to go back and make changes to the dataset. So we will move on to the next step in the exploratory analysis workflow.\n\n\nInterrogate\nNow that we have our data in a tidy format, we can move on to the next step in the exploratory analysis workflow: interrogating the data. We will submit the selected variables to descriptive or unsupervised learning methods to provide quantitative measures to evaluate.\n\nFrequency\n\nWhat are the most frequent words across time periods?\n\nWe have already made some progress on this question in the inspection phase, but now we can do it again with the updated dataset.\n\n# Get the most frequent lemmas by decade --------------------------\n\nsotu_lemmas_tfidf_df &lt;-\n  sotu_terms_df |&gt;\n  count(decade, lemma) |&gt; # Count the lemmas by decade\n  bind_tf_idf(lemma, decade, n) |&gt; # Calculate the tf-idf\n  arrange(decade, desc(tf_idf))\n\n# Preview\nsotu_lemmas_tfidf_df\n\n# A tibble: 26,435 × 6\n   decade lemma            n       tf   idf   tf_idf\n    &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   1950 armament        16 0.000321 1.95  0.000625\n 2   1950 imperialism      9 0.000181 1.95  0.000351\n 3   1950 shall          107 0.00215  0.154 0.000331\n 4   1950 disarmament     13 0.000261 1.25  0.000327\n 5   1950 mineral          8 0.000160 1.95  0.000312\n 6   1950 mobilization    12 0.000241 1.25  0.000302\n 7   1950 survivor        12 0.000241 1.25  0.000302\n 8   1950 expenditure     42 0.000842 0.336 0.000283\n 9   1950 adequate        25 0.000501 0.560 0.000281\n10   1950 constantly      11 0.000221 1.25  0.000276\n# ℹ 26,425 more rows\n\n\nNow we can visualize the top 10 lemmas for each decade, as we did above, but for the lemmas instead of the words and for seven full decades.\n\nsotu_lemmas_tfidf_df |&gt;\n  group_by(decade) |&gt;\n  slice_max(n = 10, tf_idf) |&gt;\n  ungroup() |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  mutate(lemma = reorder_within(lemma, desc(tf_idf), decade)) |&gt;\n  ggplot(aes(lemma, tf_idf, fill = decade)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~decade, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 4: Visualize the top 10 lemmas by decade\n\n\n\n\n\n\n\nDistribution\n\nHow does the distribution of words change across time periods?\n\nOK. Now let’s focus on word frequency distributions over time. We will return to the sotu_terms_df.\n\nsotu_terms_df\n\n# A tibble: 368,586 × 6\n   president decade  year party      word      lemma    \n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n 1 Truman      1950  1950 Democratic mr        mr       \n 2 Truman      1950  1950 Democratic president president\n 3 Truman      1950  1950 Democratic mr        mr       \n 4 Truman      1950  1950 Democratic speaker   speaker  \n 5 Truman      1950  1950 Democratic members   member   \n 6 Truman      1950  1950 Democratic of        of       \n 7 Truman      1950  1950 Democratic the       the      \n 8 Truman      1950  1950 Democratic congress  congress \n 9 Truman      1950  1950 Democratic a         a        \n10 Truman      1950  1950 Democratic year      year     \n# ℹ 368,576 more rows\n\n\nWe want to get a sense of how the word distributions change over time. We need to calculate the frequency of words for each year, first off. So we need to go back to the sotu_terms_df dataset and group by year and word and then count the number of times each word occurs.\nSince we will lose the lemma variable in this process, we will add it back after the \\(tf\\)-\\(idf\\) transformation by using the mutate() function and the textstem::lemmatize_words() function.\n\nsotu_terms_tfidf_df &lt;-\n  sotu_terms_df |&gt;\n  count(year, word) |&gt; # Count the words by year\n  bind_tf_idf(word, year, n) |&gt; # Calculate the tf-idf\n  mutate(lemma = textstem::lemmatize_words(word)) |&gt; # Lemmatize the words\n  arrange(year, desc(tf_idf))\n\n# Preview\nsotu_terms_tfidf_df\n\n# A tibble: 97,213 × 7\n    year word               n       tf   idf  tf_idf lemma         \n   &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         \n 1  1950 enjoyment          3 0.000585 3.54  0.00207 enjoyment     \n 2  1950 rent               3 0.000585 2.85  0.00167 rend          \n 3  1950 widespread         3 0.000585 2.85  0.00167 widespread    \n 4  1950 underdeveloped     2 0.000390 4.23  0.00165 underdeveloped\n 5  1950 ideals             7 0.00136  1.14  0.00156 ideal         \n 6  1950 transmit           3 0.000585 2.62  0.00153 transmit      \n 7  1950 peoples            8 0.00156  0.976 0.00152 people        \n 8  1950 democratic        12 0.00234  0.623 0.00146 democratic    \n 9  1950 expenditures       7 0.00136  1.06  0.00144 expenditure   \n10  1950 businessmen        3 0.000585 2.44  0.00143 businessman   \n# ℹ 97,203 more rows\n\n\nWith this format, we can visualize the distinctiveness of words over time. All we need to do is to filter the data to the lemmas we are interested first.\nLet’s just start with some random poltical-oriented words.\n\nplot_terms &lt;- c(\"crime\", \"law\", \"free\", \"terror\", \"family\", \"government\")\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.25) +\n  facet_wrap(~lemma, scales = \"free_y\")\n\n\n\n\n\n\n\nFigure 5: Distictiveness of political words over time\n\n\n\n\n\nWe can see in Figure 5 that the distinctiveness of these lemmas varies over time. Now, in this plot I’ve used a small span value for the geom_smooth() function to get a sense of the more fine-grained changes over time. However, this is may be too fine-grained. We can adjust this by increasing the span value. Let’s try a span value of 0.5.\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  facet_wrap(~lemma, scales = \"free_y\")\n\n\n\n\n\n\n\nFigure 6: Distinctiveness of political words over time\n\n\n\n\n\nFigure @ref(fig:sotu-terms-tfidf-df-political-smooth-detail) seems to be picking up on some of the more general word usage trends over time.\nAnother thing to note about the way we plotted the data is that we used the facet_wrap() function to create a separate plot for each word but we used the scales = \"free_y\" allowing the y-axis to vary for each plot. This means we are not comparing the y-axis values across plots and thus can not say anything about the differing magnitudes from a visual inspection.\nTo address this, we can remove the scales = \"free_y\" argument and use the default which will fix the x- and y-axis scales across plots.\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  facet_wrap(~lemma)\n\n\n\n\n\n\n\nFigure 7: Distinctiveness of political words over time\n\n\n\n\n\nIn Figure 7, we can clearly see that the magnitude of the words “crime” and “terror” are much higher than the other words we happend to select. Furthermore, these words have interesting patterns. In particular, “terror” has two peaks on around 1980 and another around the turn of the century. “Crime” also has two distinctive peaks on in the 1970s and one in the 1990s.\nThe terms that I selected before were somewhat arbitrary. How can I identify the words that have changed the most drastically over these years from the data itself? We can do this by creating a term-document matrix with and then calculating the standard deviation of the \\(tf\\)-\\(idf\\) values for each word. This will give us a sense of the words that have changed the most over time.\n\n# Create TDM with words-year and tf-idf values\nsotu_word_tfidf_mat &lt;-\n  sotu_terms_tfidf_df |&gt;\n  cast_tdm(word, year, tf_idf) |&gt;\n  as.matrix()\n\n# Preview\ndim(sotu_word_tfidf_mat)\n\n[1] 13109    69\n\nsotu_word_tfidf_mat[1:5, 1:5]\n\n                Docs\nTerms               1950    1951     1952 1953 1954\n  enjoyment      0.00207 0.00000 0.000661    0    0\n  rent           0.00167 0.00000 0.000000    0    0\n  widespread     0.00167 0.00000 0.000000    0    0\n  underdeveloped 0.00165 0.00000 0.000000    0    0\n  ideals         0.00156 0.00171 0.000853    0    0\n\n\nTo calculate the standard deviation of the \\(tf\\)-\\(idf\\) values for each word, we can use the apply() function to iterate over each row of the matrix and calculate the standard deviation of the values in each row. You can think of the apply() function as a cousin of the map() function. The apply() function iterates over the rows or columns of a matrix or data frame and applies a function to each row or column. We choose whether the function is applied to the rows or columns with the MARGIN argument. We can set MARGIN = 1 to apply the function to the rows and MARGIN = 2 to apply the function to the columns.\n\n# Calculate the standard deviation of the tf-idf values for each word\nsotu_words_sd &lt;-\n  apply(sotu_word_tfidf_mat, MARGIN = 1, FUN = sd, na.rm = TRUE)\n\n# Preview seed words\nsotu_words_sd |&gt;\n  sort(decreasing = TRUE) |&gt;\n  head(100)\n\n     vietnam      hussein       saddam         salt         iraq        shall \n    0.001188     0.001138     0.001121     0.001067     0.000976     0.000887 \n         oil        iraqi       that's   inspectors        qaida       terror \n    0.000798     0.000786     0.000733     0.000715     0.000702     0.000662 \n  terrorists         it's        crude       soviet    terrorist     activity \n    0.000651     0.000647     0.000635     0.000633     0.000632     0.000630 \n    covenant arrangements      session           al  disarmament       steven \n    0.000611     0.000609     0.000596     0.000596     0.000589     0.000588 \n     wartime      barrels         isil    mentioned        ought        elvin \n    0.000573     0.000569     0.000567     0.000559     0.000558     0.000556 \n      iraqis         ryan        we're         kids        let's          gun \n    0.000551     0.000539     0.000539     0.000535     0.000533     0.000532 \n     rebekah           cj        camps      empower         cory        we've \n    0.000531     0.000527     0.000524     0.000523     0.000522     0.000518 \n         92d  afghanistan    childcare        100th        music         gulf \n    0.000516     0.000514     0.000513     0.000507     0.000500     0.000494 \n        21st   extremists   foundation      picture     beguiled  contemplate \n    0.000491     0.000489     0.000487     0.000486     0.000473     0.000473 \n   enumerate    hurriedly   hysterical  ingredients        smile       smiles \n    0.000473     0.000473     0.000473     0.000473     0.000473     0.000473 \n    wagehour     josefina        shi'a        alice     alliance    seventies \n    0.000473     0.000471     0.000468     0.000468     0.000467     0.000464 \n      herman       joshua      matthew        julie         11th       border \n    0.000463     0.000463     0.000463     0.000462     0.000461     0.000459 \n     persian    recommend    communist        mayor    nicaragua       planes \n    0.000456     0.000453     0.000447     0.000443     0.000442     0.000442 \n      trevor        corey       kenton      preston        seong     property \n    0.000440     0.000439     0.000439     0.000439     0.000439     0.000439 \n     regimes       disarm      mention    peacetime   localities    objective \n    0.000437     0.000436     0.000436     0.000433     0.000429     0.000428 \n         i'm       surtax        banks       pounds       foster          she \n    0.000424     0.000423     0.000423     0.000422     0.000421     0.000418 \n        isis  sandinistas  discharging         gold \n    0.000417     0.000416     0.000416     0.000415 \n\n\nNow we can choose from the top words that have changed the most over time. Here’s another selection of words based on the standard deviation of the \\(tf\\)-\\(idf\\) values.\n\nplot_terms &lt;- c(\"equality\", \"right\", \"community\", \"child\", \"woman\", \"man\")\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~lemma)\n\n\n\n\n\n\n\nFigure 8: Distinctiveness of words over time\n\n\n\n\n\nWe can see that the words I selected based on the standard deviation, in Figure 8, can either increase, decrease, or fluctuate over time.\n\n\nMeaning\n\nHow does the meaning of words change across time periods?\n\nFor this approach we need to turn to word embeddings. Word embeddings have been show to capture distributional semantics –that is the meaning of words based on their distribution in a corpus (Hamilton, Leskovec, and Jurafsky 2016).\nSince our research question is aimed a change over time we are performing a diachronic analysis. This means that we will need to create word embeddings for each time period, identify the common vocabulary across time periods, and then align the word embeddings to a common space before we can compare them.\nLet’s load some packages that we will need.\n\nlibrary(fs) # for file system functions\nlibrary(PsychWordVec) # for working with word embeddings\nlibrary(purrr) # for iterating over lists\n\nLet’s first start by creating sub-corpora for each decade. We will write these to disk so that we can use them again if necessary. Note that data or datasets that are generated in the process of analysis are often stored in an analysis/ folder inside of the main data folder to keep them separate from the other data acquired or derived in the project.\nWe will use the str_c() function to summarize the words for each address into a single string by decade. We will then use the write_lines() function to write the string to a text file. The pwalk() function from {purrr} is a convenient way to iterate over multiple arguments (decade and address in this case) in a function without returning a value to the console. The p in pwalk() stands for “parallel” and indicates that the function will iterate over the arguments in parallel.\n# Create sub-corpora for each decade and write to disk ------------\nsotu_terms_df |&gt;\n  summarize(address = str_c(lemma, collapse = \" \"), .by = decade) |&gt;\n  select(decade, address) |&gt;\n  pwalk(\\(decade, address) {\n    file_name &lt;- str_c(\"../data/analysis/sotu/\", decade, \"s.txt\")\n    write_lines(address, file_name)\n  })\nNow I have written a function to read the text files, train the word embeddings for each, write the word embeddings to disk, and then load them back as VectorSpaceModel objects in a list.\n\n\n\n\n\n\n Tip\nA note on the training of the word embeddings. There are two main approaches to training word embeddings: Continuous Bag of Words (CBOW) and Skip-gram. CBOW is better for more common words and larger datasets. Skip-gram is better for less common words and smaller datasets. Given the varying sizes of the sub-corpora, Skip-gram might be more suitable as it may capture more nuances in less frequent words. Furthermore, the number of dimensions is a hyperparameter that needs to be tuned. The default is 50, but I have chosen 100 as we are attempting to capture more nuanced changes in the language of the SOTU.\n\n\n\n\ncreate_embeddings &lt;- function(dir_path, dims = 100) {\n  # Get the text file paths\n  txt_files &lt;- dir_ls(dir_path, regexp = \"\\\\.txt$\")\n  # Train the word embeddings\n  models &lt;-\n    txt_files |&gt;\n    map(\\(file) {\n      train_wordvec(\n        text = file,\n        dims = 100,\n        normalize = TRUE\n      )\n    })\n  # Modify the list names\n  names(models) &lt;-\n    names(models) |&gt;\n    basename() |&gt;\n    str_remove(\"\\\\.txt\")\n\n  # Convert to embed matrices\n  models &lt;- map(models, as_embed)\n\n  return(models)\n}\n\nWe can now apply the create_embeddings() custom function to the decade sub-corpora text files in ../analysis/sotu/.\nsotu_vec_mods &lt;- create_embeddings(\"../analysis/sotu/\")\nEach word embedding model is a matrix with the words as the rows and the dimensions as the columns as an element of the sotu_vec_mods list. Each of these elements have the name of the decade. We can extract an element from a list using the pluck() function from {purrr}.\n\n# Extract an element from a list\nsotu_vec_mods |&gt; pluck(\"1950s\")\n\n                        dim1 ...     dim100\n   1: the             0.2910 ... &lt;100 dims&gt;\n   2: of              0.2229 ... &lt;100 dims&gt;\n   3: and             0.2818 ... &lt;100 dims&gt;\n   4: be              0.1370 ... &lt;100 dims&gt;\n   5: to              0.2336 ... &lt;100 dims&gt;\n-----                                      \n1164: appear          0.2471 ... &lt;100 dims&gt;\n1165: allegiance      0.2689 ... &lt;100 dims&gt;\n1166: accumulate      0.2158 ... &lt;100 dims&gt;\n1167: accomplishment  0.2241 ... &lt;100 dims&gt;\n1168: accept          0.2440 ... &lt;100 dims&gt;\n\n\n\n\n\n\n\n\n Tip\nSince we have our models in a list, we will be using {purrr} functions quite a bit. Here’s a quick summary of some of the {purrr} functions we will be using.\n\nmap() iterates over a list and applies a function to each element of the list. It returns a list.\nwalk() iterates over a list and applies a function to each element of the list. It does not return a value.\n\nEach of these has a parallel version, pmap() and pwalk(), which iterate over multiple lists in parallel, and a version that iterates over named lists, imap() and iwalk(). The p in pmap() and pwalk() stands for “parallel” and indicates that the function will iterate over the arguments in parallel. The i in imap() and iwalk() stands for “indexed” and indicates that the function will iterate over the arguments in parallel and return the index of the list element.\n\n\n\nThese embeddings can be explored in a number of ways. For example, we can get the words closest to a given word in the vector space for each decade. {PsychWordVec} has a function, most_similar(), that will do this for us. We can use the map() function to iterate over each model in the list and get the words with the most similar vectors. We will then use the str_c() function to summarize the words into a single string for each decade.\n\n# Get the closest words\nsotu_vec_mods |&gt;\n  map(\\(mod) {\n    most_similar(mod, \"freedom\", verbose = FALSE) # get words similar to \"freedom\"\n  }) |&gt;\n  map(\\(res) {\n    str_c(res$word, collapse = \", \") # summarize the words into a single string\n  })\n\n$`1950s`\n[1] \"justice, peace, man, not, spirit, ideal, peaceful, fight, us, preserve\"\n\n$`1960s`\n[1] \"free, unity, berlin, asia, independence, course, communist, nation, europe, both\"\n\n$`1970s`\n[1] \"defend, close, destroy, found, shape, side, deter, honor, root, international\"\n\n$`1980s`\n[1] \"democracy, peace, struggle, defend, everywhere, democratic, secure, country, fighter, free\"\n\n$`1990s`\n[1] \"liberty, define, all, lead, perfect, share, force, communism, shape, latin\"\n\n$`2000s`\n[1] \"east, democracy, friend, liberty, woman, determine, military, middle, region, remain\"\n\n$`2010s`\n[1] \"heart, vision, remind, hero, hopeful, dignity, capitol, celebrate, journey, honor\"\n\n\nThe previous example shows that the closest words to “freedom” in each decade in a synchronic manner. We can inspect these synchronic changes and draw conclusions from them. However, we are interested in diachronic changes. To do this, we will need to align the word embeddings to a common space.\nNow we will identify the common words across the decades and subset the word embeddings to the common vocabulary. The map() function iterates over each model in the list and returns the rownames with rownames() (words) for each model. The reduce() function then iterates over the list of words and returns the intersection of the words across the models with intersect().\n\n# Extract the common vocabulary -----------------------------------\ncommon_vocab &lt;-\n  sotu_vec_mods |&gt;\n  map(rownames) |&gt;\n  reduce(intersect)\n\nlength(common_vocab)\n\n[1] 534\n\nhead(common_vocab)\n\n[1] \"the\" \"of\"  \"and\" \"be\"  \"to\"  \"in\" \n\n\nThere are 534 words in the common vocabulary. Now we can subset the word embeddings to the common vocabulary. We will use the map() function to iterate over each model in the list and subset the model to the common vocabulary using the common_vocab variable as part of the bracket notation subset.\n\n# Subset the models to the common vocabulary ----------------------\nsotu_vec_common_mods &lt;-\n  sotu_vec_mods |&gt;\n  map(\\(mod) {\n    mod[common_vocab, ] # subset each model to the common vocabulary\n  })\n\nsotu_vec_common_mods |&gt; pluck(\"1950s\")\n\n                   dim1 ...     dim100\n  1: the         0.2910 ... &lt;100 dims&gt;\n  2: of          0.2229 ... &lt;100 dims&gt;\n  3: and         0.2818 ... &lt;100 dims&gt;\n  4: be          0.1370 ... &lt;100 dims&gt;\n  5: to          0.2336 ... &lt;100 dims&gt;\n----                                  \n530: happen      0.2186 ... &lt;100 dims&gt;\n531: everything  0.2229 ... &lt;100 dims&gt;\n532: different   0.2373 ... &lt;100 dims&gt;\n533: debate      0.2244 ... &lt;100 dims&gt;\n534: city        0.2320 ... &lt;100 dims&gt;\n\nsotu_vec_common_mods |&gt; pluck(\"2010s\")\n\n                   dim1 ...     dim100\n  1: the        -0.1464 ... &lt;100 dims&gt;\n  2: of         -0.1187 ... &lt;100 dims&gt;\n  3: and        -0.1090 ... &lt;100 dims&gt;\n  4: be         -0.1162 ... &lt;100 dims&gt;\n  5: to         -0.0842 ... &lt;100 dims&gt;\n----                                  \n530: happen     -0.1037 ... &lt;100 dims&gt;\n531: everything -0.0982 ... &lt;100 dims&gt;\n532: different  -0.1032 ... &lt;100 dims&gt;\n533: debate     -0.1111 ... &lt;100 dims&gt;\n534: city       -0.1179 ... &lt;100 dims&gt;\n\n\nSo now each of the models in the list has the same vocabulary and the same number of dimensions, 100.\nNow we can align the models to a common space. The reason that the models need to be aligned is that the word embeddings are trained on different corpora. This means that the words will be represented in different spaces. We will use the Orthogonal Procrustes solution to align the models to a common coordinate space. {PsychWordVec} (Bao 2023) has a function, orth_procrustes(), that will do this for us. In the process of aligning the models, the models are converted to plain matrices so we will need to convert them back to embed matrix objects.\n\n# Align the models to a common space\nsotu_aligned_mods &lt;-\n  sotu_vec_common_mods |&gt;\n  map(\\(mod) {\n    orth_procrustes(sotu_vec_common_mods[[1]], mod) # align to the first model\n  }) |&gt;\n  map(\\(mod) {\n    emb &lt;- as_embed(mod) # convert to a embed matrix object\n    emb\n  })\n\nHaving a model for each decade which is aligned in vocabulary and space, we can now use these models to compare words across time. There are a number of ways we can compare words across time.\nIn our first approach, let’s consider the semantic displacement of words over time in the vector space. We will do this by calculating the cosine difference between the word embeddings for a word in each decade. Once we have the cosine difference, we can visualize the change over time.\n\n# Calculate the cosine difference between the models --------------\nword &lt;- \"freedom\"\n\nword_vectors &lt;-\n  sotu_aligned_mods |&gt;\n  map(\\(mod) {\n    mod[word, ]\n  })\n\ndifferences &lt;-\n  word_vectors |&gt;\n  map(\\(vec) {\n    cos_dist(vec, word_vectors[[1]])\n  })\n\ndifferences\n\n$`1950s`\n[1] 0\n\n$`1960s`\n[1] 0.0236\n\n$`1970s`\n[1] 0.0437\n\n$`1980s`\n[1] 0.0354\n\n$`1990s`\n[1] 0.0203\n\n$`2000s`\n[1] 0.0207\n\n$`2010s`\n[1] 0.0333\n\n\nWe now have a list with the cosine difference for each decade compared to the first decade (“1950s”). We can visualize this with a line plot where the x-axis is the decade and the y-axis is the cosine difference. We will add a linear trend line to the plot to get a sense of the overall trend.\nI’ll write a function to get the differences for a word and then return a data frame with the decade and the difference. This will make it easier to visualize the differences for multiple words.\n\n# Function to get the cosine difference over time\n\nget_cosine_diff &lt;- function(word, models) {\n  word_vectors &lt;- map(models, \\(mod) {\n    mod[word, ]\n  })\n\n  differences &lt;- map(word_vectors, \\(vec) {\n    cos_dist(vec, word_vectors[[1]])\n  })\n\n  tibble(word, decade = basename(names(differences)), difference = unlist(differences))\n}\n\nget_cosine_diff(word = \"freedom\", models = sotu_aligned_mods)\n\n# A tibble: 7 × 3\n  word    decade difference\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n1 freedom 1950s      0     \n2 freedom 1960s      0.0236\n3 freedom 1970s      0.0437\n4 freedom 1980s      0.0354\n5 freedom 1990s      0.0203\n6 freedom 2000s      0.0207\n7 freedom 2010s      0.0333\n\n\nLet’s create a function which performs this for us and can plot multiple words at the same time.\n\nplot_words &lt;-\n  c(\"freedom\", \"nation\", \"country\", \"america\")\n\nplot_words |&gt;\n  map(get_cosine_diff, models = sotu_aligned_mods) |&gt;\n  bind_rows() |&gt;\n  arrange(decade) |&gt;\n  ggplot(aes(decade, difference, group = word, color = word)) +\n  geom_smooth(se = FALSE, span = 1) +\n  labs(title = word)\n\n\n\n\n\n\n\nFigure 9\n\n\n\n\n\nWe can then focus in on a particular word and the nearest words to that word in each decade. We will use the map() function to iterate over each model in the list and get the closest words to a word. We will then use the str_c() function to summarize the words into a single string for each decade.\n\n# Function to get the closest words to a word ---------------------\nword &lt;- \"america\"\n\nsotu_vec_common_mods |&gt;\n  map(\\(mod) {\n    most_similar(mod, word, verbose = FALSE)\n  }) |&gt;\n  map(\\(mod) {\n    str_c(mod$word, collapse = \", \")\n  }) |&gt;\n  enframe(name = \"decade\", value = \"words\") |&gt;\n  unnest(words)\n\n# A tibble: 7 × 2\n  decade words                                                                  \n  &lt;chr&gt;  &lt;chr&gt;                                                                  \n1 1950s  people, upon, life, democracy, standard, american, fail, within, deep,…\n2 1960s  leadership, agency, whole, serious, basic, historic, respect, responsi…\n3 1970s  where, liberty, determine, future, faith, happen, always, achieve, tru…\n4 1980s  never, great, let, ready, nation, faith, future, we, hope, struggle    \n5 1990s  liberty, freedom, important, duty, moment, arm, ahead, great, always, …\n6 2000s  remember, common, moment, first, rest, sense, beyond, share, reach, ch…\n7 2010s  strong, state, once, may, believe, union, your, among, citizen, forward\n\n\nAnother approach is to visualize the vector space that words occupy over time. To do this we will collapse the word embeddings for each decade into a single matrix. We will append the decade to each word as not to lose the decade information. The we will extract the first two principal components of the matrix, so that we can visualize the data in two dimensions. We will then plot the data with a scatter plot where the x-axis is the first principal component and the y-axis is the second principal component. We will label the points with the words.\n\n# Visualize the vector space of words over time -------------------\nsotu_joined_mods &lt;-\n  sotu_aligned_mods |&gt;\n  imap(\\(mod, index) {\n    rownames(mod) &lt;- str_c(rownames(mod), \"_\", index)\n    mod\n  }) |&gt;\n  reduce(rbind) |&gt;\n  as_embed()\n\nPCA on the word embeddings, yes! With the aligned models the results are much more sensible and interesting. The individual words are grouped closer together across time, in general, but there are exceptions.\n\nsotu_joined_pca &lt;-\n  sotu_joined_mods |&gt;\n  scale() |&gt;\n  prcomp()\n\nsotu_pca_df &lt;-\n  as_tibble(sotu_joined_pca$x[, 1:2]) |&gt;\n  mutate(word = names(sotu_joined_pca$x[, 1]))\n\nsotu_pca_df |&gt;\n  filter(str_detect(word, \"^(nation|country|america)_\")) |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = word)) +\n  geom_point() +\n  ggrepel::geom_text_repel()\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\nThese kinds of visualizations can be very useful for exploring the data and drawing conclusions."
  },
  {
    "objectID": "recipes/recipe-08/index.html#summary",
    "href": "recipes/recipe-08/index.html#summary",
    "title": "08. Employing exploratory methods",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we have explored the State of the Union addresses from 1950 to 2019. We have used a number of tools and techniques to explore the data and draw conclusions. We have used {tidytext} to tokenize and lemmatize the words in the addresses. We have used {word2vec} to train word embeddings for each decade. We have used {PsychWordVec} to align the word embeddings to a common space. We have used {wordVectors} to explore the word embeddings. We have used {ggplot2} to visualize the data.\nThese strategies, and others, can be used to explore these questions or other questions in more depth. Exploratory analysis is where your creativity and curiosity can shine."
  },
  {
    "objectID": "recipes/recipe-08/index.html#check-your-understanding",
    "href": "recipes/recipe-08/index.html#check-your-understanding",
    "title": "08. Employing exploratory methods",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\nIn text analysis,  is used to transform the effect of words that are common across all time periods and promote the effect of words that are more distinctive to each time period.\nWhat is the correct method to use when adding a linear trend line to a plot in ggplot2? geom_smooth(method = ‘lm’)geom_line()geom_bar()geom_point()\nTRUEFALSE When working with lists, walk() is like map() but does not return a value.\nTRUEFALSE When creating word embeddings, the CBOW model is better suited for less common words and smaller datasets compared to Skip-gram.\nThe process of reducing the number of features in a dataset while retaining as much information as possible is known as  reduction."
  },
  {
    "objectID": "recipes/recipe-08/index.html#lab-preparation",
    "href": "recipes/recipe-08/index.html#lab-preparation",
    "title": "08. Employing exploratory methods",
    "section": "Lab preparation",
    "text": "Lab preparation\n\nIn preparation for Lab 8, review and and ensure that you are familiar with the following concepts:\n\nTokenizing text\nGenerating frequency and dispersion measures\nCreating Term-Document Matrices\nUsing {purrr} to iterate over lists\nVisualizations with {ggplot2}\n\nIn this lab, you will have the opportunity to apply the concepts from the materials in this chapter to a new dataset. You should consider the dataset and the questions that you want to ask of the data. You should also consider the tools and techniques that you will use to explore the data and draw conclusions. You will be asked to submit your code and a brief report of your findings."
  },
  {
    "objectID": "R/Library/openai/NEWS.html",
    "href": "R/Library/openai/NEWS.html",
    "title": "openai 0.4.1",
    "section": "",
    "text": "openai 0.4.1\n\nRelax validation of model argument in functions create_chat_completion(), create_fine_tune(), create_moderation(), create_embedding(), create_transcription(), and create_translation(). Otherwise, each time OpenAI will roll out a new model, the list of models has to be updated\n\n\n\nopenai 0.4.0\n\nAdd endpoints create_chat_completion(), create_transcription(), and create_translation()\nDowngrade R dependence to 3.5\nRemove redundant options of upload_file()’s argument purpose, namely \"search\", \"answers\", and \"classifications\"\nUpdate links in documentation\n\n\n\nopenai 0.3.0\n\nRemove outdated endpoints create_answer(), create_classification(), and create_search()\nDeprecate retrieve_engine() and list_engines()\nDeprecate engine_id argument in create_completion(), create_edit(), and create_embedding()\n\n\n\nopenai 0.2.0\n\nAdd new DALL-E functions, namely create_image(), create_image_edit(), and create_image_variation()\nAdd the new function create_moderation() that checks whether the input violates OpenAI’s Content Policy\nAdd new model-related functions, namely list_models() and retrieve_model()\n\n\n\nopenai 0.1.0\n\nInitial version"
  },
  {
    "objectID": "R/Library/tidytext/doc/tf_idf.html",
    "href": "R/Library/tidytext/doc/tf_idf.html",
    "title": "Term Frequency and Inverse Document Frequency (tf-idf) Using Tidy Data Principles",
    "section": "",
    "text": "A central question in text mining and natural language processing is how to quantify what a document is about. Can we do this by looking at the words that make up the document? One measure of how important a word may be is its term frequency (tf), how frequently a word occurs in a document. There are words in a document, however, that occur many times but may not be important; in English, these are probably words like “the”, “is”, “of”, and so forth. We might take the approach of adding words like these to a list of stop words and removing them before analysis, but it is possible that some of these words might be more important in some documents than others. A list of stop words is not a sophisticated approach to adjusting term frequency for commonly used words.\nAnother approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf, the frequency of a term adjusted for how rarely it is used. It is intended to measure how important a word is to a document in a collection (or corpus) of documents. It is a rule-of-thumb or heuristic quantity; while it has proved useful in text mining, search engines, etc., its theoretical foundations are considered less than firm by information theory experts. The inverse document frequency for any given term is defined as\n\\[idf(\\text{term}) = \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)}\\]\nWe can use tidy data principles, as described in the main vignette, to approach tf-idf analysis and use consistent, effective tools to quantify how important various terms are in a document that is part of a collection.\nLet’s look at the published novels of Jane Austen and examine first term frequency, then tf-idf. We can start just by using dplyr verbs such as group_by and join. What are the most commonly used words in Jane Austen’s novels? (Let’s also calculate the total words in each novel here, for later use.)\n\nlibrary(dplyr)\nlibrary(janeaustenr)\nlibrary(tidytext)\nbook_words &lt;- austen_books() %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  count(book, word, sort = TRUE)\n\ntotal_words &lt;- book_words %&gt;% group_by(book) %&gt;% summarize(total = sum(n))\nbook_words &lt;- left_join(book_words, total_words)\nbook_words\n\n# A tibble: 40,379 × 4\n   book              word      n  total\n   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt;  &lt;int&gt;\n 1 Mansfield Park    the    6206 160460\n 2 Mansfield Park    to     5475 160460\n 3 Mansfield Park    and    5438 160460\n 4 Emma              to     5239 160996\n 5 Emma              the    5201 160996\n 6 Emma              and    4896 160996\n 7 Mansfield Park    of     4778 160460\n 8 Pride & Prejudice the    4331 122204\n 9 Emma              of     4291 160996\n10 Pride & Prejudice to     4162 122204\n# ℹ 40,369 more rows\n\n\nThe usual suspects are here, “the”, “and”, “to”, and so forth. Let’s look at the distribution of n/total for each novel, the number of times a word appears in a novel divided by the total number of terms (words) in that novel. This is exactly what term frequency is.\n\nlibrary(ggplot2)\nggplot(book_words, aes(n/total, fill = book)) +\n  geom_histogram(show.legend = FALSE) +\n  scale_x_continuous(limits = c(NA, 0.0009)) +\n  facet_wrap(vars(book), ncol = 2, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThere are very long tails to the right for these novels (those extremely common words!) that we have not shown in these plots. These plots exhibit similar distributions for all the novels, with many words that occur rarely and fewer words that occur frequently. The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents, in this case, the group of Jane Austen’s novels as a whole. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. Let’s do that now.\n\nbook_words &lt;- book_words %&gt;%\n  bind_tf_idf(word, book, n)\nbook_words\n\n# A tibble: 40,379 × 7\n   book              word      n  total     tf   idf tf_idf\n   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Mansfield Park    the    6206 160460 0.0387     0      0\n 2 Mansfield Park    to     5475 160460 0.0341     0      0\n 3 Mansfield Park    and    5438 160460 0.0339     0      0\n 4 Emma              to     5239 160996 0.0325     0      0\n 5 Emma              the    5201 160996 0.0323     0      0\n 6 Emma              and    4896 160996 0.0304     0      0\n 7 Mansfield Park    of     4778 160460 0.0298     0      0\n 8 Pride & Prejudice the    4331 122204 0.0354     0      0\n 9 Emma              of     4291 160996 0.0267     0      0\n10 Pride & Prejudice to     4162 122204 0.0341     0      0\n# ℹ 40,369 more rows\n\n\nNotice that idf and thus tf-idf are zero for these extremely common words. These are all words that appear in all six of Jane Austen’s novels, so the idf term (which will then be the natural log of 1) is zero. The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection. Let’s look at terms with high tf-idf in Jane Austen’s works.\n\nbook_words %&gt;%\n  select(-total) %&gt;%\n  arrange(desc(tf_idf))\n\n# A tibble: 40,379 × 6\n   book                word          n      tf   idf  tf_idf\n   &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Sense & Sensibility elinor      623 0.00519  1.79 0.00931\n 2 Sense & Sensibility marianne    492 0.00410  1.79 0.00735\n 3 Mansfield Park      crawford    493 0.00307  1.79 0.00551\n 4 Pride & Prejudice   darcy       373 0.00305  1.79 0.00547\n 5 Persuasion          elliot      254 0.00304  1.79 0.00544\n 6 Emma                emma        786 0.00488  1.10 0.00536\n 7 Northanger Abbey    tilney      196 0.00252  1.79 0.00452\n 8 Emma                weston      389 0.00242  1.79 0.00433\n 9 Pride & Prejudice   bennet      294 0.00241  1.79 0.00431\n10 Persuasion          wentworth   191 0.00228  1.79 0.00409\n# ℹ 40,369 more rows\n\n\nHere we see all proper nouns, names that are in fact important in these novels. None of them occur in all of novels, and they are important, characteristic words for each text. Some of the values for idf are the same for different terms because there are 6 documents in this corpus and we are seeing the numerical value for \\(\\ln(6/1)\\), \\(\\ln(6/2)\\), etc. Let’s look specifically at Pride and Prejudice.\n\nbook_words %&gt;%\n  filter(book == \"Pride & Prejudice\") %&gt;%\n  select(-total) %&gt;%\n  arrange(desc(tf_idf))\n\n# A tibble: 6,538 × 6\n   book              word          n       tf   idf  tf_idf\n   &lt;fct&gt;             &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Pride & Prejudice darcy       373 0.00305  1.79  0.00547\n 2 Pride & Prejudice bennet      294 0.00241  1.79  0.00431\n 3 Pride & Prejudice bingley     257 0.00210  1.79  0.00377\n 4 Pride & Prejudice elizabeth   597 0.00489  0.693 0.00339\n 5 Pride & Prejudice wickham     162 0.00133  1.79  0.00238\n 6 Pride & Prejudice collins     156 0.00128  1.79  0.00229\n 7 Pride & Prejudice lydia       133 0.00109  1.79  0.00195\n 8 Pride & Prejudice lizzy        95 0.000777 1.79  0.00139\n 9 Pride & Prejudice longbourn    88 0.000720 1.79  0.00129\n10 Pride & Prejudice gardiner     84 0.000687 1.79  0.00123\n# ℹ 6,528 more rows\n\n\nThese words are, as measured by tf-idf, the most important to Pride and Prejudice and most readers would likely agree."
  },
  {
    "objectID": "R/Library/tidytext/doc/tidytext.html",
    "href": "R/Library/tidytext/doc/tidytext.html",
    "title": "Introduction to tidytext",
    "section": "",
    "text": "The Life-Changing Magic of Tidying Text\nUsing tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use. Much of the infrastructure needed for text mining with tidy data frames already exists in packages like dplyr, broom, tidyr and ggplot2. In this package, we provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages. Check out our book to learn more about text mining using tidy data principles.\n\n\nA few first tidy text mining examples\nThe novels of Jane Austen can be so tidy! Let’s use the text of Jane Austen’s 6 completed, published novels from the janeaustenr package, and transform them into a tidy format. janeaustenr provides them as a one-row-per-line format:\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(line = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE)))) %&gt;%\n  ungroup()\n\noriginal_books\n\nTo work with this as a tidy dataset, we need to restructure it as one-token-per-row format. The unnest_tokens function is a way to convert a dataframe with a text column to be one-token-per-row. Here let’s tokenize to a new word column from the existing text column:\n\nlibrary(tidytext)\ntidy_books &lt;- original_books %&gt;%\n  unnest_tokens(output = word, input = text)\n\ntidy_books\n\nThis function uses the tokenizers package to separate each line into words. The default tokenizing is for words, but other options include characters, ngrams, sentences, lines, paragraphs, or separation around a regex pattern.\nNow that the data is in one-word-per-row format, we can manipulate it with tidy tools like dplyr. We can remove stop words (accessible in a tidy form with the function get_stopwords()) with an anti_join.\n\ncleaned_books &lt;- tidy_books %&gt;%\n  anti_join(get_stopwords())\n\nWe can also use count to find the most common words in all the books as a whole.\n\ncleaned_books %&gt;%\n  count(word, sort = TRUE) \n\nSentiment analysis can be done as an inner join. Sentiment lexicons are available via the get_sentiments() function. Let’s look at the words with a positive score from the lexicon of Bing Liu and collaborators. What are the most common positive words in Emma?\n\npositive &lt;- get_sentiments(\"bing\") %&gt;%\n  filter(sentiment == \"positive\")\n\ntidy_books %&gt;%\n  filter(book == \"Emma\") %&gt;%\n  semi_join(positive) %&gt;%\n  count(word, sort = TRUE)\n\nOr instead we could examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the same lexicon, then count the number of positive and negative words in defined sections of each novel.\n\nlibrary(tidyr)\nbing &lt;- get_sentiments(\"bing\")\n\njaneaustensentiment &lt;- tidy_books %&gt;%\n  inner_join(bing, relationship = \"many-to-many\") %&gt;%\n  count(book, index = line %/% 80, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n\nNow we can plot these sentiment scores across the plot trajectory of each novel.\n\nlibrary(ggplot2)\n\nggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  facet_wrap(vars(book), ncol = 2, scales = \"free_x\")\n\n\n\nMost common positive and negative words\nOne advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment.\n\nbing_word_counts &lt;- tidy_books %&gt;%\n  inner_join(bing, relationship = \"many-to-many\") %&gt;%\n  count(word, sentiment, sort = TRUE)\n\nbing_word_counts\n\nThis can be shown visually, and we can pipe straight into ggplot2 because of the way we are consistently using tools built for handling tidy data frames.\n\nbing_word_counts %&gt;%\n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(vars(sentiment), scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\", y = NULL)\n\nThis lets us spot an anomaly in the sentiment analysis; the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom stop-words list using bind_rows.\n\n\nWordclouds\nWe’ve seen that this tidy text mining approach works well with ggplot2, but having our data in a tidy format is useful for other plots as well.\nFor example, consider the wordcloud package. Let’s look at the most common words in Jane Austen’s works as a whole again.\n\nlibrary(wordcloud)\n\ncleaned_books %&gt;%\n  count(word) %&gt;%\n  with(wordcloud(word, n, max.words = 100))\n\nIn other functions, such as comparison.cloud, you may need to turn it into a matrix with reshape2’s acast. Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud, this can all be done with joins, piping, and dplyr because our data is in tidy format.\n\nlibrary(reshape2)\n\ntidy_books %&gt;%\n  inner_join(bing) %&gt;%\n  count(word, sentiment, sort = TRUE) %&gt;%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\nLooking at units beyond just words\nLots of useful work can be done by tokenizing at the word level, but sometimes it is useful or necessary to look at different units of text. For example, some sentiment analysis algorithms look beyond only unigrams (i.e. single words) to try to understand the sentiment of a sentence as a whole. These algorithms try to understand that\n\nI am not having a good day.\n\nis a sad sentence, not a happy one, because of negation. The Stanford CoreNLP tools and the sentimentr R package are examples of such sentiment analysis algorithms. For these, we may want to tokenize text into sentences.\n\nPandP_sentences &lt;- tibble(text = prideprejudice) %&gt;% \n  unnest_tokens(output = sentence, input = text, token = \"sentences\")\n\nLet’s look at just one.\n\nPandP_sentences$sentence[2]\n\nThe sentence tokenizing does seem to have a bit of trouble with UTF-8 encoded text, especially with sections of dialogue; it does much better with punctuation in ASCII.\nAnother option in unnest_tokens is to split into tokens using a regex pattern. We could use this, for example, to split the text of Jane Austen’s novels into a data frame by chapter.\n\nausten_chapters &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  unnest_tokens(chapter, text, token = \"regex\", pattern = \"Chapter|CHAPTER [\\\\dIVXLC]\") %&gt;%\n  ungroup()\n\nausten_chapters %&gt;% \n  group_by(book) %&gt;% \n  summarise(chapters = n())\n\nWe have recovered the correct number of chapters in each novel (plus an “extra” row for each novel title). In this data frame, each row corresponds to one chapter.\nNear the beginning of this vignette, we used a similar regex to find where all the chapters were in Austen’s novels for a tidy data frame organized by one-word-per-row. We can use tidy text analysis to ask questions such as what are the most negative chapters in each of Jane Austen’s novels? First, let’s get the list of negative words from the Bing lexicon. Second, let’s make a dataframe of how many words are in each chapter so we can normalize for the length of chapters. Then, let’s find the number of negative words in each chapter and divide by the total words in each chapter. Which chapter has the highest proportion of negative words?\n\nbingnegative &lt;- get_sentiments(\"bing\") %&gt;%\n  filter(sentiment == \"negative\")\n\nwordcounts &lt;- tidy_books %&gt;%\n  group_by(book, chapter) %&gt;%\n  summarize(words = n())\n\ntidy_books %&gt;%\n  semi_join(bingnegative) %&gt;%\n  group_by(book, chapter) %&gt;%\n  summarize(negativewords = n()) %&gt;%\n  left_join(wordcounts, by = c(\"book\", \"chapter\")) %&gt;%\n  mutate(ratio = negativewords/words) %&gt;%\n  filter(chapter != 0) %&gt;%\n  slice_max(ratio, n = 1)\n\nThese are the chapters with the most negative words in each book, normalized for number of words in the chapter. What is happening in these chapters? In Chapter 43 of Sense and Sensibility Marianne is seriously ill, near death, and in Chapter 34 of Pride and Prejudice Mr. Darcy proposes for the first time (so badly!). Chapter 46 of Mansfield Park is almost the end, when everyone learns of Henry’s scandalous adultery, Chapter 15 of Emma is when horrifying Mr. Elton proposes, and in Chapter 21 of Northanger Abbey Catherine is deep in her Gothic faux fantasy of murder, etc. Chapter 4 of Persuasion is when the reader gets the full flashback of Anne refusing Captain Wentworth and how sad she was and what a terrible mistake she realized it to be."
  },
  {
    "objectID": "R/Library/gutenbergr/doc/intro.html",
    "href": "R/Library/gutenbergr/doc/intro.html",
    "title": "gutenbergr: Search and download public domain texts from Project Gutenberg",
    "section": "",
    "text": "The gutenbergr package helps you download and process public domain works from the Project Gutenberg collection. This includes both tools for downloading books (and stripping header/footer information), and a complete dataset of Project Gutenberg metadata that can be used to find words of interest. Includes:\n\nA function gutenberg_download() that downloads one or more works from Project Gutenberg by ID: e.g., gutenberg_download(84) downloads the text of Frankenstein.\nMetadata for all Project Gutenberg works as R datasets, so that they can be searched and filtered:\n\ngutenberg_metadata contains information about each work, pairing Gutenberg ID with title, author, language, etc\ngutenberg_authors contains information about each author, such as aliases and birth/death year\ngutenberg_subjects contains pairings of works with Library of Congress subjects and topics\n\n\n\nProject Gutenberg Metadata\nThis package contains metadata for all Project Gutenberg works as R datasets, so that you can search and filter for particular works before downloading.\nThe dataset gutenberg_metadata contains information about each work, pairing Gutenberg ID with title, author, language, etc:\n\nlibrary(gutenbergr)\nlibrary(dplyr)\ngutenberg_metadata\n#&gt; # A tibble: 72,569 × 8\n#&gt;    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n#&gt;           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n#&gt;  1            1 \"The De… Jeffe…                1638 en       \"Politics/American…\n#&gt;  2            2 \"The Un… Unite…                   1 en       \"Politics/American…\n#&gt;  3            3 \"John F… Kenne…                1666 en       \"\"                 \n#&gt;  4            4 \"Lincol… Linco…                   3 en       \"US Civil War\"     \n#&gt;  5            5 \"The Un… Unite…                   1 en       \"United States/Pol…\n#&gt;  6            6 \"Give M… Henry…                   4 en       \"American Revoluti…\n#&gt;  7            7 \"The Ma… &lt;NA&gt;                    NA en       \"\"                 \n#&gt;  8            8 \"Abraha… Linco…                   3 en       \"US Civil War\"     \n#&gt;  9            9 \"Abraha… Linco…                   3 en       \"US Civil War\"     \n#&gt; 10           10 \"The Ki… &lt;NA&gt;                    NA en       \"Banned Books List…\n#&gt; # ℹ 72,559 more rows\n#&gt; # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\nFor example, you could find the Gutenberg ID(s) of Jane Austen’s Persuasion by doing:\n\n\ngutenberg_metadata %&gt;%\n  filter(title == \"Persuasion\")\n#&gt; # A tibble: 3 × 8\n#&gt;   gutenberg_id title     author gutenberg_author_id language gutenberg_bookshelf\n#&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n#&gt; 1          105 Persuasi… Auste…                  68 en       \"\"                 \n#&gt; 2        22963 Persuasi… Auste…                  68 en       \"\"                 \n#&gt; 3        36777 Persuasi… Auste…                  68 fr       \"FR Littérature\"   \n#&gt; # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\nIn many analyses, you may want to filter just for English works, avoid duplicates, and include only books that have text that can be downloaded. The gutenberg_works() function does this pre-filtering:\n\ngutenberg_works()\n#&gt; # A tibble: 44,042 × 8\n#&gt;    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n#&gt;           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n#&gt;  1            1 \"The De… Jeffe…                1638 en       \"Politics/American…\n#&gt;  2            2 \"The Un… Unite…                   1 en       \"Politics/American…\n#&gt;  3            3 \"John F… Kenne…                1666 en       \"\"                 \n#&gt;  4            4 \"Lincol… Linco…                   3 en       \"US Civil War\"     \n#&gt;  5            5 \"The Un… Unite…                   1 en       \"United States/Pol…\n#&gt;  6            6 \"Give M… Henry…                   4 en       \"American Revoluti…\n#&gt;  7            7 \"The Ma… &lt;NA&gt;                    NA en       \"\"                 \n#&gt;  8            8 \"Abraha… Linco…                   3 en       \"US Civil War\"     \n#&gt;  9            9 \"Abraha… Linco…                   3 en       \"US Civil War\"     \n#&gt; 10           10 \"The Ki… &lt;NA&gt;                    NA en       \"Banned Books List…\n#&gt; # ℹ 44,032 more rows\n#&gt; # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\nIt also allows you to perform filtering as an argument:\n\ngutenberg_works(author == \"Austen, Jane\")\n#&gt; # A tibble: 11 × 8\n#&gt;    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n#&gt;           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n#&gt;  1          105 \"Persua… Auste…                  68 en       \"\"                 \n#&gt;  2          121 \"Northa… Auste…                  68 en       \"Gothic Fiction\"   \n#&gt;  3          141 \"Mansfi… Auste…                  68 en       \"\"                 \n#&gt;  4          158 \"Emma\"   Auste…                  68 en       \"\"                 \n#&gt;  5          946 \"Lady S… Auste…                  68 en       \"\"                 \n#&gt;  6         1212 \"Love a… Auste…                  68 en       \"\"                 \n#&gt;  7         1342 \"Pride … Auste…                  68 en       \"Best Books Ever L…\n#&gt;  8        21839 \"Sense … Auste…                  68 en       \"\"                 \n#&gt;  9        31100 \"The Co… Auste…                  68 en       \"\"                 \n#&gt; 10        37431 \"Pride … Auste…                  68 en       \"\"                 \n#&gt; 11        42078 \"The Le… Auste…                  68 en       \"\"                 \n#&gt; # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\n# or with a regular expression\n\nlibrary(stringr)\ngutenberg_works(str_detect(author, \"Austen\"))\n#&gt; # A tibble: 16 × 8\n#&gt;    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n#&gt;           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n#&gt;  1          105 \"Persua… Auste…                  68 en       \"\"                 \n#&gt;  2          121 \"Northa… Auste…                  68 en       \"Gothic Fiction\"   \n#&gt;  3          141 \"Mansfi… Auste…                  68 en       \"\"                 \n#&gt;  4          158 \"Emma\"   Auste…                  68 en       \"\"                 \n#&gt;  5          946 \"Lady S… Auste…                  68 en       \"\"                 \n#&gt;  6         1212 \"Love a… Auste…                  68 en       \"\"                 \n#&gt;  7         1342 \"Pride … Auste…                  68 en       \"Best Books Ever L…\n#&gt;  8        17797 \"Memoir… Auste…                7603 en       \"\"                 \n#&gt;  9        21839 \"Sense … Auste…                  68 en       \"\"                 \n#&gt; 10        22536 \"Jane A… Auste…               25392 en       \"\"                 \n#&gt; 11        22536 \"Jane A… Auste…               25393 en       \"\"                 \n#&gt; 12        31100 \"The Co… Auste…                  68 en       \"\"                 \n#&gt; 13        33513 \"The Fr… Auste…               36446 en       \"\"                 \n#&gt; 14        37431 \"Pride … Auste…                  68 en       \"\"                 \n#&gt; 15        39897 \"Discov… Layar…               40288 en       \"\"                 \n#&gt; 16        42078 \"The Le… Auste…                  68 en       \"\"                 \n#&gt; # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\nThe meta-data currently in the package was last updated on 19 December 2022.\n\n\nDownloading books by ID\nThe function gutenberg_download() downloads one or more works from Project Gutenberg based on their ID. For example, we earlier saw that one version of Persuasion has ID 105 (see the URL here), so gutenberg_download(105) downloads this text.\n\npersuasion &lt;- gutenberg_download(105)\n\n\npersuasion\n#&gt; # A tibble: 8,328 × 2\n#&gt;    gutenberg_id text         \n#&gt;           &lt;int&gt; &lt;chr&gt;        \n#&gt;  1          105 \"Persuasion\" \n#&gt;  2          105 \"\"           \n#&gt;  3          105 \"\"           \n#&gt;  4          105 \"by\"         \n#&gt;  5          105 \"\"           \n#&gt;  6          105 \"Jane Austen\"\n#&gt;  7          105 \"\"           \n#&gt;  8          105 \"(1818)\"     \n#&gt;  9          105 \"\"           \n#&gt; 10          105 \"\"           \n#&gt; # ℹ 8,318 more rows\n\nNotice it is returned as a tbl_df (a type of data frame) including two variables: gutenberg_id (useful if multiple books are returned), and a character vector of the text, one row per line.\nYou can also provide gutenberg_download() a vector of IDs to download multiple books. For example, to download Renascence, and Other Poems (book 109) along with Persuasion, do:\n\nbooks &lt;- gutenberg_download(c(109, 105), meta_fields = \"title\")\n\n\nbooks\n#&gt; # A tibble: 9,550 × 3\n#&gt;    gutenberg_id text          title     \n#&gt;           &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;     \n#&gt;  1          105 \"Persuasion\"  Persuasion\n#&gt;  2          105 \"\"            Persuasion\n#&gt;  3          105 \"\"            Persuasion\n#&gt;  4          105 \"by\"          Persuasion\n#&gt;  5          105 \"\"            Persuasion\n#&gt;  6          105 \"Jane Austen\" Persuasion\n#&gt;  7          105 \"\"            Persuasion\n#&gt;  8          105 \"(1818)\"      Persuasion\n#&gt;  9          105 \"\"            Persuasion\n#&gt; 10          105 \"\"            Persuasion\n#&gt; # ℹ 9,540 more rows\n\nNotice that the meta_fields argument allows us to add one or more additional fields from the gutenberg_metadata to the downloaded text, such as title or author.\n\nbooks %&gt;%\n  count(title)\n#&gt; # A tibble: 2 × 2\n#&gt;   title                           n\n#&gt;   &lt;chr&gt;                       &lt;int&gt;\n#&gt; 1 Persuasion                   8328\n#&gt; 2 Renascence, and Other Poems  1222\n\n\n\nOther meta-datasets\nYou may want to select books based on information other than their title or author, such as their genre or topic. gutenberg_subjects contains pairings of works with Library of Congress subjects and topics. “lcc” means Library of Congress Classification, while “lcsh” means Library of Congress subject headings:\n\ngutenberg_subjects\n#&gt; # A tibble: 231,741 × 3\n#&gt;    gutenberg_id subject_type subject                                            \n#&gt;           &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                                              \n#&gt;  1            1 lcsh         United States -- History -- Revolution, 1775-1783 …\n#&gt;  2            1 lcsh         United States. Declaration of Independence         \n#&gt;  3            1 lcc          E201                                               \n#&gt;  4            1 lcc          JK                                                 \n#&gt;  5            2 lcsh         Civil rights -- United States -- Sources           \n#&gt;  6            2 lcsh         United States. Constitution. 1st-10th Amendments   \n#&gt;  7            2 lcc          JK                                                 \n#&gt;  8            2 lcc          KF                                                 \n#&gt;  9            3 lcsh         United States -- Foreign relations -- 1961-1963    \n#&gt; 10            3 lcsh         Presidents -- United States -- Inaugural addresses \n#&gt; # ℹ 231,731 more rows\n\nThis is useful for extracting texts from a particular topic or genre, such as detective stories, or a particular character, such as Sherlock Holmes. The gutenberg_id column can then be used to download these texts or to link with other metadata.\n\ngutenberg_subjects %&gt;%\n  filter(subject == \"Detective and mystery stories\")\n#&gt; # A tibble: 811 × 3\n#&gt;    gutenberg_id subject_type subject                      \n#&gt;           &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                        \n#&gt;  1          170 lcsh         Detective and mystery stories\n#&gt;  2          173 lcsh         Detective and mystery stories\n#&gt;  3          244 lcsh         Detective and mystery stories\n#&gt;  4          305 lcsh         Detective and mystery stories\n#&gt;  5          330 lcsh         Detective and mystery stories\n#&gt;  6          481 lcsh         Detective and mystery stories\n#&gt;  7          547 lcsh         Detective and mystery stories\n#&gt;  8          863 lcsh         Detective and mystery stories\n#&gt;  9          905 lcsh         Detective and mystery stories\n#&gt; 10         1155 lcsh         Detective and mystery stories\n#&gt; # ℹ 801 more rows\n\ngutenberg_subjects %&gt;%\n  filter(grepl(\"Holmes, Sherlock\", subject))\n#&gt; # A tibble: 54 × 3\n#&gt;    gutenberg_id subject_type subject                                           \n#&gt;           &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                                             \n#&gt;  1          108 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  2          221 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  3          244 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  4          834 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  5         1661 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  6         2097 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  7         2343 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  8         2344 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt;  9         2345 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt; 10         2346 lcsh         Holmes, Sherlock (Fictitious character) -- Fiction\n#&gt; # ℹ 44 more rows\n\ngutenberg_authors contains information about each author, such as aliases and birth/death year:\n\ngutenberg_authors\n#&gt; # A tibble: 23,980 × 7\n#&gt;    gutenberg_author_id author        alias birthdate deathdate wikipedia aliases\n#&gt;                  &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;     &lt;int&gt;     &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  \n#&gt;  1                   1 United States U.S.…        NA        NA https://… U.S.A. \n#&gt;  2                   3 Lincoln, Abr… &lt;NA&gt;       1809      1865 https://… United…\n#&gt;  3                   4 Henry, Patri… &lt;NA&gt;       1736      1799 https://… &lt;NA&gt;   \n#&gt;  4                   5 Adam, Paul    &lt;NA&gt;       1849      1931 https://… &lt;NA&gt;   \n#&gt;  5                   7 Carroll, Lew… Dodg…      1832      1898 https://… Dodgso…\n#&gt;  6                   8 United State… &lt;NA&gt;         NA        NA https://… Agency…\n#&gt;  7                   9 Melville, He… Melv…      1819      1891 https://… Melvil…\n#&gt;  8                  10 Barrie, J. M… &lt;NA&gt;       1860      1937 https://… Barrie…\n#&gt;  9                  11 Church of Je… &lt;NA&gt;         NA        NA https://… &lt;NA&gt;   \n#&gt; 10                  12 Smith, Josep… Smit…      1805      1844 https://… Smith,…\n#&gt; # ℹ 23,970 more rows\n\n\n\nAnalysis\nWhat’s next after retrieving a book’s text? Well, having the book as a data frame is especially useful for working with the tidytext package for text analysis.\n\nlibrary(tidytext)\n\nwords &lt;- books %&gt;%\n  unnest_tokens(word, text)\n\nwords\n#&gt; # A tibble: 90,532 × 3\n#&gt;    gutenberg_id title      word      \n#&gt;           &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;     \n#&gt;  1          105 Persuasion persuasion\n#&gt;  2          105 Persuasion by        \n#&gt;  3          105 Persuasion jane      \n#&gt;  4          105 Persuasion austen    \n#&gt;  5          105 Persuasion 1818      \n#&gt;  6          105 Persuasion chapter   \n#&gt;  7          105 Persuasion 1         \n#&gt;  8          105 Persuasion sir       \n#&gt;  9          105 Persuasion walter    \n#&gt; 10          105 Persuasion elliot    \n#&gt; # ℹ 90,522 more rows\n\nword_counts &lt;- words %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  count(title, word, sort = TRUE)\n\nword_counts\n#&gt; # A tibble: 6,620 × 3\n#&gt;    title      word          n\n#&gt;    &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;\n#&gt;  1 Persuasion anne        447\n#&gt;  2 Persuasion captain     303\n#&gt;  3 Persuasion elliot      254\n#&gt;  4 Persuasion lady        214\n#&gt;  5 Persuasion wentworth   191\n#&gt;  6 Persuasion charles     155\n#&gt;  7 Persuasion time        152\n#&gt;  8 Persuasion sir         149\n#&gt;  9 Persuasion miss        125\n#&gt; 10 Persuasion walter      123\n#&gt; # ℹ 6,610 more rows\n\nYou may also find these resources useful:\n\nThe Natural Language Processing CRAN View suggests many R packages related to text mining, especially around the tm package\nYou could match the wikipedia column in gutenberg_author to Wikipedia content with the WikipediR package or to pageview statistics with the wikipediatrend package\nIf you’re considering an analysis based on author name, you may find the humaniformat (for extraction of first names) and gender (prediction of gender from first names) packages useful. (Note that humaniformat has a format_reverse function for reversing “Last, First” names)."
  },
  {
    "objectID": "R/Library/triebeard/doc/r_radix.html",
    "href": "R/Library/triebeard/doc/r_radix.html",
    "title": "Radix trees in R",
    "section": "",
    "text": "A radix tree, or trie, is a data structure optimised for storing key-value pairs in a way optimised for searching. This makes them very, very good for efficiently matching data against keys, and retrieving the values associated with those keys.\ntriebeard provides an implementation of tries for R (and one that can be used in Rcpp development, too, if that’s your thing) so that useRs can take advantage of the fast, efficient and user-friendly matching that they allow."
  },
  {
    "objectID": "R/Library/triebeard/doc/r_radix.html#radix-usage",
    "href": "R/Library/triebeard/doc/r_radix.html#radix-usage",
    "title": "Radix trees in R",
    "section": "Radix usage",
    "text": "Radix usage\nSuppose we have observations in a dataset that are labelled, with a 2-3 letter code that identifies the facility the sample came from:\n\nlabels &lt;- c(\"AO-1002\", \"AEO-1004\", \"AAI-1009\", \"AFT-1403\", \"QZ-9065\", \"QZ-1021\", \"RF-0901\",\n            \"AO-1099\", \"AFT-1101\", \"QZ-4933\")\n\nWe know the facility each code maps to, and we want to be able to map the labels to that - not over 10 entries but over hundreds, or thousands, or hundreds of thousands. Tries are a great way of doing that: we treat the codes as keys and the full facility names as values. So let’s make a trie to do this matching, and then, well, match:\n\nlibrary(triebeard)\ntrie &lt;- trie(keys = c(\"AO\", \"AEO\", \"AAI\", \"AFT\", \"QZ\", \"RF\"),\n             values = c(\"Audobon\", \"Atlanta\", \"Ann Arbor\", \"Austin\", \"Queensland\", \"Raleigh\"))\n\nlongest_match(trie = trie, to_match = labels)\n\n [1] \"Audobon\"    \"Atlanta\"    \"Ann Arbor\"  \"Austin\"     \"Queensland\" \"Queensland\" \"Raleigh\"    \"Audobon\"    \"Austin\"    \n[10] \"Queensland\"\n\nThis pulls out, for each label, the trie value where the associated key has the longest prefix-match to the label. We can also just grab all the values where the key starts with, say, A:\n\nprefix_match(trie = trie, to_match = \"A\")\n\n[[1]]\n[1] \"Ann Arbor\" \"Atlanta\"   \"Austin\"    \"Audobon\"  \n\nAnd finally if we want we can match very, very fuzzily using “greedy” matching:\n\ngreedy_match(trie = trie, to_match = \"AO\")\n\n[[1]]\n[1] \"Ann Arbor\" \"Atlanta\"   \"Austin\"    \"Audobon\"  \n\nThese operations are very, very efficient. If we use longest-match as an example, since that’s the most useful thing, with a one-million element vector of things to match against:\n\nlibrary(triebeard)\nlibrary(microbenchmark)\n\ntrie &lt;- trie(keys = c(\"AO\", \"AEO\", \"AAI\", \"AFT\", \"QZ\", \"RF\"),\n             values = c(\"Audobon\", \"Atlanta\", \"Ann Arbor\", \"Austin\", \"Queensland\", \"Raleigh\"))\n\nlabels &lt;- rep(c(\"AO-1002\", \"AEO-1004\", \"AAI-1009\", \"AFT-1403\", \"QZ-9065\", \"QZ-1021\", \"RF-0901\",\n                \"AO-1099\", \"AFT-1101\", \"QZ-4933\"), 100000)\n\nmicrobenchmark({longest_match(trie = trie, to_match = labels)})\n\nUnit: milliseconds\n                                                  expr      min       lq     mean   median       uq      max neval\n {     longest_match(trie = trie, to_match = labels) } 284.6457 285.5902 289.5342 286.8775 288.4564 327.3878   100\n\nI think we can call &lt;300 milliseconds for a million matches against an entire set of possible values pretty fast."
  },
  {
    "objectID": "R/Library/triebeard/doc/r_radix.html#radix-modification",
    "href": "R/Library/triebeard/doc/r_radix.html#radix-modification",
    "title": "Radix trees in R",
    "section": "Radix modification",
    "text": "Radix modification\nThere’s always the possibility that (horror of horrors) you’ll have to add or remove entries from the trie. Fear not; you can do just that with trie_add and trie_remove respectively, both of which silently modify the trie they’re provided with to add or remove whatever key-value pairs you provide:\n\nto_match = \"198.0.0.1\"\ntrie_inst &lt;- trie(keys = \"197\", values = \"fake range\")\n\nlongest_match(trie_inst, to_match)\n[1] NA\n\ntrie_add(trie_inst, keys = \"198\", values = \"home range\")\nlongest_match(trie_inst, to_match)\n[1] \"home range\"\n\ntrie_remove(trie_inst, keys = \"198\")\nlongest_match(trie_inst, to_match)\n[1] NA"
  },
  {
    "objectID": "R/Library/triebeard/doc/r_radix.html#metadata-and-coercion",
    "href": "R/Library/triebeard/doc/r_radix.html#metadata-and-coercion",
    "title": "Radix trees in R",
    "section": "Metadata and coercion",
    "text": "Metadata and coercion\nYou can also extract information from tries without using them. dim, str, print and length all work for tries, and you can use get_keys(trie) and get_values(trie) to extract, respectively, the keys and values from a trie object.\nIn addition, you can also coerce tries into other R data structures, specifically lists and data.frames:\n\ntrie &lt;- trie(keys = c(\"AO\", \"AEO\", \"AAI\", \"AFT\", \"QZ\", \"RF\"),\n             values = c(\"Audobon\", \"Atlanta\", \"Ann Arbor\", \"Austin\", \"Queensland\", \"Raleigh\"))\n\nstr(as.data.frame(trie))\n'data.frame':   6 obs. of  2 variables:\n $ keys  : chr  \"AAI\" \"AEO\" \"AFT\" \"AO\" ...\n $ values: chr  \"Ann Arbor\" \"Atlanta\" \"Austin\" \"Audobon\" ...\n\nstr(as.list(trie))\n\nList of 2\n $ keys  : chr [1:6] \"AAI\" \"AEO\" \"AFT\" \"AO\" ...\n $ values: chr [1:6] \"Ann Arbor\" \"Atlanta\" \"Austin\" \"Audobon\" ...\n\n\nOther trie operations\nIf you have ideas for other trie-like structures, or functions that would be useful with these tries, the best approach is to either request it or add it!"
  },
  {
    "objectID": "R/Library/qtkit/NEWS.html",
    "href": "R/Library/qtkit/NEWS.html",
    "title": "qtkit (development version)",
    "section": "",
    "text": "qtkit (development version)\n\n\nqtkit 0.10.0\n\nInitial CRAN submission."
  },
  {
    "objectID": "R/Library/here/doc/rmarkdown.html",
    "href": "R/Library/here/doc/rmarkdown.html",
    "title": "Using here with rmarkdown",
    "section": "",
    "text": "The here package enables easy file referencing by using the top-level directory of a file project to easily build file paths. This article demonstrates the case where the working directory is set to a subdirectory of the project root, for instance when rendering an R Markdown document that lives in a subdirectory. See vignette(\"here\") for a more general introduction."
  },
  {
    "objectID": "R/Library/here/doc/rmarkdown.html#rmarkdown-starts-in-a-subdirectory",
    "href": "R/Library/here/doc/rmarkdown.html#rmarkdown-starts-in-a-subdirectory",
    "title": "Using here with rmarkdown",
    "section": "rmarkdown starts in a subdirectory",
    "text": "rmarkdown starts in a subdirectory\nFor demonstration, this article uses a data analysis project that lives in /Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project on my machine. This is the project root. The path will most likely be different on your machine, the here package helps deal with this situation.\nThe project has the following structure:\n\n#&gt; /Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\n#&gt; ├── analysis\n#&gt; │   ├── report.Rmd\n#&gt; │   ├── report.html\n#&gt; │   └── report_cache\n#&gt; │       └── html\n#&gt; │           ├── __packages\n#&gt; │           ├── setup_0c1d92767c6b3fa18b692c9308b5a9ee.RData\n#&gt; │           ├── setup_0c1d92767c6b3fa18b692c9308b5a9ee.rdb\n#&gt; │           ├── setup_0c1d92767c6b3fa18b692c9308b5a9ee.rdx\n#&gt; │           ├── unnamed-chunk-1_54733d1c24c19e45f2d9528df7ddd3cd.RData\n#&gt; │           ├── unnamed-chunk-1_54733d1c24c19e45f2d9528df7ddd3cd.rdb\n#&gt; │           └── unnamed-chunk-1_54733d1c24c19e45f2d9528df7ddd3cd.rdx\n#&gt; ├── data\n#&gt; │   └── penguins.csv\n#&gt; ├── demo-project.Rproj\n#&gt; └── prepare\n#&gt;     └── penguins.R\n\nWhen report.Rmd is rendered, the working directory is internally set to &lt;project root&gt;/analysis by rmarkdown:\n\nsetwd(file.path(project_path, \"analysis\"))\n\n\ngetwd()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/analysis\"\n\nHowever, penguins.csv still lives in the data/ subdirectory. The report requires the penguins.csv file to work."
  },
  {
    "objectID": "R/Library/here/doc/rmarkdown.html#here-always-uses-project-relative-paths",
    "href": "R/Library/here/doc/rmarkdown.html#here-always-uses-project-relative-paths",
    "title": "Using here with rmarkdown",
    "section": "here always uses project-relative paths",
    "text": "here always uses project-relative paths\nTo render report.Rmd, you would have to ensure the path to penguins.csv is relative to the analysis/ directory - i.e., ../data/penguins.csv. The chunks would knit properly, but could not be run in the console since the working directory in the console isn’t analysis/.\nThe here package circumvents this issue by always referring to the project root:\n\nhere::i_am(\"analysis/report.Rmd\")\n\nAll files accessed by report.Rmd should be referred to using here():\n\nlibrary(here)\nhere(\"data\", \"penguins.csv\")\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/data/penguins.csv\"\nhere(\"data/penguins.csv\")\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/data/penguins.csv\"\n\nThis ensures that penguins.csv can be read both when the report is knit and when the code is run interactively in the console."
  },
  {
    "objectID": "R/Library/here/NEWS.html",
    "href": "R/Library/here/NEWS.html",
    "title": "here 1.0.1 (2020-12-13)",
    "section": "",
    "text": "Adapt to testthat 3.0.1."
  },
  {
    "objectID": "R/Library/here/NEWS.html#features",
    "href": "R/Library/here/NEWS.html#features",
    "title": "here 1.0.1 (2020-12-13)",
    "section": "Features",
    "text": "Features\n\nNew here::i_am() offers a new recommended way to declare the project root. Instead of relying on special files or directories that indicate the project root, each script and report now can declare its own location relative to the project root (#27).\ndr_here() shows the initial and current working directory (#18, #55)."
  },
  {
    "objectID": "R/Library/here/NEWS.html#documentation",
    "href": "R/Library/here/NEWS.html#documentation",
    "title": "here 1.0.1 (2020-12-13)",
    "section": "Documentation",
    "text": "Documentation\n\nImprove readme and add “Getting started” vignette (#44, (sharlagelfand?)). Extend “Getting started” vignette with a detailed description of the behavior and use cases (#55).\nUpdate ?here to address conflict with lubridate::here() (#37, (nzgwynn?)).\nNew “rmarkdown” vignette (#55).\nset_here() is superseded in favor of here::i_am() (#61).\nMore explicit documentation for the ... argument to here() (#7)."
  },
  {
    "objectID": "R/Library/here/NEWS.html#internal",
    "href": "R/Library/here/NEWS.html#internal",
    "title": "here 1.0.1 (2020-12-13)",
    "section": "Internal",
    "text": "Internal\n\nAdd testthat tests (#57).\nUse GitHub Actions for checks (#52).\nRequires rprojroot &gt;= 2.0.1.\nRe-license as MIT (#50).\nAdded Jennifer Bryan as contributor (#32)."
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html",
    "title": "Introduction to the tokenizers Package",
    "section": "",
    "text": "In natural language processing, tokenization is the process of breaking human-readable text into machine readable components. The most obvious way to tokenize a text is to split the text into words. But there are many other ways to tokenize a text, the most useful of which are provided by this package.\nThe tokenizers in this package have a consistent interface. They all take either a character vector of any length, or a list where each element is a character vector of length one. The idea is that each element comprises a text. Then each function returns a list with the same length as the input vector, where each element in the list contains the tokens generated by the function. If the input character vector or list is named, then the names are preserved, so that the names can serve as identifiers.\nUsing the following sample text, the rest of this vignette demonstrates the different kinds of tokenizers in this package.\n\nlibrary(tokenizers)\noptions(max.print = 25)\n\njames &lt;- paste0(\n  \"The question thus becomes a verbal one\\n\",\n  \"again; and our knowledge of all these early stages of thought and feeling\\n\",\n  \"is in any case so conjectural and imperfect that farther discussion would\\n\",\n  \"not be worth while.\\n\",\n  \"\\n\",\n  \"Religion, therefore, as I now ask you arbitrarily to take it, shall mean\\n\",\n  \"for us _the feelings, acts, and experiences of individual men in their\\n\",\n  \"solitude, so far as they apprehend themselves to stand in relation to\\n\",\n  \"whatever they may consider the divine_. Since the relation may be either\\n\",\n  \"moral, physical, or ritual, it is evident that out of religion in the\\n\",\n  \"sense in which we take it, theologies, philosophies, and ecclesiastical\\n\",\n  \"organizations may secondarily grow.\\n\"\n)"
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#package-overview",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#package-overview",
    "title": "Introduction to the tokenizers Package",
    "section": "",
    "text": "In natural language processing, tokenization is the process of breaking human-readable text into machine readable components. The most obvious way to tokenize a text is to split the text into words. But there are many other ways to tokenize a text, the most useful of which are provided by this package.\nThe tokenizers in this package have a consistent interface. They all take either a character vector of any length, or a list where each element is a character vector of length one. The idea is that each element comprises a text. Then each function returns a list with the same length as the input vector, where each element in the list contains the tokens generated by the function. If the input character vector or list is named, then the names are preserved, so that the names can serve as identifiers.\nUsing the following sample text, the rest of this vignette demonstrates the different kinds of tokenizers in this package.\n\nlibrary(tokenizers)\noptions(max.print = 25)\n\njames &lt;- paste0(\n  \"The question thus becomes a verbal one\\n\",\n  \"again; and our knowledge of all these early stages of thought and feeling\\n\",\n  \"is in any case so conjectural and imperfect that farther discussion would\\n\",\n  \"not be worth while.\\n\",\n  \"\\n\",\n  \"Religion, therefore, as I now ask you arbitrarily to take it, shall mean\\n\",\n  \"for us _the feelings, acts, and experiences of individual men in their\\n\",\n  \"solitude, so far as they apprehend themselves to stand in relation to\\n\",\n  \"whatever they may consider the divine_. Since the relation may be either\\n\",\n  \"moral, physical, or ritual, it is evident that out of religion in the\\n\",\n  \"sense in which we take it, theologies, philosophies, and ecclesiastical\\n\",\n  \"organizations may secondarily grow.\\n\"\n)"
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#character-and-character-shingle-tokenizers",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#character-and-character-shingle-tokenizers",
    "title": "Introduction to the tokenizers Package",
    "section": "Character and character-shingle tokenizers",
    "text": "Character and character-shingle tokenizers\nThe character tokenizer splits texts into individual characters.\n\ntokenize_characters(james)[[1]] \n\n [1] \"t\" \"h\" \"e\" \"q\" \"u\" \"e\" \"s\" \"t\" \"i\" \"o\" \"n\" \"t\" \"h\" \"u\" \"s\" \"b\" \"e\" \"c\" \"o\"\n[20] \"m\" \"e\" \"s\" \"a\" \"v\" \"e\"\n [ reached getOption(\"max.print\") -- omitted 517 entries ]\n\n\nYou can also tokenize into character-based shingles.\n\ntokenize_character_shingles(james, n = 3, n_min = 3, \n                            strip_non_alphanum = FALSE)[[1]][1:20]\n\n [1] \"the\" \"he \" \"e q\" \" qu\" \"que\" \"ues\" \"est\" \"sti\" \"tio\" \"ion\" \"on \" \"n t\"\n[13] \" th\" \"thu\" \"hus\" \"us \" \"s b\" \" be\" \"bec\" \"eco\""
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#word-and-word-stem-tokenizers",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#word-and-word-stem-tokenizers",
    "title": "Introduction to the tokenizers Package",
    "section": "Word and word-stem tokenizers",
    "text": "Word and word-stem tokenizers\nThe word tokenizer splits texts into words.\n\ntokenize_words(james)\n\n[[1]]\n [1] \"the\"       \"question\"  \"thus\"      \"becomes\"   \"a\"         \"verbal\"   \n [7] \"one\"       \"again\"     \"and\"       \"our\"       \"knowledge\" \"of\"       \n[13] \"all\"       \"these\"     \"early\"     \"stages\"    \"of\"        \"thought\"  \n[19] \"and\"       \"feeling\"   \"is\"        \"in\"        \"any\"       \"case\"     \n[25] \"so\"       \n [ reached getOption(\"max.print\") -- omitted 87 entries ]\n\n\nWord stemming is provided by the SnowballC package.\n\ntokenize_word_stems(james)\n\n[[1]]\n [1] \"the\"      \"question\" \"thus\"     \"becom\"    \"a\"        \"verbal\"  \n [7] \"one\"      \"again\"    \"and\"      \"our\"      \"knowledg\" \"of\"      \n[13] \"all\"      \"these\"    \"earli\"    \"stage\"    \"of\"       \"thought\" \n[19] \"and\"      \"feel\"     \"is\"       \"in\"       \"ani\"      \"case\"    \n[25] \"so\"      \n [ reached getOption(\"max.print\") -- omitted 87 entries ]\n\n\nYou can also provide a vector of stopwords which will be omitted. The stopwords package, which contains stopwords for many languages from several sources, is recommended. This argument also works with the n-gram and skip n-gram tokenizers.\n\nlibrary(stopwords)\ntokenize_words(james, stopwords = stopwords::stopwords(\"en\"))\n\n[[1]]\n [1] \"question\"       \"thus\"           \"becomes\"        \"verbal\"        \n [5] \"one\"            \"knowledge\"      \"early\"          \"stages\"        \n [9] \"thought\"        \"feeling\"        \"case\"           \"conjectural\"   \n[13] \"imperfect\"      \"farther\"        \"discussion\"     \"worth\"         \n[17] \"religion\"       \"therefore\"      \"now\"            \"ask\"           \n[21] \"arbitrarily\"    \"take\"           \"shall\"          \"mean\"          \n[25] \"us\"             \"_the\"           \"feelings\"       \"acts\"          \n[29] \"experiences\"    \"individual\"     \"men\"            \"solitude\"      \n[33] \"far\"            \"apprehend\"      \"stand\"          \"relation\"      \n[37] \"whatever\"       \"may\"            \"consider\"       \"divine_\"       \n[41] \"since\"          \"relation\"       \"may\"            \"either\"        \n[45] \"moral\"          \"physical\"       \"ritual\"         \"evident\"       \n[49] \"religion\"       \"sense\"          \"take\"           \"theologies\"    \n[53] \"philosophies\"   \"ecclesiastical\" \"organizations\"  \"may\"           \n[57] \"secondarily\"    \"grow\"          \n\n\nAn alternative word stemmer often used in NLP that preserves punctuation and separates common English contractions is the Penn Treebank tokenizer.\n\ntokenize_ptb(james)\n\n[[1]]\n  [1] \"The\"            \"question\"       \"thus\"           \"becomes\"       \n  [5] \"a\"              \"verbal\"         \"one\"            \"again\"         \n  [9] \";\"              \"and\"            \"our\"            \"knowledge\"     \n [13] \"of\"             \"all\"            \"these\"          \"early\"         \n [17] \"stages\"         \"of\"             \"thought\"        \"and\"           \n [21] \"feeling\"        \"is\"             \"in\"             \"any\"           \n [25] \"case\"           \"so\"             \"conjectural\"    \"and\"           \n [29] \"imperfect\"      \"that\"           \"farther\"        \"discussion\"    \n [33] \"would\"          \"not\"            \"be\"             \"worth\"         \n [37] \"while.\"         \"Religion\"       \",\"              \"therefore\"     \n [41] \",\"              \"as\"             \"I\"              \"now\"           \n [45] \"ask\"            \"you\"            \"arbitrarily\"    \"to\"            \n [49] \"take\"           \"it\"             \",\"              \"shall\"         \n [53] \"mean\"           \"for\"            \"us\"             \"_the\"          \n [57] \"feelings\"       \",\"              \"acts\"           \",\"             \n [61] \"and\"            \"experiences\"    \"of\"             \"individual\"    \n [65] \"men\"            \"in\"             \"their\"          \"solitude\"      \n [69] \",\"              \"so\"             \"far\"            \"as\"            \n [73] \"they\"           \"apprehend\"      \"themselves\"     \"to\"            \n [77] \"stand\"          \"in\"             \"relation\"       \"to\"            \n [81] \"whatever\"       \"they\"           \"may\"            \"consider\"      \n [85] \"the\"            \"divine_.\"       \"Since\"          \"the\"           \n [89] \"relation\"       \"may\"            \"be\"             \"either\"        \n [93] \"moral\"          \",\"              \"physical\"       \",\"             \n [97] \"or\"             \"ritual\"         \",\"              \"it\"            \n[101] \"is\"             \"evident\"        \"that\"           \"out\"           \n[105] \"of\"             \"religion\"       \"in\"             \"the\"           \n[109] \"sense\"          \"in\"             \"which\"          \"we\"            \n[113] \"take\"           \"it\"             \",\"              \"theologies\"    \n[117] \",\"              \"philosophies\"   \",\"              \"and\"           \n[121] \"ecclesiastical\" \"organizations\"  \"may\"            \"secondarily\"   \n[125] \"grow\"           \".\""
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#n-gram-and-skip-n-gram-tokenizers",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#n-gram-and-skip-n-gram-tokenizers",
    "title": "Introduction to the tokenizers Package",
    "section": "N-gram and skip n-gram tokenizers",
    "text": "N-gram and skip n-gram tokenizers\nAn n-gram is a contiguous sequence of words containing at least n_min words and at most n words. This function will generate all such combinations of n-grams, omitting stopwords if desired.\n\ntokenize_ngrams(james, n = 5, n_min = 2,\n                stopwords = stopwords::stopwords(\"en\"))\n\n[[1]]\n  [1] \"question thus\"                                            \n  [2] \"question thus becomes\"                                    \n  [3] \"question thus becomes verbal\"                             \n  [4] \"question thus becomes verbal one\"                         \n  [5] \"thus becomes\"                                             \n  [6] \"thus becomes verbal\"                                      \n  [7] \"thus becomes verbal one\"                                  \n  [8] \"thus becomes verbal one knowledge\"                        \n  [9] \"becomes verbal\"                                           \n [10] \"becomes verbal one\"                                       \n [11] \"becomes verbal one knowledge\"                             \n [12] \"becomes verbal one knowledge early\"                       \n [13] \"verbal one\"                                               \n [14] \"verbal one knowledge\"                                     \n [15] \"verbal one knowledge early\"                               \n [16] \"verbal one knowledge early stages\"                        \n [17] \"one knowledge\"                                            \n [18] \"one knowledge early\"                                      \n [19] \"one knowledge early stages\"                               \n [20] \"one knowledge early stages thought\"                       \n [21] \"knowledge early\"                                          \n [22] \"knowledge early stages\"                                   \n [23] \"knowledge early stages thought\"                           \n [24] \"knowledge early stages thought feeling\"                   \n [25] \"early stages\"                                             \n [26] \"early stages thought\"                                     \n [27] \"early stages thought feeling\"                             \n [28] \"early stages thought feeling case\"                        \n [29] \"stages thought\"                                           \n [30] \"stages thought feeling\"                                   \n [31] \"stages thought feeling case\"                              \n [32] \"stages thought feeling case conjectural\"                  \n [33] \"thought feeling\"                                          \n [34] \"thought feeling case\"                                     \n [35] \"thought feeling case conjectural\"                         \n [36] \"thought feeling case conjectural imperfect\"               \n [37] \"feeling case\"                                             \n [38] \"feeling case conjectural\"                                 \n [39] \"feeling case conjectural imperfect\"                       \n [40] \"feeling case conjectural imperfect farther\"               \n [41] \"case conjectural\"                                         \n [42] \"case conjectural imperfect\"                               \n [43] \"case conjectural imperfect farther\"                       \n [44] \"case conjectural imperfect farther discussion\"            \n [45] \"conjectural imperfect\"                                    \n [46] \"conjectural imperfect farther\"                            \n [47] \"conjectural imperfect farther discussion\"                 \n [48] \"conjectural imperfect farther discussion worth\"           \n [49] \"imperfect farther\"                                        \n [50] \"imperfect farther discussion\"                             \n [51] \"imperfect farther discussion worth\"                       \n [52] \"imperfect farther discussion worth religion\"              \n [53] \"farther discussion\"                                       \n [54] \"farther discussion worth\"                                 \n [55] \"farther discussion worth religion\"                        \n [56] \"farther discussion worth religion therefore\"              \n [57] \"discussion worth\"                                         \n [58] \"discussion worth religion\"                                \n [59] \"discussion worth religion therefore\"                      \n [60] \"discussion worth religion therefore now\"                  \n [61] \"worth religion\"                                           \n [62] \"worth religion therefore\"                                 \n [63] \"worth religion therefore now\"                             \n [64] \"worth religion therefore now ask\"                         \n [65] \"religion therefore\"                                       \n [66] \"religion therefore now\"                                   \n [67] \"religion therefore now ask\"                               \n [68] \"religion therefore now ask arbitrarily\"                   \n [69] \"therefore now\"                                            \n [70] \"therefore now ask\"                                        \n [71] \"therefore now ask arbitrarily\"                            \n [72] \"therefore now ask arbitrarily take\"                       \n [73] \"now ask\"                                                  \n [74] \"now ask arbitrarily\"                                      \n [75] \"now ask arbitrarily take\"                                 \n [76] \"now ask arbitrarily take shall\"                           \n [77] \"ask arbitrarily\"                                          \n [78] \"ask arbitrarily take\"                                     \n [79] \"ask arbitrarily take shall\"                               \n [80] \"ask arbitrarily take shall mean\"                          \n [81] \"arbitrarily take\"                                         \n [82] \"arbitrarily take shall\"                                   \n [83] \"arbitrarily take shall mean\"                              \n [84] \"arbitrarily take shall mean us\"                           \n [85] \"take shall\"                                               \n [86] \"take shall mean\"                                          \n [87] \"take shall mean us\"                                       \n [88] \"take shall mean us _the\"                                  \n [89] \"shall mean\"                                               \n [90] \"shall mean us\"                                            \n [91] \"shall mean us _the\"                                       \n [92] \"shall mean us _the feelings\"                              \n [93] \"mean us\"                                                  \n [94] \"mean us _the\"                                             \n [95] \"mean us _the feelings\"                                    \n [96] \"mean us _the feelings acts\"                               \n [97] \"us _the\"                                                  \n [98] \"us _the feelings\"                                         \n [99] \"us _the feelings acts\"                                    \n[100] \"us _the feelings acts experiences\"                        \n[101] \"_the feelings\"                                            \n[102] \"_the feelings acts\"                                       \n[103] \"_the feelings acts experiences\"                           \n[104] \"_the feelings acts experiences individual\"                \n[105] \"feelings acts\"                                            \n[106] \"feelings acts experiences\"                                \n[107] \"feelings acts experiences individual\"                     \n[108] \"feelings acts experiences individual men\"                 \n[109] \"acts experiences\"                                         \n[110] \"acts experiences individual\"                              \n[111] \"acts experiences individual men\"                          \n[112] \"acts experiences individual men solitude\"                 \n[113] \"experiences individual\"                                   \n[114] \"experiences individual men\"                               \n[115] \"experiences individual men solitude\"                      \n[116] \"experiences individual men solitude far\"                  \n[117] \"individual men\"                                           \n[118] \"individual men solitude\"                                  \n[119] \"individual men solitude far\"                              \n[120] \"individual men solitude far apprehend\"                    \n[121] \"men solitude\"                                             \n[122] \"men solitude far\"                                         \n[123] \"men solitude far apprehend\"                               \n[124] \"men solitude far apprehend stand\"                         \n[125] \"solitude far\"                                             \n[126] \"solitude far apprehend\"                                   \n[127] \"solitude far apprehend stand\"                             \n[128] \"solitude far apprehend stand relation\"                    \n[129] \"far apprehend\"                                            \n[130] \"far apprehend stand\"                                      \n[131] \"far apprehend stand relation\"                             \n[132] \"far apprehend stand relation whatever\"                    \n[133] \"apprehend stand\"                                          \n[134] \"apprehend stand relation\"                                 \n[135] \"apprehend stand relation whatever\"                        \n[136] \"apprehend stand relation whatever may\"                    \n[137] \"stand relation\"                                           \n[138] \"stand relation whatever\"                                  \n[139] \"stand relation whatever may\"                              \n[140] \"stand relation whatever may consider\"                     \n[141] \"relation whatever\"                                        \n[142] \"relation whatever may\"                                    \n[143] \"relation whatever may consider\"                           \n[144] \"relation whatever may consider divine_\"                   \n[145] \"whatever may\"                                             \n[146] \"whatever may consider\"                                    \n[147] \"whatever may consider divine_\"                            \n[148] \"whatever may consider divine_ since\"                      \n[149] \"may consider\"                                             \n[150] \"may consider divine_\"                                     \n[151] \"may consider divine_ since\"                               \n[152] \"may consider divine_ since relation\"                      \n[153] \"consider divine_\"                                         \n[154] \"consider divine_ since\"                                   \n[155] \"consider divine_ since relation\"                          \n[156] \"consider divine_ since relation may\"                      \n[157] \"divine_ since\"                                            \n[158] \"divine_ since relation\"                                   \n[159] \"divine_ since relation may\"                               \n[160] \"divine_ since relation may either\"                        \n[161] \"since relation\"                                           \n[162] \"since relation may\"                                       \n[163] \"since relation may either\"                                \n[164] \"since relation may either moral\"                          \n[165] \"relation may\"                                             \n[166] \"relation may either\"                                      \n[167] \"relation may either moral\"                                \n[168] \"relation may either moral physical\"                       \n[169] \"may either\"                                               \n[170] \"may either moral\"                                         \n[171] \"may either moral physical\"                                \n[172] \"may either moral physical ritual\"                         \n[173] \"either moral\"                                             \n[174] \"either moral physical\"                                    \n[175] \"either moral physical ritual\"                             \n[176] \"either moral physical ritual evident\"                     \n[177] \"moral physical\"                                           \n[178] \"moral physical ritual\"                                    \n[179] \"moral physical ritual evident\"                            \n[180] \"moral physical ritual evident religion\"                   \n[181] \"physical ritual\"                                          \n[182] \"physical ritual evident\"                                  \n[183] \"physical ritual evident religion\"                         \n[184] \"physical ritual evident religion sense\"                   \n[185] \"ritual evident\"                                           \n[186] \"ritual evident religion\"                                  \n[187] \"ritual evident religion sense\"                            \n[188] \"ritual evident religion sense take\"                       \n[189] \"evident religion\"                                         \n[190] \"evident religion sense\"                                   \n[191] \"evident religion sense take\"                              \n[192] \"evident religion sense take theologies\"                   \n[193] \"religion sense\"                                           \n[194] \"religion sense take\"                                      \n[195] \"religion sense take theologies\"                           \n[196] \"religion sense take theologies philosophies\"              \n[197] \"sense take\"                                               \n[198] \"sense take theologies\"                                    \n[199] \"sense take theologies philosophies\"                       \n[200] \"sense take theologies philosophies ecclesiastical\"        \n[201] \"take theologies\"                                          \n[202] \"take theologies philosophies\"                             \n[203] \"take theologies philosophies ecclesiastical\"              \n[204] \"take theologies philosophies ecclesiastical organizations\"\n[205] \"theologies philosophies\"                                  \n[206] \"theologies philosophies ecclesiastical\"                   \n[207] \"theologies philosophies ecclesiastical organizations\"     \n[208] \"theologies philosophies ecclesiastical organizations may\" \n[209] \"philosophies ecclesiastical\"                              \n[210] \"philosophies ecclesiastical organizations\"                \n[211] \"philosophies ecclesiastical organizations may\"            \n[212] \"philosophies ecclesiastical organizations may secondarily\"\n[213] \"ecclesiastical organizations\"                             \n[214] \"ecclesiastical organizations may\"                         \n[215] \"ecclesiastical organizations may secondarily\"             \n[216] \"ecclesiastical organizations may secondarily grow\"        \n[217] \"organizations may\"                                        \n[218] \"organizations may secondarily\"                            \n[219] \"organizations may secondarily grow\"                       \n[220] \"may secondarily\"                                          \n[221] \"may secondarily grow\"                                     \n[222] \"secondarily grow\"                                         \n\n\nA skip n-gram is like an n-gram in that it takes the n and n_min parameters. But rather than returning contiguous sequences of words, it will also return sequences of n-grams skipping words with gaps between 0 and the value of k. This function generates all such sequences, again omitting stopwords if desired. Note that the number of tokens returned can be very large.\n\ntokenize_skip_ngrams(james, n = 5, n_min = 2, k = 2,\n                     stopwords = stopwords::stopwords(\"en\"))\n\n[[1]]\n   [1] \"question thus\"                                                   \n   [2] \"question becomes\"                                                \n   [3] \"question verbal\"                                                 \n   [4] \"question thus becomes\"                                           \n   [5] \"question thus verbal\"                                            \n   [6] \"question thus one\"                                               \n   [7] \"question becomes verbal\"                                         \n   [8] \"question becomes one\"                                            \n   [9] \"question becomes knowledge\"                                      \n  [10] \"question verbal one\"                                             \n  [11] \"question verbal knowledge\"                                       \n  [12] \"question verbal early\"                                           \n  [13] \"question thus becomes verbal\"                                    \n  [14] \"question thus becomes one\"                                       \n  [15] \"question thus becomes knowledge\"                                 \n  [16] \"question thus verbal one\"                                        \n  [17] \"question thus verbal knowledge\"                                  \n  [18] \"question thus verbal early\"                                      \n  [19] \"question thus one knowledge\"                                     \n  [20] \"question thus one early\"                                         \n  [21] \"question thus one stages\"                                        \n  [22] \"question becomes verbal one\"                                     \n  [23] \"question becomes verbal knowledge\"                               \n  [24] \"question becomes verbal early\"                                   \n  [25] \"question becomes one knowledge\"                                  \n  [26] \"question becomes one early\"                                      \n  [27] \"question becomes one stages\"                                     \n  [28] \"question becomes knowledge early\"                                \n  [29] \"question becomes knowledge stages\"                               \n  [30] \"question becomes knowledge thought\"                              \n  [31] \"question verbal one knowledge\"                                   \n  [32] \"question verbal one early\"                                       \n  [33] \"question verbal one stages\"                                      \n  [34] \"question verbal knowledge early\"                                 \n  [35] \"question verbal knowledge stages\"                                \n  [36] \"question verbal knowledge thought\"                               \n  [37] \"question verbal early stages\"                                    \n  [38] \"question verbal early thought\"                                   \n  [39] \"question verbal early feeling\"                                   \n  [40] \"question thus becomes verbal one\"                                \n  [41] \"question thus becomes verbal knowledge\"                          \n  [42] \"question thus becomes verbal early\"                              \n  [43] \"question thus becomes one knowledge\"                             \n  [44] \"question thus becomes one early\"                                 \n  [45] \"question thus becomes one stages\"                                \n  [46] \"question thus becomes knowledge early\"                           \n  [47] \"question thus becomes knowledge stages\"                          \n  [48] \"question thus becomes knowledge thought\"                         \n  [49] \"question thus verbal one knowledge\"                              \n  [50] \"question thus verbal one early\"                                  \n  [51] \"question thus verbal one stages\"                                 \n  [52] \"question thus verbal knowledge early\"                            \n  [53] \"question thus verbal knowledge stages\"                           \n  [54] \"question thus verbal knowledge thought\"                          \n  [55] \"question thus verbal early stages\"                               \n  [56] \"question thus verbal early thought\"                              \n  [57] \"question thus verbal early feeling\"                              \n  [58] \"question thus one knowledge early\"                               \n  [59] \"question thus one knowledge stages\"                              \n  [60] \"question thus one knowledge thought\"                             \n  [61] \"question thus one early stages\"                                  \n  [62] \"question thus one early thought\"                                 \n  [63] \"question thus one early feeling\"                                 \n  [64] \"question thus one stages thought\"                                \n  [65] \"question thus one stages feeling\"                                \n  [66] \"question thus one stages case\"                                   \n  [67] \"question becomes verbal one knowledge\"                           \n  [68] \"question becomes verbal one early\"                               \n  [69] \"question becomes verbal one stages\"                              \n  [70] \"question becomes verbal knowledge early\"                         \n  [71] \"question becomes verbal knowledge stages\"                        \n  [72] \"question becomes verbal knowledge thought\"                       \n  [73] \"question becomes verbal early stages\"                            \n  [74] \"question becomes verbal early thought\"                           \n  [75] \"question becomes verbal early feeling\"                           \n  [76] \"question becomes one knowledge early\"                            \n  [77] \"question becomes one knowledge stages\"                           \n  [78] \"question becomes one knowledge thought\"                          \n  [79] \"question becomes one early stages\"                               \n  [80] \"question becomes one early thought\"                              \n  [81] \"question becomes one early feeling\"                              \n  [82] \"question becomes one stages thought\"                             \n  [83] \"question becomes one stages feeling\"                             \n  [84] \"question becomes one stages case\"                                \n  [85] \"question becomes knowledge early stages\"                         \n  [86] \"question becomes knowledge early thought\"                        \n  [87] \"question becomes knowledge early feeling\"                        \n  [88] \"question becomes knowledge stages thought\"                       \n  [89] \"question becomes knowledge stages feeling\"                       \n  [90] \"question becomes knowledge stages case\"                          \n  [91] \"question becomes knowledge thought feeling\"                      \n  [92] \"question becomes knowledge thought case\"                         \n  [93] \"question becomes knowledge thought conjectural\"                  \n  [94] \"question verbal one knowledge early\"                             \n  [95] \"question verbal one knowledge stages\"                            \n  [96] \"question verbal one knowledge thought\"                           \n  [97] \"question verbal one early stages\"                                \n  [98] \"question verbal one early thought\"                               \n  [99] \"question verbal one early feeling\"                               \n [100] \"question verbal one stages thought\"                              \n [101] \"question verbal one stages feeling\"                              \n [102] \"question verbal one stages case\"                                 \n [103] \"question verbal knowledge early stages\"                          \n [104] \"question verbal knowledge early thought\"                         \n [105] \"question verbal knowledge early feeling\"                         \n [106] \"question verbal knowledge stages thought\"                        \n [107] \"question verbal knowledge stages feeling\"                        \n [108] \"question verbal knowledge stages case\"                           \n [109] \"question verbal knowledge thought feeling\"                       \n [110] \"question verbal knowledge thought case\"                          \n [111] \"question verbal knowledge thought conjectural\"                   \n [112] \"question verbal early stages thought\"                            \n [113] \"question verbal early stages feeling\"                            \n [114] \"question verbal early stages case\"                               \n [115] \"question verbal early thought feeling\"                           \n [116] \"question verbal early thought case\"                              \n [117] \"question verbal early thought conjectural\"                       \n [118] \"question verbal early feeling case\"                              \n [119] \"question verbal early feeling conjectural\"                       \n [120] \"question verbal early feeling imperfect\"                         \n [121] \"thus becomes\"                                                    \n [122] \"thus verbal\"                                                     \n [123] \"thus one\"                                                        \n [124] \"thus becomes verbal\"                                             \n [125] \"thus becomes one\"                                                \n [126] \"thus becomes knowledge\"                                          \n [127] \"thus verbal one\"                                                 \n [128] \"thus verbal knowledge\"                                           \n [129] \"thus verbal early\"                                               \n [130] \"thus one knowledge\"                                              \n [131] \"thus one early\"                                                  \n [132] \"thus one stages\"                                                 \n [133] \"thus becomes verbal one\"                                         \n [134] \"thus becomes verbal knowledge\"                                   \n [135] \"thus becomes verbal early\"                                       \n [136] \"thus becomes one knowledge\"                                      \n [137] \"thus becomes one early\"                                          \n [138] \"thus becomes one stages\"                                         \n [139] \"thus becomes knowledge early\"                                    \n [140] \"thus becomes knowledge stages\"                                   \n [141] \"thus becomes knowledge thought\"                                  \n [142] \"thus verbal one knowledge\"                                       \n [143] \"thus verbal one early\"                                           \n [144] \"thus verbal one stages\"                                          \n [145] \"thus verbal knowledge early\"                                     \n [146] \"thus verbal knowledge stages\"                                    \n [147] \"thus verbal knowledge thought\"                                   \n [148] \"thus verbal early stages\"                                        \n [149] \"thus verbal early thought\"                                       \n [150] \"thus verbal early feeling\"                                       \n [151] \"thus one knowledge early\"                                        \n [152] \"thus one knowledge stages\"                                       \n [153] \"thus one knowledge thought\"                                      \n [154] \"thus one early stages\"                                           \n [155] \"thus one early thought\"                                          \n [156] \"thus one early feeling\"                                          \n [157] \"thus one stages thought\"                                         \n [158] \"thus one stages feeling\"                                         \n [159] \"thus one stages case\"                                            \n [160] \"thus becomes verbal one knowledge\"                               \n [161] \"thus becomes verbal one early\"                                   \n [162] \"thus becomes verbal one stages\"                                  \n [163] \"thus becomes verbal knowledge early\"                             \n [164] \"thus becomes verbal knowledge stages\"                            \n [165] \"thus becomes verbal knowledge thought\"                           \n [166] \"thus becomes verbal early stages\"                                \n [167] \"thus becomes verbal early thought\"                               \n [168] \"thus becomes verbal early feeling\"                               \n [169] \"thus becomes one knowledge early\"                                \n [170] \"thus becomes one knowledge stages\"                               \n [171] \"thus becomes one knowledge thought\"                              \n [172] \"thus becomes one early stages\"                                   \n [173] \"thus becomes one early thought\"                                  \n [174] \"thus becomes one early feeling\"                                  \n [175] \"thus becomes one stages thought\"                                 \n [176] \"thus becomes one stages feeling\"                                 \n [177] \"thus becomes one stages case\"                                    \n [178] \"thus becomes knowledge early stages\"                             \n [179] \"thus becomes knowledge early thought\"                            \n [180] \"thus becomes knowledge early feeling\"                            \n [181] \"thus becomes knowledge stages thought\"                           \n [182] \"thus becomes knowledge stages feeling\"                           \n [183] \"thus becomes knowledge stages case\"                              \n [184] \"thus becomes knowledge thought feeling\"                          \n [185] \"thus becomes knowledge thought case\"                             \n [186] \"thus becomes knowledge thought conjectural\"                      \n [187] \"thus verbal one knowledge early\"                                 \n [188] \"thus verbal one knowledge stages\"                                \n [189] \"thus verbal one knowledge thought\"                               \n [190] \"thus verbal one early stages\"                                    \n [191] \"thus verbal one early thought\"                                   \n [192] \"thus verbal one early feeling\"                                   \n [193] \"thus verbal one stages thought\"                                  \n [194] \"thus verbal one stages feeling\"                                  \n [195] \"thus verbal one stages case\"                                     \n [196] \"thus verbal knowledge early stages\"                              \n [197] \"thus verbal knowledge early thought\"                             \n [198] \"thus verbal knowledge early feeling\"                             \n [199] \"thus verbal knowledge stages thought\"                            \n [200] \"thus verbal knowledge stages feeling\"                            \n [201] \"thus verbal knowledge stages case\"                               \n [202] \"thus verbal knowledge thought feeling\"                           \n [203] \"thus verbal knowledge thought case\"                              \n [204] \"thus verbal knowledge thought conjectural\"                       \n [205] \"thus verbal early stages thought\"                                \n [206] \"thus verbal early stages feeling\"                                \n [207] \"thus verbal early stages case\"                                   \n [208] \"thus verbal early thought feeling\"                               \n [209] \"thus verbal early thought case\"                                  \n [210] \"thus verbal early thought conjectural\"                           \n [211] \"thus verbal early feeling case\"                                  \n [212] \"thus verbal early feeling conjectural\"                           \n [213] \"thus verbal early feeling imperfect\"                             \n [214] \"thus one knowledge early stages\"                                 \n [215] \"thus one knowledge early thought\"                                \n [216] \"thus one knowledge early feeling\"                                \n [217] \"thus one knowledge stages thought\"                               \n [218] \"thus one knowledge stages feeling\"                               \n [219] \"thus one knowledge stages case\"                                  \n [220] \"thus one knowledge thought feeling\"                              \n [221] \"thus one knowledge thought case\"                                 \n [222] \"thus one knowledge thought conjectural\"                          \n [223] \"thus one early stages thought\"                                   \n [224] \"thus one early stages feeling\"                                   \n [225] \"thus one early stages case\"                                      \n [226] \"thus one early thought feeling\"                                  \n [227] \"thus one early thought case\"                                     \n [228] \"thus one early thought conjectural\"                              \n [229] \"thus one early feeling case\"                                     \n [230] \"thus one early feeling conjectural\"                              \n [231] \"thus one early feeling imperfect\"                                \n [232] \"thus one stages thought feeling\"                                 \n [233] \"thus one stages thought case\"                                    \n [234] \"thus one stages thought conjectural\"                             \n [235] \"thus one stages feeling case\"                                    \n [236] \"thus one stages feeling conjectural\"                             \n [237] \"thus one stages feeling imperfect\"                               \n [238] \"thus one stages case conjectural\"                                \n [239] \"thus one stages case imperfect\"                                  \n [240] \"thus one stages case farther\"                                    \n [241] \"becomes verbal\"                                                  \n [242] \"becomes one\"                                                     \n [243] \"becomes knowledge\"                                               \n [244] \"becomes verbal one\"                                              \n [245] \"becomes verbal knowledge\"                                        \n [246] \"becomes verbal early\"                                            \n [247] \"becomes one knowledge\"                                           \n [248] \"becomes one early\"                                               \n [249] \"becomes one stages\"                                              \n [250] \"becomes knowledge early\"                                         \n [251] \"becomes knowledge stages\"                                        \n [252] \"becomes knowledge thought\"                                       \n [253] \"becomes verbal one knowledge\"                                    \n [254] \"becomes verbal one early\"                                        \n [255] \"becomes verbal one stages\"                                       \n [256] \"becomes verbal knowledge early\"                                  \n [257] \"becomes verbal knowledge stages\"                                 \n [258] \"becomes verbal knowledge thought\"                                \n [259] \"becomes verbal early stages\"                                     \n [260] \"becomes verbal early thought\"                                    \n [261] \"becomes verbal early feeling\"                                    \n [262] \"becomes one knowledge early\"                                     \n [263] \"becomes one knowledge stages\"                                    \n [264] \"becomes one knowledge thought\"                                   \n [265] \"becomes one early stages\"                                        \n [266] \"becomes one early thought\"                                       \n [267] \"becomes one early feeling\"                                       \n [268] \"becomes one stages thought\"                                      \n [269] \"becomes one stages feeling\"                                      \n [270] \"becomes one stages case\"                                         \n [271] \"becomes knowledge early stages\"                                  \n [272] \"becomes knowledge early thought\"                                 \n [273] \"becomes knowledge early feeling\"                                 \n [274] \"becomes knowledge stages thought\"                                \n [275] \"becomes knowledge stages feeling\"                                \n [276] \"becomes knowledge stages case\"                                   \n [277] \"becomes knowledge thought feeling\"                               \n [278] \"becomes knowledge thought case\"                                  \n [279] \"becomes knowledge thought conjectural\"                           \n [280] \"becomes verbal one knowledge early\"                              \n [281] \"becomes verbal one knowledge stages\"                             \n [282] \"becomes verbal one knowledge thought\"                            \n [283] \"becomes verbal one early stages\"                                 \n [284] \"becomes verbal one early thought\"                                \n [285] \"becomes verbal one early feeling\"                                \n [286] \"becomes verbal one stages thought\"                               \n [287] \"becomes verbal one stages feeling\"                               \n [288] \"becomes verbal one stages case\"                                  \n [289] \"becomes verbal knowledge early stages\"                           \n [290] \"becomes verbal knowledge early thought\"                          \n [291] \"becomes verbal knowledge early feeling\"                          \n [292] \"becomes verbal knowledge stages thought\"                         \n [293] \"becomes verbal knowledge stages feeling\"                         \n [294] \"becomes verbal knowledge stages case\"                            \n [295] \"becomes verbal knowledge thought feeling\"                        \n [296] \"becomes verbal knowledge thought case\"                           \n [297] \"becomes verbal knowledge thought conjectural\"                    \n [298] \"becomes verbal early stages thought\"                             \n [299] \"becomes verbal early stages feeling\"                             \n [300] \"becomes verbal early stages case\"                                \n [301] \"becomes verbal early thought feeling\"                            \n [302] \"becomes verbal early thought case\"                               \n [303] \"becomes verbal early thought conjectural\"                        \n [304] \"becomes verbal early feeling case\"                               \n [305] \"becomes verbal early feeling conjectural\"                        \n [306] \"becomes verbal early feeling imperfect\"                          \n [307] \"becomes one knowledge early stages\"                              \n [308] \"becomes one knowledge early thought\"                             \n [309] \"becomes one knowledge early feeling\"                             \n [310] \"becomes one knowledge stages thought\"                            \n [311] \"becomes one knowledge stages feeling\"                            \n [312] \"becomes one knowledge stages case\"                               \n [313] \"becomes one knowledge thought feeling\"                           \n [314] \"becomes one knowledge thought case\"                              \n [315] \"becomes one knowledge thought conjectural\"                       \n [316] \"becomes one early stages thought\"                                \n [317] \"becomes one early stages feeling\"                                \n [318] \"becomes one early stages case\"                                   \n [319] \"becomes one early thought feeling\"                               \n [320] \"becomes one early thought case\"                                  \n [321] \"becomes one early thought conjectural\"                           \n [322] \"becomes one early feeling case\"                                  \n [323] \"becomes one early feeling conjectural\"                           \n [324] \"becomes one early feeling imperfect\"                             \n [325] \"becomes one stages thought feeling\"                              \n [326] \"becomes one stages thought case\"                                 \n [327] \"becomes one stages thought conjectural\"                          \n [328] \"becomes one stages feeling case\"                                 \n [329] \"becomes one stages feeling conjectural\"                          \n [330] \"becomes one stages feeling imperfect\"                            \n [331] \"becomes one stages case conjectural\"                             \n [332] \"becomes one stages case imperfect\"                               \n [333] \"becomes one stages case farther\"                                 \n [334] \"becomes knowledge early stages thought\"                          \n [335] \"becomes knowledge early stages feeling\"                          \n [336] \"becomes knowledge early stages case\"                             \n [337] \"becomes knowledge early thought feeling\"                         \n [338] \"becomes knowledge early thought case\"                            \n [339] \"becomes knowledge early thought conjectural\"                     \n [340] \"becomes knowledge early feeling case\"                            \n [341] \"becomes knowledge early feeling conjectural\"                     \n [342] \"becomes knowledge early feeling imperfect\"                       \n [343] \"becomes knowledge stages thought feeling\"                        \n [344] \"becomes knowledge stages thought case\"                           \n [345] \"becomes knowledge stages thought conjectural\"                    \n [346] \"becomes knowledge stages feeling case\"                           \n [347] \"becomes knowledge stages feeling conjectural\"                    \n [348] \"becomes knowledge stages feeling imperfect\"                      \n [349] \"becomes knowledge stages case conjectural\"                       \n [350] \"becomes knowledge stages case imperfect\"                         \n [351] \"becomes knowledge stages case farther\"                           \n [352] \"becomes knowledge thought feeling case\"                          \n [353] \"becomes knowledge thought feeling conjectural\"                   \n [354] \"becomes knowledge thought feeling imperfect\"                     \n [355] \"becomes knowledge thought case conjectural\"                      \n [356] \"becomes knowledge thought case imperfect\"                        \n [357] \"becomes knowledge thought case farther\"                          \n [358] \"becomes knowledge thought conjectural imperfect\"                 \n [359] \"becomes knowledge thought conjectural farther\"                   \n [360] \"becomes knowledge thought conjectural discussion\"                \n [361] \"verbal one\"                                                      \n [362] \"verbal knowledge\"                                                \n [363] \"verbal early\"                                                    \n [364] \"verbal one knowledge\"                                            \n [365] \"verbal one early\"                                                \n [366] \"verbal one stages\"                                               \n [367] \"verbal knowledge early\"                                          \n [368] \"verbal knowledge stages\"                                         \n [369] \"verbal knowledge thought\"                                        \n [370] \"verbal early stages\"                                             \n [371] \"verbal early thought\"                                            \n [372] \"verbal early feeling\"                                            \n [373] \"verbal one knowledge early\"                                      \n [374] \"verbal one knowledge stages\"                                     \n [375] \"verbal one knowledge thought\"                                    \n [376] \"verbal one early stages\"                                         \n [377] \"verbal one early thought\"                                        \n [378] \"verbal one early feeling\"                                        \n [379] \"verbal one stages thought\"                                       \n [380] \"verbal one stages feeling\"                                       \n [381] \"verbal one stages case\"                                          \n [382] \"verbal knowledge early stages\"                                   \n [383] \"verbal knowledge early thought\"                                  \n [384] \"verbal knowledge early feeling\"                                  \n [385] \"verbal knowledge stages thought\"                                 \n [386] \"verbal knowledge stages feeling\"                                 \n [387] \"verbal knowledge stages case\"                                    \n [388] \"verbal knowledge thought feeling\"                                \n [389] \"verbal knowledge thought case\"                                   \n [390] \"verbal knowledge thought conjectural\"                            \n [391] \"verbal early stages thought\"                                     \n [392] \"verbal early stages feeling\"                                     \n [393] \"verbal early stages case\"                                        \n [394] \"verbal early thought feeling\"                                    \n [395] \"verbal early thought case\"                                       \n [396] \"verbal early thought conjectural\"                                \n [397] \"verbal early feeling case\"                                       \n [398] \"verbal early feeling conjectural\"                                \n [399] \"verbal early feeling imperfect\"                                  \n [400] \"verbal one knowledge early stages\"                               \n [401] \"verbal one knowledge early thought\"                              \n [402] \"verbal one knowledge early feeling\"                              \n [403] \"verbal one knowledge stages thought\"                             \n [404] \"verbal one knowledge stages feeling\"                             \n [405] \"verbal one knowledge stages case\"                                \n [406] \"verbal one knowledge thought feeling\"                            \n [407] \"verbal one knowledge thought case\"                               \n [408] \"verbal one knowledge thought conjectural\"                        \n [409] \"verbal one early stages thought\"                                 \n [410] \"verbal one early stages feeling\"                                 \n [411] \"verbal one early stages case\"                                    \n [412] \"verbal one early thought feeling\"                                \n [413] \"verbal one early thought case\"                                   \n [414] \"verbal one early thought conjectural\"                            \n [415] \"verbal one early feeling case\"                                   \n [416] \"verbal one early feeling conjectural\"                            \n [417] \"verbal one early feeling imperfect\"                              \n [418] \"verbal one stages thought feeling\"                               \n [419] \"verbal one stages thought case\"                                  \n [420] \"verbal one stages thought conjectural\"                           \n [421] \"verbal one stages feeling case\"                                  \n [422] \"verbal one stages feeling conjectural\"                           \n [423] \"verbal one stages feeling imperfect\"                             \n [424] \"verbal one stages case conjectural\"                              \n [425] \"verbal one stages case imperfect\"                                \n [426] \"verbal one stages case farther\"                                  \n [427] \"verbal knowledge early stages thought\"                           \n [428] \"verbal knowledge early stages feeling\"                           \n [429] \"verbal knowledge early stages case\"                              \n [430] \"verbal knowledge early thought feeling\"                          \n [431] \"verbal knowledge early thought case\"                             \n [432] \"verbal knowledge early thought conjectural\"                      \n [433] \"verbal knowledge early feeling case\"                             \n [434] \"verbal knowledge early feeling conjectural\"                      \n [435] \"verbal knowledge early feeling imperfect\"                        \n [436] \"verbal knowledge stages thought feeling\"                         \n [437] \"verbal knowledge stages thought case\"                            \n [438] \"verbal knowledge stages thought conjectural\"                     \n [439] \"verbal knowledge stages feeling case\"                            \n [440] \"verbal knowledge stages feeling conjectural\"                     \n [441] \"verbal knowledge stages feeling imperfect\"                       \n [442] \"verbal knowledge stages case conjectural\"                        \n [443] \"verbal knowledge stages case imperfect\"                          \n [444] \"verbal knowledge stages case farther\"                            \n [445] \"verbal knowledge thought feeling case\"                           \n [446] \"verbal knowledge thought feeling conjectural\"                    \n [447] \"verbal knowledge thought feeling imperfect\"                      \n [448] \"verbal knowledge thought case conjectural\"                       \n [449] \"verbal knowledge thought case imperfect\"                         \n [450] \"verbal knowledge thought case farther\"                           \n [451] \"verbal knowledge thought conjectural imperfect\"                  \n [452] \"verbal knowledge thought conjectural farther\"                    \n [453] \"verbal knowledge thought conjectural discussion\"                 \n [454] \"verbal early stages thought feeling\"                             \n [455] \"verbal early stages thought case\"                                \n [456] \"verbal early stages thought conjectural\"                         \n [457] \"verbal early stages feeling case\"                                \n [458] \"verbal early stages feeling conjectural\"                         \n [459] \"verbal early stages feeling imperfect\"                           \n [460] \"verbal early stages case conjectural\"                            \n [461] \"verbal early stages case imperfect\"                              \n [462] \"verbal early stages case farther\"                                \n [463] \"verbal early thought feeling case\"                               \n [464] \"verbal early thought feeling conjectural\"                        \n [465] \"verbal early thought feeling imperfect\"                          \n [466] \"verbal early thought case conjectural\"                           \n [467] \"verbal early thought case imperfect\"                             \n [468] \"verbal early thought case farther\"                               \n [469] \"verbal early thought conjectural imperfect\"                      \n [470] \"verbal early thought conjectural farther\"                        \n [471] \"verbal early thought conjectural discussion\"                     \n [472] \"verbal early feeling case conjectural\"                           \n [473] \"verbal early feeling case imperfect\"                             \n [474] \"verbal early feeling case farther\"                               \n [475] \"verbal early feeling conjectural imperfect\"                      \n [476] \"verbal early feeling conjectural farther\"                        \n [477] \"verbal early feeling conjectural discussion\"                     \n [478] \"verbal early feeling imperfect farther\"                          \n [479] \"verbal early feeling imperfect discussion\"                       \n [480] \"verbal early feeling imperfect worth\"                            \n [481] \"one knowledge\"                                                   \n [482] \"one early\"                                                       \n [483] \"one stages\"                                                      \n [484] \"one knowledge early\"                                             \n [485] \"one knowledge stages\"                                            \n [486] \"one knowledge thought\"                                           \n [487] \"one early stages\"                                                \n [488] \"one early thought\"                                               \n [489] \"one early feeling\"                                               \n [490] \"one stages thought\"                                              \n [491] \"one stages feeling\"                                              \n [492] \"one stages case\"                                                 \n [493] \"one knowledge early stages\"                                      \n [494] \"one knowledge early thought\"                                     \n [495] \"one knowledge early feeling\"                                     \n [496] \"one knowledge stages thought\"                                    \n [497] \"one knowledge stages feeling\"                                    \n [498] \"one knowledge stages case\"                                       \n [499] \"one knowledge thought feeling\"                                   \n [500] \"one knowledge thought case\"                                      \n [501] \"one knowledge thought conjectural\"                               \n [502] \"one early stages thought\"                                        \n [503] \"one early stages feeling\"                                        \n [504] \"one early stages case\"                                           \n [505] \"one early thought feeling\"                                       \n [506] \"one early thought case\"                                          \n [507] \"one early thought conjectural\"                                   \n [508] \"one early feeling case\"                                          \n [509] \"one early feeling conjectural\"                                   \n [510] \"one early feeling imperfect\"                                     \n [511] \"one stages thought feeling\"                                      \n [512] \"one stages thought case\"                                         \n [513] \"one stages thought conjectural\"                                  \n [514] \"one stages feeling case\"                                         \n [515] \"one stages feeling conjectural\"                                  \n [516] \"one stages feeling imperfect\"                                    \n [517] \"one stages case conjectural\"                                     \n [518] \"one stages case imperfect\"                                       \n [519] \"one stages case farther\"                                         \n [520] \"one knowledge early stages thought\"                              \n [521] \"one knowledge early stages feeling\"                              \n [522] \"one knowledge early stages case\"                                 \n [523] \"one knowledge early thought feeling\"                             \n [524] \"one knowledge early thought case\"                                \n [525] \"one knowledge early thought conjectural\"                         \n [526] \"one knowledge early feeling case\"                                \n [527] \"one knowledge early feeling conjectural\"                         \n [528] \"one knowledge early feeling imperfect\"                           \n [529] \"one knowledge stages thought feeling\"                            \n [530] \"one knowledge stages thought case\"                               \n [531] \"one knowledge stages thought conjectural\"                        \n [532] \"one knowledge stages feeling case\"                               \n [533] \"one knowledge stages feeling conjectural\"                        \n [534] \"one knowledge stages feeling imperfect\"                          \n [535] \"one knowledge stages case conjectural\"                           \n [536] \"one knowledge stages case imperfect\"                             \n [537] \"one knowledge stages case farther\"                               \n [538] \"one knowledge thought feeling case\"                              \n [539] \"one knowledge thought feeling conjectural\"                       \n [540] \"one knowledge thought feeling imperfect\"                         \n [541] \"one knowledge thought case conjectural\"                          \n [542] \"one knowledge thought case imperfect\"                            \n [543] \"one knowledge thought case farther\"                              \n [544] \"one knowledge thought conjectural imperfect\"                     \n [545] \"one knowledge thought conjectural farther\"                       \n [546] \"one knowledge thought conjectural discussion\"                    \n [547] \"one early stages thought feeling\"                                \n [548] \"one early stages thought case\"                                   \n [549] \"one early stages thought conjectural\"                            \n [550] \"one early stages feeling case\"                                   \n [551] \"one early stages feeling conjectural\"                            \n [552] \"one early stages feeling imperfect\"                              \n [553] \"one early stages case conjectural\"                               \n [554] \"one early stages case imperfect\"                                 \n [555] \"one early stages case farther\"                                   \n [556] \"one early thought feeling case\"                                  \n [557] \"one early thought feeling conjectural\"                           \n [558] \"one early thought feeling imperfect\"                             \n [559] \"one early thought case conjectural\"                              \n [560] \"one early thought case imperfect\"                                \n [561] \"one early thought case farther\"                                  \n [562] \"one early thought conjectural imperfect\"                         \n [563] \"one early thought conjectural farther\"                           \n [564] \"one early thought conjectural discussion\"                        \n [565] \"one early feeling case conjectural\"                              \n [566] \"one early feeling case imperfect\"                                \n [567] \"one early feeling case farther\"                                  \n [568] \"one early feeling conjectural imperfect\"                         \n [569] \"one early feeling conjectural farther\"                           \n [570] \"one early feeling conjectural discussion\"                        \n [571] \"one early feeling imperfect farther\"                             \n [572] \"one early feeling imperfect discussion\"                          \n [573] \"one early feeling imperfect worth\"                               \n [574] \"one stages thought feeling case\"                                 \n [575] \"one stages thought feeling conjectural\"                          \n [576] \"one stages thought feeling imperfect\"                            \n [577] \"one stages thought case conjectural\"                             \n [578] \"one stages thought case imperfect\"                               \n [579] \"one stages thought case farther\"                                 \n [580] \"one stages thought conjectural imperfect\"                        \n [581] \"one stages thought conjectural farther\"                          \n [582] \"one stages thought conjectural discussion\"                       \n [583] \"one stages feeling case conjectural\"                             \n [584] \"one stages feeling case imperfect\"                               \n [585] \"one stages feeling case farther\"                                 \n [586] \"one stages feeling conjectural imperfect\"                        \n [587] \"one stages feeling conjectural farther\"                          \n [588] \"one stages feeling conjectural discussion\"                       \n [589] \"one stages feeling imperfect farther\"                            \n [590] \"one stages feeling imperfect discussion\"                         \n [591] \"one stages feeling imperfect worth\"                              \n [592] \"one stages case conjectural imperfect\"                           \n [593] \"one stages case conjectural farther\"                             \n [594] \"one stages case conjectural discussion\"                          \n [595] \"one stages case imperfect farther\"                               \n [596] \"one stages case imperfect discussion\"                            \n [597] \"one stages case imperfect worth\"                                 \n [598] \"one stages case farther discussion\"                              \n [599] \"one stages case farther worth\"                                   \n [600] \"one stages case farther religion\"                                \n [601] \"knowledge early\"                                                 \n [602] \"knowledge stages\"                                                \n [603] \"knowledge thought\"                                               \n [604] \"knowledge early stages\"                                          \n [605] \"knowledge early thought\"                                         \n [606] \"knowledge early feeling\"                                         \n [607] \"knowledge stages thought\"                                        \n [608] \"knowledge stages feeling\"                                        \n [609] \"knowledge stages case\"                                           \n [610] \"knowledge thought feeling\"                                       \n [611] \"knowledge thought case\"                                          \n [612] \"knowledge thought conjectural\"                                   \n [613] \"knowledge early stages thought\"                                  \n [614] \"knowledge early stages feeling\"                                  \n [615] \"knowledge early stages case\"                                     \n [616] \"knowledge early thought feeling\"                                 \n [617] \"knowledge early thought case\"                                    \n [618] \"knowledge early thought conjectural\"                             \n [619] \"knowledge early feeling case\"                                    \n [620] \"knowledge early feeling conjectural\"                             \n [621] \"knowledge early feeling imperfect\"                               \n [622] \"knowledge stages thought feeling\"                                \n [623] \"knowledge stages thought case\"                                   \n [624] \"knowledge stages thought conjectural\"                            \n [625] \"knowledge stages feeling case\"                                   \n [626] \"knowledge stages feeling conjectural\"                            \n [627] \"knowledge stages feeling imperfect\"                              \n [628] \"knowledge stages case conjectural\"                               \n [629] \"knowledge stages case imperfect\"                                 \n [630] \"knowledge stages case farther\"                                   \n [631] \"knowledge thought feeling case\"                                  \n [632] \"knowledge thought feeling conjectural\"                           \n [633] \"knowledge thought feeling imperfect\"                             \n [634] \"knowledge thought case conjectural\"                              \n [635] \"knowledge thought case imperfect\"                                \n [636] \"knowledge thought case farther\"                                  \n [637] \"knowledge thought conjectural imperfect\"                         \n [638] \"knowledge thought conjectural farther\"                           \n [639] \"knowledge thought conjectural discussion\"                        \n [640] \"knowledge early stages thought feeling\"                          \n [641] \"knowledge early stages thought case\"                             \n [642] \"knowledge early stages thought conjectural\"                      \n [643] \"knowledge early stages feeling case\"                             \n [644] \"knowledge early stages feeling conjectural\"                      \n [645] \"knowledge early stages feeling imperfect\"                        \n [646] \"knowledge early stages case conjectural\"                         \n [647] \"knowledge early stages case imperfect\"                           \n [648] \"knowledge early stages case farther\"                             \n [649] \"knowledge early thought feeling case\"                            \n [650] \"knowledge early thought feeling conjectural\"                     \n [651] \"knowledge early thought feeling imperfect\"                       \n [652] \"knowledge early thought case conjectural\"                        \n [653] \"knowledge early thought case imperfect\"                          \n [654] \"knowledge early thought case farther\"                            \n [655] \"knowledge early thought conjectural imperfect\"                   \n [656] \"knowledge early thought conjectural farther\"                     \n [657] \"knowledge early thought conjectural discussion\"                  \n [658] \"knowledge early feeling case conjectural\"                        \n [659] \"knowledge early feeling case imperfect\"                          \n [660] \"knowledge early feeling case farther\"                            \n [661] \"knowledge early feeling conjectural imperfect\"                   \n [662] \"knowledge early feeling conjectural farther\"                     \n [663] \"knowledge early feeling conjectural discussion\"                  \n [664] \"knowledge early feeling imperfect farther\"                       \n [665] \"knowledge early feeling imperfect discussion\"                    \n [666] \"knowledge early feeling imperfect worth\"                         \n [667] \"knowledge stages thought feeling case\"                           \n [668] \"knowledge stages thought feeling conjectural\"                    \n [669] \"knowledge stages thought feeling imperfect\"                      \n [670] \"knowledge stages thought case conjectural\"                       \n [671] \"knowledge stages thought case imperfect\"                         \n [672] \"knowledge stages thought case farther\"                           \n [673] \"knowledge stages thought conjectural imperfect\"                  \n [674] \"knowledge stages thought conjectural farther\"                    \n [675] \"knowledge stages thought conjectural discussion\"                 \n [676] \"knowledge stages feeling case conjectural\"                       \n [677] \"knowledge stages feeling case imperfect\"                         \n [678] \"knowledge stages feeling case farther\"                           \n [679] \"knowledge stages feeling conjectural imperfect\"                  \n [680] \"knowledge stages feeling conjectural farther\"                    \n [681] \"knowledge stages feeling conjectural discussion\"                 \n [682] \"knowledge stages feeling imperfect farther\"                      \n [683] \"knowledge stages feeling imperfect discussion\"                   \n [684] \"knowledge stages feeling imperfect worth\"                        \n [685] \"knowledge stages case conjectural imperfect\"                     \n [686] \"knowledge stages case conjectural farther\"                       \n [687] \"knowledge stages case conjectural discussion\"                    \n [688] \"knowledge stages case imperfect farther\"                         \n [689] \"knowledge stages case imperfect discussion\"                      \n [690] \"knowledge stages case imperfect worth\"                           \n [691] \"knowledge stages case farther discussion\"                        \n [692] \"knowledge stages case farther worth\"                             \n [693] \"knowledge stages case farther religion\"                          \n [694] \"knowledge thought feeling case conjectural\"                      \n [695] \"knowledge thought feeling case imperfect\"                        \n [696] \"knowledge thought feeling case farther\"                          \n [697] \"knowledge thought feeling conjectural imperfect\"                 \n [698] \"knowledge thought feeling conjectural farther\"                   \n [699] \"knowledge thought feeling conjectural discussion\"                \n [700] \"knowledge thought feeling imperfect farther\"                     \n [701] \"knowledge thought feeling imperfect discussion\"                  \n [702] \"knowledge thought feeling imperfect worth\"                       \n [703] \"knowledge thought case conjectural imperfect\"                    \n [704] \"knowledge thought case conjectural farther\"                      \n [705] \"knowledge thought case conjectural discussion\"                   \n [706] \"knowledge thought case imperfect farther\"                        \n [707] \"knowledge thought case imperfect discussion\"                     \n [708] \"knowledge thought case imperfect worth\"                          \n [709] \"knowledge thought case farther discussion\"                       \n [710] \"knowledge thought case farther worth\"                            \n [711] \"knowledge thought case farther religion\"                         \n [712] \"knowledge thought conjectural imperfect farther\"                 \n [713] \"knowledge thought conjectural imperfect discussion\"              \n [714] \"knowledge thought conjectural imperfect worth\"                   \n [715] \"knowledge thought conjectural farther discussion\"                \n [716] \"knowledge thought conjectural farther worth\"                     \n [717] \"knowledge thought conjectural farther religion\"                  \n [718] \"knowledge thought conjectural discussion worth\"                  \n [719] \"knowledge thought conjectural discussion religion\"               \n [720] \"knowledge thought conjectural discussion therefore\"              \n [721] \"early stages\"                                                    \n [722] \"early thought\"                                                   \n [723] \"early feeling\"                                                   \n [724] \"early stages thought\"                                            \n [725] \"early stages feeling\"                                            \n [726] \"early stages case\"                                               \n [727] \"early thought feeling\"                                           \n [728] \"early thought case\"                                              \n [729] \"early thought conjectural\"                                       \n [730] \"early feeling case\"                                              \n [731] \"early feeling conjectural\"                                       \n [732] \"early feeling imperfect\"                                         \n [733] \"early stages thought feeling\"                                    \n [734] \"early stages thought case\"                                       \n [735] \"early stages thought conjectural\"                                \n [736] \"early stages feeling case\"                                       \n [737] \"early stages feeling conjectural\"                                \n [738] \"early stages feeling imperfect\"                                  \n [739] \"early stages case conjectural\"                                   \n [740] \"early stages case imperfect\"                                     \n [741] \"early stages case farther\"                                       \n [742] \"early thought feeling case\"                                      \n [743] \"early thought feeling conjectural\"                               \n [744] \"early thought feeling imperfect\"                                 \n [745] \"early thought case conjectural\"                                  \n [746] \"early thought case imperfect\"                                    \n [747] \"early thought case farther\"                                      \n [748] \"early thought conjectural imperfect\"                             \n [749] \"early thought conjectural farther\"                               \n [750] \"early thought conjectural discussion\"                            \n [751] \"early feeling case conjectural\"                                  \n [752] \"early feeling case imperfect\"                                    \n [753] \"early feeling case farther\"                                      \n [754] \"early feeling conjectural imperfect\"                             \n [755] \"early feeling conjectural farther\"                               \n [756] \"early feeling conjectural discussion\"                            \n [757] \"early feeling imperfect farther\"                                 \n [758] \"early feeling imperfect discussion\"                              \n [759] \"early feeling imperfect worth\"                                   \n [760] \"early stages thought feeling case\"                               \n [761] \"early stages thought feeling conjectural\"                        \n [762] \"early stages thought feeling imperfect\"                          \n [763] \"early stages thought case conjectural\"                           \n [764] \"early stages thought case imperfect\"                             \n [765] \"early stages thought case farther\"                               \n [766] \"early stages thought conjectural imperfect\"                      \n [767] \"early stages thought conjectural farther\"                        \n [768] \"early stages thought conjectural discussion\"                     \n [769] \"early stages feeling case conjectural\"                           \n [770] \"early stages feeling case imperfect\"                             \n [771] \"early stages feeling case farther\"                               \n [772] \"early stages feeling conjectural imperfect\"                      \n [773] \"early stages feeling conjectural farther\"                        \n [774] \"early stages feeling conjectural discussion\"                     \n [775] \"early stages feeling imperfect farther\"                          \n [776] \"early stages feeling imperfect discussion\"                       \n [777] \"early stages feeling imperfect worth\"                            \n [778] \"early stages case conjectural imperfect\"                         \n [779] \"early stages case conjectural farther\"                           \n [780] \"early stages case conjectural discussion\"                        \n [781] \"early stages case imperfect farther\"                             \n [782] \"early stages case imperfect discussion\"                          \n [783] \"early stages case imperfect worth\"                               \n [784] \"early stages case farther discussion\"                            \n [785] \"early stages case farther worth\"                                 \n [786] \"early stages case farther religion\"                              \n [787] \"early thought feeling case conjectural\"                          \n [788] \"early thought feeling case imperfect\"                            \n [789] \"early thought feeling case farther\"                              \n [790] \"early thought feeling conjectural imperfect\"                     \n [791] \"early thought feeling conjectural farther\"                       \n [792] \"early thought feeling conjectural discussion\"                    \n [793] \"early thought feeling imperfect farther\"                         \n [794] \"early thought feeling imperfect discussion\"                      \n [795] \"early thought feeling imperfect worth\"                           \n [796] \"early thought case conjectural imperfect\"                        \n [797] \"early thought case conjectural farther\"                          \n [798] \"early thought case conjectural discussion\"                       \n [799] \"early thought case imperfect farther\"                            \n [800] \"early thought case imperfect discussion\"                         \n [801] \"early thought case imperfect worth\"                              \n [802] \"early thought case farther discussion\"                           \n [803] \"early thought case farther worth\"                                \n [804] \"early thought case farther religion\"                             \n [805] \"early thought conjectural imperfect farther\"                     \n [806] \"early thought conjectural imperfect discussion\"                  \n [807] \"early thought conjectural imperfect worth\"                       \n [808] \"early thought conjectural farther discussion\"                    \n [809] \"early thought conjectural farther worth\"                         \n [810] \"early thought conjectural farther religion\"                      \n [811] \"early thought conjectural discussion worth\"                      \n [812] \"early thought conjectural discussion religion\"                   \n [813] \"early thought conjectural discussion therefore\"                  \n [814] \"early feeling case conjectural imperfect\"                        \n [815] \"early feeling case conjectural farther\"                          \n [816] \"early feeling case conjectural discussion\"                       \n [817] \"early feeling case imperfect farther\"                            \n [818] \"early feeling case imperfect discussion\"                         \n [819] \"early feeling case imperfect worth\"                              \n [820] \"early feeling case farther discussion\"                           \n [821] \"early feeling case farther worth\"                                \n [822] \"early feeling case farther religion\"                             \n [823] \"early feeling conjectural imperfect farther\"                     \n [824] \"early feeling conjectural imperfect discussion\"                  \n [825] \"early feeling conjectural imperfect worth\"                       \n [826] \"early feeling conjectural farther discussion\"                    \n [827] \"early feeling conjectural farther worth\"                         \n [828] \"early feeling conjectural farther religion\"                      \n [829] \"early feeling conjectural discussion worth\"                      \n [830] \"early feeling conjectural discussion religion\"                   \n [831] \"early feeling conjectural discussion therefore\"                  \n [832] \"early feeling imperfect farther discussion\"                      \n [833] \"early feeling imperfect farther worth\"                           \n [834] \"early feeling imperfect farther religion\"                        \n [835] \"early feeling imperfect discussion worth\"                        \n [836] \"early feeling imperfect discussion religion\"                     \n [837] \"early feeling imperfect discussion therefore\"                    \n [838] \"early feeling imperfect worth religion\"                          \n [839] \"early feeling imperfect worth therefore\"                         \n [840] \"early feeling imperfect worth now\"                               \n [841] \"stages thought\"                                                  \n [842] \"stages feeling\"                                                  \n [843] \"stages case\"                                                     \n [844] \"stages thought feeling\"                                          \n [845] \"stages thought case\"                                             \n [846] \"stages thought conjectural\"                                      \n [847] \"stages feeling case\"                                             \n [848] \"stages feeling conjectural\"                                      \n [849] \"stages feeling imperfect\"                                        \n [850] \"stages case conjectural\"                                         \n [851] \"stages case imperfect\"                                           \n [852] \"stages case farther\"                                             \n [853] \"stages thought feeling case\"                                     \n [854] \"stages thought feeling conjectural\"                              \n [855] \"stages thought feeling imperfect\"                                \n [856] \"stages thought case conjectural\"                                 \n [857] \"stages thought case imperfect\"                                   \n [858] \"stages thought case farther\"                                     \n [859] \"stages thought conjectural imperfect\"                            \n [860] \"stages thought conjectural farther\"                              \n [861] \"stages thought conjectural discussion\"                           \n [862] \"stages feeling case conjectural\"                                 \n [863] \"stages feeling case imperfect\"                                   \n [864] \"stages feeling case farther\"                                     \n [865] \"stages feeling conjectural imperfect\"                            \n [866] \"stages feeling conjectural farther\"                              \n [867] \"stages feeling conjectural discussion\"                           \n [868] \"stages feeling imperfect farther\"                                \n [869] \"stages feeling imperfect discussion\"                             \n [870] \"stages feeling imperfect worth\"                                  \n [871] \"stages case conjectural imperfect\"                               \n [872] \"stages case conjectural farther\"                                 \n [873] \"stages case conjectural discussion\"                              \n [874] \"stages case imperfect farther\"                                   \n [875] \"stages case imperfect discussion\"                                \n [876] \"stages case imperfect worth\"                                     \n [877] \"stages case farther discussion\"                                  \n [878] \"stages case farther worth\"                                       \n [879] \"stages case farther religion\"                                    \n [880] \"stages thought feeling case conjectural\"                         \n [881] \"stages thought feeling case imperfect\"                           \n [882] \"stages thought feeling case farther\"                             \n [883] \"stages thought feeling conjectural imperfect\"                    \n [884] \"stages thought feeling conjectural farther\"                      \n [885] \"stages thought feeling conjectural discussion\"                   \n [886] \"stages thought feeling imperfect farther\"                        \n [887] \"stages thought feeling imperfect discussion\"                     \n [888] \"stages thought feeling imperfect worth\"                          \n [889] \"stages thought case conjectural imperfect\"                       \n [890] \"stages thought case conjectural farther\"                         \n [891] \"stages thought case conjectural discussion\"                      \n [892] \"stages thought case imperfect farther\"                           \n [893] \"stages thought case imperfect discussion\"                        \n [894] \"stages thought case imperfect worth\"                             \n [895] \"stages thought case farther discussion\"                          \n [896] \"stages thought case farther worth\"                               \n [897] \"stages thought case farther religion\"                            \n [898] \"stages thought conjectural imperfect farther\"                    \n [899] \"stages thought conjectural imperfect discussion\"                 \n [900] \"stages thought conjectural imperfect worth\"                      \n [901] \"stages thought conjectural farther discussion\"                   \n [902] \"stages thought conjectural farther worth\"                        \n [903] \"stages thought conjectural farther religion\"                     \n [904] \"stages thought conjectural discussion worth\"                     \n [905] \"stages thought conjectural discussion religion\"                  \n [906] \"stages thought conjectural discussion therefore\"                 \n [907] \"stages feeling case conjectural imperfect\"                       \n [908] \"stages feeling case conjectural farther\"                         \n [909] \"stages feeling case conjectural discussion\"                      \n [910] \"stages feeling case imperfect farther\"                           \n [911] \"stages feeling case imperfect discussion\"                        \n [912] \"stages feeling case imperfect worth\"                             \n [913] \"stages feeling case farther discussion\"                          \n [914] \"stages feeling case farther worth\"                               \n [915] \"stages feeling case farther religion\"                            \n [916] \"stages feeling conjectural imperfect farther\"                    \n [917] \"stages feeling conjectural imperfect discussion\"                 \n [918] \"stages feeling conjectural imperfect worth\"                      \n [919] \"stages feeling conjectural farther discussion\"                   \n [920] \"stages feeling conjectural farther worth\"                        \n [921] \"stages feeling conjectural farther religion\"                     \n [922] \"stages feeling conjectural discussion worth\"                     \n [923] \"stages feeling conjectural discussion religion\"                  \n [924] \"stages feeling conjectural discussion therefore\"                 \n [925] \"stages feeling imperfect farther discussion\"                     \n [926] \"stages feeling imperfect farther worth\"                          \n [927] \"stages feeling imperfect farther religion\"                       \n [928] \"stages feeling imperfect discussion worth\"                       \n [929] \"stages feeling imperfect discussion religion\"                    \n [930] \"stages feeling imperfect discussion therefore\"                   \n [931] \"stages feeling imperfect worth religion\"                         \n [932] \"stages feeling imperfect worth therefore\"                        \n [933] \"stages feeling imperfect worth now\"                              \n [934] \"stages case conjectural imperfect farther\"                       \n [935] \"stages case conjectural imperfect discussion\"                    \n [936] \"stages case conjectural imperfect worth\"                         \n [937] \"stages case conjectural farther discussion\"                      \n [938] \"stages case conjectural farther worth\"                           \n [939] \"stages case conjectural farther religion\"                        \n [940] \"stages case conjectural discussion worth\"                        \n [941] \"stages case conjectural discussion religion\"                     \n [942] \"stages case conjectural discussion therefore\"                    \n [943] \"stages case imperfect farther discussion\"                        \n [944] \"stages case imperfect farther worth\"                             \n [945] \"stages case imperfect farther religion\"                          \n [946] \"stages case imperfect discussion worth\"                          \n [947] \"stages case imperfect discussion religion\"                       \n [948] \"stages case imperfect discussion therefore\"                      \n [949] \"stages case imperfect worth religion\"                            \n [950] \"stages case imperfect worth therefore\"                           \n [951] \"stages case imperfect worth now\"                                 \n [952] \"stages case farther discussion worth\"                            \n [953] \"stages case farther discussion religion\"                         \n [954] \"stages case farther discussion therefore\"                        \n [955] \"stages case farther worth religion\"                              \n [956] \"stages case farther worth therefore\"                             \n [957] \"stages case farther worth now\"                                   \n [958] \"stages case farther religion therefore\"                          \n [959] \"stages case farther religion now\"                                \n [960] \"stages case farther religion ask\"                                \n [961] \"thought feeling\"                                                 \n [962] \"thought case\"                                                    \n [963] \"thought conjectural\"                                             \n [964] \"thought feeling case\"                                            \n [965] \"thought feeling conjectural\"                                     \n [966] \"thought feeling imperfect\"                                       \n [967] \"thought case conjectural\"                                        \n [968] \"thought case imperfect\"                                          \n [969] \"thought case farther\"                                            \n [970] \"thought conjectural imperfect\"                                   \n [971] \"thought conjectural farther\"                                     \n [972] \"thought conjectural discussion\"                                  \n [973] \"thought feeling case conjectural\"                                \n [974] \"thought feeling case imperfect\"                                  \n [975] \"thought feeling case farther\"                                    \n [976] \"thought feeling conjectural imperfect\"                           \n [977] \"thought feeling conjectural farther\"                             \n [978] \"thought feeling conjectural discussion\"                          \n [979] \"thought feeling imperfect farther\"                               \n [980] \"thought feeling imperfect discussion\"                            \n [981] \"thought feeling imperfect worth\"                                 \n [982] \"thought case conjectural imperfect\"                              \n [983] \"thought case conjectural farther\"                                \n [984] \"thought case conjectural discussion\"                             \n [985] \"thought case imperfect farther\"                                  \n [986] \"thought case imperfect discussion\"                               \n [987] \"thought case imperfect worth\"                                    \n [988] \"thought case farther discussion\"                                 \n [989] \"thought case farther worth\"                                      \n [990] \"thought case farther religion\"                                   \n [991] \"thought conjectural imperfect farther\"                           \n [992] \"thought conjectural imperfect discussion\"                        \n [993] \"thought conjectural imperfect worth\"                             \n [994] \"thought conjectural farther discussion\"                          \n [995] \"thought conjectural farther worth\"                               \n [996] \"thought conjectural farther religion\"                            \n [997] \"thought conjectural discussion worth\"                            \n [998] \"thought conjectural discussion religion\"                         \n [999] \"thought conjectural discussion therefore\"                        \n[1000] \"thought feeling case conjectural imperfect\"                      \n[1001] \"thought feeling case conjectural farther\"                        \n[1002] \"thought feeling case conjectural discussion\"                     \n[1003] \"thought feeling case imperfect farther\"                          \n[1004] \"thought feeling case imperfect discussion\"                       \n[1005] \"thought feeling case imperfect worth\"                            \n[1006] \"thought feeling case farther discussion\"                         \n[1007] \"thought feeling case farther worth\"                              \n[1008] \"thought feeling case farther religion\"                           \n[1009] \"thought feeling conjectural imperfect farther\"                   \n[1010] \"thought feeling conjectural imperfect discussion\"                \n[1011] \"thought feeling conjectural imperfect worth\"                     \n[1012] \"thought feeling conjectural farther discussion\"                  \n[1013] \"thought feeling conjectural farther worth\"                       \n[1014] \"thought feeling conjectural farther religion\"                    \n[1015] \"thought feeling conjectural discussion worth\"                    \n[1016] \"thought feeling conjectural discussion religion\"                 \n[1017] \"thought feeling conjectural discussion therefore\"                \n[1018] \"thought feeling imperfect farther discussion\"                    \n[1019] \"thought feeling imperfect farther worth\"                         \n[1020] \"thought feeling imperfect farther religion\"                      \n[1021] \"thought feeling imperfect discussion worth\"                      \n[1022] \"thought feeling imperfect discussion religion\"                   \n[1023] \"thought feeling imperfect discussion therefore\"                  \n[1024] \"thought feeling imperfect worth religion\"                        \n[1025] \"thought feeling imperfect worth therefore\"                       \n[1026] \"thought feeling imperfect worth now\"                             \n[1027] \"thought case conjectural imperfect farther\"                      \n[1028] \"thought case conjectural imperfect discussion\"                   \n[1029] \"thought case conjectural imperfect worth\"                        \n[1030] \"thought case conjectural farther discussion\"                     \n[1031] \"thought case conjectural farther worth\"                          \n[1032] \"thought case conjectural farther religion\"                       \n[1033] \"thought case conjectural discussion worth\"                       \n[1034] \"thought case conjectural discussion religion\"                    \n[1035] \"thought case conjectural discussion therefore\"                   \n[1036] \"thought case imperfect farther discussion\"                       \n[1037] \"thought case imperfect farther worth\"                            \n[1038] \"thought case imperfect farther religion\"                         \n[1039] \"thought case imperfect discussion worth\"                         \n[1040] \"thought case imperfect discussion religion\"                      \n[1041] \"thought case imperfect discussion therefore\"                     \n[1042] \"thought case imperfect worth religion\"                           \n[1043] \"thought case imperfect worth therefore\"                          \n[1044] \"thought case imperfect worth now\"                                \n[1045] \"thought case farther discussion worth\"                           \n[1046] \"thought case farther discussion religion\"                        \n[1047] \"thought case farther discussion therefore\"                       \n[1048] \"thought case farther worth religion\"                             \n[1049] \"thought case farther worth therefore\"                            \n[1050] \"thought case farther worth now\"                                  \n[1051] \"thought case farther religion therefore\"                         \n[1052] \"thought case farther religion now\"                               \n[1053] \"thought case farther religion ask\"                               \n[1054] \"thought conjectural imperfect farther discussion\"                \n[1055] \"thought conjectural imperfect farther worth\"                     \n[1056] \"thought conjectural imperfect farther religion\"                  \n[1057] \"thought conjectural imperfect discussion worth\"                  \n[1058] \"thought conjectural imperfect discussion religion\"               \n[1059] \"thought conjectural imperfect discussion therefore\"              \n[1060] \"thought conjectural imperfect worth religion\"                    \n[1061] \"thought conjectural imperfect worth therefore\"                   \n[1062] \"thought conjectural imperfect worth now\"                         \n[1063] \"thought conjectural farther discussion worth\"                    \n[1064] \"thought conjectural farther discussion religion\"                 \n[1065] \"thought conjectural farther discussion therefore\"                \n[1066] \"thought conjectural farther worth religion\"                      \n[1067] \"thought conjectural farther worth therefore\"                     \n[1068] \"thought conjectural farther worth now\"                           \n[1069] \"thought conjectural farther religion therefore\"                  \n[1070] \"thought conjectural farther religion now\"                        \n[1071] \"thought conjectural farther religion ask\"                        \n[1072] \"thought conjectural discussion worth religion\"                   \n[1073] \"thought conjectural discussion worth therefore\"                  \n[1074] \"thought conjectural discussion worth now\"                        \n[1075] \"thought conjectural discussion religion therefore\"               \n[1076] \"thought conjectural discussion religion now\"                     \n[1077] \"thought conjectural discussion religion ask\"                     \n[1078] \"thought conjectural discussion therefore now\"                    \n[1079] \"thought conjectural discussion therefore ask\"                    \n[1080] \"thought conjectural discussion therefore arbitrarily\"            \n[1081] \"feeling case\"                                                    \n[1082] \"feeling conjectural\"                                             \n[1083] \"feeling imperfect\"                                               \n[1084] \"feeling case conjectural\"                                        \n[1085] \"feeling case imperfect\"                                          \n[1086] \"feeling case farther\"                                            \n[1087] \"feeling conjectural imperfect\"                                   \n[1088] \"feeling conjectural farther\"                                     \n[1089] \"feeling conjectural discussion\"                                  \n[1090] \"feeling imperfect farther\"                                       \n[1091] \"feeling imperfect discussion\"                                    \n[1092] \"feeling imperfect worth\"                                         \n[1093] \"feeling case conjectural imperfect\"                              \n[1094] \"feeling case conjectural farther\"                                \n[1095] \"feeling case conjectural discussion\"                             \n[1096] \"feeling case imperfect farther\"                                  \n[1097] \"feeling case imperfect discussion\"                               \n[1098] \"feeling case imperfect worth\"                                    \n[1099] \"feeling case farther discussion\"                                 \n[1100] \"feeling case farther worth\"                                      \n[1101] \"feeling case farther religion\"                                   \n[1102] \"feeling conjectural imperfect farther\"                           \n[1103] \"feeling conjectural imperfect discussion\"                        \n[1104] \"feeling conjectural imperfect worth\"                             \n[1105] \"feeling conjectural farther discussion\"                          \n[1106] \"feeling conjectural farther worth\"                               \n[1107] \"feeling conjectural farther religion\"                            \n[1108] \"feeling conjectural discussion worth\"                            \n[1109] \"feeling conjectural discussion religion\"                         \n[1110] \"feeling conjectural discussion therefore\"                        \n[1111] \"feeling imperfect farther discussion\"                            \n[1112] \"feeling imperfect farther worth\"                                 \n[1113] \"feeling imperfect farther religion\"                              \n[1114] \"feeling imperfect discussion worth\"                              \n[1115] \"feeling imperfect discussion religion\"                           \n[1116] \"feeling imperfect discussion therefore\"                          \n[1117] \"feeling imperfect worth religion\"                                \n[1118] \"feeling imperfect worth therefore\"                               \n[1119] \"feeling imperfect worth now\"                                     \n[1120] \"feeling case conjectural imperfect farther\"                      \n[1121] \"feeling case conjectural imperfect discussion\"                   \n[1122] \"feeling case conjectural imperfect worth\"                        \n[1123] \"feeling case conjectural farther discussion\"                     \n[1124] \"feeling case conjectural farther worth\"                          \n[1125] \"feeling case conjectural farther religion\"                       \n[1126] \"feeling case conjectural discussion worth\"                       \n[1127] \"feeling case conjectural discussion religion\"                    \n[1128] \"feeling case conjectural discussion therefore\"                   \n[1129] \"feeling case imperfect farther discussion\"                       \n[1130] \"feeling case imperfect farther worth\"                            \n[1131] \"feeling case imperfect farther religion\"                         \n[1132] \"feeling case imperfect discussion worth\"                         \n[1133] \"feeling case imperfect discussion religion\"                      \n[1134] \"feeling case imperfect discussion therefore\"                     \n[1135] \"feeling case imperfect worth religion\"                           \n[1136] \"feeling case imperfect worth therefore\"                          \n[1137] \"feeling case imperfect worth now\"                                \n[1138] \"feeling case farther discussion worth\"                           \n[1139] \"feeling case farther discussion religion\"                        \n[1140] \"feeling case farther discussion therefore\"                       \n[1141] \"feeling case farther worth religion\"                             \n[1142] \"feeling case farther worth therefore\"                            \n[1143] \"feeling case farther worth now\"                                  \n[1144] \"feeling case farther religion therefore\"                         \n[1145] \"feeling case farther religion now\"                               \n[1146] \"feeling case farther religion ask\"                               \n[1147] \"feeling conjectural imperfect farther discussion\"                \n[1148] \"feeling conjectural imperfect farther worth\"                     \n[1149] \"feeling conjectural imperfect farther religion\"                  \n[1150] \"feeling conjectural imperfect discussion worth\"                  \n[1151] \"feeling conjectural imperfect discussion religion\"               \n[1152] \"feeling conjectural imperfect discussion therefore\"              \n[1153] \"feeling conjectural imperfect worth religion\"                    \n[1154] \"feeling conjectural imperfect worth therefore\"                   \n[1155] \"feeling conjectural imperfect worth now\"                         \n[1156] \"feeling conjectural farther discussion worth\"                    \n[1157] \"feeling conjectural farther discussion religion\"                 \n[1158] \"feeling conjectural farther discussion therefore\"                \n[1159] \"feeling conjectural farther worth religion\"                      \n[1160] \"feeling conjectural farther worth therefore\"                     \n[1161] \"feeling conjectural farther worth now\"                           \n[1162] \"feeling conjectural farther religion therefore\"                  \n[1163] \"feeling conjectural farther religion now\"                        \n[1164] \"feeling conjectural farther religion ask\"                        \n[1165] \"feeling conjectural discussion worth religion\"                   \n[1166] \"feeling conjectural discussion worth therefore\"                  \n[1167] \"feeling conjectural discussion worth now\"                        \n[1168] \"feeling conjectural discussion religion therefore\"               \n[1169] \"feeling conjectural discussion religion now\"                     \n[1170] \"feeling conjectural discussion religion ask\"                     \n[1171] \"feeling conjectural discussion therefore now\"                    \n[1172] \"feeling conjectural discussion therefore ask\"                    \n[1173] \"feeling conjectural discussion therefore arbitrarily\"            \n[1174] \"feeling imperfect farther discussion worth\"                      \n[1175] \"feeling imperfect farther discussion religion\"                   \n[1176] \"feeling imperfect farther discussion therefore\"                  \n[1177] \"feeling imperfect farther worth religion\"                        \n[1178] \"feeling imperfect farther worth therefore\"                       \n[1179] \"feeling imperfect farther worth now\"                             \n[1180] \"feeling imperfect farther religion therefore\"                    \n[1181] \"feeling imperfect farther religion now\"                          \n[1182] \"feeling imperfect farther religion ask\"                          \n[1183] \"feeling imperfect discussion worth religion\"                     \n[1184] \"feeling imperfect discussion worth therefore\"                    \n[1185] \"feeling imperfect discussion worth now\"                          \n[1186] \"feeling imperfect discussion religion therefore\"                 \n[1187] \"feeling imperfect discussion religion now\"                       \n[1188] \"feeling imperfect discussion religion ask\"                       \n[1189] \"feeling imperfect discussion therefore now\"                      \n[1190] \"feeling imperfect discussion therefore ask\"                      \n[1191] \"feeling imperfect discussion therefore arbitrarily\"              \n[1192] \"feeling imperfect worth religion therefore\"                      \n[1193] \"feeling imperfect worth religion now\"                            \n[1194] \"feeling imperfect worth religion ask\"                            \n[1195] \"feeling imperfect worth therefore now\"                           \n[1196] \"feeling imperfect worth therefore ask\"                           \n[1197] \"feeling imperfect worth therefore arbitrarily\"                   \n[1198] \"feeling imperfect worth now ask\"                                 \n[1199] \"feeling imperfect worth now arbitrarily\"                         \n[1200] \"feeling imperfect worth now take\"                                \n[1201] \"case conjectural\"                                                \n[1202] \"case imperfect\"                                                  \n[1203] \"case farther\"                                                    \n[1204] \"case conjectural imperfect\"                                      \n[1205] \"case conjectural farther\"                                        \n[1206] \"case conjectural discussion\"                                     \n[1207] \"case imperfect farther\"                                          \n[1208] \"case imperfect discussion\"                                       \n[1209] \"case imperfect worth\"                                            \n[1210] \"case farther discussion\"                                         \n[1211] \"case farther worth\"                                              \n[1212] \"case farther religion\"                                           \n[1213] \"case conjectural imperfect farther\"                              \n[1214] \"case conjectural imperfect discussion\"                           \n[1215] \"case conjectural imperfect worth\"                                \n[1216] \"case conjectural farther discussion\"                             \n[1217] \"case conjectural farther worth\"                                  \n[1218] \"case conjectural farther religion\"                               \n[1219] \"case conjectural discussion worth\"                               \n[1220] \"case conjectural discussion religion\"                            \n[1221] \"case conjectural discussion therefore\"                           \n[1222] \"case imperfect farther discussion\"                               \n[1223] \"case imperfect farther worth\"                                    \n[1224] \"case imperfect farther religion\"                                 \n[1225] \"case imperfect discussion worth\"                                 \n[1226] \"case imperfect discussion religion\"                              \n[1227] \"case imperfect discussion therefore\"                             \n[1228] \"case imperfect worth religion\"                                   \n[1229] \"case imperfect worth therefore\"                                  \n[1230] \"case imperfect worth now\"                                        \n[1231] \"case farther discussion worth\"                                   \n[1232] \"case farther discussion religion\"                                \n[1233] \"case farther discussion therefore\"                               \n[1234] \"case farther worth religion\"                                     \n[1235] \"case farther worth therefore\"                                    \n[1236] \"case farther worth now\"                                          \n[1237] \"case farther religion therefore\"                                 \n[1238] \"case farther religion now\"                                       \n[1239] \"case farther religion ask\"                                       \n[1240] \"case conjectural imperfect farther discussion\"                   \n[1241] \"case conjectural imperfect farther worth\"                        \n[1242] \"case conjectural imperfect farther religion\"                     \n[1243] \"case conjectural imperfect discussion worth\"                     \n[1244] \"case conjectural imperfect discussion religion\"                  \n[1245] \"case conjectural imperfect discussion therefore\"                 \n[1246] \"case conjectural imperfect worth religion\"                       \n[1247] \"case conjectural imperfect worth therefore\"                      \n[1248] \"case conjectural imperfect worth now\"                            \n[1249] \"case conjectural farther discussion worth\"                       \n[1250] \"case conjectural farther discussion religion\"                    \n[1251] \"case conjectural farther discussion therefore\"                   \n[1252] \"case conjectural farther worth religion\"                         \n[1253] \"case conjectural farther worth therefore\"                        \n[1254] \"case conjectural farther worth now\"                              \n[1255] \"case conjectural farther religion therefore\"                     \n[1256] \"case conjectural farther religion now\"                           \n[1257] \"case conjectural farther religion ask\"                           \n[1258] \"case conjectural discussion worth religion\"                      \n[1259] \"case conjectural discussion worth therefore\"                     \n[1260] \"case conjectural discussion worth now\"                           \n[1261] \"case conjectural discussion religion therefore\"                  \n[1262] \"case conjectural discussion religion now\"                        \n[1263] \"case conjectural discussion religion ask\"                        \n[1264] \"case conjectural discussion therefore now\"                       \n[1265] \"case conjectural discussion therefore ask\"                       \n[1266] \"case conjectural discussion therefore arbitrarily\"               \n[1267] \"case imperfect farther discussion worth\"                         \n[1268] \"case imperfect farther discussion religion\"                      \n[1269] \"case imperfect farther discussion therefore\"                     \n[1270] \"case imperfect farther worth religion\"                           \n[1271] \"case imperfect farther worth therefore\"                          \n[1272] \"case imperfect farther worth now\"                                \n[1273] \"case imperfect farther religion therefore\"                       \n[1274] \"case imperfect farther religion now\"                             \n[1275] \"case imperfect farther religion ask\"                             \n[1276] \"case imperfect discussion worth religion\"                        \n[1277] \"case imperfect discussion worth therefore\"                       \n[1278] \"case imperfect discussion worth now\"                             \n[1279] \"case imperfect discussion religion therefore\"                    \n[1280] \"case imperfect discussion religion now\"                          \n[1281] \"case imperfect discussion religion ask\"                          \n[1282] \"case imperfect discussion therefore now\"                         \n[1283] \"case imperfect discussion therefore ask\"                         \n[1284] \"case imperfect discussion therefore arbitrarily\"                 \n[1285] \"case imperfect worth religion therefore\"                         \n[1286] \"case imperfect worth religion now\"                               \n[1287] \"case imperfect worth religion ask\"                               \n[1288] \"case imperfect worth therefore now\"                              \n[1289] \"case imperfect worth therefore ask\"                              \n[1290] \"case imperfect worth therefore arbitrarily\"                      \n[1291] \"case imperfect worth now ask\"                                    \n[1292] \"case imperfect worth now arbitrarily\"                            \n[1293] \"case imperfect worth now take\"                                   \n[1294] \"case farther discussion worth religion\"                          \n[1295] \"case farther discussion worth therefore\"                         \n[1296] \"case farther discussion worth now\"                               \n[1297] \"case farther discussion religion therefore\"                      \n[1298] \"case farther discussion religion now\"                            \n[1299] \"case farther discussion religion ask\"                            \n[1300] \"case farther discussion therefore now\"                           \n[1301] \"case farther discussion therefore ask\"                           \n[1302] \"case farther discussion therefore arbitrarily\"                   \n[1303] \"case farther worth religion therefore\"                           \n[1304] \"case farther worth religion now\"                                 \n[1305] \"case farther worth religion ask\"                                 \n[1306] \"case farther worth therefore now\"                                \n[1307] \"case farther worth therefore ask\"                                \n[1308] \"case farther worth therefore arbitrarily\"                        \n[1309] \"case farther worth now ask\"                                      \n[1310] \"case farther worth now arbitrarily\"                              \n[1311] \"case farther worth now take\"                                     \n[1312] \"case farther religion therefore now\"                             \n[1313] \"case farther religion therefore ask\"                             \n[1314] \"case farther religion therefore arbitrarily\"                     \n[1315] \"case farther religion now ask\"                                   \n[1316] \"case farther religion now arbitrarily\"                           \n[1317] \"case farther religion now take\"                                  \n[1318] \"case farther religion ask arbitrarily\"                           \n[1319] \"case farther religion ask take\"                                  \n[1320] \"case farther religion ask shall\"                                 \n[1321] \"conjectural imperfect\"                                           \n[1322] \"conjectural farther\"                                             \n[1323] \"conjectural discussion\"                                          \n[1324] \"conjectural imperfect farther\"                                   \n[1325] \"conjectural imperfect discussion\"                                \n[1326] \"conjectural imperfect worth\"                                     \n[1327] \"conjectural farther discussion\"                                  \n[1328] \"conjectural farther worth\"                                       \n[1329] \"conjectural farther religion\"                                    \n[1330] \"conjectural discussion worth\"                                    \n[1331] \"conjectural discussion religion\"                                 \n[1332] \"conjectural discussion therefore\"                                \n[1333] \"conjectural imperfect farther discussion\"                        \n[1334] \"conjectural imperfect farther worth\"                             \n[1335] \"conjectural imperfect farther religion\"                          \n[1336] \"conjectural imperfect discussion worth\"                          \n[1337] \"conjectural imperfect discussion religion\"                       \n[1338] \"conjectural imperfect discussion therefore\"                      \n[1339] \"conjectural imperfect worth religion\"                            \n[1340] \"conjectural imperfect worth therefore\"                           \n[1341] \"conjectural imperfect worth now\"                                 \n[1342] \"conjectural farther discussion worth\"                            \n[1343] \"conjectural farther discussion religion\"                         \n[1344] \"conjectural farther discussion therefore\"                        \n[1345] \"conjectural farther worth religion\"                              \n[1346] \"conjectural farther worth therefore\"                             \n[1347] \"conjectural farther worth now\"                                   \n[1348] \"conjectural farther religion therefore\"                          \n[1349] \"conjectural farther religion now\"                                \n[1350] \"conjectural farther religion ask\"                                \n[1351] \"conjectural discussion worth religion\"                           \n[1352] \"conjectural discussion worth therefore\"                          \n[1353] \"conjectural discussion worth now\"                                \n[1354] \"conjectural discussion religion therefore\"                       \n[1355] \"conjectural discussion religion now\"                             \n[1356] \"conjectural discussion religion ask\"                             \n[1357] \"conjectural discussion therefore now\"                            \n[1358] \"conjectural discussion therefore ask\"                            \n[1359] \"conjectural discussion therefore arbitrarily\"                    \n[1360] \"conjectural imperfect farther discussion worth\"                  \n[1361] \"conjectural imperfect farther discussion religion\"               \n[1362] \"conjectural imperfect farther discussion therefore\"              \n[1363] \"conjectural imperfect farther worth religion\"                    \n[1364] \"conjectural imperfect farther worth therefore\"                   \n[1365] \"conjectural imperfect farther worth now\"                         \n[1366] \"conjectural imperfect farther religion therefore\"                \n[1367] \"conjectural imperfect farther religion now\"                      \n[1368] \"conjectural imperfect farther religion ask\"                      \n[1369] \"conjectural imperfect discussion worth religion\"                 \n[1370] \"conjectural imperfect discussion worth therefore\"                \n[1371] \"conjectural imperfect discussion worth now\"                      \n[1372] \"conjectural imperfect discussion religion therefore\"             \n[1373] \"conjectural imperfect discussion religion now\"                   \n[1374] \"conjectural imperfect discussion religion ask\"                   \n[1375] \"conjectural imperfect discussion therefore now\"                  \n[1376] \"conjectural imperfect discussion therefore ask\"                  \n[1377] \"conjectural imperfect discussion therefore arbitrarily\"          \n[1378] \"conjectural imperfect worth religion therefore\"                  \n[1379] \"conjectural imperfect worth religion now\"                        \n[1380] \"conjectural imperfect worth religion ask\"                        \n[1381] \"conjectural imperfect worth therefore now\"                       \n[1382] \"conjectural imperfect worth therefore ask\"                       \n[1383] \"conjectural imperfect worth therefore arbitrarily\"               \n[1384] \"conjectural imperfect worth now ask\"                             \n[1385] \"conjectural imperfect worth now arbitrarily\"                     \n[1386] \"conjectural imperfect worth now take\"                            \n[1387] \"conjectural farther discussion worth religion\"                   \n[1388] \"conjectural farther discussion worth therefore\"                  \n[1389] \"conjectural farther discussion worth now\"                        \n[1390] \"conjectural farther discussion religion therefore\"               \n[1391] \"conjectural farther discussion religion now\"                     \n[1392] \"conjectural farther discussion religion ask\"                     \n[1393] \"conjectural farther discussion therefore now\"                    \n[1394] \"conjectural farther discussion therefore ask\"                    \n[1395] \"conjectural farther discussion therefore arbitrarily\"            \n[1396] \"conjectural farther worth religion therefore\"                    \n[1397] \"conjectural farther worth religion now\"                          \n[1398] \"conjectural farther worth religion ask\"                          \n[1399] \"conjectural farther worth therefore now\"                         \n[1400] \"conjectural farther worth therefore ask\"                         \n[1401] \"conjectural farther worth therefore arbitrarily\"                 \n[1402] \"conjectural farther worth now ask\"                               \n[1403] \"conjectural farther worth now arbitrarily\"                       \n[1404] \"conjectural farther worth now take\"                              \n[1405] \"conjectural farther religion therefore now\"                      \n[1406] \"conjectural farther religion therefore ask\"                      \n[1407] \"conjectural farther religion therefore arbitrarily\"              \n[1408] \"conjectural farther religion now ask\"                            \n[1409] \"conjectural farther religion now arbitrarily\"                    \n[1410] \"conjectural farther religion now take\"                           \n[1411] \"conjectural farther religion ask arbitrarily\"                    \n[1412] \"conjectural farther religion ask take\"                           \n[1413] \"conjectural farther religion ask shall\"                          \n[1414] \"conjectural discussion worth religion therefore\"                 \n[1415] \"conjectural discussion worth religion now\"                       \n[1416] \"conjectural discussion worth religion ask\"                       \n[1417] \"conjectural discussion worth therefore now\"                      \n[1418] \"conjectural discussion worth therefore ask\"                      \n[1419] \"conjectural discussion worth therefore arbitrarily\"              \n[1420] \"conjectural discussion worth now ask\"                            \n[1421] \"conjectural discussion worth now arbitrarily\"                    \n[1422] \"conjectural discussion worth now take\"                           \n[1423] \"conjectural discussion religion therefore now\"                   \n[1424] \"conjectural discussion religion therefore ask\"                   \n[1425] \"conjectural discussion religion therefore arbitrarily\"           \n[1426] \"conjectural discussion religion now ask\"                         \n[1427] \"conjectural discussion religion now arbitrarily\"                 \n[1428] \"conjectural discussion religion now take\"                        \n[1429] \"conjectural discussion religion ask arbitrarily\"                 \n[1430] \"conjectural discussion religion ask take\"                        \n[1431] \"conjectural discussion religion ask shall\"                       \n[1432] \"conjectural discussion therefore now ask\"                        \n[1433] \"conjectural discussion therefore now arbitrarily\"                \n[1434] \"conjectural discussion therefore now take\"                       \n[1435] \"conjectural discussion therefore ask arbitrarily\"                \n[1436] \"conjectural discussion therefore ask take\"                       \n[1437] \"conjectural discussion therefore ask shall\"                      \n[1438] \"conjectural discussion therefore arbitrarily take\"               \n[1439] \"conjectural discussion therefore arbitrarily shall\"              \n[1440] \"conjectural discussion therefore arbitrarily mean\"               \n[1441] \"imperfect farther\"                                               \n[1442] \"imperfect discussion\"                                            \n[1443] \"imperfect worth\"                                                 \n[1444] \"imperfect farther discussion\"                                    \n[1445] \"imperfect farther worth\"                                         \n[1446] \"imperfect farther religion\"                                      \n[1447] \"imperfect discussion worth\"                                      \n[1448] \"imperfect discussion religion\"                                   \n[1449] \"imperfect discussion therefore\"                                  \n[1450] \"imperfect worth religion\"                                        \n[1451] \"imperfect worth therefore\"                                       \n[1452] \"imperfect worth now\"                                             \n[1453] \"imperfect farther discussion worth\"                              \n[1454] \"imperfect farther discussion religion\"                           \n[1455] \"imperfect farther discussion therefore\"                          \n[1456] \"imperfect farther worth religion\"                                \n[1457] \"imperfect farther worth therefore\"                               \n[1458] \"imperfect farther worth now\"                                     \n[1459] \"imperfect farther religion therefore\"                            \n[1460] \"imperfect farther religion now\"                                  \n[1461] \"imperfect farther religion ask\"                                  \n[1462] \"imperfect discussion worth religion\"                             \n[1463] \"imperfect discussion worth therefore\"                            \n[1464] \"imperfect discussion worth now\"                                  \n[1465] \"imperfect discussion religion therefore\"                         \n[1466] \"imperfect discussion religion now\"                               \n[1467] \"imperfect discussion religion ask\"                               \n[1468] \"imperfect discussion therefore now\"                              \n[1469] \"imperfect discussion therefore ask\"                              \n[1470] \"imperfect discussion therefore arbitrarily\"                      \n[1471] \"imperfect worth religion therefore\"                              \n[1472] \"imperfect worth religion now\"                                    \n[1473] \"imperfect worth religion ask\"                                    \n[1474] \"imperfect worth therefore now\"                                   \n[1475] \"imperfect worth therefore ask\"                                   \n[1476] \"imperfect worth therefore arbitrarily\"                           \n[1477] \"imperfect worth now ask\"                                         \n[1478] \"imperfect worth now arbitrarily\"                                 \n[1479] \"imperfect worth now take\"                                        \n[1480] \"imperfect farther discussion worth religion\"                     \n[1481] \"imperfect farther discussion worth therefore\"                    \n[1482] \"imperfect farther discussion worth now\"                          \n[1483] \"imperfect farther discussion religion therefore\"                 \n[1484] \"imperfect farther discussion religion now\"                       \n[1485] \"imperfect farther discussion religion ask\"                       \n[1486] \"imperfect farther discussion therefore now\"                      \n[1487] \"imperfect farther discussion therefore ask\"                      \n[1488] \"imperfect farther discussion therefore arbitrarily\"              \n[1489] \"imperfect farther worth religion therefore\"                      \n[1490] \"imperfect farther worth religion now\"                            \n[1491] \"imperfect farther worth religion ask\"                            \n[1492] \"imperfect farther worth therefore now\"                           \n[1493] \"imperfect farther worth therefore ask\"                           \n[1494] \"imperfect farther worth therefore arbitrarily\"                   \n[1495] \"imperfect farther worth now ask\"                                 \n[1496] \"imperfect farther worth now arbitrarily\"                         \n[1497] \"imperfect farther worth now take\"                                \n[1498] \"imperfect farther religion therefore now\"                        \n[1499] \"imperfect farther religion therefore ask\"                        \n[1500] \"imperfect farther religion therefore arbitrarily\"                \n[1501] \"imperfect farther religion now ask\"                              \n[1502] \"imperfect farther religion now arbitrarily\"                      \n[1503] \"imperfect farther religion now take\"                             \n[1504] \"imperfect farther religion ask arbitrarily\"                      \n[1505] \"imperfect farther religion ask take\"                             \n[1506] \"imperfect farther religion ask shall\"                            \n[1507] \"imperfect discussion worth religion therefore\"                   \n[1508] \"imperfect discussion worth religion now\"                         \n[1509] \"imperfect discussion worth religion ask\"                         \n[1510] \"imperfect discussion worth therefore now\"                        \n[1511] \"imperfect discussion worth therefore ask\"                        \n[1512] \"imperfect discussion worth therefore arbitrarily\"                \n[1513] \"imperfect discussion worth now ask\"                              \n[1514] \"imperfect discussion worth now arbitrarily\"                      \n[1515] \"imperfect discussion worth now take\"                             \n[1516] \"imperfect discussion religion therefore now\"                     \n[1517] \"imperfect discussion religion therefore ask\"                     \n[1518] \"imperfect discussion religion therefore arbitrarily\"             \n[1519] \"imperfect discussion religion now ask\"                           \n[1520] \"imperfect discussion religion now arbitrarily\"                   \n[1521] \"imperfect discussion religion now take\"                          \n[1522] \"imperfect discussion religion ask arbitrarily\"                   \n[1523] \"imperfect discussion religion ask take\"                          \n[1524] \"imperfect discussion religion ask shall\"                         \n[1525] \"imperfect discussion therefore now ask\"                          \n[1526] \"imperfect discussion therefore now arbitrarily\"                  \n[1527] \"imperfect discussion therefore now take\"                         \n[1528] \"imperfect discussion therefore ask arbitrarily\"                  \n[1529] \"imperfect discussion therefore ask take\"                         \n[1530] \"imperfect discussion therefore ask shall\"                        \n[1531] \"imperfect discussion therefore arbitrarily take\"                 \n[1532] \"imperfect discussion therefore arbitrarily shall\"                \n[1533] \"imperfect discussion therefore arbitrarily mean\"                 \n[1534] \"imperfect worth religion therefore now\"                          \n[1535] \"imperfect worth religion therefore ask\"                          \n[1536] \"imperfect worth religion therefore arbitrarily\"                  \n[1537] \"imperfect worth religion now ask\"                                \n[1538] \"imperfect worth religion now arbitrarily\"                        \n[1539] \"imperfect worth religion now take\"                               \n[1540] \"imperfect worth religion ask arbitrarily\"                        \n[1541] \"imperfect worth religion ask take\"                               \n[1542] \"imperfect worth religion ask shall\"                              \n[1543] \"imperfect worth therefore now ask\"                               \n[1544] \"imperfect worth therefore now arbitrarily\"                       \n[1545] \"imperfect worth therefore now take\"                              \n[1546] \"imperfect worth therefore ask arbitrarily\"                       \n[1547] \"imperfect worth therefore ask take\"                              \n[1548] \"imperfect worth therefore ask shall\"                             \n[1549] \"imperfect worth therefore arbitrarily take\"                      \n[1550] \"imperfect worth therefore arbitrarily shall\"                     \n[1551] \"imperfect worth therefore arbitrarily mean\"                      \n[1552] \"imperfect worth now ask arbitrarily\"                             \n[1553] \"imperfect worth now ask take\"                                    \n[1554] \"imperfect worth now ask shall\"                                   \n[1555] \"imperfect worth now arbitrarily take\"                            \n[1556] \"imperfect worth now arbitrarily shall\"                           \n[1557] \"imperfect worth now arbitrarily mean\"                            \n[1558] \"imperfect worth now take shall\"                                  \n[1559] \"imperfect worth now take mean\"                                   \n[1560] \"imperfect worth now take us\"                                     \n[1561] \"farther discussion\"                                              \n[1562] \"farther worth\"                                                   \n[1563] \"farther religion\"                                                \n[1564] \"farther discussion worth\"                                        \n[1565] \"farther discussion religion\"                                     \n[1566] \"farther discussion therefore\"                                    \n[1567] \"farther worth religion\"                                          \n[1568] \"farther worth therefore\"                                         \n[1569] \"farther worth now\"                                               \n[1570] \"farther religion therefore\"                                      \n[1571] \"farther religion now\"                                            \n[1572] \"farther religion ask\"                                            \n[1573] \"farther discussion worth religion\"                               \n[1574] \"farther discussion worth therefore\"                              \n[1575] \"farther discussion worth now\"                                    \n[1576] \"farther discussion religion therefore\"                           \n[1577] \"farther discussion religion now\"                                 \n[1578] \"farther discussion religion ask\"                                 \n[1579] \"farther discussion therefore now\"                                \n[1580] \"farther discussion therefore ask\"                                \n[1581] \"farther discussion therefore arbitrarily\"                        \n[1582] \"farther worth religion therefore\"                                \n[1583] \"farther worth religion now\"                                      \n[1584] \"farther worth religion ask\"                                      \n[1585] \"farther worth therefore now\"                                     \n[1586] \"farther worth therefore ask\"                                     \n[1587] \"farther worth therefore arbitrarily\"                             \n[1588] \"farther worth now ask\"                                           \n[1589] \"farther worth now arbitrarily\"                                   \n[1590] \"farther worth now take\"                                          \n[1591] \"farther religion therefore now\"                                  \n[1592] \"farther religion therefore ask\"                                  \n[1593] \"farther religion therefore arbitrarily\"                          \n[1594] \"farther religion now ask\"                                        \n[1595] \"farther religion now arbitrarily\"                                \n[1596] \"farther religion now take\"                                       \n[1597] \"farther religion ask arbitrarily\"                                \n[1598] \"farther religion ask take\"                                       \n[1599] \"farther religion ask shall\"                                      \n[1600] \"farther discussion worth religion therefore\"                     \n[1601] \"farther discussion worth religion now\"                           \n[1602] \"farther discussion worth religion ask\"                           \n[1603] \"farther discussion worth therefore now\"                          \n[1604] \"farther discussion worth therefore ask\"                          \n[1605] \"farther discussion worth therefore arbitrarily\"                  \n[1606] \"farther discussion worth now ask\"                                \n[1607] \"farther discussion worth now arbitrarily\"                        \n[1608] \"farther discussion worth now take\"                               \n[1609] \"farther discussion religion therefore now\"                       \n[1610] \"farther discussion religion therefore ask\"                       \n[1611] \"farther discussion religion therefore arbitrarily\"               \n[1612] \"farther discussion religion now ask\"                             \n[1613] \"farther discussion religion now arbitrarily\"                     \n[1614] \"farther discussion religion now take\"                            \n[1615] \"farther discussion religion ask arbitrarily\"                     \n[1616] \"farther discussion religion ask take\"                            \n[1617] \"farther discussion religion ask shall\"                           \n[1618] \"farther discussion therefore now ask\"                            \n[1619] \"farther discussion therefore now arbitrarily\"                    \n[1620] \"farther discussion therefore now take\"                           \n[1621] \"farther discussion therefore ask arbitrarily\"                    \n[1622] \"farther discussion therefore ask take\"                           \n[1623] \"farther discussion therefore ask shall\"                          \n[1624] \"farther discussion therefore arbitrarily take\"                   \n[1625] \"farther discussion therefore arbitrarily shall\"                  \n[1626] \"farther discussion therefore arbitrarily mean\"                   \n[1627] \"farther worth religion therefore now\"                            \n[1628] \"farther worth religion therefore ask\"                            \n[1629] \"farther worth religion therefore arbitrarily\"                    \n[1630] \"farther worth religion now ask\"                                  \n[1631] \"farther worth religion now arbitrarily\"                          \n[1632] \"farther worth religion now take\"                                 \n[1633] \"farther worth religion ask arbitrarily\"                          \n[1634] \"farther worth religion ask take\"                                 \n[1635] \"farther worth religion ask shall\"                                \n[1636] \"farther worth therefore now ask\"                                 \n[1637] \"farther worth therefore now arbitrarily\"                         \n[1638] \"farther worth therefore now take\"                                \n[1639] \"farther worth therefore ask arbitrarily\"                         \n[1640] \"farther worth therefore ask take\"                                \n[1641] \"farther worth therefore ask shall\"                               \n[1642] \"farther worth therefore arbitrarily take\"                        \n[1643] \"farther worth therefore arbitrarily shall\"                       \n[1644] \"farther worth therefore arbitrarily mean\"                        \n[1645] \"farther worth now ask arbitrarily\"                               \n[1646] \"farther worth now ask take\"                                      \n[1647] \"farther worth now ask shall\"                                     \n[1648] \"farther worth now arbitrarily take\"                              \n[1649] \"farther worth now arbitrarily shall\"                             \n[1650] \"farther worth now arbitrarily mean\"                              \n[1651] \"farther worth now take shall\"                                    \n[1652] \"farther worth now take mean\"                                     \n[1653] \"farther worth now take us\"                                       \n[1654] \"farther religion therefore now ask\"                              \n[1655] \"farther religion therefore now arbitrarily\"                      \n[1656] \"farther religion therefore now take\"                             \n[1657] \"farther religion therefore ask arbitrarily\"                      \n[1658] \"farther religion therefore ask take\"                             \n[1659] \"farther religion therefore ask shall\"                            \n[1660] \"farther religion therefore arbitrarily take\"                     \n[1661] \"farther religion therefore arbitrarily shall\"                    \n[1662] \"farther religion therefore arbitrarily mean\"                     \n[1663] \"farther religion now ask arbitrarily\"                            \n[1664] \"farther religion now ask take\"                                   \n[1665] \"farther religion now ask shall\"                                  \n[1666] \"farther religion now arbitrarily take\"                           \n[1667] \"farther religion now arbitrarily shall\"                          \n[1668] \"farther religion now arbitrarily mean\"                           \n[1669] \"farther religion now take shall\"                                 \n[1670] \"farther religion now take mean\"                                  \n[1671] \"farther religion now take us\"                                    \n[1672] \"farther religion ask arbitrarily take\"                           \n[1673] \"farther religion ask arbitrarily shall\"                          \n[1674] \"farther religion ask arbitrarily mean\"                           \n[1675] \"farther religion ask take shall\"                                 \n[1676] \"farther religion ask take mean\"                                  \n[1677] \"farther religion ask take us\"                                    \n[1678] \"farther religion ask shall mean\"                                 \n[1679] \"farther religion ask shall us\"                                   \n[1680] \"farther religion ask shall _the\"                                 \n[1681] \"discussion worth\"                                                \n[1682] \"discussion religion\"                                             \n[1683] \"discussion therefore\"                                            \n[1684] \"discussion worth religion\"                                       \n[1685] \"discussion worth therefore\"                                      \n[1686] \"discussion worth now\"                                            \n[1687] \"discussion religion therefore\"                                   \n[1688] \"discussion religion now\"                                         \n[1689] \"discussion religion ask\"                                         \n[1690] \"discussion therefore now\"                                        \n[1691] \"discussion therefore ask\"                                        \n[1692] \"discussion therefore arbitrarily\"                                \n[1693] \"discussion worth religion therefore\"                             \n[1694] \"discussion worth religion now\"                                   \n[1695] \"discussion worth religion ask\"                                   \n[1696] \"discussion worth therefore now\"                                  \n[1697] \"discussion worth therefore ask\"                                  \n[1698] \"discussion worth therefore arbitrarily\"                          \n[1699] \"discussion worth now ask\"                                        \n[1700] \"discussion worth now arbitrarily\"                                \n[1701] \"discussion worth now take\"                                       \n[1702] \"discussion religion therefore now\"                               \n[1703] \"discussion religion therefore ask\"                               \n[1704] \"discussion religion therefore arbitrarily\"                       \n[1705] \"discussion religion now ask\"                                     \n[1706] \"discussion religion now arbitrarily\"                             \n[1707] \"discussion religion now take\"                                    \n[1708] \"discussion religion ask arbitrarily\"                             \n[1709] \"discussion religion ask take\"                                    \n[1710] \"discussion religion ask shall\"                                   \n[1711] \"discussion therefore now ask\"                                    \n[1712] \"discussion therefore now arbitrarily\"                            \n[1713] \"discussion therefore now take\"                                   \n[1714] \"discussion therefore ask arbitrarily\"                            \n[1715] \"discussion therefore ask take\"                                   \n[1716] \"discussion therefore ask shall\"                                  \n[1717] \"discussion therefore arbitrarily take\"                           \n[1718] \"discussion therefore arbitrarily shall\"                          \n[1719] \"discussion therefore arbitrarily mean\"                           \n[1720] \"discussion worth religion therefore now\"                         \n[1721] \"discussion worth religion therefore ask\"                         \n[1722] \"discussion worth religion therefore arbitrarily\"                 \n[1723] \"discussion worth religion now ask\"                               \n[1724] \"discussion worth religion now arbitrarily\"                       \n[1725] \"discussion worth religion now take\"                              \n[1726] \"discussion worth religion ask arbitrarily\"                       \n[1727] \"discussion worth religion ask take\"                              \n[1728] \"discussion worth religion ask shall\"                             \n[1729] \"discussion worth therefore now ask\"                              \n[1730] \"discussion worth therefore now arbitrarily\"                      \n[1731] \"discussion worth therefore now take\"                             \n[1732] \"discussion worth therefore ask arbitrarily\"                      \n[1733] \"discussion worth therefore ask take\"                             \n[1734] \"discussion worth therefore ask shall\"                            \n[1735] \"discussion worth therefore arbitrarily take\"                     \n[1736] \"discussion worth therefore arbitrarily shall\"                    \n[1737] \"discussion worth therefore arbitrarily mean\"                     \n[1738] \"discussion worth now ask arbitrarily\"                            \n[1739] \"discussion worth now ask take\"                                   \n[1740] \"discussion worth now ask shall\"                                  \n[1741] \"discussion worth now arbitrarily take\"                           \n[1742] \"discussion worth now arbitrarily shall\"                          \n[1743] \"discussion worth now arbitrarily mean\"                           \n[1744] \"discussion worth now take shall\"                                 \n[1745] \"discussion worth now take mean\"                                  \n[1746] \"discussion worth now take us\"                                    \n[1747] \"discussion religion therefore now ask\"                           \n[1748] \"discussion religion therefore now arbitrarily\"                   \n[1749] \"discussion religion therefore now take\"                          \n[1750] \"discussion religion therefore ask arbitrarily\"                   \n[1751] \"discussion religion therefore ask take\"                          \n[1752] \"discussion religion therefore ask shall\"                         \n[1753] \"discussion religion therefore arbitrarily take\"                  \n[1754] \"discussion religion therefore arbitrarily shall\"                 \n[1755] \"discussion religion therefore arbitrarily mean\"                  \n[1756] \"discussion religion now ask arbitrarily\"                         \n[1757] \"discussion religion now ask take\"                                \n[1758] \"discussion religion now ask shall\"                               \n[1759] \"discussion religion now arbitrarily take\"                        \n[1760] \"discussion religion now arbitrarily shall\"                       \n[1761] \"discussion religion now arbitrarily mean\"                        \n[1762] \"discussion religion now take shall\"                              \n[1763] \"discussion religion now take mean\"                               \n[1764] \"discussion religion now take us\"                                 \n[1765] \"discussion religion ask arbitrarily take\"                        \n[1766] \"discussion religion ask arbitrarily shall\"                       \n[1767] \"discussion religion ask arbitrarily mean\"                        \n[1768] \"discussion religion ask take shall\"                              \n[1769] \"discussion religion ask take mean\"                               \n[1770] \"discussion religion ask take us\"                                 \n[1771] \"discussion religion ask shall mean\"                              \n[1772] \"discussion religion ask shall us\"                                \n[1773] \"discussion religion ask shall _the\"                              \n[1774] \"discussion therefore now ask arbitrarily\"                        \n[1775] \"discussion therefore now ask take\"                               \n[1776] \"discussion therefore now ask shall\"                              \n[1777] \"discussion therefore now arbitrarily take\"                       \n[1778] \"discussion therefore now arbitrarily shall\"                      \n[1779] \"discussion therefore now arbitrarily mean\"                       \n[1780] \"discussion therefore now take shall\"                             \n[1781] \"discussion therefore now take mean\"                              \n[1782] \"discussion therefore now take us\"                                \n[1783] \"discussion therefore ask arbitrarily take\"                       \n[1784] \"discussion therefore ask arbitrarily shall\"                      \n[1785] \"discussion therefore ask arbitrarily mean\"                       \n[1786] \"discussion therefore ask take shall\"                             \n[1787] \"discussion therefore ask take mean\"                              \n[1788] \"discussion therefore ask take us\"                                \n[1789] \"discussion therefore ask shall mean\"                             \n[1790] \"discussion therefore ask shall us\"                               \n[1791] \"discussion therefore ask shall _the\"                             \n[1792] \"discussion therefore arbitrarily take shall\"                     \n[1793] \"discussion therefore arbitrarily take mean\"                      \n[1794] \"discussion therefore arbitrarily take us\"                        \n[1795] \"discussion therefore arbitrarily shall mean\"                     \n[1796] \"discussion therefore arbitrarily shall us\"                       \n[1797] \"discussion therefore arbitrarily shall _the\"                     \n[1798] \"discussion therefore arbitrarily mean us\"                        \n[1799] \"discussion therefore arbitrarily mean _the\"                      \n[1800] \"discussion therefore arbitrarily mean feelings\"                  \n[1801] \"worth religion\"                                                  \n[1802] \"worth therefore\"                                                 \n[1803] \"worth now\"                                                       \n[1804] \"worth religion therefore\"                                        \n[1805] \"worth religion now\"                                              \n[1806] \"worth religion ask\"                                              \n[1807] \"worth therefore now\"                                             \n[1808] \"worth therefore ask\"                                             \n[1809] \"worth therefore arbitrarily\"                                     \n[1810] \"worth now ask\"                                                   \n[1811] \"worth now arbitrarily\"                                           \n[1812] \"worth now take\"                                                  \n[1813] \"worth religion therefore now\"                                    \n[1814] \"worth religion therefore ask\"                                    \n[1815] \"worth religion therefore arbitrarily\"                            \n[1816] \"worth religion now ask\"                                          \n[1817] \"worth religion now arbitrarily\"                                  \n[1818] \"worth religion now take\"                                         \n[1819] \"worth religion ask arbitrarily\"                                  \n[1820] \"worth religion ask take\"                                         \n[1821] \"worth religion ask shall\"                                        \n[1822] \"worth therefore now ask\"                                         \n[1823] \"worth therefore now arbitrarily\"                                 \n[1824] \"worth therefore now take\"                                        \n[1825] \"worth therefore ask arbitrarily\"                                 \n[1826] \"worth therefore ask take\"                                        \n[1827] \"worth therefore ask shall\"                                       \n[1828] \"worth therefore arbitrarily take\"                                \n[1829] \"worth therefore arbitrarily shall\"                               \n[1830] \"worth therefore arbitrarily mean\"                                \n[1831] \"worth now ask arbitrarily\"                                       \n[1832] \"worth now ask take\"                                              \n[1833] \"worth now ask shall\"                                             \n[1834] \"worth now arbitrarily take\"                                      \n[1835] \"worth now arbitrarily shall\"                                     \n[1836] \"worth now arbitrarily mean\"                                      \n[1837] \"worth now take shall\"                                            \n[1838] \"worth now take mean\"                                             \n[1839] \"worth now take us\"                                               \n[1840] \"worth religion therefore now ask\"                                \n[1841] \"worth religion therefore now arbitrarily\"                        \n[1842] \"worth religion therefore now take\"                               \n[1843] \"worth religion therefore ask arbitrarily\"                        \n[1844] \"worth religion therefore ask take\"                               \n[1845] \"worth religion therefore ask shall\"                              \n[1846] \"worth religion therefore arbitrarily take\"                       \n[1847] \"worth religion therefore arbitrarily shall\"                      \n[1848] \"worth religion therefore arbitrarily mean\"                       \n[1849] \"worth religion now ask arbitrarily\"                              \n[1850] \"worth religion now ask take\"                                     \n[1851] \"worth religion now ask shall\"                                    \n[1852] \"worth religion now arbitrarily take\"                             \n[1853] \"worth religion now arbitrarily shall\"                            \n[1854] \"worth religion now arbitrarily mean\"                             \n[1855] \"worth religion now take shall\"                                   \n[1856] \"worth religion now take mean\"                                    \n[1857] \"worth religion now take us\"                                      \n[1858] \"worth religion ask arbitrarily take\"                             \n[1859] \"worth religion ask arbitrarily shall\"                            \n[1860] \"worth religion ask arbitrarily mean\"                             \n[1861] \"worth religion ask take shall\"                                   \n[1862] \"worth religion ask take mean\"                                    \n[1863] \"worth religion ask take us\"                                      \n[1864] \"worth religion ask shall mean\"                                   \n[1865] \"worth religion ask shall us\"                                     \n[1866] \"worth religion ask shall _the\"                                   \n[1867] \"worth therefore now ask arbitrarily\"                             \n[1868] \"worth therefore now ask take\"                                    \n[1869] \"worth therefore now ask shall\"                                   \n[1870] \"worth therefore now arbitrarily take\"                            \n[1871] \"worth therefore now arbitrarily shall\"                           \n[1872] \"worth therefore now arbitrarily mean\"                            \n[1873] \"worth therefore now take shall\"                                  \n[1874] \"worth therefore now take mean\"                                   \n[1875] \"worth therefore now take us\"                                     \n[1876] \"worth therefore ask arbitrarily take\"                            \n[1877] \"worth therefore ask arbitrarily shall\"                           \n[1878] \"worth therefore ask arbitrarily mean\"                            \n[1879] \"worth therefore ask take shall\"                                  \n[1880] \"worth therefore ask take mean\"                                   \n[1881] \"worth therefore ask take us\"                                     \n[1882] \"worth therefore ask shall mean\"                                  \n[1883] \"worth therefore ask shall us\"                                    \n[1884] \"worth therefore ask shall _the\"                                  \n[1885] \"worth therefore arbitrarily take shall\"                          \n[1886] \"worth therefore arbitrarily take mean\"                           \n[1887] \"worth therefore arbitrarily take us\"                             \n[1888] \"worth therefore arbitrarily shall mean\"                          \n[1889] \"worth therefore arbitrarily shall us\"                            \n[1890] \"worth therefore arbitrarily shall _the\"                          \n[1891] \"worth therefore arbitrarily mean us\"                             \n[1892] \"worth therefore arbitrarily mean _the\"                           \n[1893] \"worth therefore arbitrarily mean feelings\"                       \n[1894] \"worth now ask arbitrarily take\"                                  \n[1895] \"worth now ask arbitrarily shall\"                                 \n[1896] \"worth now ask arbitrarily mean\"                                  \n[1897] \"worth now ask take shall\"                                        \n[1898] \"worth now ask take mean\"                                         \n[1899] \"worth now ask take us\"                                           \n[1900] \"worth now ask shall mean\"                                        \n[1901] \"worth now ask shall us\"                                          \n[1902] \"worth now ask shall _the\"                                        \n[1903] \"worth now arbitrarily take shall\"                                \n[1904] \"worth now arbitrarily take mean\"                                 \n[1905] \"worth now arbitrarily take us\"                                   \n[1906] \"worth now arbitrarily shall mean\"                                \n[1907] \"worth now arbitrarily shall us\"                                  \n[1908] \"worth now arbitrarily shall _the\"                                \n[1909] \"worth now arbitrarily mean us\"                                   \n[1910] \"worth now arbitrarily mean _the\"                                 \n[1911] \"worth now arbitrarily mean feelings\"                             \n[1912] \"worth now take shall mean\"                                       \n[1913] \"worth now take shall us\"                                         \n[1914] \"worth now take shall _the\"                                       \n[1915] \"worth now take mean us\"                                          \n[1916] \"worth now take mean _the\"                                        \n[1917] \"worth now take mean feelings\"                                    \n[1918] \"worth now take us _the\"                                          \n[1919] \"worth now take us feelings\"                                      \n[1920] \"worth now take us acts\"                                          \n[1921] \"religion therefore\"                                              \n[1922] \"religion now\"                                                    \n[1923] \"religion ask\"                                                    \n[1924] \"religion therefore now\"                                          \n[1925] \"religion therefore ask\"                                          \n[1926] \"religion therefore arbitrarily\"                                  \n[1927] \"religion now ask\"                                                \n[1928] \"religion now arbitrarily\"                                        \n[1929] \"religion now take\"                                               \n[1930] \"religion ask arbitrarily\"                                        \n[1931] \"religion ask take\"                                               \n[1932] \"religion ask shall\"                                              \n[1933] \"religion therefore now ask\"                                      \n[1934] \"religion therefore now arbitrarily\"                              \n[1935] \"religion therefore now take\"                                     \n[1936] \"religion therefore ask arbitrarily\"                              \n[1937] \"religion therefore ask take\"                                     \n[1938] \"religion therefore ask shall\"                                    \n[1939] \"religion therefore arbitrarily take\"                             \n[1940] \"religion therefore arbitrarily shall\"                            \n[1941] \"religion therefore arbitrarily mean\"                             \n[1942] \"religion now ask arbitrarily\"                                    \n[1943] \"religion now ask take\"                                           \n[1944] \"religion now ask shall\"                                          \n[1945] \"religion now arbitrarily take\"                                   \n[1946] \"religion now arbitrarily shall\"                                  \n[1947] \"religion now arbitrarily mean\"                                   \n[1948] \"religion now take shall\"                                         \n[1949] \"religion now take mean\"                                          \n[1950] \"religion now take us\"                                            \n[1951] \"religion ask arbitrarily take\"                                   \n[1952] \"religion ask arbitrarily shall\"                                  \n[1953] \"religion ask arbitrarily mean\"                                   \n[1954] \"religion ask take shall\"                                         \n[1955] \"religion ask take mean\"                                          \n[1956] \"religion ask take us\"                                            \n[1957] \"religion ask shall mean\"                                         \n[1958] \"religion ask shall us\"                                           \n[1959] \"religion ask shall _the\"                                         \n[1960] \"religion therefore now ask arbitrarily\"                          \n[1961] \"religion therefore now ask take\"                                 \n[1962] \"religion therefore now ask shall\"                                \n[1963] \"religion therefore now arbitrarily take\"                         \n[1964] \"religion therefore now arbitrarily shall\"                        \n[1965] \"religion therefore now arbitrarily mean\"                         \n[1966] \"religion therefore now take shall\"                               \n[1967] \"religion therefore now take mean\"                                \n[1968] \"religion therefore now take us\"                                  \n[1969] \"religion therefore ask arbitrarily take\"                         \n[1970] \"religion therefore ask arbitrarily shall\"                        \n[1971] \"religion therefore ask arbitrarily mean\"                         \n[1972] \"religion therefore ask take shall\"                               \n[1973] \"religion therefore ask take mean\"                                \n[1974] \"religion therefore ask take us\"                                  \n[1975] \"religion therefore ask shall mean\"                               \n[1976] \"religion therefore ask shall us\"                                 \n[1977] \"religion therefore ask shall _the\"                               \n[1978] \"religion therefore arbitrarily take shall\"                       \n[1979] \"religion therefore arbitrarily take mean\"                        \n[1980] \"religion therefore arbitrarily take us\"                          \n[1981] \"religion therefore arbitrarily shall mean\"                       \n[1982] \"religion therefore arbitrarily shall us\"                         \n[1983] \"religion therefore arbitrarily shall _the\"                       \n[1984] \"religion therefore arbitrarily mean us\"                          \n[1985] \"religion therefore arbitrarily mean _the\"                        \n[1986] \"religion therefore arbitrarily mean feelings\"                    \n[1987] \"religion now ask arbitrarily take\"                               \n[1988] \"religion now ask arbitrarily shall\"                              \n[1989] \"religion now ask arbitrarily mean\"                               \n[1990] \"religion now ask take shall\"                                     \n[1991] \"religion now ask take mean\"                                      \n[1992] \"religion now ask take us\"                                        \n[1993] \"religion now ask shall mean\"                                     \n[1994] \"religion now ask shall us\"                                       \n[1995] \"religion now ask shall _the\"                                     \n[1996] \"religion now arbitrarily take shall\"                             \n[1997] \"religion now arbitrarily take mean\"                              \n[1998] \"religion now arbitrarily take us\"                                \n[1999] \"religion now arbitrarily shall mean\"                             \n[2000] \"religion now arbitrarily shall us\"                               \n[2001] \"religion now arbitrarily shall _the\"                             \n[2002] \"religion now arbitrarily mean us\"                                \n[2003] \"religion now arbitrarily mean _the\"                              \n[2004] \"religion now arbitrarily mean feelings\"                          \n[2005] \"religion now take shall mean\"                                    \n[2006] \"religion now take shall us\"                                      \n[2007] \"religion now take shall _the\"                                    \n[2008] \"religion now take mean us\"                                       \n[2009] \"religion now take mean _the\"                                     \n[2010] \"religion now take mean feelings\"                                 \n[2011] \"religion now take us _the\"                                       \n[2012] \"religion now take us feelings\"                                   \n[2013] \"religion now take us acts\"                                       \n[2014] \"religion ask arbitrarily take shall\"                             \n[2015] \"religion ask arbitrarily take mean\"                              \n[2016] \"religion ask arbitrarily take us\"                                \n[2017] \"religion ask arbitrarily shall mean\"                             \n[2018] \"religion ask arbitrarily shall us\"                               \n[2019] \"religion ask arbitrarily shall _the\"                             \n[2020] \"religion ask arbitrarily mean us\"                                \n[2021] \"religion ask arbitrarily mean _the\"                              \n[2022] \"religion ask arbitrarily mean feelings\"                          \n[2023] \"religion ask take shall mean\"                                    \n[2024] \"religion ask take shall us\"                                      \n[2025] \"religion ask take shall _the\"                                    \n[2026] \"religion ask take mean us\"                                       \n[2027] \"religion ask take mean _the\"                                     \n[2028] \"religion ask take mean feelings\"                                 \n[2029] \"religion ask take us _the\"                                       \n[2030] \"religion ask take us feelings\"                                   \n[2031] \"religion ask take us acts\"                                       \n[2032] \"religion ask shall mean us\"                                      \n[2033] \"religion ask shall mean _the\"                                    \n[2034] \"religion ask shall mean feelings\"                                \n[2035] \"religion ask shall us _the\"                                      \n[2036] \"religion ask shall us feelings\"                                  \n[2037] \"religion ask shall us acts\"                                      \n[2038] \"religion ask shall _the feelings\"                                \n[2039] \"religion ask shall _the acts\"                                    \n[2040] \"religion ask shall _the experiences\"                             \n[2041] \"therefore now\"                                                   \n[2042] \"therefore ask\"                                                   \n[2043] \"therefore arbitrarily\"                                           \n[2044] \"therefore now ask\"                                               \n[2045] \"therefore now arbitrarily\"                                       \n[2046] \"therefore now take\"                                              \n[2047] \"therefore ask arbitrarily\"                                       \n[2048] \"therefore ask take\"                                              \n[2049] \"therefore ask shall\"                                             \n[2050] \"therefore arbitrarily take\"                                      \n[2051] \"therefore arbitrarily shall\"                                     \n[2052] \"therefore arbitrarily mean\"                                      \n[2053] \"therefore now ask arbitrarily\"                                   \n[2054] \"therefore now ask take\"                                          \n[2055] \"therefore now ask shall\"                                         \n[2056] \"therefore now arbitrarily take\"                                  \n[2057] \"therefore now arbitrarily shall\"                                 \n[2058] \"therefore now arbitrarily mean\"                                  \n[2059] \"therefore now take shall\"                                        \n[2060] \"therefore now take mean\"                                         \n[2061] \"therefore now take us\"                                           \n[2062] \"therefore ask arbitrarily take\"                                  \n[2063] \"therefore ask arbitrarily shall\"                                 \n[2064] \"therefore ask arbitrarily mean\"                                  \n[2065] \"therefore ask take shall\"                                        \n[2066] \"therefore ask take mean\"                                         \n[2067] \"therefore ask take us\"                                           \n[2068] \"therefore ask shall mean\"                                        \n[2069] \"therefore ask shall us\"                                          \n[2070] \"therefore ask shall _the\"                                        \n[2071] \"therefore arbitrarily take shall\"                                \n[2072] \"therefore arbitrarily take mean\"                                 \n[2073] \"therefore arbitrarily take us\"                                   \n[2074] \"therefore arbitrarily shall mean\"                                \n[2075] \"therefore arbitrarily shall us\"                                  \n[2076] \"therefore arbitrarily shall _the\"                                \n[2077] \"therefore arbitrarily mean us\"                                   \n[2078] \"therefore arbitrarily mean _the\"                                 \n[2079] \"therefore arbitrarily mean feelings\"                             \n[2080] \"therefore now ask arbitrarily take\"                              \n[2081] \"therefore now ask arbitrarily shall\"                             \n[2082] \"therefore now ask arbitrarily mean\"                              \n[2083] \"therefore now ask take shall\"                                    \n[2084] \"therefore now ask take mean\"                                     \n[2085] \"therefore now ask take us\"                                       \n[2086] \"therefore now ask shall mean\"                                    \n[2087] \"therefore now ask shall us\"                                      \n[2088] \"therefore now ask shall _the\"                                    \n[2089] \"therefore now arbitrarily take shall\"                            \n[2090] \"therefore now arbitrarily take mean\"                             \n[2091] \"therefore now arbitrarily take us\"                               \n[2092] \"therefore now arbitrarily shall mean\"                            \n[2093] \"therefore now arbitrarily shall us\"                              \n[2094] \"therefore now arbitrarily shall _the\"                            \n[2095] \"therefore now arbitrarily mean us\"                               \n[2096] \"therefore now arbitrarily mean _the\"                             \n[2097] \"therefore now arbitrarily mean feelings\"                         \n[2098] \"therefore now take shall mean\"                                   \n[2099] \"therefore now take shall us\"                                     \n[2100] \"therefore now take shall _the\"                                   \n[2101] \"therefore now take mean us\"                                      \n[2102] \"therefore now take mean _the\"                                    \n[2103] \"therefore now take mean feelings\"                                \n[2104] \"therefore now take us _the\"                                      \n[2105] \"therefore now take us feelings\"                                  \n[2106] \"therefore now take us acts\"                                      \n[2107] \"therefore ask arbitrarily take shall\"                            \n[2108] \"therefore ask arbitrarily take mean\"                             \n[2109] \"therefore ask arbitrarily take us\"                               \n[2110] \"therefore ask arbitrarily shall mean\"                            \n[2111] \"therefore ask arbitrarily shall us\"                              \n[2112] \"therefore ask arbitrarily shall _the\"                            \n[2113] \"therefore ask arbitrarily mean us\"                               \n[2114] \"therefore ask arbitrarily mean _the\"                             \n[2115] \"therefore ask arbitrarily mean feelings\"                         \n[2116] \"therefore ask take shall mean\"                                   \n[2117] \"therefore ask take shall us\"                                     \n[2118] \"therefore ask take shall _the\"                                   \n[2119] \"therefore ask take mean us\"                                      \n[2120] \"therefore ask take mean _the\"                                    \n[2121] \"therefore ask take mean feelings\"                                \n[2122] \"therefore ask take us _the\"                                      \n[2123] \"therefore ask take us feelings\"                                  \n[2124] \"therefore ask take us acts\"                                      \n[2125] \"therefore ask shall mean us\"                                     \n[2126] \"therefore ask shall mean _the\"                                   \n[2127] \"therefore ask shall mean feelings\"                               \n[2128] \"therefore ask shall us _the\"                                     \n[2129] \"therefore ask shall us feelings\"                                 \n[2130] \"therefore ask shall us acts\"                                     \n[2131] \"therefore ask shall _the feelings\"                               \n[2132] \"therefore ask shall _the acts\"                                   \n[2133] \"therefore ask shall _the experiences\"                            \n[2134] \"therefore arbitrarily take shall mean\"                           \n[2135] \"therefore arbitrarily take shall us\"                             \n[2136] \"therefore arbitrarily take shall _the\"                           \n[2137] \"therefore arbitrarily take mean us\"                              \n[2138] \"therefore arbitrarily take mean _the\"                            \n[2139] \"therefore arbitrarily take mean feelings\"                        \n[2140] \"therefore arbitrarily take us _the\"                              \n[2141] \"therefore arbitrarily take us feelings\"                          \n[2142] \"therefore arbitrarily take us acts\"                              \n[2143] \"therefore arbitrarily shall mean us\"                             \n[2144] \"therefore arbitrarily shall mean _the\"                           \n[2145] \"therefore arbitrarily shall mean feelings\"                       \n[2146] \"therefore arbitrarily shall us _the\"                             \n[2147] \"therefore arbitrarily shall us feelings\"                         \n[2148] \"therefore arbitrarily shall us acts\"                             \n[2149] \"therefore arbitrarily shall _the feelings\"                       \n[2150] \"therefore arbitrarily shall _the acts\"                           \n[2151] \"therefore arbitrarily shall _the experiences\"                    \n[2152] \"therefore arbitrarily mean us _the\"                              \n[2153] \"therefore arbitrarily mean us feelings\"                          \n[2154] \"therefore arbitrarily mean us acts\"                              \n[2155] \"therefore arbitrarily mean _the feelings\"                        \n[2156] \"therefore arbitrarily mean _the acts\"                            \n[2157] \"therefore arbitrarily mean _the experiences\"                     \n[2158] \"therefore arbitrarily mean feelings acts\"                        \n[2159] \"therefore arbitrarily mean feelings experiences\"                 \n[2160] \"therefore arbitrarily mean feelings individual\"                  \n[2161] \"now ask\"                                                         \n[2162] \"now arbitrarily\"                                                 \n[2163] \"now take\"                                                        \n[2164] \"now ask arbitrarily\"                                             \n[2165] \"now ask take\"                                                    \n[2166] \"now ask shall\"                                                   \n[2167] \"now arbitrarily take\"                                            \n[2168] \"now arbitrarily shall\"                                           \n[2169] \"now arbitrarily mean\"                                            \n[2170] \"now take shall\"                                                  \n[2171] \"now take mean\"                                                   \n[2172] \"now take us\"                                                     \n[2173] \"now ask arbitrarily take\"                                        \n[2174] \"now ask arbitrarily shall\"                                       \n[2175] \"now ask arbitrarily mean\"                                        \n[2176] \"now ask take shall\"                                              \n[2177] \"now ask take mean\"                                               \n[2178] \"now ask take us\"                                                 \n[2179] \"now ask shall mean\"                                              \n[2180] \"now ask shall us\"                                                \n[2181] \"now ask shall _the\"                                              \n[2182] \"now arbitrarily take shall\"                                      \n[2183] \"now arbitrarily take mean\"                                       \n[2184] \"now arbitrarily take us\"                                         \n[2185] \"now arbitrarily shall mean\"                                      \n[2186] \"now arbitrarily shall us\"                                        \n[2187] \"now arbitrarily shall _the\"                                      \n[2188] \"now arbitrarily mean us\"                                         \n[2189] \"now arbitrarily mean _the\"                                       \n[2190] \"now arbitrarily mean feelings\"                                   \n[2191] \"now take shall mean\"                                             \n[2192] \"now take shall us\"                                               \n[2193] \"now take shall _the\"                                             \n[2194] \"now take mean us\"                                                \n[2195] \"now take mean _the\"                                              \n[2196] \"now take mean feelings\"                                          \n[2197] \"now take us _the\"                                                \n[2198] \"now take us feelings\"                                            \n[2199] \"now take us acts\"                                                \n[2200] \"now ask arbitrarily take shall\"                                  \n[2201] \"now ask arbitrarily take mean\"                                   \n[2202] \"now ask arbitrarily take us\"                                     \n[2203] \"now ask arbitrarily shall mean\"                                  \n[2204] \"now ask arbitrarily shall us\"                                    \n[2205] \"now ask arbitrarily shall _the\"                                  \n[2206] \"now ask arbitrarily mean us\"                                     \n[2207] \"now ask arbitrarily mean _the\"                                   \n[2208] \"now ask arbitrarily mean feelings\"                               \n[2209] \"now ask take shall mean\"                                         \n[2210] \"now ask take shall us\"                                           \n[2211] \"now ask take shall _the\"                                         \n[2212] \"now ask take mean us\"                                            \n[2213] \"now ask take mean _the\"                                          \n[2214] \"now ask take mean feelings\"                                      \n[2215] \"now ask take us _the\"                                            \n[2216] \"now ask take us feelings\"                                        \n[2217] \"now ask take us acts\"                                            \n[2218] \"now ask shall mean us\"                                           \n[2219] \"now ask shall mean _the\"                                         \n[2220] \"now ask shall mean feelings\"                                     \n[2221] \"now ask shall us _the\"                                           \n[2222] \"now ask shall us feelings\"                                       \n[2223] \"now ask shall us acts\"                                           \n[2224] \"now ask shall _the feelings\"                                     \n[2225] \"now ask shall _the acts\"                                         \n[2226] \"now ask shall _the experiences\"                                  \n[2227] \"now arbitrarily take shall mean\"                                 \n[2228] \"now arbitrarily take shall us\"                                   \n[2229] \"now arbitrarily take shall _the\"                                 \n[2230] \"now arbitrarily take mean us\"                                    \n[2231] \"now arbitrarily take mean _the\"                                  \n[2232] \"now arbitrarily take mean feelings\"                              \n[2233] \"now arbitrarily take us _the\"                                    \n[2234] \"now arbitrarily take us feelings\"                                \n[2235] \"now arbitrarily take us acts\"                                    \n[2236] \"now arbitrarily shall mean us\"                                   \n[2237] \"now arbitrarily shall mean _the\"                                 \n[2238] \"now arbitrarily shall mean feelings\"                             \n[2239] \"now arbitrarily shall us _the\"                                   \n[2240] \"now arbitrarily shall us feelings\"                               \n[2241] \"now arbitrarily shall us acts\"                                   \n[2242] \"now arbitrarily shall _the feelings\"                             \n[2243] \"now arbitrarily shall _the acts\"                                 \n[2244] \"now arbitrarily shall _the experiences\"                          \n[2245] \"now arbitrarily mean us _the\"                                    \n[2246] \"now arbitrarily mean us feelings\"                                \n[2247] \"now arbitrarily mean us acts\"                                    \n[2248] \"now arbitrarily mean _the feelings\"                              \n[2249] \"now arbitrarily mean _the acts\"                                  \n[2250] \"now arbitrarily mean _the experiences\"                           \n[2251] \"now arbitrarily mean feelings acts\"                              \n[2252] \"now arbitrarily mean feelings experiences\"                       \n[2253] \"now arbitrarily mean feelings individual\"                        \n[2254] \"now take shall mean us\"                                          \n[2255] \"now take shall mean _the\"                                        \n[2256] \"now take shall mean feelings\"                                    \n[2257] \"now take shall us _the\"                                          \n[2258] \"now take shall us feelings\"                                      \n[2259] \"now take shall us acts\"                                          \n[2260] \"now take shall _the feelings\"                                    \n[2261] \"now take shall _the acts\"                                        \n[2262] \"now take shall _the experiences\"                                 \n[2263] \"now take mean us _the\"                                           \n[2264] \"now take mean us feelings\"                                       \n[2265] \"now take mean us acts\"                                           \n[2266] \"now take mean _the feelings\"                                     \n[2267] \"now take mean _the acts\"                                         \n[2268] \"now take mean _the experiences\"                                  \n[2269] \"now take mean feelings acts\"                                     \n[2270] \"now take mean feelings experiences\"                              \n[2271] \"now take mean feelings individual\"                               \n[2272] \"now take us _the feelings\"                                       \n[2273] \"now take us _the acts\"                                           \n[2274] \"now take us _the experiences\"                                    \n[2275] \"now take us feelings acts\"                                       \n[2276] \"now take us feelings experiences\"                                \n[2277] \"now take us feelings individual\"                                 \n[2278] \"now take us acts experiences\"                                    \n[2279] \"now take us acts individual\"                                     \n[2280] \"now take us acts men\"                                            \n[2281] \"ask arbitrarily\"                                                 \n[2282] \"ask take\"                                                        \n[2283] \"ask shall\"                                                       \n[2284] \"ask arbitrarily take\"                                            \n[2285] \"ask arbitrarily shall\"                                           \n[2286] \"ask arbitrarily mean\"                                            \n[2287] \"ask take shall\"                                                  \n[2288] \"ask take mean\"                                                   \n[2289] \"ask take us\"                                                     \n[2290] \"ask shall mean\"                                                  \n[2291] \"ask shall us\"                                                    \n[2292] \"ask shall _the\"                                                  \n[2293] \"ask arbitrarily take shall\"                                      \n[2294] \"ask arbitrarily take mean\"                                       \n[2295] \"ask arbitrarily take us\"                                         \n[2296] \"ask arbitrarily shall mean\"                                      \n[2297] \"ask arbitrarily shall us\"                                        \n[2298] \"ask arbitrarily shall _the\"                                      \n[2299] \"ask arbitrarily mean us\"                                         \n[2300] \"ask arbitrarily mean _the\"                                       \n[2301] \"ask arbitrarily mean feelings\"                                   \n[2302] \"ask take shall mean\"                                             \n[2303] \"ask take shall us\"                                               \n[2304] \"ask take shall _the\"                                             \n[2305] \"ask take mean us\"                                                \n[2306] \"ask take mean _the\"                                              \n[2307] \"ask take mean feelings\"                                          \n[2308] \"ask take us _the\"                                                \n[2309] \"ask take us feelings\"                                            \n[2310] \"ask take us acts\"                                                \n[2311] \"ask shall mean us\"                                               \n[2312] \"ask shall mean _the\"                                             \n[2313] \"ask shall mean feelings\"                                         \n[2314] \"ask shall us _the\"                                               \n[2315] \"ask shall us feelings\"                                           \n[2316] \"ask shall us acts\"                                               \n[2317] \"ask shall _the feelings\"                                         \n[2318] \"ask shall _the acts\"                                             \n[2319] \"ask shall _the experiences\"                                      \n[2320] \"ask arbitrarily take shall mean\"                                 \n[2321] \"ask arbitrarily take shall us\"                                   \n[2322] \"ask arbitrarily take shall _the\"                                 \n[2323] \"ask arbitrarily take mean us\"                                    \n[2324] \"ask arbitrarily take mean _the\"                                  \n[2325] \"ask arbitrarily take mean feelings\"                              \n[2326] \"ask arbitrarily take us _the\"                                    \n[2327] \"ask arbitrarily take us feelings\"                                \n[2328] \"ask arbitrarily take us acts\"                                    \n[2329] \"ask arbitrarily shall mean us\"                                   \n[2330] \"ask arbitrarily shall mean _the\"                                 \n[2331] \"ask arbitrarily shall mean feelings\"                             \n[2332] \"ask arbitrarily shall us _the\"                                   \n[2333] \"ask arbitrarily shall us feelings\"                               \n[2334] \"ask arbitrarily shall us acts\"                                   \n[2335] \"ask arbitrarily shall _the feelings\"                             \n[2336] \"ask arbitrarily shall _the acts\"                                 \n[2337] \"ask arbitrarily shall _the experiences\"                          \n[2338] \"ask arbitrarily mean us _the\"                                    \n[2339] \"ask arbitrarily mean us feelings\"                                \n[2340] \"ask arbitrarily mean us acts\"                                    \n[2341] \"ask arbitrarily mean _the feelings\"                              \n[2342] \"ask arbitrarily mean _the acts\"                                  \n[2343] \"ask arbitrarily mean _the experiences\"                           \n[2344] \"ask arbitrarily mean feelings acts\"                              \n[2345] \"ask arbitrarily mean feelings experiences\"                       \n[2346] \"ask arbitrarily mean feelings individual\"                        \n[2347] \"ask take shall mean us\"                                          \n[2348] \"ask take shall mean _the\"                                        \n[2349] \"ask take shall mean feelings\"                                    \n[2350] \"ask take shall us _the\"                                          \n[2351] \"ask take shall us feelings\"                                      \n[2352] \"ask take shall us acts\"                                          \n[2353] \"ask take shall _the feelings\"                                    \n[2354] \"ask take shall _the acts\"                                        \n[2355] \"ask take shall _the experiences\"                                 \n[2356] \"ask take mean us _the\"                                           \n[2357] \"ask take mean us feelings\"                                       \n[2358] \"ask take mean us acts\"                                           \n[2359] \"ask take mean _the feelings\"                                     \n[2360] \"ask take mean _the acts\"                                         \n[2361] \"ask take mean _the experiences\"                                  \n[2362] \"ask take mean feelings acts\"                                     \n[2363] \"ask take mean feelings experiences\"                              \n[2364] \"ask take mean feelings individual\"                               \n[2365] \"ask take us _the feelings\"                                       \n[2366] \"ask take us _the acts\"                                           \n[2367] \"ask take us _the experiences\"                                    \n[2368] \"ask take us feelings acts\"                                       \n[2369] \"ask take us feelings experiences\"                                \n[2370] \"ask take us feelings individual\"                                 \n[2371] \"ask take us acts experiences\"                                    \n[2372] \"ask take us acts individual\"                                     \n[2373] \"ask take us acts men\"                                            \n[2374] \"ask shall mean us _the\"                                          \n[2375] \"ask shall mean us feelings\"                                      \n[2376] \"ask shall mean us acts\"                                          \n[2377] \"ask shall mean _the feelings\"                                    \n[2378] \"ask shall mean _the acts\"                                        \n[2379] \"ask shall mean _the experiences\"                                 \n[2380] \"ask shall mean feelings acts\"                                    \n[2381] \"ask shall mean feelings experiences\"                             \n[2382] \"ask shall mean feelings individual\"                              \n[2383] \"ask shall us _the feelings\"                                      \n[2384] \"ask shall us _the acts\"                                          \n[2385] \"ask shall us _the experiences\"                                   \n[2386] \"ask shall us feelings acts\"                                      \n[2387] \"ask shall us feelings experiences\"                               \n[2388] \"ask shall us feelings individual\"                                \n[2389] \"ask shall us acts experiences\"                                   \n[2390] \"ask shall us acts individual\"                                    \n[2391] \"ask shall us acts men\"                                           \n[2392] \"ask shall _the feelings acts\"                                    \n[2393] \"ask shall _the feelings experiences\"                             \n[2394] \"ask shall _the feelings individual\"                              \n[2395] \"ask shall _the acts experiences\"                                 \n[2396] \"ask shall _the acts individual\"                                  \n[2397] \"ask shall _the acts men\"                                         \n[2398] \"ask shall _the experiences individual\"                           \n[2399] \"ask shall _the experiences men\"                                  \n[2400] \"ask shall _the experiences solitude\"                             \n[2401] \"arbitrarily take\"                                                \n[2402] \"arbitrarily shall\"                                               \n[2403] \"arbitrarily mean\"                                                \n[2404] \"arbitrarily take shall\"                                          \n[2405] \"arbitrarily take mean\"                                           \n[2406] \"arbitrarily take us\"                                             \n[2407] \"arbitrarily shall mean\"                                          \n[2408] \"arbitrarily shall us\"                                            \n[2409] \"arbitrarily shall _the\"                                          \n[2410] \"arbitrarily mean us\"                                             \n[2411] \"arbitrarily mean _the\"                                           \n[2412] \"arbitrarily mean feelings\"                                       \n[2413] \"arbitrarily take shall mean\"                                     \n[2414] \"arbitrarily take shall us\"                                       \n[2415] \"arbitrarily take shall _the\"                                     \n[2416] \"arbitrarily take mean us\"                                        \n[2417] \"arbitrarily take mean _the\"                                      \n[2418] \"arbitrarily take mean feelings\"                                  \n[2419] \"arbitrarily take us _the\"                                        \n[2420] \"arbitrarily take us feelings\"                                    \n[2421] \"arbitrarily take us acts\"                                        \n[2422] \"arbitrarily shall mean us\"                                       \n[2423] \"arbitrarily shall mean _the\"                                     \n[2424] \"arbitrarily shall mean feelings\"                                 \n[2425] \"arbitrarily shall us _the\"                                       \n[2426] \"arbitrarily shall us feelings\"                                   \n[2427] \"arbitrarily shall us acts\"                                       \n[2428] \"arbitrarily shall _the feelings\"                                 \n[2429] \"arbitrarily shall _the acts\"                                     \n[2430] \"arbitrarily shall _the experiences\"                              \n[2431] \"arbitrarily mean us _the\"                                        \n[2432] \"arbitrarily mean us feelings\"                                    \n[2433] \"arbitrarily mean us acts\"                                        \n[2434] \"arbitrarily mean _the feelings\"                                  \n[2435] \"arbitrarily mean _the acts\"                                      \n[2436] \"arbitrarily mean _the experiences\"                               \n[2437] \"arbitrarily mean feelings acts\"                                  \n[2438] \"arbitrarily mean feelings experiences\"                           \n[2439] \"arbitrarily mean feelings individual\"                            \n[2440] \"arbitrarily take shall mean us\"                                  \n[2441] \"arbitrarily take shall mean _the\"                                \n[2442] \"arbitrarily take shall mean feelings\"                            \n[2443] \"arbitrarily take shall us _the\"                                  \n[2444] \"arbitrarily take shall us feelings\"                              \n[2445] \"arbitrarily take shall us acts\"                                  \n[2446] \"arbitrarily take shall _the feelings\"                            \n[2447] \"arbitrarily take shall _the acts\"                                \n[2448] \"arbitrarily take shall _the experiences\"                         \n[2449] \"arbitrarily take mean us _the\"                                   \n[2450] \"arbitrarily take mean us feelings\"                               \n[2451] \"arbitrarily take mean us acts\"                                   \n[2452] \"arbitrarily take mean _the feelings\"                             \n[2453] \"arbitrarily take mean _the acts\"                                 \n[2454] \"arbitrarily take mean _the experiences\"                          \n[2455] \"arbitrarily take mean feelings acts\"                             \n[2456] \"arbitrarily take mean feelings experiences\"                      \n[2457] \"arbitrarily take mean feelings individual\"                       \n[2458] \"arbitrarily take us _the feelings\"                               \n[2459] \"arbitrarily take us _the acts\"                                   \n[2460] \"arbitrarily take us _the experiences\"                            \n[2461] \"arbitrarily take us feelings acts\"                               \n[2462] \"arbitrarily take us feelings experiences\"                        \n[2463] \"arbitrarily take us feelings individual\"                         \n[2464] \"arbitrarily take us acts experiences\"                            \n[2465] \"arbitrarily take us acts individual\"                             \n[2466] \"arbitrarily take us acts men\"                                    \n[2467] \"arbitrarily shall mean us _the\"                                  \n[2468] \"arbitrarily shall mean us feelings\"                              \n[2469] \"arbitrarily shall mean us acts\"                                  \n[2470] \"arbitrarily shall mean _the feelings\"                            \n[2471] \"arbitrarily shall mean _the acts\"                                \n[2472] \"arbitrarily shall mean _the experiences\"                         \n[2473] \"arbitrarily shall mean feelings acts\"                            \n[2474] \"arbitrarily shall mean feelings experiences\"                     \n[2475] \"arbitrarily shall mean feelings individual\"                      \n[2476] \"arbitrarily shall us _the feelings\"                              \n[2477] \"arbitrarily shall us _the acts\"                                  \n[2478] \"arbitrarily shall us _the experiences\"                           \n[2479] \"arbitrarily shall us feelings acts\"                              \n[2480] \"arbitrarily shall us feelings experiences\"                       \n[2481] \"arbitrarily shall us feelings individual\"                        \n[2482] \"arbitrarily shall us acts experiences\"                           \n[2483] \"arbitrarily shall us acts individual\"                            \n[2484] \"arbitrarily shall us acts men\"                                   \n[2485] \"arbitrarily shall _the feelings acts\"                            \n[2486] \"arbitrarily shall _the feelings experiences\"                     \n[2487] \"arbitrarily shall _the feelings individual\"                      \n[2488] \"arbitrarily shall _the acts experiences\"                         \n[2489] \"arbitrarily shall _the acts individual\"                          \n[2490] \"arbitrarily shall _the acts men\"                                 \n[2491] \"arbitrarily shall _the experiences individual\"                   \n[2492] \"arbitrarily shall _the experiences men\"                          \n[2493] \"arbitrarily shall _the experiences solitude\"                     \n[2494] \"arbitrarily mean us _the feelings\"                               \n[2495] \"arbitrarily mean us _the acts\"                                   \n[2496] \"arbitrarily mean us _the experiences\"                            \n[2497] \"arbitrarily mean us feelings acts\"                               \n[2498] \"arbitrarily mean us feelings experiences\"                        \n[2499] \"arbitrarily mean us feelings individual\"                         \n[2500] \"arbitrarily mean us acts experiences\"                            \n[2501] \"arbitrarily mean us acts individual\"                             \n[2502] \"arbitrarily mean us acts men\"                                    \n[2503] \"arbitrarily mean _the feelings acts\"                             \n[2504] \"arbitrarily mean _the feelings experiences\"                      \n[2505] \"arbitrarily mean _the feelings individual\"                       \n[2506] \"arbitrarily mean _the acts experiences\"                          \n[2507] \"arbitrarily mean _the acts individual\"                           \n[2508] \"arbitrarily mean _the acts men\"                                  \n[2509] \"arbitrarily mean _the experiences individual\"                    \n[2510] \"arbitrarily mean _the experiences men\"                           \n[2511] \"arbitrarily mean _the experiences solitude\"                      \n[2512] \"arbitrarily mean feelings acts experiences\"                      \n[2513] \"arbitrarily mean feelings acts individual\"                       \n[2514] \"arbitrarily mean feelings acts men\"                              \n[2515] \"arbitrarily mean feelings experiences individual\"                \n[2516] \"arbitrarily mean feelings experiences men\"                       \n[2517] \"arbitrarily mean feelings experiences solitude\"                  \n[2518] \"arbitrarily mean feelings individual men\"                        \n[2519] \"arbitrarily mean feelings individual solitude\"                   \n[2520] \"arbitrarily mean feelings individual far\"                        \n[2521] \"take shall\"                                                      \n[2522] \"take mean\"                                                       \n[2523] \"take us\"                                                         \n[2524] \"take shall mean\"                                                 \n[2525] \"take shall us\"                                                   \n[2526] \"take shall _the\"                                                 \n[2527] \"take mean us\"                                                    \n[2528] \"take mean _the\"                                                  \n[2529] \"take mean feelings\"                                              \n[2530] \"take us _the\"                                                    \n[2531] \"take us feelings\"                                                \n[2532] \"take us acts\"                                                    \n[2533] \"take shall mean us\"                                              \n[2534] \"take shall mean _the\"                                            \n[2535] \"take shall mean feelings\"                                        \n[2536] \"take shall us _the\"                                              \n[2537] \"take shall us feelings\"                                          \n[2538] \"take shall us acts\"                                              \n[2539] \"take shall _the feelings\"                                        \n[2540] \"take shall _the acts\"                                            \n[2541] \"take shall _the experiences\"                                     \n[2542] \"take mean us _the\"                                               \n[2543] \"take mean us feelings\"                                           \n[2544] \"take mean us acts\"                                               \n[2545] \"take mean _the feelings\"                                         \n[2546] \"take mean _the acts\"                                             \n[2547] \"take mean _the experiences\"                                      \n[2548] \"take mean feelings acts\"                                         \n[2549] \"take mean feelings experiences\"                                  \n[2550] \"take mean feelings individual\"                                   \n[2551] \"take us _the feelings\"                                           \n[2552] \"take us _the acts\"                                               \n[2553] \"take us _the experiences\"                                        \n[2554] \"take us feelings acts\"                                           \n[2555] \"take us feelings experiences\"                                    \n[2556] \"take us feelings individual\"                                     \n[2557] \"take us acts experiences\"                                        \n[2558] \"take us acts individual\"                                         \n[2559] \"take us acts men\"                                                \n[2560] \"take shall mean us _the\"                                         \n[2561] \"take shall mean us feelings\"                                     \n[2562] \"take shall mean us acts\"                                         \n[2563] \"take shall mean _the feelings\"                                   \n[2564] \"take shall mean _the acts\"                                       \n[2565] \"take shall mean _the experiences\"                                \n[2566] \"take shall mean feelings acts\"                                   \n[2567] \"take shall mean feelings experiences\"                            \n[2568] \"take shall mean feelings individual\"                             \n[2569] \"take shall us _the feelings\"                                     \n[2570] \"take shall us _the acts\"                                         \n[2571] \"take shall us _the experiences\"                                  \n[2572] \"take shall us feelings acts\"                                     \n[2573] \"take shall us feelings experiences\"                              \n[2574] \"take shall us feelings individual\"                               \n[2575] \"take shall us acts experiences\"                                  \n[2576] \"take shall us acts individual\"                                   \n[2577] \"take shall us acts men\"                                          \n[2578] \"take shall _the feelings acts\"                                   \n[2579] \"take shall _the feelings experiences\"                            \n[2580] \"take shall _the feelings individual\"                             \n[2581] \"take shall _the acts experiences\"                                \n[2582] \"take shall _the acts individual\"                                 \n[2583] \"take shall _the acts men\"                                        \n[2584] \"take shall _the experiences individual\"                          \n[2585] \"take shall _the experiences men\"                                 \n[2586] \"take shall _the experiences solitude\"                            \n[2587] \"take mean us _the feelings\"                                      \n[2588] \"take mean us _the acts\"                                          \n[2589] \"take mean us _the experiences\"                                   \n[2590] \"take mean us feelings acts\"                                      \n[2591] \"take mean us feelings experiences\"                               \n[2592] \"take mean us feelings individual\"                                \n[2593] \"take mean us acts experiences\"                                   \n[2594] \"take mean us acts individual\"                                    \n[2595] \"take mean us acts men\"                                           \n[2596] \"take mean _the feelings acts\"                                    \n[2597] \"take mean _the feelings experiences\"                             \n[2598] \"take mean _the feelings individual\"                              \n[2599] \"take mean _the acts experiences\"                                 \n[2600] \"take mean _the acts individual\"                                  \n[2601] \"take mean _the acts men\"                                         \n[2602] \"take mean _the experiences individual\"                           \n[2603] \"take mean _the experiences men\"                                  \n[2604] \"take mean _the experiences solitude\"                             \n[2605] \"take mean feelings acts experiences\"                             \n[2606] \"take mean feelings acts individual\"                              \n[2607] \"take mean feelings acts men\"                                     \n[2608] \"take mean feelings experiences individual\"                       \n[2609] \"take mean feelings experiences men\"                              \n[2610] \"take mean feelings experiences solitude\"                         \n[2611] \"take mean feelings individual men\"                               \n[2612] \"take mean feelings individual solitude\"                          \n[2613] \"take mean feelings individual far\"                               \n[2614] \"take us _the feelings acts\"                                      \n[2615] \"take us _the feelings experiences\"                               \n[2616] \"take us _the feelings individual\"                                \n[2617] \"take us _the acts experiences\"                                   \n[2618] \"take us _the acts individual\"                                    \n[2619] \"take us _the acts men\"                                           \n[2620] \"take us _the experiences individual\"                             \n[2621] \"take us _the experiences men\"                                    \n[2622] \"take us _the experiences solitude\"                               \n[2623] \"take us feelings acts experiences\"                               \n[2624] \"take us feelings acts individual\"                                \n[2625] \"take us feelings acts men\"                                       \n[2626] \"take us feelings experiences individual\"                         \n[2627] \"take us feelings experiences men\"                                \n[2628] \"take us feelings experiences solitude\"                           \n[2629] \"take us feelings individual men\"                                 \n[2630] \"take us feelings individual solitude\"                            \n[2631] \"take us feelings individual far\"                                 \n[2632] \"take us acts experiences individual\"                             \n[2633] \"take us acts experiences men\"                                    \n[2634] \"take us acts experiences solitude\"                               \n[2635] \"take us acts individual men\"                                     \n[2636] \"take us acts individual solitude\"                                \n[2637] \"take us acts individual far\"                                     \n[2638] \"take us acts men solitude\"                                       \n[2639] \"take us acts men far\"                                            \n[2640] \"take us acts men apprehend\"                                      \n[2641] \"shall mean\"                                                      \n[2642] \"shall us\"                                                        \n[2643] \"shall _the\"                                                      \n[2644] \"shall mean us\"                                                   \n[2645] \"shall mean _the\"                                                 \n[2646] \"shall mean feelings\"                                             \n[2647] \"shall us _the\"                                                   \n[2648] \"shall us feelings\"                                               \n[2649] \"shall us acts\"                                                   \n[2650] \"shall _the feelings\"                                             \n[2651] \"shall _the acts\"                                                 \n[2652] \"shall _the experiences\"                                          \n[2653] \"shall mean us _the\"                                              \n[2654] \"shall mean us feelings\"                                          \n[2655] \"shall mean us acts\"                                              \n[2656] \"shall mean _the feelings\"                                        \n[2657] \"shall mean _the acts\"                                            \n[2658] \"shall mean _the experiences\"                                     \n[2659] \"shall mean feelings acts\"                                        \n[2660] \"shall mean feelings experiences\"                                 \n[2661] \"shall mean feelings individual\"                                  \n[2662] \"shall us _the feelings\"                                          \n[2663] \"shall us _the acts\"                                              \n[2664] \"shall us _the experiences\"                                       \n[2665] \"shall us feelings acts\"                                          \n[2666] \"shall us feelings experiences\"                                   \n[2667] \"shall us feelings individual\"                                    \n[2668] \"shall us acts experiences\"                                       \n[2669] \"shall us acts individual\"                                        \n[2670] \"shall us acts men\"                                               \n[2671] \"shall _the feelings acts\"                                        \n[2672] \"shall _the feelings experiences\"                                 \n[2673] \"shall _the feelings individual\"                                  \n[2674] \"shall _the acts experiences\"                                     \n[2675] \"shall _the acts individual\"                                      \n[2676] \"shall _the acts men\"                                             \n[2677] \"shall _the experiences individual\"                               \n[2678] \"shall _the experiences men\"                                      \n[2679] \"shall _the experiences solitude\"                                 \n[2680] \"shall mean us _the feelings\"                                     \n[2681] \"shall mean us _the acts\"                                         \n[2682] \"shall mean us _the experiences\"                                  \n[2683] \"shall mean us feelings acts\"                                     \n[2684] \"shall mean us feelings experiences\"                              \n[2685] \"shall mean us feelings individual\"                               \n[2686] \"shall mean us acts experiences\"                                  \n[2687] \"shall mean us acts individual\"                                   \n[2688] \"shall mean us acts men\"                                          \n[2689] \"shall mean _the feelings acts\"                                   \n[2690] \"shall mean _the feelings experiences\"                            \n[2691] \"shall mean _the feelings individual\"                             \n[2692] \"shall mean _the acts experiences\"                                \n[2693] \"shall mean _the acts individual\"                                 \n[2694] \"shall mean _the acts men\"                                        \n[2695] \"shall mean _the experiences individual\"                          \n[2696] \"shall mean _the experiences men\"                                 \n[2697] \"shall mean _the experiences solitude\"                            \n[2698] \"shall mean feelings acts experiences\"                            \n[2699] \"shall mean feelings acts individual\"                             \n[2700] \"shall mean feelings acts men\"                                    \n[2701] \"shall mean feelings experiences individual\"                      \n[2702] \"shall mean feelings experiences men\"                             \n[2703] \"shall mean feelings experiences solitude\"                        \n[2704] \"shall mean feelings individual men\"                              \n[2705] \"shall mean feelings individual solitude\"                         \n[2706] \"shall mean feelings individual far\"                              \n[2707] \"shall us _the feelings acts\"                                     \n[2708] \"shall us _the feelings experiences\"                              \n[2709] \"shall us _the feelings individual\"                               \n[2710] \"shall us _the acts experiences\"                                  \n[2711] \"shall us _the acts individual\"                                   \n[2712] \"shall us _the acts men\"                                          \n[2713] \"shall us _the experiences individual\"                            \n[2714] \"shall us _the experiences men\"                                   \n[2715] \"shall us _the experiences solitude\"                              \n[2716] \"shall us feelings acts experiences\"                              \n[2717] \"shall us feelings acts individual\"                               \n[2718] \"shall us feelings acts men\"                                      \n[2719] \"shall us feelings experiences individual\"                        \n[2720] \"shall us feelings experiences men\"                               \n[2721] \"shall us feelings experiences solitude\"                          \n[2722] \"shall us feelings individual men\"                                \n[2723] \"shall us feelings individual solitude\"                           \n[2724] \"shall us feelings individual far\"                                \n[2725] \"shall us acts experiences individual\"                            \n[2726] \"shall us acts experiences men\"                                   \n[2727] \"shall us acts experiences solitude\"                              \n[2728] \"shall us acts individual men\"                                    \n[2729] \"shall us acts individual solitude\"                               \n[2730] \"shall us acts individual far\"                                    \n[2731] \"shall us acts men solitude\"                                      \n[2732] \"shall us acts men far\"                                           \n[2733] \"shall us acts men apprehend\"                                     \n[2734] \"shall _the feelings acts experiences\"                            \n[2735] \"shall _the feelings acts individual\"                             \n[2736] \"shall _the feelings acts men\"                                    \n[2737] \"shall _the feelings experiences individual\"                      \n[2738] \"shall _the feelings experiences men\"                             \n[2739] \"shall _the feelings experiences solitude\"                        \n[2740] \"shall _the feelings individual men\"                              \n[2741] \"shall _the feelings individual solitude\"                         \n[2742] \"shall _the feelings individual far\"                              \n[2743] \"shall _the acts experiences individual\"                          \n[2744] \"shall _the acts experiences men\"                                 \n[2745] \"shall _the acts experiences solitude\"                            \n[2746] \"shall _the acts individual men\"                                  \n[2747] \"shall _the acts individual solitude\"                             \n[2748] \"shall _the acts individual far\"                                  \n[2749] \"shall _the acts men solitude\"                                    \n[2750] \"shall _the acts men far\"                                         \n[2751] \"shall _the acts men apprehend\"                                   \n[2752] \"shall _the experiences individual men\"                           \n[2753] \"shall _the experiences individual solitude\"                      \n[2754] \"shall _the experiences individual far\"                           \n[2755] \"shall _the experiences men solitude\"                             \n[2756] \"shall _the experiences men far\"                                  \n[2757] \"shall _the experiences men apprehend\"                            \n[2758] \"shall _the experiences solitude far\"                             \n[2759] \"shall _the experiences solitude apprehend\"                       \n[2760] \"shall _the experiences solitude stand\"                           \n[2761] \"mean us\"                                                         \n[2762] \"mean _the\"                                                       \n[2763] \"mean feelings\"                                                   \n[2764] \"mean us _the\"                                                    \n[2765] \"mean us feelings\"                                                \n[2766] \"mean us acts\"                                                    \n[2767] \"mean _the feelings\"                                              \n[2768] \"mean _the acts\"                                                  \n[2769] \"mean _the experiences\"                                           \n[2770] \"mean feelings acts\"                                              \n[2771] \"mean feelings experiences\"                                       \n[2772] \"mean feelings individual\"                                        \n[2773] \"mean us _the feelings\"                                           \n[2774] \"mean us _the acts\"                                               \n[2775] \"mean us _the experiences\"                                        \n[2776] \"mean us feelings acts\"                                           \n[2777] \"mean us feelings experiences\"                                    \n[2778] \"mean us feelings individual\"                                     \n[2779] \"mean us acts experiences\"                                        \n[2780] \"mean us acts individual\"                                         \n[2781] \"mean us acts men\"                                                \n[2782] \"mean _the feelings acts\"                                         \n[2783] \"mean _the feelings experiences\"                                  \n[2784] \"mean _the feelings individual\"                                   \n[2785] \"mean _the acts experiences\"                                      \n[2786] \"mean _the acts individual\"                                       \n[2787] \"mean _the acts men\"                                              \n[2788] \"mean _the experiences individual\"                                \n[2789] \"mean _the experiences men\"                                       \n[2790] \"mean _the experiences solitude\"                                  \n[2791] \"mean feelings acts experiences\"                                  \n[2792] \"mean feelings acts individual\"                                   \n[2793] \"mean feelings acts men\"                                          \n[2794] \"mean feelings experiences individual\"                            \n[2795] \"mean feelings experiences men\"                                   \n[2796] \"mean feelings experiences solitude\"                              \n[2797] \"mean feelings individual men\"                                    \n[2798] \"mean feelings individual solitude\"                               \n[2799] \"mean feelings individual far\"                                    \n[2800] \"mean us _the feelings acts\"                                      \n[2801] \"mean us _the feelings experiences\"                               \n[2802] \"mean us _the feelings individual\"                                \n[2803] \"mean us _the acts experiences\"                                   \n[2804] \"mean us _the acts individual\"                                    \n[2805] \"mean us _the acts men\"                                           \n[2806] \"mean us _the experiences individual\"                             \n[2807] \"mean us _the experiences men\"                                    \n[2808] \"mean us _the experiences solitude\"                               \n[2809] \"mean us feelings acts experiences\"                               \n[2810] \"mean us feelings acts individual\"                                \n[2811] \"mean us feelings acts men\"                                       \n[2812] \"mean us feelings experiences individual\"                         \n[2813] \"mean us feelings experiences men\"                                \n[2814] \"mean us feelings experiences solitude\"                           \n[2815] \"mean us feelings individual men\"                                 \n[2816] \"mean us feelings individual solitude\"                            \n[2817] \"mean us feelings individual far\"                                 \n[2818] \"mean us acts experiences individual\"                             \n[2819] \"mean us acts experiences men\"                                    \n[2820] \"mean us acts experiences solitude\"                               \n[2821] \"mean us acts individual men\"                                     \n[2822] \"mean us acts individual solitude\"                                \n[2823] \"mean us acts individual far\"                                     \n[2824] \"mean us acts men solitude\"                                       \n[2825] \"mean us acts men far\"                                            \n[2826] \"mean us acts men apprehend\"                                      \n[2827] \"mean _the feelings acts experiences\"                             \n[2828] \"mean _the feelings acts individual\"                              \n[2829] \"mean _the feelings acts men\"                                     \n[2830] \"mean _the feelings experiences individual\"                       \n[2831] \"mean _the feelings experiences men\"                              \n[2832] \"mean _the feelings experiences solitude\"                         \n[2833] \"mean _the feelings individual men\"                               \n[2834] \"mean _the feelings individual solitude\"                          \n[2835] \"mean _the feelings individual far\"                               \n[2836] \"mean _the acts experiences individual\"                           \n[2837] \"mean _the acts experiences men\"                                  \n[2838] \"mean _the acts experiences solitude\"                             \n[2839] \"mean _the acts individual men\"                                   \n[2840] \"mean _the acts individual solitude\"                              \n[2841] \"mean _the acts individual far\"                                   \n[2842] \"mean _the acts men solitude\"                                     \n[2843] \"mean _the acts men far\"                                          \n[2844] \"mean _the acts men apprehend\"                                    \n[2845] \"mean _the experiences individual men\"                            \n[2846] \"mean _the experiences individual solitude\"                       \n[2847] \"mean _the experiences individual far\"                            \n[2848] \"mean _the experiences men solitude\"                              \n[2849] \"mean _the experiences men far\"                                   \n[2850] \"mean _the experiences men apprehend\"                             \n[2851] \"mean _the experiences solitude far\"                              \n[2852] \"mean _the experiences solitude apprehend\"                        \n[2853] \"mean _the experiences solitude stand\"                            \n[2854] \"mean feelings acts experiences individual\"                       \n[2855] \"mean feelings acts experiences men\"                              \n[2856] \"mean feelings acts experiences solitude\"                         \n[2857] \"mean feelings acts individual men\"                               \n[2858] \"mean feelings acts individual solitude\"                          \n[2859] \"mean feelings acts individual far\"                               \n[2860] \"mean feelings acts men solitude\"                                 \n[2861] \"mean feelings acts men far\"                                      \n[2862] \"mean feelings acts men apprehend\"                                \n[2863] \"mean feelings experiences individual men\"                        \n[2864] \"mean feelings experiences individual solitude\"                   \n[2865] \"mean feelings experiences individual far\"                        \n[2866] \"mean feelings experiences men solitude\"                          \n[2867] \"mean feelings experiences men far\"                               \n[2868] \"mean feelings experiences men apprehend\"                         \n[2869] \"mean feelings experiences solitude far\"                          \n[2870] \"mean feelings experiences solitude apprehend\"                    \n[2871] \"mean feelings experiences solitude stand\"                        \n[2872] \"mean feelings individual men solitude\"                           \n[2873] \"mean feelings individual men far\"                                \n[2874] \"mean feelings individual men apprehend\"                          \n[2875] \"mean feelings individual solitude far\"                           \n[2876] \"mean feelings individual solitude apprehend\"                     \n[2877] \"mean feelings individual solitude stand\"                         \n[2878] \"mean feelings individual far apprehend\"                          \n[2879] \"mean feelings individual far stand\"                              \n[2880] \"mean feelings individual far relation\"                           \n[2881] \"us _the\"                                                         \n[2882] \"us feelings\"                                                     \n[2883] \"us acts\"                                                         \n[2884] \"us _the feelings\"                                                \n[2885] \"us _the acts\"                                                    \n[2886] \"us _the experiences\"                                             \n[2887] \"us feelings acts\"                                                \n[2888] \"us feelings experiences\"                                         \n[2889] \"us feelings individual\"                                          \n[2890] \"us acts experiences\"                                             \n[2891] \"us acts individual\"                                              \n[2892] \"us acts men\"                                                     \n[2893] \"us _the feelings acts\"                                           \n[2894] \"us _the feelings experiences\"                                    \n[2895] \"us _the feelings individual\"                                     \n[2896] \"us _the acts experiences\"                                        \n[2897] \"us _the acts individual\"                                         \n[2898] \"us _the acts men\"                                                \n[2899] \"us _the experiences individual\"                                  \n[2900] \"us _the experiences men\"                                         \n[2901] \"us _the experiences solitude\"                                    \n[2902] \"us feelings acts experiences\"                                    \n[2903] \"us feelings acts individual\"                                     \n[2904] \"us feelings acts men\"                                            \n[2905] \"us feelings experiences individual\"                              \n[2906] \"us feelings experiences men\"                                     \n[2907] \"us feelings experiences solitude\"                                \n[2908] \"us feelings individual men\"                                      \n[2909] \"us feelings individual solitude\"                                 \n[2910] \"us feelings individual far\"                                      \n[2911] \"us acts experiences individual\"                                  \n[2912] \"us acts experiences men\"                                         \n[2913] \"us acts experiences solitude\"                                    \n[2914] \"us acts individual men\"                                          \n[2915] \"us acts individual solitude\"                                     \n[2916] \"us acts individual far\"                                          \n[2917] \"us acts men solitude\"                                            \n[2918] \"us acts men far\"                                                 \n[2919] \"us acts men apprehend\"                                           \n[2920] \"us _the feelings acts experiences\"                               \n[2921] \"us _the feelings acts individual\"                                \n[2922] \"us _the feelings acts men\"                                       \n[2923] \"us _the feelings experiences individual\"                         \n[2924] \"us _the feelings experiences men\"                                \n[2925] \"us _the feelings experiences solitude\"                           \n[2926] \"us _the feelings individual men\"                                 \n[2927] \"us _the feelings individual solitude\"                            \n[2928] \"us _the feelings individual far\"                                 \n[2929] \"us _the acts experiences individual\"                             \n[2930] \"us _the acts experiences men\"                                    \n[2931] \"us _the acts experiences solitude\"                               \n[2932] \"us _the acts individual men\"                                     \n[2933] \"us _the acts individual solitude\"                                \n[2934] \"us _the acts individual far\"                                     \n[2935] \"us _the acts men solitude\"                                       \n[2936] \"us _the acts men far\"                                            \n[2937] \"us _the acts men apprehend\"                                      \n[2938] \"us _the experiences individual men\"                              \n[2939] \"us _the experiences individual solitude\"                         \n[2940] \"us _the experiences individual far\"                              \n[2941] \"us _the experiences men solitude\"                                \n[2942] \"us _the experiences men far\"                                     \n[2943] \"us _the experiences men apprehend\"                               \n[2944] \"us _the experiences solitude far\"                                \n[2945] \"us _the experiences solitude apprehend\"                          \n[2946] \"us _the experiences solitude stand\"                              \n[2947] \"us feelings acts experiences individual\"                         \n[2948] \"us feelings acts experiences men\"                                \n[2949] \"us feelings acts experiences solitude\"                           \n[2950] \"us feelings acts individual men\"                                 \n[2951] \"us feelings acts individual solitude\"                            \n[2952] \"us feelings acts individual far\"                                 \n[2953] \"us feelings acts men solitude\"                                   \n[2954] \"us feelings acts men far\"                                        \n[2955] \"us feelings acts men apprehend\"                                  \n[2956] \"us feelings experiences individual men\"                          \n[2957] \"us feelings experiences individual solitude\"                     \n[2958] \"us feelings experiences individual far\"                          \n[2959] \"us feelings experiences men solitude\"                            \n[2960] \"us feelings experiences men far\"                                 \n[2961] \"us feelings experiences men apprehend\"                           \n[2962] \"us feelings experiences solitude far\"                            \n[2963] \"us feelings experiences solitude apprehend\"                      \n[2964] \"us feelings experiences solitude stand\"                          \n[2965] \"us feelings individual men solitude\"                             \n[2966] \"us feelings individual men far\"                                  \n[2967] \"us feelings individual men apprehend\"                            \n[2968] \"us feelings individual solitude far\"                             \n[2969] \"us feelings individual solitude apprehend\"                       \n[2970] \"us feelings individual solitude stand\"                           \n[2971] \"us feelings individual far apprehend\"                            \n[2972] \"us feelings individual far stand\"                                \n[2973] \"us feelings individual far relation\"                             \n[2974] \"us acts experiences individual men\"                              \n[2975] \"us acts experiences individual solitude\"                         \n[2976] \"us acts experiences individual far\"                              \n[2977] \"us acts experiences men solitude\"                                \n[2978] \"us acts experiences men far\"                                     \n[2979] \"us acts experiences men apprehend\"                               \n[2980] \"us acts experiences solitude far\"                                \n[2981] \"us acts experiences solitude apprehend\"                          \n[2982] \"us acts experiences solitude stand\"                              \n[2983] \"us acts individual men solitude\"                                 \n[2984] \"us acts individual men far\"                                      \n[2985] \"us acts individual men apprehend\"                                \n[2986] \"us acts individual solitude far\"                                 \n[2987] \"us acts individual solitude apprehend\"                           \n[2988] \"us acts individual solitude stand\"                               \n[2989] \"us acts individual far apprehend\"                                \n[2990] \"us acts individual far stand\"                                    \n[2991] \"us acts individual far relation\"                                 \n[2992] \"us acts men solitude far\"                                        \n[2993] \"us acts men solitude apprehend\"                                  \n[2994] \"us acts men solitude stand\"                                      \n[2995] \"us acts men far apprehend\"                                       \n[2996] \"us acts men far stand\"                                           \n[2997] \"us acts men far relation\"                                        \n[2998] \"us acts men apprehend stand\"                                     \n[2999] \"us acts men apprehend relation\"                                  \n[3000] \"us acts men apprehend whatever\"                                  \n[3001] \"_the feelings\"                                                   \n[3002] \"_the acts\"                                                       \n[3003] \"_the experiences\"                                                \n[3004] \"_the feelings acts\"                                              \n[3005] \"_the feelings experiences\"                                       \n[3006] \"_the feelings individual\"                                        \n[3007] \"_the acts experiences\"                                           \n[3008] \"_the acts individual\"                                            \n[3009] \"_the acts men\"                                                   \n[3010] \"_the experiences individual\"                                     \n[3011] \"_the experiences men\"                                            \n[3012] \"_the experiences solitude\"                                       \n[3013] \"_the feelings acts experiences\"                                  \n[3014] \"_the feelings acts individual\"                                   \n[3015] \"_the feelings acts men\"                                          \n[3016] \"_the feelings experiences individual\"                            \n[3017] \"_the feelings experiences men\"                                   \n[3018] \"_the feelings experiences solitude\"                              \n[3019] \"_the feelings individual men\"                                    \n[3020] \"_the feelings individual solitude\"                               \n[3021] \"_the feelings individual far\"                                    \n[3022] \"_the acts experiences individual\"                                \n[3023] \"_the acts experiences men\"                                       \n[3024] \"_the acts experiences solitude\"                                  \n[3025] \"_the acts individual men\"                                        \n[3026] \"_the acts individual solitude\"                                   \n[3027] \"_the acts individual far\"                                        \n[3028] \"_the acts men solitude\"                                          \n[3029] \"_the acts men far\"                                               \n[3030] \"_the acts men apprehend\"                                         \n[3031] \"_the experiences individual men\"                                 \n[3032] \"_the experiences individual solitude\"                            \n[3033] \"_the experiences individual far\"                                 \n[3034] \"_the experiences men solitude\"                                   \n[3035] \"_the experiences men far\"                                        \n[3036] \"_the experiences men apprehend\"                                  \n[3037] \"_the experiences solitude far\"                                   \n[3038] \"_the experiences solitude apprehend\"                             \n[3039] \"_the experiences solitude stand\"                                 \n[3040] \"_the feelings acts experiences individual\"                       \n[3041] \"_the feelings acts experiences men\"                              \n[3042] \"_the feelings acts experiences solitude\"                         \n[3043] \"_the feelings acts individual men\"                               \n[3044] \"_the feelings acts individual solitude\"                          \n[3045] \"_the feelings acts individual far\"                               \n[3046] \"_the feelings acts men solitude\"                                 \n[3047] \"_the feelings acts men far\"                                      \n[3048] \"_the feelings acts men apprehend\"                                \n[3049] \"_the feelings experiences individual men\"                        \n[3050] \"_the feelings experiences individual solitude\"                   \n[3051] \"_the feelings experiences individual far\"                        \n[3052] \"_the feelings experiences men solitude\"                          \n[3053] \"_the feelings experiences men far\"                               \n[3054] \"_the feelings experiences men apprehend\"                         \n[3055] \"_the feelings experiences solitude far\"                          \n[3056] \"_the feelings experiences solitude apprehend\"                    \n[3057] \"_the feelings experiences solitude stand\"                        \n[3058] \"_the feelings individual men solitude\"                           \n[3059] \"_the feelings individual men far\"                                \n[3060] \"_the feelings individual men apprehend\"                          \n[3061] \"_the feelings individual solitude far\"                           \n[3062] \"_the feelings individual solitude apprehend\"                     \n[3063] \"_the feelings individual solitude stand\"                         \n[3064] \"_the feelings individual far apprehend\"                          \n[3065] \"_the feelings individual far stand\"                              \n[3066] \"_the feelings individual far relation\"                           \n[3067] \"_the acts experiences individual men\"                            \n[3068] \"_the acts experiences individual solitude\"                       \n[3069] \"_the acts experiences individual far\"                            \n[3070] \"_the acts experiences men solitude\"                              \n[3071] \"_the acts experiences men far\"                                   \n[3072] \"_the acts experiences men apprehend\"                             \n[3073] \"_the acts experiences solitude far\"                              \n[3074] \"_the acts experiences solitude apprehend\"                        \n[3075] \"_the acts experiences solitude stand\"                            \n[3076] \"_the acts individual men solitude\"                               \n[3077] \"_the acts individual men far\"                                    \n[3078] \"_the acts individual men apprehend\"                              \n[3079] \"_the acts individual solitude far\"                               \n[3080] \"_the acts individual solitude apprehend\"                         \n[3081] \"_the acts individual solitude stand\"                             \n[3082] \"_the acts individual far apprehend\"                              \n[3083] \"_the acts individual far stand\"                                  \n[3084] \"_the acts individual far relation\"                               \n[3085] \"_the acts men solitude far\"                                      \n[3086] \"_the acts men solitude apprehend\"                                \n[3087] \"_the acts men solitude stand\"                                    \n[3088] \"_the acts men far apprehend\"                                     \n[3089] \"_the acts men far stand\"                                         \n[3090] \"_the acts men far relation\"                                      \n[3091] \"_the acts men apprehend stand\"                                   \n[3092] \"_the acts men apprehend relation\"                                \n[3093] \"_the acts men apprehend whatever\"                                \n[3094] \"_the experiences individual men solitude\"                        \n[3095] \"_the experiences individual men far\"                             \n[3096] \"_the experiences individual men apprehend\"                       \n[3097] \"_the experiences individual solitude far\"                        \n[3098] \"_the experiences individual solitude apprehend\"                  \n[3099] \"_the experiences individual solitude stand\"                      \n[3100] \"_the experiences individual far apprehend\"                       \n[3101] \"_the experiences individual far stand\"                           \n[3102] \"_the experiences individual far relation\"                        \n[3103] \"_the experiences men solitude far\"                               \n[3104] \"_the experiences men solitude apprehend\"                         \n[3105] \"_the experiences men solitude stand\"                             \n[3106] \"_the experiences men far apprehend\"                              \n[3107] \"_the experiences men far stand\"                                  \n[3108] \"_the experiences men far relation\"                               \n[3109] \"_the experiences men apprehend stand\"                            \n[3110] \"_the experiences men apprehend relation\"                         \n[3111] \"_the experiences men apprehend whatever\"                         \n[3112] \"_the experiences solitude far apprehend\"                         \n[3113] \"_the experiences solitude far stand\"                             \n[3114] \"_the experiences solitude far relation\"                          \n[3115] \"_the experiences solitude apprehend stand\"                       \n[3116] \"_the experiences solitude apprehend relation\"                    \n[3117] \"_the experiences solitude apprehend whatever\"                    \n[3118] \"_the experiences solitude stand relation\"                        \n[3119] \"_the experiences solitude stand whatever\"                        \n[3120] \"_the experiences solitude stand may\"                             \n[3121] \"feelings acts\"                                                   \n[3122] \"feelings experiences\"                                            \n[3123] \"feelings individual\"                                             \n[3124] \"feelings acts experiences\"                                       \n[3125] \"feelings acts individual\"                                        \n[3126] \"feelings acts men\"                                               \n[3127] \"feelings experiences individual\"                                 \n[3128] \"feelings experiences men\"                                        \n[3129] \"feelings experiences solitude\"                                   \n[3130] \"feelings individual men\"                                         \n[3131] \"feelings individual solitude\"                                    \n[3132] \"feelings individual far\"                                         \n[3133] \"feelings acts experiences individual\"                            \n[3134] \"feelings acts experiences men\"                                   \n[3135] \"feelings acts experiences solitude\"                              \n[3136] \"feelings acts individual men\"                                    \n[3137] \"feelings acts individual solitude\"                               \n[3138] \"feelings acts individual far\"                                    \n[3139] \"feelings acts men solitude\"                                      \n[3140] \"feelings acts men far\"                                           \n[3141] \"feelings acts men apprehend\"                                     \n[3142] \"feelings experiences individual men\"                             \n[3143] \"feelings experiences individual solitude\"                        \n[3144] \"feelings experiences individual far\"                             \n[3145] \"feelings experiences men solitude\"                               \n[3146] \"feelings experiences men far\"                                    \n[3147] \"feelings experiences men apprehend\"                              \n[3148] \"feelings experiences solitude far\"                               \n[3149] \"feelings experiences solitude apprehend\"                         \n[3150] \"feelings experiences solitude stand\"                             \n[3151] \"feelings individual men solitude\"                                \n[3152] \"feelings individual men far\"                                     \n[3153] \"feelings individual men apprehend\"                               \n[3154] \"feelings individual solitude far\"                                \n[3155] \"feelings individual solitude apprehend\"                          \n[3156] \"feelings individual solitude stand\"                              \n[3157] \"feelings individual far apprehend\"                               \n[3158] \"feelings individual far stand\"                                   \n[3159] \"feelings individual far relation\"                                \n[3160] \"feelings acts experiences individual men\"                        \n[3161] \"feelings acts experiences individual solitude\"                   \n[3162] \"feelings acts experiences individual far\"                        \n[3163] \"feelings acts experiences men solitude\"                          \n[3164] \"feelings acts experiences men far\"                               \n[3165] \"feelings acts experiences men apprehend\"                         \n[3166] \"feelings acts experiences solitude far\"                          \n[3167] \"feelings acts experiences solitude apprehend\"                    \n[3168] \"feelings acts experiences solitude stand\"                        \n[3169] \"feelings acts individual men solitude\"                           \n[3170] \"feelings acts individual men far\"                                \n[3171] \"feelings acts individual men apprehend\"                          \n[3172] \"feelings acts individual solitude far\"                           \n[3173] \"feelings acts individual solitude apprehend\"                     \n[3174] \"feelings acts individual solitude stand\"                         \n[3175] \"feelings acts individual far apprehend\"                          \n[3176] \"feelings acts individual far stand\"                              \n[3177] \"feelings acts individual far relation\"                           \n[3178] \"feelings acts men solitude far\"                                  \n[3179] \"feelings acts men solitude apprehend\"                            \n[3180] \"feelings acts men solitude stand\"                                \n[3181] \"feelings acts men far apprehend\"                                 \n[3182] \"feelings acts men far stand\"                                     \n[3183] \"feelings acts men far relation\"                                  \n[3184] \"feelings acts men apprehend stand\"                               \n[3185] \"feelings acts men apprehend relation\"                            \n[3186] \"feelings acts men apprehend whatever\"                            \n[3187] \"feelings experiences individual men solitude\"                    \n[3188] \"feelings experiences individual men far\"                         \n[3189] \"feelings experiences individual men apprehend\"                   \n[3190] \"feelings experiences individual solitude far\"                    \n[3191] \"feelings experiences individual solitude apprehend\"              \n[3192] \"feelings experiences individual solitude stand\"                  \n[3193] \"feelings experiences individual far apprehend\"                   \n[3194] \"feelings experiences individual far stand\"                       \n[3195] \"feelings experiences individual far relation\"                    \n[3196] \"feelings experiences men solitude far\"                           \n[3197] \"feelings experiences men solitude apprehend\"                     \n[3198] \"feelings experiences men solitude stand\"                         \n[3199] \"feelings experiences men far apprehend\"                          \n[3200] \"feelings experiences men far stand\"                              \n[3201] \"feelings experiences men far relation\"                           \n[3202] \"feelings experiences men apprehend stand\"                        \n[3203] \"feelings experiences men apprehend relation\"                     \n[3204] \"feelings experiences men apprehend whatever\"                     \n[3205] \"feelings experiences solitude far apprehend\"                     \n[3206] \"feelings experiences solitude far stand\"                         \n[3207] \"feelings experiences solitude far relation\"                      \n[3208] \"feelings experiences solitude apprehend stand\"                   \n[3209] \"feelings experiences solitude apprehend relation\"                \n[3210] \"feelings experiences solitude apprehend whatever\"                \n[3211] \"feelings experiences solitude stand relation\"                    \n[3212] \"feelings experiences solitude stand whatever\"                    \n[3213] \"feelings experiences solitude stand may\"                         \n[3214] \"feelings individual men solitude far\"                            \n[3215] \"feelings individual men solitude apprehend\"                      \n[3216] \"feelings individual men solitude stand\"                          \n[3217] \"feelings individual men far apprehend\"                           \n[3218] \"feelings individual men far stand\"                               \n[3219] \"feelings individual men far relation\"                            \n[3220] \"feelings individual men apprehend stand\"                         \n[3221] \"feelings individual men apprehend relation\"                      \n[3222] \"feelings individual men apprehend whatever\"                      \n[3223] \"feelings individual solitude far apprehend\"                      \n[3224] \"feelings individual solitude far stand\"                          \n[3225] \"feelings individual solitude far relation\"                       \n[3226] \"feelings individual solitude apprehend stand\"                    \n[3227] \"feelings individual solitude apprehend relation\"                 \n[3228] \"feelings individual solitude apprehend whatever\"                 \n[3229] \"feelings individual solitude stand relation\"                     \n[3230] \"feelings individual solitude stand whatever\"                     \n[3231] \"feelings individual solitude stand may\"                          \n[3232] \"feelings individual far apprehend stand\"                         \n[3233] \"feelings individual far apprehend relation\"                      \n[3234] \"feelings individual far apprehend whatever\"                      \n[3235] \"feelings individual far stand relation\"                          \n[3236] \"feelings individual far stand whatever\"                          \n[3237] \"feelings individual far stand may\"                               \n[3238] \"feelings individual far relation whatever\"                       \n[3239] \"feelings individual far relation may\"                            \n[3240] \"feelings individual far relation consider\"                       \n[3241] \"acts experiences\"                                                \n[3242] \"acts individual\"                                                 \n[3243] \"acts men\"                                                        \n[3244] \"acts experiences individual\"                                     \n[3245] \"acts experiences men\"                                            \n[3246] \"acts experiences solitude\"                                       \n[3247] \"acts individual men\"                                             \n[3248] \"acts individual solitude\"                                        \n[3249] \"acts individual far\"                                             \n[3250] \"acts men solitude\"                                               \n[3251] \"acts men far\"                                                    \n[3252] \"acts men apprehend\"                                              \n[3253] \"acts experiences individual men\"                                 \n[3254] \"acts experiences individual solitude\"                            \n[3255] \"acts experiences individual far\"                                 \n[3256] \"acts experiences men solitude\"                                   \n[3257] \"acts experiences men far\"                                        \n[3258] \"acts experiences men apprehend\"                                  \n[3259] \"acts experiences solitude far\"                                   \n[3260] \"acts experiences solitude apprehend\"                             \n[3261] \"acts experiences solitude stand\"                                 \n[3262] \"acts individual men solitude\"                                    \n[3263] \"acts individual men far\"                                         \n[3264] \"acts individual men apprehend\"                                   \n[3265] \"acts individual solitude far\"                                    \n[3266] \"acts individual solitude apprehend\"                              \n[3267] \"acts individual solitude stand\"                                  \n[3268] \"acts individual far apprehend\"                                   \n[3269] \"acts individual far stand\"                                       \n[3270] \"acts individual far relation\"                                    \n[3271] \"acts men solitude far\"                                           \n[3272] \"acts men solitude apprehend\"                                     \n[3273] \"acts men solitude stand\"                                         \n[3274] \"acts men far apprehend\"                                          \n[3275] \"acts men far stand\"                                              \n[3276] \"acts men far relation\"                                           \n[3277] \"acts men apprehend stand\"                                        \n[3278] \"acts men apprehend relation\"                                     \n[3279] \"acts men apprehend whatever\"                                     \n[3280] \"acts experiences individual men solitude\"                        \n[3281] \"acts experiences individual men far\"                             \n[3282] \"acts experiences individual men apprehend\"                       \n[3283] \"acts experiences individual solitude far\"                        \n[3284] \"acts experiences individual solitude apprehend\"                  \n[3285] \"acts experiences individual solitude stand\"                      \n[3286] \"acts experiences individual far apprehend\"                       \n[3287] \"acts experiences individual far stand\"                           \n[3288] \"acts experiences individual far relation\"                        \n[3289] \"acts experiences men solitude far\"                               \n[3290] \"acts experiences men solitude apprehend\"                         \n[3291] \"acts experiences men solitude stand\"                             \n[3292] \"acts experiences men far apprehend\"                              \n[3293] \"acts experiences men far stand\"                                  \n[3294] \"acts experiences men far relation\"                               \n[3295] \"acts experiences men apprehend stand\"                            \n[3296] \"acts experiences men apprehend relation\"                         \n[3297] \"acts experiences men apprehend whatever\"                         \n[3298] \"acts experiences solitude far apprehend\"                         \n[3299] \"acts experiences solitude far stand\"                             \n[3300] \"acts experiences solitude far relation\"                          \n[3301] \"acts experiences solitude apprehend stand\"                       \n[3302] \"acts experiences solitude apprehend relation\"                    \n[3303] \"acts experiences solitude apprehend whatever\"                    \n[3304] \"acts experiences solitude stand relation\"                        \n[3305] \"acts experiences solitude stand whatever\"                        \n[3306] \"acts experiences solitude stand may\"                             \n[3307] \"acts individual men solitude far\"                                \n[3308] \"acts individual men solitude apprehend\"                          \n[3309] \"acts individual men solitude stand\"                              \n[3310] \"acts individual men far apprehend\"                               \n[3311] \"acts individual men far stand\"                                   \n[3312] \"acts individual men far relation\"                                \n[3313] \"acts individual men apprehend stand\"                             \n[3314] \"acts individual men apprehend relation\"                          \n[3315] \"acts individual men apprehend whatever\"                          \n[3316] \"acts individual solitude far apprehend\"                          \n[3317] \"acts individual solitude far stand\"                              \n[3318] \"acts individual solitude far relation\"                           \n[3319] \"acts individual solitude apprehend stand\"                        \n[3320] \"acts individual solitude apprehend relation\"                     \n[3321] \"acts individual solitude apprehend whatever\"                     \n[3322] \"acts individual solitude stand relation\"                         \n[3323] \"acts individual solitude stand whatever\"                         \n[3324] \"acts individual solitude stand may\"                              \n[3325] \"acts individual far apprehend stand\"                             \n[3326] \"acts individual far apprehend relation\"                          \n[3327] \"acts individual far apprehend whatever\"                          \n[3328] \"acts individual far stand relation\"                              \n[3329] \"acts individual far stand whatever\"                              \n[3330] \"acts individual far stand may\"                                   \n[3331] \"acts individual far relation whatever\"                           \n[3332] \"acts individual far relation may\"                                \n[3333] \"acts individual far relation consider\"                           \n[3334] \"acts men solitude far apprehend\"                                 \n[3335] \"acts men solitude far stand\"                                     \n[3336] \"acts men solitude far relation\"                                  \n[3337] \"acts men solitude apprehend stand\"                               \n[3338] \"acts men solitude apprehend relation\"                            \n[3339] \"acts men solitude apprehend whatever\"                            \n[3340] \"acts men solitude stand relation\"                                \n[3341] \"acts men solitude stand whatever\"                                \n[3342] \"acts men solitude stand may\"                                     \n[3343] \"acts men far apprehend stand\"                                    \n[3344] \"acts men far apprehend relation\"                                 \n[3345] \"acts men far apprehend whatever\"                                 \n[3346] \"acts men far stand relation\"                                     \n[3347] \"acts men far stand whatever\"                                     \n[3348] \"acts men far stand may\"                                          \n[3349] \"acts men far relation whatever\"                                  \n[3350] \"acts men far relation may\"                                       \n[3351] \"acts men far relation consider\"                                  \n[3352] \"acts men apprehend stand relation\"                               \n[3353] \"acts men apprehend stand whatever\"                               \n[3354] \"acts men apprehend stand may\"                                    \n[3355] \"acts men apprehend relation whatever\"                            \n[3356] \"acts men apprehend relation may\"                                 \n[3357] \"acts men apprehend relation consider\"                            \n[3358] \"acts men apprehend whatever may\"                                 \n[3359] \"acts men apprehend whatever consider\"                            \n[3360] \"acts men apprehend whatever divine_\"                             \n[3361] \"experiences individual\"                                          \n[3362] \"experiences men\"                                                 \n[3363] \"experiences solitude\"                                            \n[3364] \"experiences individual men\"                                      \n[3365] \"experiences individual solitude\"                                 \n[3366] \"experiences individual far\"                                      \n[3367] \"experiences men solitude\"                                        \n[3368] \"experiences men far\"                                             \n[3369] \"experiences men apprehend\"                                       \n[3370] \"experiences solitude far\"                                        \n[3371] \"experiences solitude apprehend\"                                  \n[3372] \"experiences solitude stand\"                                      \n[3373] \"experiences individual men solitude\"                             \n[3374] \"experiences individual men far\"                                  \n[3375] \"experiences individual men apprehend\"                            \n[3376] \"experiences individual solitude far\"                             \n[3377] \"experiences individual solitude apprehend\"                       \n[3378] \"experiences individual solitude stand\"                           \n[3379] \"experiences individual far apprehend\"                            \n[3380] \"experiences individual far stand\"                                \n[3381] \"experiences individual far relation\"                             \n[3382] \"experiences men solitude far\"                                    \n[3383] \"experiences men solitude apprehend\"                              \n[3384] \"experiences men solitude stand\"                                  \n[3385] \"experiences men far apprehend\"                                   \n[3386] \"experiences men far stand\"                                       \n[3387] \"experiences men far relation\"                                    \n[3388] \"experiences men apprehend stand\"                                 \n[3389] \"experiences men apprehend relation\"                              \n[3390] \"experiences men apprehend whatever\"                              \n[3391] \"experiences solitude far apprehend\"                              \n[3392] \"experiences solitude far stand\"                                  \n[3393] \"experiences solitude far relation\"                               \n[3394] \"experiences solitude apprehend stand\"                            \n[3395] \"experiences solitude apprehend relation\"                         \n[3396] \"experiences solitude apprehend whatever\"                         \n[3397] \"experiences solitude stand relation\"                             \n[3398] \"experiences solitude stand whatever\"                             \n[3399] \"experiences solitude stand may\"                                  \n[3400] \"experiences individual men solitude far\"                         \n[3401] \"experiences individual men solitude apprehend\"                   \n[3402] \"experiences individual men solitude stand\"                       \n[3403] \"experiences individual men far apprehend\"                        \n[3404] \"experiences individual men far stand\"                            \n[3405] \"experiences individual men far relation\"                         \n[3406] \"experiences individual men apprehend stand\"                      \n[3407] \"experiences individual men apprehend relation\"                   \n[3408] \"experiences individual men apprehend whatever\"                   \n[3409] \"experiences individual solitude far apprehend\"                   \n[3410] \"experiences individual solitude far stand\"                       \n[3411] \"experiences individual solitude far relation\"                    \n[3412] \"experiences individual solitude apprehend stand\"                 \n[3413] \"experiences individual solitude apprehend relation\"              \n[3414] \"experiences individual solitude apprehend whatever\"              \n[3415] \"experiences individual solitude stand relation\"                  \n[3416] \"experiences individual solitude stand whatever\"                  \n[3417] \"experiences individual solitude stand may\"                       \n[3418] \"experiences individual far apprehend stand\"                      \n[3419] \"experiences individual far apprehend relation\"                   \n[3420] \"experiences individual far apprehend whatever\"                   \n[3421] \"experiences individual far stand relation\"                       \n[3422] \"experiences individual far stand whatever\"                       \n[3423] \"experiences individual far stand may\"                            \n[3424] \"experiences individual far relation whatever\"                    \n[3425] \"experiences individual far relation may\"                         \n[3426] \"experiences individual far relation consider\"                    \n[3427] \"experiences men solitude far apprehend\"                          \n[3428] \"experiences men solitude far stand\"                              \n[3429] \"experiences men solitude far relation\"                           \n[3430] \"experiences men solitude apprehend stand\"                        \n[3431] \"experiences men solitude apprehend relation\"                     \n[3432] \"experiences men solitude apprehend whatever\"                     \n[3433] \"experiences men solitude stand relation\"                         \n[3434] \"experiences men solitude stand whatever\"                         \n[3435] \"experiences men solitude stand may\"                              \n[3436] \"experiences men far apprehend stand\"                             \n[3437] \"experiences men far apprehend relation\"                          \n[3438] \"experiences men far apprehend whatever\"                          \n[3439] \"experiences men far stand relation\"                              \n[3440] \"experiences men far stand whatever\"                              \n[3441] \"experiences men far stand may\"                                   \n[3442] \"experiences men far relation whatever\"                           \n[3443] \"experiences men far relation may\"                                \n[3444] \"experiences men far relation consider\"                           \n[3445] \"experiences men apprehend stand relation\"                        \n[3446] \"experiences men apprehend stand whatever\"                        \n[3447] \"experiences men apprehend stand may\"                             \n[3448] \"experiences men apprehend relation whatever\"                     \n[3449] \"experiences men apprehend relation may\"                          \n[3450] \"experiences men apprehend relation consider\"                     \n[3451] \"experiences men apprehend whatever may\"                          \n[3452] \"experiences men apprehend whatever consider\"                     \n[3453] \"experiences men apprehend whatever divine_\"                      \n[3454] \"experiences solitude far apprehend stand\"                        \n[3455] \"experiences solitude far apprehend relation\"                     \n[3456] \"experiences solitude far apprehend whatever\"                     \n[3457] \"experiences solitude far stand relation\"                         \n[3458] \"experiences solitude far stand whatever\"                         \n[3459] \"experiences solitude far stand may\"                              \n[3460] \"experiences solitude far relation whatever\"                      \n[3461] \"experiences solitude far relation may\"                           \n[3462] \"experiences solitude far relation consider\"                      \n[3463] \"experiences solitude apprehend stand relation\"                   \n[3464] \"experiences solitude apprehend stand whatever\"                   \n[3465] \"experiences solitude apprehend stand may\"                        \n[3466] \"experiences solitude apprehend relation whatever\"                \n[3467] \"experiences solitude apprehend relation may\"                     \n[3468] \"experiences solitude apprehend relation consider\"                \n[3469] \"experiences solitude apprehend whatever may\"                     \n[3470] \"experiences solitude apprehend whatever consider\"                \n[3471] \"experiences solitude apprehend whatever divine_\"                 \n[3472] \"experiences solitude stand relation whatever\"                    \n[3473] \"experiences solitude stand relation may\"                         \n[3474] \"experiences solitude stand relation consider\"                    \n[3475] \"experiences solitude stand whatever may\"                         \n[3476] \"experiences solitude stand whatever consider\"                    \n[3477] \"experiences solitude stand whatever divine_\"                     \n[3478] \"experiences solitude stand may consider\"                         \n[3479] \"experiences solitude stand may divine_\"                          \n[3480] \"experiences solitude stand may since\"                            \n[3481] \"individual men\"                                                  \n[3482] \"individual solitude\"                                             \n[3483] \"individual far\"                                                  \n[3484] \"individual men solitude\"                                         \n[3485] \"individual men far\"                                              \n[3486] \"individual men apprehend\"                                        \n[3487] \"individual solitude far\"                                         \n[3488] \"individual solitude apprehend\"                                   \n[3489] \"individual solitude stand\"                                       \n[3490] \"individual far apprehend\"                                        \n[3491] \"individual far stand\"                                            \n[3492] \"individual far relation\"                                         \n[3493] \"individual men solitude far\"                                     \n[3494] \"individual men solitude apprehend\"                               \n[3495] \"individual men solitude stand\"                                   \n[3496] \"individual men far apprehend\"                                    \n[3497] \"individual men far stand\"                                        \n[3498] \"individual men far relation\"                                     \n[3499] \"individual men apprehend stand\"                                  \n[3500] \"individual men apprehend relation\"                               \n[3501] \"individual men apprehend whatever\"                               \n[3502] \"individual solitude far apprehend\"                               \n[3503] \"individual solitude far stand\"                                   \n[3504] \"individual solitude far relation\"                                \n[3505] \"individual solitude apprehend stand\"                             \n[3506] \"individual solitude apprehend relation\"                          \n[3507] \"individual solitude apprehend whatever\"                          \n[3508] \"individual solitude stand relation\"                              \n[3509] \"individual solitude stand whatever\"                              \n[3510] \"individual solitude stand may\"                                   \n[3511] \"individual far apprehend stand\"                                  \n[3512] \"individual far apprehend relation\"                               \n[3513] \"individual far apprehend whatever\"                               \n[3514] \"individual far stand relation\"                                   \n[3515] \"individual far stand whatever\"                                   \n[3516] \"individual far stand may\"                                        \n[3517] \"individual far relation whatever\"                                \n[3518] \"individual far relation may\"                                     \n[3519] \"individual far relation consider\"                                \n[3520] \"individual men solitude far apprehend\"                           \n[3521] \"individual men solitude far stand\"                               \n[3522] \"individual men solitude far relation\"                            \n[3523] \"individual men solitude apprehend stand\"                         \n[3524] \"individual men solitude apprehend relation\"                      \n[3525] \"individual men solitude apprehend whatever\"                      \n[3526] \"individual men solitude stand relation\"                          \n[3527] \"individual men solitude stand whatever\"                          \n[3528] \"individual men solitude stand may\"                               \n[3529] \"individual men far apprehend stand\"                              \n[3530] \"individual men far apprehend relation\"                           \n[3531] \"individual men far apprehend whatever\"                           \n[3532] \"individual men far stand relation\"                               \n[3533] \"individual men far stand whatever\"                               \n[3534] \"individual men far stand may\"                                    \n[3535] \"individual men far relation whatever\"                            \n[3536] \"individual men far relation may\"                                 \n[3537] \"individual men far relation consider\"                            \n[3538] \"individual men apprehend stand relation\"                         \n[3539] \"individual men apprehend stand whatever\"                         \n[3540] \"individual men apprehend stand may\"                              \n[3541] \"individual men apprehend relation whatever\"                      \n[3542] \"individual men apprehend relation may\"                           \n[3543] \"individual men apprehend relation consider\"                      \n[3544] \"individual men apprehend whatever may\"                           \n[3545] \"individual men apprehend whatever consider\"                      \n[3546] \"individual men apprehend whatever divine_\"                       \n[3547] \"individual solitude far apprehend stand\"                         \n[3548] \"individual solitude far apprehend relation\"                      \n[3549] \"individual solitude far apprehend whatever\"                      \n[3550] \"individual solitude far stand relation\"                          \n[3551] \"individual solitude far stand whatever\"                          \n[3552] \"individual solitude far stand may\"                               \n[3553] \"individual solitude far relation whatever\"                       \n[3554] \"individual solitude far relation may\"                            \n[3555] \"individual solitude far relation consider\"                       \n[3556] \"individual solitude apprehend stand relation\"                    \n[3557] \"individual solitude apprehend stand whatever\"                    \n[3558] \"individual solitude apprehend stand may\"                         \n[3559] \"individual solitude apprehend relation whatever\"                 \n[3560] \"individual solitude apprehend relation may\"                      \n[3561] \"individual solitude apprehend relation consider\"                 \n[3562] \"individual solitude apprehend whatever may\"                      \n[3563] \"individual solitude apprehend whatever consider\"                 \n[3564] \"individual solitude apprehend whatever divine_\"                  \n[3565] \"individual solitude stand relation whatever\"                     \n[3566] \"individual solitude stand relation may\"                          \n[3567] \"individual solitude stand relation consider\"                     \n[3568] \"individual solitude stand whatever may\"                          \n[3569] \"individual solitude stand whatever consider\"                     \n[3570] \"individual solitude stand whatever divine_\"                      \n[3571] \"individual solitude stand may consider\"                          \n[3572] \"individual solitude stand may divine_\"                           \n[3573] \"individual solitude stand may since\"                             \n[3574] \"individual far apprehend stand relation\"                         \n[3575] \"individual far apprehend stand whatever\"                         \n[3576] \"individual far apprehend stand may\"                              \n[3577] \"individual far apprehend relation whatever\"                      \n[3578] \"individual far apprehend relation may\"                           \n[3579] \"individual far apprehend relation consider\"                      \n[3580] \"individual far apprehend whatever may\"                           \n[3581] \"individual far apprehend whatever consider\"                      \n[3582] \"individual far apprehend whatever divine_\"                       \n[3583] \"individual far stand relation whatever\"                          \n[3584] \"individual far stand relation may\"                               \n[3585] \"individual far stand relation consider\"                          \n[3586] \"individual far stand whatever may\"                               \n[3587] \"individual far stand whatever consider\"                          \n[3588] \"individual far stand whatever divine_\"                           \n[3589] \"individual far stand may consider\"                               \n[3590] \"individual far stand may divine_\"                                \n[3591] \"individual far stand may since\"                                  \n[3592] \"individual far relation whatever may\"                            \n[3593] \"individual far relation whatever consider\"                       \n[3594] \"individual far relation whatever divine_\"                        \n[3595] \"individual far relation may consider\"                            \n[3596] \"individual far relation may divine_\"                             \n[3597] \"individual far relation may since\"                               \n[3598] \"individual far relation consider divine_\"                        \n[3599] \"individual far relation consider since\"                          \n[3600] \"individual far relation consider relation\"                       \n[3601] \"men solitude\"                                                    \n[3602] \"men far\"                                                         \n[3603] \"men apprehend\"                                                   \n[3604] \"men solitude far\"                                                \n[3605] \"men solitude apprehend\"                                          \n[3606] \"men solitude stand\"                                              \n[3607] \"men far apprehend\"                                               \n[3608] \"men far stand\"                                                   \n[3609] \"men far relation\"                                                \n[3610] \"men apprehend stand\"                                             \n[3611] \"men apprehend relation\"                                          \n[3612] \"men apprehend whatever\"                                          \n[3613] \"men solitude far apprehend\"                                      \n[3614] \"men solitude far stand\"                                          \n[3615] \"men solitude far relation\"                                       \n[3616] \"men solitude apprehend stand\"                                    \n[3617] \"men solitude apprehend relation\"                                 \n[3618] \"men solitude apprehend whatever\"                                 \n[3619] \"men solitude stand relation\"                                     \n[3620] \"men solitude stand whatever\"                                     \n[3621] \"men solitude stand may\"                                          \n[3622] \"men far apprehend stand\"                                         \n[3623] \"men far apprehend relation\"                                      \n[3624] \"men far apprehend whatever\"                                      \n[3625] \"men far stand relation\"                                          \n[3626] \"men far stand whatever\"                                          \n[3627] \"men far stand may\"                                               \n[3628] \"men far relation whatever\"                                       \n[3629] \"men far relation may\"                                            \n[3630] \"men far relation consider\"                                       \n[3631] \"men apprehend stand relation\"                                    \n[3632] \"men apprehend stand whatever\"                                    \n[3633] \"men apprehend stand may\"                                         \n[3634] \"men apprehend relation whatever\"                                 \n[3635] \"men apprehend relation may\"                                      \n[3636] \"men apprehend relation consider\"                                 \n[3637] \"men apprehend whatever may\"                                      \n[3638] \"men apprehend whatever consider\"                                 \n[3639] \"men apprehend whatever divine_\"                                  \n[3640] \"men solitude far apprehend stand\"                                \n[3641] \"men solitude far apprehend relation\"                             \n[3642] \"men solitude far apprehend whatever\"                             \n[3643] \"men solitude far stand relation\"                                 \n[3644] \"men solitude far stand whatever\"                                 \n[3645] \"men solitude far stand may\"                                      \n[3646] \"men solitude far relation whatever\"                              \n[3647] \"men solitude far relation may\"                                   \n[3648] \"men solitude far relation consider\"                              \n[3649] \"men solitude apprehend stand relation\"                           \n[3650] \"men solitude apprehend stand whatever\"                           \n[3651] \"men solitude apprehend stand may\"                                \n[3652] \"men solitude apprehend relation whatever\"                        \n[3653] \"men solitude apprehend relation may\"                             \n[3654] \"men solitude apprehend relation consider\"                        \n[3655] \"men solitude apprehend whatever may\"                             \n[3656] \"men solitude apprehend whatever consider\"                        \n[3657] \"men solitude apprehend whatever divine_\"                         \n[3658] \"men solitude stand relation whatever\"                            \n[3659] \"men solitude stand relation may\"                                 \n[3660] \"men solitude stand relation consider\"                            \n[3661] \"men solitude stand whatever may\"                                 \n[3662] \"men solitude stand whatever consider\"                            \n[3663] \"men solitude stand whatever divine_\"                             \n[3664] \"men solitude stand may consider\"                                 \n[3665] \"men solitude stand may divine_\"                                  \n[3666] \"men solitude stand may since\"                                    \n[3667] \"men far apprehend stand relation\"                                \n[3668] \"men far apprehend stand whatever\"                                \n[3669] \"men far apprehend stand may\"                                     \n[3670] \"men far apprehend relation whatever\"                             \n[3671] \"men far apprehend relation may\"                                  \n[3672] \"men far apprehend relation consider\"                             \n[3673] \"men far apprehend whatever may\"                                  \n[3674] \"men far apprehend whatever consider\"                             \n[3675] \"men far apprehend whatever divine_\"                              \n[3676] \"men far stand relation whatever\"                                 \n[3677] \"men far stand relation may\"                                      \n[3678] \"men far stand relation consider\"                                 \n[3679] \"men far stand whatever may\"                                      \n[3680] \"men far stand whatever consider\"                                 \n[3681] \"men far stand whatever divine_\"                                  \n[3682] \"men far stand may consider\"                                      \n[3683] \"men far stand may divine_\"                                       \n[3684] \"men far stand may since\"                                         \n[3685] \"men far relation whatever may\"                                   \n[3686] \"men far relation whatever consider\"                              \n[3687] \"men far relation whatever divine_\"                               \n[3688] \"men far relation may consider\"                                   \n[3689] \"men far relation may divine_\"                                    \n[3690] \"men far relation may since\"                                      \n[3691] \"men far relation consider divine_\"                               \n[3692] \"men far relation consider since\"                                 \n[3693] \"men far relation consider relation\"                              \n[3694] \"men apprehend stand relation whatever\"                           \n[3695] \"men apprehend stand relation may\"                                \n[3696] \"men apprehend stand relation consider\"                           \n[3697] \"men apprehend stand whatever may\"                                \n[3698] \"men apprehend stand whatever consider\"                           \n[3699] \"men apprehend stand whatever divine_\"                            \n[3700] \"men apprehend stand may consider\"                                \n[3701] \"men apprehend stand may divine_\"                                 \n[3702] \"men apprehend stand may since\"                                   \n[3703] \"men apprehend relation whatever may\"                             \n[3704] \"men apprehend relation whatever consider\"                        \n[3705] \"men apprehend relation whatever divine_\"                         \n[3706] \"men apprehend relation may consider\"                             \n[3707] \"men apprehend relation may divine_\"                              \n[3708] \"men apprehend relation may since\"                                \n[3709] \"men apprehend relation consider divine_\"                         \n[3710] \"men apprehend relation consider since\"                           \n[3711] \"men apprehend relation consider relation\"                        \n[3712] \"men apprehend whatever may consider\"                             \n[3713] \"men apprehend whatever may divine_\"                              \n[3714] \"men apprehend whatever may since\"                                \n[3715] \"men apprehend whatever consider divine_\"                         \n[3716] \"men apprehend whatever consider since\"                           \n[3717] \"men apprehend whatever consider relation\"                        \n[3718] \"men apprehend whatever divine_ since\"                            \n[3719] \"men apprehend whatever divine_ relation\"                         \n[3720] \"men apprehend whatever divine_ may\"                              \n[3721] \"solitude far\"                                                    \n[3722] \"solitude apprehend\"                                              \n[3723] \"solitude stand\"                                                  \n[3724] \"solitude far apprehend\"                                          \n[3725] \"solitude far stand\"                                              \n[3726] \"solitude far relation\"                                           \n[3727] \"solitude apprehend stand\"                                        \n[3728] \"solitude apprehend relation\"                                     \n[3729] \"solitude apprehend whatever\"                                     \n[3730] \"solitude stand relation\"                                         \n[3731] \"solitude stand whatever\"                                         \n[3732] \"solitude stand may\"                                              \n[3733] \"solitude far apprehend stand\"                                    \n[3734] \"solitude far apprehend relation\"                                 \n[3735] \"solitude far apprehend whatever\"                                 \n[3736] \"solitude far stand relation\"                                     \n[3737] \"solitude far stand whatever\"                                     \n[3738] \"solitude far stand may\"                                          \n[3739] \"solitude far relation whatever\"                                  \n[3740] \"solitude far relation may\"                                       \n[3741] \"solitude far relation consider\"                                  \n[3742] \"solitude apprehend stand relation\"                               \n[3743] \"solitude apprehend stand whatever\"                               \n[3744] \"solitude apprehend stand may\"                                    \n[3745] \"solitude apprehend relation whatever\"                            \n[3746] \"solitude apprehend relation may\"                                 \n[3747] \"solitude apprehend relation consider\"                            \n[3748] \"solitude apprehend whatever may\"                                 \n[3749] \"solitude apprehend whatever consider\"                            \n[3750] \"solitude apprehend whatever divine_\"                             \n[3751] \"solitude stand relation whatever\"                                \n[3752] \"solitude stand relation may\"                                     \n[3753] \"solitude stand relation consider\"                                \n[3754] \"solitude stand whatever may\"                                     \n[3755] \"solitude stand whatever consider\"                                \n[3756] \"solitude stand whatever divine_\"                                 \n[3757] \"solitude stand may consider\"                                     \n[3758] \"solitude stand may divine_\"                                      \n[3759] \"solitude stand may since\"                                        \n[3760] \"solitude far apprehend stand relation\"                           \n[3761] \"solitude far apprehend stand whatever\"                           \n[3762] \"solitude far apprehend stand may\"                                \n[3763] \"solitude far apprehend relation whatever\"                        \n[3764] \"solitude far apprehend relation may\"                             \n[3765] \"solitude far apprehend relation consider\"                        \n[3766] \"solitude far apprehend whatever may\"                             \n[3767] \"solitude far apprehend whatever consider\"                        \n[3768] \"solitude far apprehend whatever divine_\"                         \n[3769] \"solitude far stand relation whatever\"                            \n[3770] \"solitude far stand relation may\"                                 \n[3771] \"solitude far stand relation consider\"                            \n[3772] \"solitude far stand whatever may\"                                 \n[3773] \"solitude far stand whatever consider\"                            \n[3774] \"solitude far stand whatever divine_\"                             \n[3775] \"solitude far stand may consider\"                                 \n[3776] \"solitude far stand may divine_\"                                  \n[3777] \"solitude far stand may since\"                                    \n[3778] \"solitude far relation whatever may\"                              \n[3779] \"solitude far relation whatever consider\"                         \n[3780] \"solitude far relation whatever divine_\"                          \n[3781] \"solitude far relation may consider\"                              \n[3782] \"solitude far relation may divine_\"                               \n[3783] \"solitude far relation may since\"                                 \n[3784] \"solitude far relation consider divine_\"                          \n[3785] \"solitude far relation consider since\"                            \n[3786] \"solitude far relation consider relation\"                         \n[3787] \"solitude apprehend stand relation whatever\"                      \n[3788] \"solitude apprehend stand relation may\"                           \n[3789] \"solitude apprehend stand relation consider\"                      \n[3790] \"solitude apprehend stand whatever may\"                           \n[3791] \"solitude apprehend stand whatever consider\"                      \n[3792] \"solitude apprehend stand whatever divine_\"                       \n[3793] \"solitude apprehend stand may consider\"                           \n[3794] \"solitude apprehend stand may divine_\"                            \n[3795] \"solitude apprehend stand may since\"                              \n[3796] \"solitude apprehend relation whatever may\"                        \n[3797] \"solitude apprehend relation whatever consider\"                   \n[3798] \"solitude apprehend relation whatever divine_\"                    \n[3799] \"solitude apprehend relation may consider\"                        \n[3800] \"solitude apprehend relation may divine_\"                         \n[3801] \"solitude apprehend relation may since\"                           \n[3802] \"solitude apprehend relation consider divine_\"                    \n[3803] \"solitude apprehend relation consider since\"                      \n[3804] \"solitude apprehend relation consider relation\"                   \n[3805] \"solitude apprehend whatever may consider\"                        \n[3806] \"solitude apprehend whatever may divine_\"                         \n[3807] \"solitude apprehend whatever may since\"                           \n[3808] \"solitude apprehend whatever consider divine_\"                    \n[3809] \"solitude apprehend whatever consider since\"                      \n[3810] \"solitude apprehend whatever consider relation\"                   \n[3811] \"solitude apprehend whatever divine_ since\"                       \n[3812] \"solitude apprehend whatever divine_ relation\"                    \n[3813] \"solitude apprehend whatever divine_ may\"                         \n[3814] \"solitude stand relation whatever may\"                            \n[3815] \"solitude stand relation whatever consider\"                       \n[3816] \"solitude stand relation whatever divine_\"                        \n[3817] \"solitude stand relation may consider\"                            \n[3818] \"solitude stand relation may divine_\"                             \n[3819] \"solitude stand relation may since\"                               \n[3820] \"solitude stand relation consider divine_\"                        \n[3821] \"solitude stand relation consider since\"                          \n[3822] \"solitude stand relation consider relation\"                       \n[3823] \"solitude stand whatever may consider\"                            \n[3824] \"solitude stand whatever may divine_\"                             \n[3825] \"solitude stand whatever may since\"                               \n[3826] \"solitude stand whatever consider divine_\"                        \n[3827] \"solitude stand whatever consider since\"                          \n[3828] \"solitude stand whatever consider relation\"                       \n[3829] \"solitude stand whatever divine_ since\"                           \n[3830] \"solitude stand whatever divine_ relation\"                        \n[3831] \"solitude stand whatever divine_ may\"                             \n[3832] \"solitude stand may consider divine_\"                             \n[3833] \"solitude stand may consider since\"                               \n[3834] \"solitude stand may consider relation\"                            \n[3835] \"solitude stand may divine_ since\"                                \n[3836] \"solitude stand may divine_ relation\"                             \n[3837] \"solitude stand may divine_ may\"                                  \n[3838] \"solitude stand may since relation\"                               \n[3839] \"solitude stand may since may\"                                    \n[3840] \"solitude stand may since either\"                                 \n[3841] \"far apprehend\"                                                   \n[3842] \"far stand\"                                                       \n[3843] \"far relation\"                                                    \n[3844] \"far apprehend stand\"                                             \n[3845] \"far apprehend relation\"                                          \n[3846] \"far apprehend whatever\"                                          \n[3847] \"far stand relation\"                                              \n[3848] \"far stand whatever\"                                              \n[3849] \"far stand may\"                                                   \n[3850] \"far relation whatever\"                                           \n[3851] \"far relation may\"                                                \n[3852] \"far relation consider\"                                           \n[3853] \"far apprehend stand relation\"                                    \n[3854] \"far apprehend stand whatever\"                                    \n[3855] \"far apprehend stand may\"                                         \n[3856] \"far apprehend relation whatever\"                                 \n[3857] \"far apprehend relation may\"                                      \n[3858] \"far apprehend relation consider\"                                 \n[3859] \"far apprehend whatever may\"                                      \n[3860] \"far apprehend whatever consider\"                                 \n[3861] \"far apprehend whatever divine_\"                                  \n[3862] \"far stand relation whatever\"                                     \n[3863] \"far stand relation may\"                                          \n[3864] \"far stand relation consider\"                                     \n[3865] \"far stand whatever may\"                                          \n[3866] \"far stand whatever consider\"                                     \n[3867] \"far stand whatever divine_\"                                      \n[3868] \"far stand may consider\"                                          \n[3869] \"far stand may divine_\"                                           \n[3870] \"far stand may since\"                                             \n[3871] \"far relation whatever may\"                                       \n[3872] \"far relation whatever consider\"                                  \n[3873] \"far relation whatever divine_\"                                   \n[3874] \"far relation may consider\"                                       \n[3875] \"far relation may divine_\"                                        \n[3876] \"far relation may since\"                                          \n[3877] \"far relation consider divine_\"                                   \n[3878] \"far relation consider since\"                                     \n[3879] \"far relation consider relation\"                                  \n[3880] \"far apprehend stand relation whatever\"                           \n[3881] \"far apprehend stand relation may\"                                \n[3882] \"far apprehend stand relation consider\"                           \n[3883] \"far apprehend stand whatever may\"                                \n[3884] \"far apprehend stand whatever consider\"                           \n[3885] \"far apprehend stand whatever divine_\"                            \n[3886] \"far apprehend stand may consider\"                                \n[3887] \"far apprehend stand may divine_\"                                 \n[3888] \"far apprehend stand may since\"                                   \n[3889] \"far apprehend relation whatever may\"                             \n[3890] \"far apprehend relation whatever consider\"                        \n[3891] \"far apprehend relation whatever divine_\"                         \n[3892] \"far apprehend relation may consider\"                             \n[3893] \"far apprehend relation may divine_\"                              \n[3894] \"far apprehend relation may since\"                                \n[3895] \"far apprehend relation consider divine_\"                         \n[3896] \"far apprehend relation consider since\"                           \n[3897] \"far apprehend relation consider relation\"                        \n[3898] \"far apprehend whatever may consider\"                             \n[3899] \"far apprehend whatever may divine_\"                              \n[3900] \"far apprehend whatever may since\"                                \n[3901] \"far apprehend whatever consider divine_\"                         \n[3902] \"far apprehend whatever consider since\"                           \n[3903] \"far apprehend whatever consider relation\"                        \n[3904] \"far apprehend whatever divine_ since\"                            \n[3905] \"far apprehend whatever divine_ relation\"                         \n[3906] \"far apprehend whatever divine_ may\"                              \n[3907] \"far stand relation whatever may\"                                 \n[3908] \"far stand relation whatever consider\"                            \n[3909] \"far stand relation whatever divine_\"                             \n[3910] \"far stand relation may consider\"                                 \n[3911] \"far stand relation may divine_\"                                  \n[3912] \"far stand relation may since\"                                    \n[3913] \"far stand relation consider divine_\"                             \n[3914] \"far stand relation consider since\"                               \n[3915] \"far stand relation consider relation\"                            \n[3916] \"far stand whatever may consider\"                                 \n[3917] \"far stand whatever may divine_\"                                  \n[3918] \"far stand whatever may since\"                                    \n[3919] \"far stand whatever consider divine_\"                             \n[3920] \"far stand whatever consider since\"                               \n[3921] \"far stand whatever consider relation\"                            \n[3922] \"far stand whatever divine_ since\"                                \n[3923] \"far stand whatever divine_ relation\"                             \n[3924] \"far stand whatever divine_ may\"                                  \n[3925] \"far stand may consider divine_\"                                  \n[3926] \"far stand may consider since\"                                    \n[3927] \"far stand may consider relation\"                                 \n[3928] \"far stand may divine_ since\"                                     \n[3929] \"far stand may divine_ relation\"                                  \n[3930] \"far stand may divine_ may\"                                       \n[3931] \"far stand may since relation\"                                    \n[3932] \"far stand may since may\"                                         \n[3933] \"far stand may since either\"                                      \n[3934] \"far relation whatever may consider\"                              \n[3935] \"far relation whatever may divine_\"                               \n[3936] \"far relation whatever may since\"                                 \n[3937] \"far relation whatever consider divine_\"                          \n[3938] \"far relation whatever consider since\"                            \n[3939] \"far relation whatever consider relation\"                         \n[3940] \"far relation whatever divine_ since\"                             \n[3941] \"far relation whatever divine_ relation\"                          \n[3942] \"far relation whatever divine_ may\"                               \n[3943] \"far relation may consider divine_\"                               \n[3944] \"far relation may consider since\"                                 \n[3945] \"far relation may consider relation\"                              \n[3946] \"far relation may divine_ since\"                                  \n[3947] \"far relation may divine_ relation\"                               \n[3948] \"far relation may divine_ may\"                                    \n[3949] \"far relation may since relation\"                                 \n[3950] \"far relation may since may\"                                      \n[3951] \"far relation may since either\"                                   \n[3952] \"far relation consider divine_ since\"                             \n[3953] \"far relation consider divine_ relation\"                          \n[3954] \"far relation consider divine_ may\"                               \n[3955] \"far relation consider since relation\"                            \n[3956] \"far relation consider since may\"                                 \n[3957] \"far relation consider since either\"                              \n[3958] \"far relation consider relation may\"                              \n[3959] \"far relation consider relation either\"                           \n[3960] \"far relation consider relation moral\"                            \n[3961] \"apprehend stand\"                                                 \n[3962] \"apprehend relation\"                                              \n[3963] \"apprehend whatever\"                                              \n[3964] \"apprehend stand relation\"                                        \n[3965] \"apprehend stand whatever\"                                        \n[3966] \"apprehend stand may\"                                             \n[3967] \"apprehend relation whatever\"                                     \n[3968] \"apprehend relation may\"                                          \n[3969] \"apprehend relation consider\"                                     \n[3970] \"apprehend whatever may\"                                          \n[3971] \"apprehend whatever consider\"                                     \n[3972] \"apprehend whatever divine_\"                                      \n[3973] \"apprehend stand relation whatever\"                               \n[3974] \"apprehend stand relation may\"                                    \n[3975] \"apprehend stand relation consider\"                               \n[3976] \"apprehend stand whatever may\"                                    \n[3977] \"apprehend stand whatever consider\"                               \n[3978] \"apprehend stand whatever divine_\"                                \n[3979] \"apprehend stand may consider\"                                    \n[3980] \"apprehend stand may divine_\"                                     \n[3981] \"apprehend stand may since\"                                       \n[3982] \"apprehend relation whatever may\"                                 \n[3983] \"apprehend relation whatever consider\"                            \n[3984] \"apprehend relation whatever divine_\"                             \n[3985] \"apprehend relation may consider\"                                 \n[3986] \"apprehend relation may divine_\"                                  \n[3987] \"apprehend relation may since\"                                    \n[3988] \"apprehend relation consider divine_\"                             \n[3989] \"apprehend relation consider since\"                               \n[3990] \"apprehend relation consider relation\"                            \n[3991] \"apprehend whatever may consider\"                                 \n[3992] \"apprehend whatever may divine_\"                                  \n[3993] \"apprehend whatever may since\"                                    \n[3994] \"apprehend whatever consider divine_\"                             \n[3995] \"apprehend whatever consider since\"                               \n[3996] \"apprehend whatever consider relation\"                            \n[3997] \"apprehend whatever divine_ since\"                                \n[3998] \"apprehend whatever divine_ relation\"                             \n[3999] \"apprehend whatever divine_ may\"                                  \n[4000] \"apprehend stand relation whatever may\"                           \n[4001] \"apprehend stand relation whatever consider\"                      \n[4002] \"apprehend stand relation whatever divine_\"                       \n[4003] \"apprehend stand relation may consider\"                           \n[4004] \"apprehend stand relation may divine_\"                            \n[4005] \"apprehend stand relation may since\"                              \n[4006] \"apprehend stand relation consider divine_\"                       \n[4007] \"apprehend stand relation consider since\"                         \n[4008] \"apprehend stand relation consider relation\"                      \n[4009] \"apprehend stand whatever may consider\"                           \n[4010] \"apprehend stand whatever may divine_\"                            \n[4011] \"apprehend stand whatever may since\"                              \n[4012] \"apprehend stand whatever consider divine_\"                       \n[4013] \"apprehend stand whatever consider since\"                         \n[4014] \"apprehend stand whatever consider relation\"                      \n[4015] \"apprehend stand whatever divine_ since\"                          \n[4016] \"apprehend stand whatever divine_ relation\"                       \n[4017] \"apprehend stand whatever divine_ may\"                            \n[4018] \"apprehend stand may consider divine_\"                            \n[4019] \"apprehend stand may consider since\"                              \n[4020] \"apprehend stand may consider relation\"                           \n[4021] \"apprehend stand may divine_ since\"                               \n[4022] \"apprehend stand may divine_ relation\"                            \n[4023] \"apprehend stand may divine_ may\"                                 \n[4024] \"apprehend stand may since relation\"                              \n[4025] \"apprehend stand may since may\"                                   \n[4026] \"apprehend stand may since either\"                                \n[4027] \"apprehend relation whatever may consider\"                        \n[4028] \"apprehend relation whatever may divine_\"                         \n[4029] \"apprehend relation whatever may since\"                           \n[4030] \"apprehend relation whatever consider divine_\"                    \n[4031] \"apprehend relation whatever consider since\"                      \n[4032] \"apprehend relation whatever consider relation\"                   \n[4033] \"apprehend relation whatever divine_ since\"                       \n[4034] \"apprehend relation whatever divine_ relation\"                    \n[4035] \"apprehend relation whatever divine_ may\"                         \n[4036] \"apprehend relation may consider divine_\"                         \n[4037] \"apprehend relation may consider since\"                           \n[4038] \"apprehend relation may consider relation\"                        \n[4039] \"apprehend relation may divine_ since\"                            \n[4040] \"apprehend relation may divine_ relation\"                         \n[4041] \"apprehend relation may divine_ may\"                              \n[4042] \"apprehend relation may since relation\"                           \n[4043] \"apprehend relation may since may\"                                \n[4044] \"apprehend relation may since either\"                             \n[4045] \"apprehend relation consider divine_ since\"                       \n[4046] \"apprehend relation consider divine_ relation\"                    \n[4047] \"apprehend relation consider divine_ may\"                         \n[4048] \"apprehend relation consider since relation\"                      \n[4049] \"apprehend relation consider since may\"                           \n[4050] \"apprehend relation consider since either\"                        \n[4051] \"apprehend relation consider relation may\"                        \n[4052] \"apprehend relation consider relation either\"                     \n[4053] \"apprehend relation consider relation moral\"                      \n[4054] \"apprehend whatever may consider divine_\"                         \n[4055] \"apprehend whatever may consider since\"                           \n[4056] \"apprehend whatever may consider relation\"                        \n[4057] \"apprehend whatever may divine_ since\"                            \n[4058] \"apprehend whatever may divine_ relation\"                         \n[4059] \"apprehend whatever may divine_ may\"                              \n[4060] \"apprehend whatever may since relation\"                           \n[4061] \"apprehend whatever may since may\"                                \n[4062] \"apprehend whatever may since either\"                             \n[4063] \"apprehend whatever consider divine_ since\"                       \n[4064] \"apprehend whatever consider divine_ relation\"                    \n[4065] \"apprehend whatever consider divine_ may\"                         \n[4066] \"apprehend whatever consider since relation\"                      \n[4067] \"apprehend whatever consider since may\"                           \n[4068] \"apprehend whatever consider since either\"                        \n[4069] \"apprehend whatever consider relation may\"                        \n[4070] \"apprehend whatever consider relation either\"                     \n[4071] \"apprehend whatever consider relation moral\"                      \n[4072] \"apprehend whatever divine_ since relation\"                       \n[4073] \"apprehend whatever divine_ since may\"                            \n[4074] \"apprehend whatever divine_ since either\"                         \n[4075] \"apprehend whatever divine_ relation may\"                         \n[4076] \"apprehend whatever divine_ relation either\"                      \n[4077] \"apprehend whatever divine_ relation moral\"                       \n[4078] \"apprehend whatever divine_ may either\"                           \n[4079] \"apprehend whatever divine_ may moral\"                            \n[4080] \"apprehend whatever divine_ may physical\"                         \n[4081] \"stand relation\"                                                  \n[4082] \"stand whatever\"                                                  \n[4083] \"stand may\"                                                       \n[4084] \"stand relation whatever\"                                         \n[4085] \"stand relation may\"                                              \n[4086] \"stand relation consider\"                                         \n[4087] \"stand whatever may\"                                              \n[4088] \"stand whatever consider\"                                         \n[4089] \"stand whatever divine_\"                                          \n[4090] \"stand may consider\"                                              \n[4091] \"stand may divine_\"                                               \n[4092] \"stand may since\"                                                 \n[4093] \"stand relation whatever may\"                                     \n[4094] \"stand relation whatever consider\"                                \n[4095] \"stand relation whatever divine_\"                                 \n[4096] \"stand relation may consider\"                                     \n[4097] \"stand relation may divine_\"                                      \n[4098] \"stand relation may since\"                                        \n[4099] \"stand relation consider divine_\"                                 \n[4100] \"stand relation consider since\"                                   \n[4101] \"stand relation consider relation\"                                \n[4102] \"stand whatever may consider\"                                     \n[4103] \"stand whatever may divine_\"                                      \n[4104] \"stand whatever may since\"                                        \n[4105] \"stand whatever consider divine_\"                                 \n[4106] \"stand whatever consider since\"                                   \n[4107] \"stand whatever consider relation\"                                \n[4108] \"stand whatever divine_ since\"                                    \n[4109] \"stand whatever divine_ relation\"                                 \n[4110] \"stand whatever divine_ may\"                                      \n[4111] \"stand may consider divine_\"                                      \n[4112] \"stand may consider since\"                                        \n[4113] \"stand may consider relation\"                                     \n[4114] \"stand may divine_ since\"                                         \n[4115] \"stand may divine_ relation\"                                      \n[4116] \"stand may divine_ may\"                                           \n[4117] \"stand may since relation\"                                        \n[4118] \"stand may since may\"                                             \n[4119] \"stand may since either\"                                          \n[4120] \"stand relation whatever may consider\"                            \n[4121] \"stand relation whatever may divine_\"                             \n[4122] \"stand relation whatever may since\"                               \n[4123] \"stand relation whatever consider divine_\"                        \n[4124] \"stand relation whatever consider since\"                          \n[4125] \"stand relation whatever consider relation\"                       \n[4126] \"stand relation whatever divine_ since\"                           \n[4127] \"stand relation whatever divine_ relation\"                        \n[4128] \"stand relation whatever divine_ may\"                             \n[4129] \"stand relation may consider divine_\"                             \n[4130] \"stand relation may consider since\"                               \n[4131] \"stand relation may consider relation\"                            \n[4132] \"stand relation may divine_ since\"                                \n[4133] \"stand relation may divine_ relation\"                             \n[4134] \"stand relation may divine_ may\"                                  \n[4135] \"stand relation may since relation\"                               \n[4136] \"stand relation may since may\"                                    \n[4137] \"stand relation may since either\"                                 \n[4138] \"stand relation consider divine_ since\"                           \n[4139] \"stand relation consider divine_ relation\"                        \n[4140] \"stand relation consider divine_ may\"                             \n[4141] \"stand relation consider since relation\"                          \n[4142] \"stand relation consider since may\"                               \n[4143] \"stand relation consider since either\"                            \n[4144] \"stand relation consider relation may\"                            \n[4145] \"stand relation consider relation either\"                         \n[4146] \"stand relation consider relation moral\"                          \n[4147] \"stand whatever may consider divine_\"                             \n[4148] \"stand whatever may consider since\"                               \n[4149] \"stand whatever may consider relation\"                            \n[4150] \"stand whatever may divine_ since\"                                \n[4151] \"stand whatever may divine_ relation\"                             \n[4152] \"stand whatever may divine_ may\"                                  \n[4153] \"stand whatever may since relation\"                               \n[4154] \"stand whatever may since may\"                                    \n[4155] \"stand whatever may since either\"                                 \n[4156] \"stand whatever consider divine_ since\"                           \n[4157] \"stand whatever consider divine_ relation\"                        \n[4158] \"stand whatever consider divine_ may\"                             \n[4159] \"stand whatever consider since relation\"                          \n[4160] \"stand whatever consider since may\"                               \n[4161] \"stand whatever consider since either\"                            \n[4162] \"stand whatever consider relation may\"                            \n[4163] \"stand whatever consider relation either\"                         \n[4164] \"stand whatever consider relation moral\"                          \n[4165] \"stand whatever divine_ since relation\"                           \n[4166] \"stand whatever divine_ since may\"                                \n[4167] \"stand whatever divine_ since either\"                             \n[4168] \"stand whatever divine_ relation may\"                             \n[4169] \"stand whatever divine_ relation either\"                          \n[4170] \"stand whatever divine_ relation moral\"                           \n[4171] \"stand whatever divine_ may either\"                               \n[4172] \"stand whatever divine_ may moral\"                                \n[4173] \"stand whatever divine_ may physical\"                             \n[4174] \"stand may consider divine_ since\"                                \n[4175] \"stand may consider divine_ relation\"                             \n[4176] \"stand may consider divine_ may\"                                  \n[4177] \"stand may consider since relation\"                               \n[4178] \"stand may consider since may\"                                    \n[4179] \"stand may consider since either\"                                 \n[4180] \"stand may consider relation may\"                                 \n[4181] \"stand may consider relation either\"                              \n[4182] \"stand may consider relation moral\"                               \n[4183] \"stand may divine_ since relation\"                                \n[4184] \"stand may divine_ since may\"                                     \n[4185] \"stand may divine_ since either\"                                  \n[4186] \"stand may divine_ relation may\"                                  \n[4187] \"stand may divine_ relation either\"                               \n[4188] \"stand may divine_ relation moral\"                                \n[4189] \"stand may divine_ may either\"                                    \n[4190] \"stand may divine_ may moral\"                                     \n[4191] \"stand may divine_ may physical\"                                  \n[4192] \"stand may since relation may\"                                    \n[4193] \"stand may since relation either\"                                 \n[4194] \"stand may since relation moral\"                                  \n[4195] \"stand may since may either\"                                      \n[4196] \"stand may since may moral\"                                       \n[4197] \"stand may since may physical\"                                    \n[4198] \"stand may since either moral\"                                    \n[4199] \"stand may since either physical\"                                 \n[4200] \"stand may since either ritual\"                                   \n[4201] \"relation whatever\"                                               \n[4202] \"relation may\"                                                    \n[4203] \"relation consider\"                                               \n[4204] \"relation whatever may\"                                           \n[4205] \"relation whatever consider\"                                      \n[4206] \"relation whatever divine_\"                                       \n[4207] \"relation may consider\"                                           \n[4208] \"relation may divine_\"                                            \n[4209] \"relation may since\"                                              \n[4210] \"relation consider divine_\"                                       \n[4211] \"relation consider since\"                                         \n[4212] \"relation consider relation\"                                      \n[4213] \"relation whatever may consider\"                                  \n[4214] \"relation whatever may divine_\"                                   \n[4215] \"relation whatever may since\"                                     \n[4216] \"relation whatever consider divine_\"                              \n[4217] \"relation whatever consider since\"                                \n[4218] \"relation whatever consider relation\"                             \n[4219] \"relation whatever divine_ since\"                                 \n[4220] \"relation whatever divine_ relation\"                              \n[4221] \"relation whatever divine_ may\"                                   \n[4222] \"relation may consider divine_\"                                   \n[4223] \"relation may consider since\"                                     \n[4224] \"relation may consider relation\"                                  \n[4225] \"relation may divine_ since\"                                      \n[4226] \"relation may divine_ relation\"                                   \n[4227] \"relation may divine_ may\"                                        \n[4228] \"relation may since relation\"                                     \n[4229] \"relation may since may\"                                          \n[4230] \"relation may since either\"                                       \n[4231] \"relation consider divine_ since\"                                 \n[4232] \"relation consider divine_ relation\"                              \n[4233] \"relation consider divine_ may\"                                   \n[4234] \"relation consider since relation\"                                \n[4235] \"relation consider since may\"                                     \n[4236] \"relation consider since either\"                                  \n[4237] \"relation consider relation may\"                                  \n[4238] \"relation consider relation either\"                               \n[4239] \"relation consider relation moral\"                                \n[4240] \"relation whatever may consider divine_\"                          \n[4241] \"relation whatever may consider since\"                            \n[4242] \"relation whatever may consider relation\"                         \n[4243] \"relation whatever may divine_ since\"                             \n[4244] \"relation whatever may divine_ relation\"                          \n[4245] \"relation whatever may divine_ may\"                               \n[4246] \"relation whatever may since relation\"                            \n[4247] \"relation whatever may since may\"                                 \n[4248] \"relation whatever may since either\"                              \n[4249] \"relation whatever consider divine_ since\"                        \n[4250] \"relation whatever consider divine_ relation\"                     \n[4251] \"relation whatever consider divine_ may\"                          \n[4252] \"relation whatever consider since relation\"                       \n[4253] \"relation whatever consider since may\"                            \n[4254] \"relation whatever consider since either\"                         \n[4255] \"relation whatever consider relation may\"                         \n[4256] \"relation whatever consider relation either\"                      \n[4257] \"relation whatever consider relation moral\"                       \n[4258] \"relation whatever divine_ since relation\"                        \n[4259] \"relation whatever divine_ since may\"                             \n[4260] \"relation whatever divine_ since either\"                          \n[4261] \"relation whatever divine_ relation may\"                          \n[4262] \"relation whatever divine_ relation either\"                       \n[4263] \"relation whatever divine_ relation moral\"                        \n[4264] \"relation whatever divine_ may either\"                            \n[4265] \"relation whatever divine_ may moral\"                             \n[4266] \"relation whatever divine_ may physical\"                          \n[4267] \"relation may consider divine_ since\"                             \n[4268] \"relation may consider divine_ relation\"                          \n[4269] \"relation may consider divine_ may\"                               \n[4270] \"relation may consider since relation\"                            \n[4271] \"relation may consider since may\"                                 \n[4272] \"relation may consider since either\"                              \n[4273] \"relation may consider relation may\"                              \n[4274] \"relation may consider relation either\"                           \n[4275] \"relation may consider relation moral\"                            \n[4276] \"relation may divine_ since relation\"                             \n[4277] \"relation may divine_ since may\"                                  \n[4278] \"relation may divine_ since either\"                               \n[4279] \"relation may divine_ relation may\"                               \n[4280] \"relation may divine_ relation either\"                            \n[4281] \"relation may divine_ relation moral\"                             \n[4282] \"relation may divine_ may either\"                                 \n[4283] \"relation may divine_ may moral\"                                  \n[4284] \"relation may divine_ may physical\"                               \n[4285] \"relation may since relation may\"                                 \n[4286] \"relation may since relation either\"                              \n[4287] \"relation may since relation moral\"                               \n[4288] \"relation may since may either\"                                   \n[4289] \"relation may since may moral\"                                    \n[4290] \"relation may since may physical\"                                 \n[4291] \"relation may since either moral\"                                 \n[4292] \"relation may since either physical\"                              \n[4293] \"relation may since either ritual\"                                \n[4294] \"relation consider divine_ since relation\"                        \n[4295] \"relation consider divine_ since may\"                             \n[4296] \"relation consider divine_ since either\"                          \n[4297] \"relation consider divine_ relation may\"                          \n[4298] \"relation consider divine_ relation either\"                       \n[4299] \"relation consider divine_ relation moral\"                        \n[4300] \"relation consider divine_ may either\"                            \n[4301] \"relation consider divine_ may moral\"                             \n[4302] \"relation consider divine_ may physical\"                          \n[4303] \"relation consider since relation may\"                            \n[4304] \"relation consider since relation either\"                         \n[4305] \"relation consider since relation moral\"                          \n[4306] \"relation consider since may either\"                              \n[4307] \"relation consider since may moral\"                               \n[4308] \"relation consider since may physical\"                            \n[4309] \"relation consider since either moral\"                            \n[4310] \"relation consider since either physical\"                         \n[4311] \"relation consider since either ritual\"                           \n[4312] \"relation consider relation may either\"                           \n[4313] \"relation consider relation may moral\"                            \n[4314] \"relation consider relation may physical\"                         \n[4315] \"relation consider relation either moral\"                         \n[4316] \"relation consider relation either physical\"                      \n[4317] \"relation consider relation either ritual\"                        \n[4318] \"relation consider relation moral physical\"                       \n[4319] \"relation consider relation moral ritual\"                         \n[4320] \"relation consider relation moral evident\"                        \n[4321] \"whatever may\"                                                    \n[4322] \"whatever consider\"                                               \n[4323] \"whatever divine_\"                                                \n[4324] \"whatever may consider\"                                           \n[4325] \"whatever may divine_\"                                            \n[4326] \"whatever may since\"                                              \n[4327] \"whatever consider divine_\"                                       \n[4328] \"whatever consider since\"                                         \n[4329] \"whatever consider relation\"                                      \n[4330] \"whatever divine_ since\"                                          \n[4331] \"whatever divine_ relation\"                                       \n[4332] \"whatever divine_ may\"                                            \n[4333] \"whatever may consider divine_\"                                   \n[4334] \"whatever may consider since\"                                     \n[4335] \"whatever may consider relation\"                                  \n[4336] \"whatever may divine_ since\"                                      \n[4337] \"whatever may divine_ relation\"                                   \n[4338] \"whatever may divine_ may\"                                        \n[4339] \"whatever may since relation\"                                     \n[4340] \"whatever may since may\"                                          \n[4341] \"whatever may since either\"                                       \n[4342] \"whatever consider divine_ since\"                                 \n[4343] \"whatever consider divine_ relation\"                              \n[4344] \"whatever consider divine_ may\"                                   \n[4345] \"whatever consider since relation\"                                \n[4346] \"whatever consider since may\"                                     \n[4347] \"whatever consider since either\"                                  \n[4348] \"whatever consider relation may\"                                  \n[4349] \"whatever consider relation either\"                               \n[4350] \"whatever consider relation moral\"                                \n[4351] \"whatever divine_ since relation\"                                 \n[4352] \"whatever divine_ since may\"                                      \n[4353] \"whatever divine_ since either\"                                   \n[4354] \"whatever divine_ relation may\"                                   \n[4355] \"whatever divine_ relation either\"                                \n[4356] \"whatever divine_ relation moral\"                                 \n[4357] \"whatever divine_ may either\"                                     \n[4358] \"whatever divine_ may moral\"                                      \n[4359] \"whatever divine_ may physical\"                                   \n[4360] \"whatever may consider divine_ since\"                             \n[4361] \"whatever may consider divine_ relation\"                          \n[4362] \"whatever may consider divine_ may\"                               \n[4363] \"whatever may consider since relation\"                            \n[4364] \"whatever may consider since may\"                                 \n[4365] \"whatever may consider since either\"                              \n[4366] \"whatever may consider relation may\"                              \n[4367] \"whatever may consider relation either\"                           \n[4368] \"whatever may consider relation moral\"                            \n[4369] \"whatever may divine_ since relation\"                             \n[4370] \"whatever may divine_ since may\"                                  \n[4371] \"whatever may divine_ since either\"                               \n[4372] \"whatever may divine_ relation may\"                               \n[4373] \"whatever may divine_ relation either\"                            \n[4374] \"whatever may divine_ relation moral\"                             \n[4375] \"whatever may divine_ may either\"                                 \n[4376] \"whatever may divine_ may moral\"                                  \n[4377] \"whatever may divine_ may physical\"                               \n[4378] \"whatever may since relation may\"                                 \n[4379] \"whatever may since relation either\"                              \n[4380] \"whatever may since relation moral\"                               \n[4381] \"whatever may since may either\"                                   \n[4382] \"whatever may since may moral\"                                    \n[4383] \"whatever may since may physical\"                                 \n[4384] \"whatever may since either moral\"                                 \n[4385] \"whatever may since either physical\"                              \n[4386] \"whatever may since either ritual\"                                \n[4387] \"whatever consider divine_ since relation\"                        \n[4388] \"whatever consider divine_ since may\"                             \n[4389] \"whatever consider divine_ since either\"                          \n[4390] \"whatever consider divine_ relation may\"                          \n[4391] \"whatever consider divine_ relation either\"                       \n[4392] \"whatever consider divine_ relation moral\"                        \n[4393] \"whatever consider divine_ may either\"                            \n[4394] \"whatever consider divine_ may moral\"                             \n[4395] \"whatever consider divine_ may physical\"                          \n[4396] \"whatever consider since relation may\"                            \n[4397] \"whatever consider since relation either\"                         \n[4398] \"whatever consider since relation moral\"                          \n[4399] \"whatever consider since may either\"                              \n[4400] \"whatever consider since may moral\"                               \n[4401] \"whatever consider since may physical\"                            \n[4402] \"whatever consider since either moral\"                            \n[4403] \"whatever consider since either physical\"                         \n[4404] \"whatever consider since either ritual\"                           \n[4405] \"whatever consider relation may either\"                           \n[4406] \"whatever consider relation may moral\"                            \n[4407] \"whatever consider relation may physical\"                         \n[4408] \"whatever consider relation either moral\"                         \n[4409] \"whatever consider relation either physical\"                      \n[4410] \"whatever consider relation either ritual\"                        \n[4411] \"whatever consider relation moral physical\"                       \n[4412] \"whatever consider relation moral ritual\"                         \n[4413] \"whatever consider relation moral evident\"                        \n[4414] \"whatever divine_ since relation may\"                             \n[4415] \"whatever divine_ since relation either\"                          \n[4416] \"whatever divine_ since relation moral\"                           \n[4417] \"whatever divine_ since may either\"                               \n[4418] \"whatever divine_ since may moral\"                                \n[4419] \"whatever divine_ since may physical\"                             \n[4420] \"whatever divine_ since either moral\"                             \n[4421] \"whatever divine_ since either physical\"                          \n[4422] \"whatever divine_ since either ritual\"                            \n[4423] \"whatever divine_ relation may either\"                            \n[4424] \"whatever divine_ relation may moral\"                             \n[4425] \"whatever divine_ relation may physical\"                          \n[4426] \"whatever divine_ relation either moral\"                          \n[4427] \"whatever divine_ relation either physical\"                       \n[4428] \"whatever divine_ relation either ritual\"                         \n[4429] \"whatever divine_ relation moral physical\"                        \n[4430] \"whatever divine_ relation moral ritual\"                          \n[4431] \"whatever divine_ relation moral evident\"                         \n[4432] \"whatever divine_ may either moral\"                               \n[4433] \"whatever divine_ may either physical\"                            \n[4434] \"whatever divine_ may either ritual\"                              \n[4435] \"whatever divine_ may moral physical\"                             \n[4436] \"whatever divine_ may moral ritual\"                               \n[4437] \"whatever divine_ may moral evident\"                              \n[4438] \"whatever divine_ may physical ritual\"                            \n[4439] \"whatever divine_ may physical evident\"                           \n[4440] \"whatever divine_ may physical religion\"                          \n[4441] \"may consider\"                                                    \n[4442] \"may divine_\"                                                     \n[4443] \"may since\"                                                       \n[4444] \"may consider divine_\"                                            \n[4445] \"may consider since\"                                              \n[4446] \"may consider relation\"                                           \n[4447] \"may divine_ since\"                                               \n[4448] \"may divine_ relation\"                                            \n[4449] \"may divine_ may\"                                                 \n[4450] \"may since relation\"                                              \n[4451] \"may since may\"                                                   \n[4452] \"may since either\"                                                \n[4453] \"may consider divine_ since\"                                      \n[4454] \"may consider divine_ relation\"                                   \n[4455] \"may consider divine_ may\"                                        \n[4456] \"may consider since relation\"                                     \n[4457] \"may consider since may\"                                          \n[4458] \"may consider since either\"                                       \n[4459] \"may consider relation may\"                                       \n[4460] \"may consider relation either\"                                    \n[4461] \"may consider relation moral\"                                     \n[4462] \"may divine_ since relation\"                                      \n[4463] \"may divine_ since may\"                                           \n[4464] \"may divine_ since either\"                                        \n[4465] \"may divine_ relation may\"                                        \n[4466] \"may divine_ relation either\"                                     \n[4467] \"may divine_ relation moral\"                                      \n[4468] \"may divine_ may either\"                                          \n[4469] \"may divine_ may moral\"                                           \n[4470] \"may divine_ may physical\"                                        \n[4471] \"may since relation may\"                                          \n[4472] \"may since relation either\"                                       \n[4473] \"may since relation moral\"                                        \n[4474] \"may since may either\"                                            \n[4475] \"may since may moral\"                                             \n[4476] \"may since may physical\"                                          \n[4477] \"may since either moral\"                                          \n[4478] \"may since either physical\"                                       \n[4479] \"may since either ritual\"                                         \n[4480] \"may consider divine_ since relation\"                             \n[4481] \"may consider divine_ since may\"                                  \n[4482] \"may consider divine_ since either\"                               \n[4483] \"may consider divine_ relation may\"                               \n[4484] \"may consider divine_ relation either\"                            \n[4485] \"may consider divine_ relation moral\"                             \n[4486] \"may consider divine_ may either\"                                 \n[4487] \"may consider divine_ may moral\"                                  \n[4488] \"may consider divine_ may physical\"                               \n[4489] \"may consider since relation may\"                                 \n[4490] \"may consider since relation either\"                              \n[4491] \"may consider since relation moral\"                               \n[4492] \"may consider since may either\"                                   \n[4493] \"may consider since may moral\"                                    \n[4494] \"may consider since may physical\"                                 \n[4495] \"may consider since either moral\"                                 \n[4496] \"may consider since either physical\"                              \n[4497] \"may consider since either ritual\"                                \n[4498] \"may consider relation may either\"                                \n[4499] \"may consider relation may moral\"                                 \n[4500] \"may consider relation may physical\"                              \n[4501] \"may consider relation either moral\"                              \n[4502] \"may consider relation either physical\"                           \n[4503] \"may consider relation either ritual\"                             \n[4504] \"may consider relation moral physical\"                            \n[4505] \"may consider relation moral ritual\"                              \n[4506] \"may consider relation moral evident\"                             \n[4507] \"may divine_ since relation may\"                                  \n[4508] \"may divine_ since relation either\"                               \n[4509] \"may divine_ since relation moral\"                                \n[4510] \"may divine_ since may either\"                                    \n[4511] \"may divine_ since may moral\"                                     \n[4512] \"may divine_ since may physical\"                                  \n[4513] \"may divine_ since either moral\"                                  \n[4514] \"may divine_ since either physical\"                               \n[4515] \"may divine_ since either ritual\"                                 \n[4516] \"may divine_ relation may either\"                                 \n[4517] \"may divine_ relation may moral\"                                  \n[4518] \"may divine_ relation may physical\"                               \n[4519] \"may divine_ relation either moral\"                               \n[4520] \"may divine_ relation either physical\"                            \n[4521] \"may divine_ relation either ritual\"                              \n[4522] \"may divine_ relation moral physical\"                             \n[4523] \"may divine_ relation moral ritual\"                               \n[4524] \"may divine_ relation moral evident\"                              \n[4525] \"may divine_ may either moral\"                                    \n[4526] \"may divine_ may either physical\"                                 \n[4527] \"may divine_ may either ritual\"                                   \n[4528] \"may divine_ may moral physical\"                                  \n[4529] \"may divine_ may moral ritual\"                                    \n[4530] \"may divine_ may moral evident\"                                   \n[4531] \"may divine_ may physical ritual\"                                 \n[4532] \"may divine_ may physical evident\"                                \n[4533] \"may divine_ may physical religion\"                               \n[4534] \"may since relation may either\"                                   \n[4535] \"may since relation may moral\"                                    \n[4536] \"may since relation may physical\"                                 \n[4537] \"may since relation either moral\"                                 \n[4538] \"may since relation either physical\"                              \n[4539] \"may since relation either ritual\"                                \n[4540] \"may since relation moral physical\"                               \n[4541] \"may since relation moral ritual\"                                 \n[4542] \"may since relation moral evident\"                                \n[4543] \"may since may either moral\"                                      \n[4544] \"may since may either physical\"                                   \n[4545] \"may since may either ritual\"                                     \n[4546] \"may since may moral physical\"                                    \n[4547] \"may since may moral ritual\"                                      \n[4548] \"may since may moral evident\"                                     \n[4549] \"may since may physical ritual\"                                   \n[4550] \"may since may physical evident\"                                  \n[4551] \"may since may physical religion\"                                 \n[4552] \"may since either moral physical\"                                 \n[4553] \"may since either moral ritual\"                                   \n[4554] \"may since either moral evident\"                                  \n[4555] \"may since either physical ritual\"                                \n[4556] \"may since either physical evident\"                               \n[4557] \"may since either physical religion\"                              \n[4558] \"may since either ritual evident\"                                 \n[4559] \"may since either ritual religion\"                                \n[4560] \"may since either ritual sense\"                                   \n[4561] \"consider divine_\"                                                \n[4562] \"consider since\"                                                  \n[4563] \"consider relation\"                                               \n[4564] \"consider divine_ since\"                                          \n[4565] \"consider divine_ relation\"                                       \n[4566] \"consider divine_ may\"                                            \n[4567] \"consider since relation\"                                         \n[4568] \"consider since may\"                                              \n[4569] \"consider since either\"                                           \n[4570] \"consider relation may\"                                           \n[4571] \"consider relation either\"                                        \n[4572] \"consider relation moral\"                                         \n[4573] \"consider divine_ since relation\"                                 \n[4574] \"consider divine_ since may\"                                      \n[4575] \"consider divine_ since either\"                                   \n[4576] \"consider divine_ relation may\"                                   \n[4577] \"consider divine_ relation either\"                                \n[4578] \"consider divine_ relation moral\"                                 \n[4579] \"consider divine_ may either\"                                     \n[4580] \"consider divine_ may moral\"                                      \n[4581] \"consider divine_ may physical\"                                   \n[4582] \"consider since relation may\"                                     \n[4583] \"consider since relation either\"                                  \n[4584] \"consider since relation moral\"                                   \n[4585] \"consider since may either\"                                       \n[4586] \"consider since may moral\"                                        \n[4587] \"consider since may physical\"                                     \n[4588] \"consider since either moral\"                                     \n[4589] \"consider since either physical\"                                  \n[4590] \"consider since either ritual\"                                    \n[4591] \"consider relation may either\"                                    \n[4592] \"consider relation may moral\"                                     \n[4593] \"consider relation may physical\"                                  \n[4594] \"consider relation either moral\"                                  \n[4595] \"consider relation either physical\"                               \n[4596] \"consider relation either ritual\"                                 \n[4597] \"consider relation moral physical\"                                \n[4598] \"consider relation moral ritual\"                                  \n[4599] \"consider relation moral evident\"                                 \n[4600] \"consider divine_ since relation may\"                             \n[4601] \"consider divine_ since relation either\"                          \n[4602] \"consider divine_ since relation moral\"                           \n[4603] \"consider divine_ since may either\"                               \n[4604] \"consider divine_ since may moral\"                                \n[4605] \"consider divine_ since may physical\"                             \n[4606] \"consider divine_ since either moral\"                             \n[4607] \"consider divine_ since either physical\"                          \n[4608] \"consider divine_ since either ritual\"                            \n[4609] \"consider divine_ relation may either\"                            \n[4610] \"consider divine_ relation may moral\"                             \n[4611] \"consider divine_ relation may physical\"                          \n[4612] \"consider divine_ relation either moral\"                          \n[4613] \"consider divine_ relation either physical\"                       \n[4614] \"consider divine_ relation either ritual\"                         \n[4615] \"consider divine_ relation moral physical\"                        \n[4616] \"consider divine_ relation moral ritual\"                          \n[4617] \"consider divine_ relation moral evident\"                         \n[4618] \"consider divine_ may either moral\"                               \n[4619] \"consider divine_ may either physical\"                            \n[4620] \"consider divine_ may either ritual\"                              \n[4621] \"consider divine_ may moral physical\"                             \n[4622] \"consider divine_ may moral ritual\"                               \n[4623] \"consider divine_ may moral evident\"                              \n[4624] \"consider divine_ may physical ritual\"                            \n[4625] \"consider divine_ may physical evident\"                           \n[4626] \"consider divine_ may physical religion\"                          \n[4627] \"consider since relation may either\"                              \n[4628] \"consider since relation may moral\"                               \n[4629] \"consider since relation may physical\"                            \n[4630] \"consider since relation either moral\"                            \n[4631] \"consider since relation either physical\"                         \n[4632] \"consider since relation either ritual\"                           \n[4633] \"consider since relation moral physical\"                          \n[4634] \"consider since relation moral ritual\"                            \n[4635] \"consider since relation moral evident\"                           \n[4636] \"consider since may either moral\"                                 \n[4637] \"consider since may either physical\"                              \n[4638] \"consider since may either ritual\"                                \n[4639] \"consider since may moral physical\"                               \n[4640] \"consider since may moral ritual\"                                 \n[4641] \"consider since may moral evident\"                                \n[4642] \"consider since may physical ritual\"                              \n[4643] \"consider since may physical evident\"                             \n[4644] \"consider since may physical religion\"                            \n[4645] \"consider since either moral physical\"                            \n[4646] \"consider since either moral ritual\"                              \n[4647] \"consider since either moral evident\"                             \n[4648] \"consider since either physical ritual\"                           \n[4649] \"consider since either physical evident\"                          \n[4650] \"consider since either physical religion\"                         \n[4651] \"consider since either ritual evident\"                            \n[4652] \"consider since either ritual religion\"                           \n[4653] \"consider since either ritual sense\"                              \n[4654] \"consider relation may either moral\"                              \n[4655] \"consider relation may either physical\"                           \n[4656] \"consider relation may either ritual\"                             \n[4657] \"consider relation may moral physical\"                            \n[4658] \"consider relation may moral ritual\"                              \n[4659] \"consider relation may moral evident\"                             \n[4660] \"consider relation may physical ritual\"                           \n[4661] \"consider relation may physical evident\"                          \n[4662] \"consider relation may physical religion\"                         \n[4663] \"consider relation either moral physical\"                         \n[4664] \"consider relation either moral ritual\"                           \n[4665] \"consider relation either moral evident\"                          \n[4666] \"consider relation either physical ritual\"                        \n[4667] \"consider relation either physical evident\"                       \n[4668] \"consider relation either physical religion\"                      \n[4669] \"consider relation either ritual evident\"                         \n[4670] \"consider relation either ritual religion\"                        \n[4671] \"consider relation either ritual sense\"                           \n[4672] \"consider relation moral physical ritual\"                         \n[4673] \"consider relation moral physical evident\"                        \n[4674] \"consider relation moral physical religion\"                       \n[4675] \"consider relation moral ritual evident\"                          \n[4676] \"consider relation moral ritual religion\"                         \n[4677] \"consider relation moral ritual sense\"                            \n[4678] \"consider relation moral evident religion\"                        \n[4679] \"consider relation moral evident sense\"                           \n[4680] \"consider relation moral evident take\"                            \n[4681] \"divine_ since\"                                                   \n[4682] \"divine_ relation\"                                                \n[4683] \"divine_ may\"                                                     \n[4684] \"divine_ since relation\"                                          \n[4685] \"divine_ since may\"                                               \n[4686] \"divine_ since either\"                                            \n[4687] \"divine_ relation may\"                                            \n[4688] \"divine_ relation either\"                                         \n[4689] \"divine_ relation moral\"                                          \n[4690] \"divine_ may either\"                                              \n[4691] \"divine_ may moral\"                                               \n[4692] \"divine_ may physical\"                                            \n[4693] \"divine_ since relation may\"                                      \n[4694] \"divine_ since relation either\"                                   \n[4695] \"divine_ since relation moral\"                                    \n[4696] \"divine_ since may either\"                                        \n[4697] \"divine_ since may moral\"                                         \n[4698] \"divine_ since may physical\"                                      \n[4699] \"divine_ since either moral\"                                      \n[4700] \"divine_ since either physical\"                                   \n[4701] \"divine_ since either ritual\"                                     \n[4702] \"divine_ relation may either\"                                     \n[4703] \"divine_ relation may moral\"                                      \n[4704] \"divine_ relation may physical\"                                   \n[4705] \"divine_ relation either moral\"                                   \n[4706] \"divine_ relation either physical\"                                \n[4707] \"divine_ relation either ritual\"                                  \n[4708] \"divine_ relation moral physical\"                                 \n[4709] \"divine_ relation moral ritual\"                                   \n[4710] \"divine_ relation moral evident\"                                  \n[4711] \"divine_ may either moral\"                                        \n[4712] \"divine_ may either physical\"                                     \n[4713] \"divine_ may either ritual\"                                       \n[4714] \"divine_ may moral physical\"                                      \n[4715] \"divine_ may moral ritual\"                                        \n[4716] \"divine_ may moral evident\"                                       \n[4717] \"divine_ may physical ritual\"                                     \n[4718] \"divine_ may physical evident\"                                    \n[4719] \"divine_ may physical religion\"                                   \n[4720] \"divine_ since relation may either\"                               \n[4721] \"divine_ since relation may moral\"                                \n[4722] \"divine_ since relation may physical\"                             \n[4723] \"divine_ since relation either moral\"                             \n[4724] \"divine_ since relation either physical\"                          \n[4725] \"divine_ since relation either ritual\"                            \n[4726] \"divine_ since relation moral physical\"                           \n[4727] \"divine_ since relation moral ritual\"                             \n[4728] \"divine_ since relation moral evident\"                            \n[4729] \"divine_ since may either moral\"                                  \n[4730] \"divine_ since may either physical\"                               \n[4731] \"divine_ since may either ritual\"                                 \n[4732] \"divine_ since may moral physical\"                                \n[4733] \"divine_ since may moral ritual\"                                  \n[4734] \"divine_ since may moral evident\"                                 \n[4735] \"divine_ since may physical ritual\"                               \n[4736] \"divine_ since may physical evident\"                              \n[4737] \"divine_ since may physical religion\"                             \n[4738] \"divine_ since either moral physical\"                             \n[4739] \"divine_ since either moral ritual\"                               \n[4740] \"divine_ since either moral evident\"                              \n[4741] \"divine_ since either physical ritual\"                            \n[4742] \"divine_ since either physical evident\"                           \n[4743] \"divine_ since either physical religion\"                          \n[4744] \"divine_ since either ritual evident\"                             \n[4745] \"divine_ since either ritual religion\"                            \n[4746] \"divine_ since either ritual sense\"                               \n[4747] \"divine_ relation may either moral\"                               \n[4748] \"divine_ relation may either physical\"                            \n[4749] \"divine_ relation may either ritual\"                              \n[4750] \"divine_ relation may moral physical\"                             \n[4751] \"divine_ relation may moral ritual\"                               \n[4752] \"divine_ relation may moral evident\"                              \n[4753] \"divine_ relation may physical ritual\"                            \n[4754] \"divine_ relation may physical evident\"                           \n[4755] \"divine_ relation may physical religion\"                          \n[4756] \"divine_ relation either moral physical\"                          \n[4757] \"divine_ relation either moral ritual\"                            \n[4758] \"divine_ relation either moral evident\"                           \n[4759] \"divine_ relation either physical ritual\"                         \n[4760] \"divine_ relation either physical evident\"                        \n[4761] \"divine_ relation either physical religion\"                       \n[4762] \"divine_ relation either ritual evident\"                          \n[4763] \"divine_ relation either ritual religion\"                         \n[4764] \"divine_ relation either ritual sense\"                            \n[4765] \"divine_ relation moral physical ritual\"                          \n[4766] \"divine_ relation moral physical evident\"                         \n[4767] \"divine_ relation moral physical religion\"                        \n[4768] \"divine_ relation moral ritual evident\"                           \n[4769] \"divine_ relation moral ritual religion\"                          \n[4770] \"divine_ relation moral ritual sense\"                             \n[4771] \"divine_ relation moral evident religion\"                         \n[4772] \"divine_ relation moral evident sense\"                            \n[4773] \"divine_ relation moral evident take\"                             \n[4774] \"divine_ may either moral physical\"                               \n[4775] \"divine_ may either moral ritual\"                                 \n[4776] \"divine_ may either moral evident\"                                \n[4777] \"divine_ may either physical ritual\"                              \n[4778] \"divine_ may either physical evident\"                             \n[4779] \"divine_ may either physical religion\"                            \n[4780] \"divine_ may either ritual evident\"                               \n[4781] \"divine_ may either ritual religion\"                              \n[4782] \"divine_ may either ritual sense\"                                 \n[4783] \"divine_ may moral physical ritual\"                               \n[4784] \"divine_ may moral physical evident\"                              \n[4785] \"divine_ may moral physical religion\"                             \n[4786] \"divine_ may moral ritual evident\"                                \n[4787] \"divine_ may moral ritual religion\"                               \n[4788] \"divine_ may moral ritual sense\"                                  \n[4789] \"divine_ may moral evident religion\"                              \n[4790] \"divine_ may moral evident sense\"                                 \n[4791] \"divine_ may moral evident take\"                                  \n[4792] \"divine_ may physical ritual evident\"                             \n[4793] \"divine_ may physical ritual religion\"                            \n[4794] \"divine_ may physical ritual sense\"                               \n[4795] \"divine_ may physical evident religion\"                           \n[4796] \"divine_ may physical evident sense\"                              \n[4797] \"divine_ may physical evident take\"                               \n[4798] \"divine_ may physical religion sense\"                             \n[4799] \"divine_ may physical religion take\"                              \n[4800] \"divine_ may physical religion theologies\"                        \n[4801] \"since relation\"                                                  \n[4802] \"since may\"                                                       \n[4803] \"since either\"                                                    \n[4804] \"since relation may\"                                              \n[4805] \"since relation either\"                                           \n[4806] \"since relation moral\"                                            \n[4807] \"since may either\"                                                \n[4808] \"since may moral\"                                                 \n[4809] \"since may physical\"                                              \n[4810] \"since either moral\"                                              \n[4811] \"since either physical\"                                           \n[4812] \"since either ritual\"                                             \n[4813] \"since relation may either\"                                       \n[4814] \"since relation may moral\"                                        \n[4815] \"since relation may physical\"                                     \n[4816] \"since relation either moral\"                                     \n[4817] \"since relation either physical\"                                  \n[4818] \"since relation either ritual\"                                    \n[4819] \"since relation moral physical\"                                   \n[4820] \"since relation moral ritual\"                                     \n[4821] \"since relation moral evident\"                                    \n[4822] \"since may either moral\"                                          \n[4823] \"since may either physical\"                                       \n[4824] \"since may either ritual\"                                         \n[4825] \"since may moral physical\"                                        \n[4826] \"since may moral ritual\"                                          \n[4827] \"since may moral evident\"                                         \n[4828] \"since may physical ritual\"                                       \n[4829] \"since may physical evident\"                                      \n[4830] \"since may physical religion\"                                     \n[4831] \"since either moral physical\"                                     \n[4832] \"since either moral ritual\"                                       \n[4833] \"since either moral evident\"                                      \n[4834] \"since either physical ritual\"                                    \n[4835] \"since either physical evident\"                                   \n[4836] \"since either physical religion\"                                  \n[4837] \"since either ritual evident\"                                     \n[4838] \"since either ritual religion\"                                    \n[4839] \"since either ritual sense\"                                       \n[4840] \"since relation may either moral\"                                 \n[4841] \"since relation may either physical\"                              \n[4842] \"since relation may either ritual\"                                \n[4843] \"since relation may moral physical\"                               \n[4844] \"since relation may moral ritual\"                                 \n[4845] \"since relation may moral evident\"                                \n[4846] \"since relation may physical ritual\"                              \n[4847] \"since relation may physical evident\"                             \n[4848] \"since relation may physical religion\"                            \n[4849] \"since relation either moral physical\"                            \n[4850] \"since relation either moral ritual\"                              \n[4851] \"since relation either moral evident\"                             \n[4852] \"since relation either physical ritual\"                           \n[4853] \"since relation either physical evident\"                          \n[4854] \"since relation either physical religion\"                         \n[4855] \"since relation either ritual evident\"                            \n[4856] \"since relation either ritual religion\"                           \n[4857] \"since relation either ritual sense\"                              \n[4858] \"since relation moral physical ritual\"                            \n[4859] \"since relation moral physical evident\"                           \n[4860] \"since relation moral physical religion\"                          \n[4861] \"since relation moral ritual evident\"                             \n[4862] \"since relation moral ritual religion\"                            \n[4863] \"since relation moral ritual sense\"                               \n[4864] \"since relation moral evident religion\"                           \n[4865] \"since relation moral evident sense\"                              \n[4866] \"since relation moral evident take\"                               \n[4867] \"since may either moral physical\"                                 \n[4868] \"since may either moral ritual\"                                   \n[4869] \"since may either moral evident\"                                  \n[4870] \"since may either physical ritual\"                                \n[4871] \"since may either physical evident\"                               \n[4872] \"since may either physical religion\"                              \n[4873] \"since may either ritual evident\"                                 \n[4874] \"since may either ritual religion\"                                \n[4875] \"since may either ritual sense\"                                   \n[4876] \"since may moral physical ritual\"                                 \n[4877] \"since may moral physical evident\"                                \n[4878] \"since may moral physical religion\"                               \n[4879] \"since may moral ritual evident\"                                  \n[4880] \"since may moral ritual religion\"                                 \n[4881] \"since may moral ritual sense\"                                    \n[4882] \"since may moral evident religion\"                                \n[4883] \"since may moral evident sense\"                                   \n[4884] \"since may moral evident take\"                                    \n[4885] \"since may physical ritual evident\"                               \n[4886] \"since may physical ritual religion\"                              \n[4887] \"since may physical ritual sense\"                                 \n[4888] \"since may physical evident religion\"                             \n[4889] \"since may physical evident sense\"                                \n[4890] \"since may physical evident take\"                                 \n[4891] \"since may physical religion sense\"                               \n[4892] \"since may physical religion take\"                                \n[4893] \"since may physical religion theologies\"                          \n[4894] \"since either moral physical ritual\"                              \n[4895] \"since either moral physical evident\"                             \n[4896] \"since either moral physical religion\"                            \n[4897] \"since either moral ritual evident\"                               \n[4898] \"since either moral ritual religion\"                              \n[4899] \"since either moral ritual sense\"                                 \n[4900] \"since either moral evident religion\"                             \n[4901] \"since either moral evident sense\"                                \n[4902] \"since either moral evident take\"                                 \n[4903] \"since either physical ritual evident\"                            \n[4904] \"since either physical ritual religion\"                           \n[4905] \"since either physical ritual sense\"                              \n[4906] \"since either physical evident religion\"                          \n[4907] \"since either physical evident sense\"                             \n[4908] \"since either physical evident take\"                              \n[4909] \"since either physical religion sense\"                            \n[4910] \"since either physical religion take\"                             \n[4911] \"since either physical religion theologies\"                       \n[4912] \"since either ritual evident religion\"                            \n[4913] \"since either ritual evident sense\"                               \n[4914] \"since either ritual evident take\"                                \n[4915] \"since either ritual religion sense\"                              \n[4916] \"since either ritual religion take\"                               \n[4917] \"since either ritual religion theologies\"                         \n[4918] \"since either ritual sense take\"                                  \n[4919] \"since either ritual sense theologies\"                            \n[4920] \"since either ritual sense philosophies\"                          \n[4921] \"relation may\"                                                    \n[4922] \"relation either\"                                                 \n[4923] \"relation moral\"                                                  \n[4924] \"relation may either\"                                             \n[4925] \"relation may moral\"                                              \n[4926] \"relation may physical\"                                           \n[4927] \"relation either moral\"                                           \n[4928] \"relation either physical\"                                        \n[4929] \"relation either ritual\"                                          \n[4930] \"relation moral physical\"                                         \n[4931] \"relation moral ritual\"                                           \n[4932] \"relation moral evident\"                                          \n[4933] \"relation may either moral\"                                       \n[4934] \"relation may either physical\"                                    \n[4935] \"relation may either ritual\"                                      \n[4936] \"relation may moral physical\"                                     \n[4937] \"relation may moral ritual\"                                       \n[4938] \"relation may moral evident\"                                      \n[4939] \"relation may physical ritual\"                                    \n[4940] \"relation may physical evident\"                                   \n[4941] \"relation may physical religion\"                                  \n[4942] \"relation either moral physical\"                                  \n[4943] \"relation either moral ritual\"                                    \n[4944] \"relation either moral evident\"                                   \n[4945] \"relation either physical ritual\"                                 \n[4946] \"relation either physical evident\"                                \n[4947] \"relation either physical religion\"                               \n[4948] \"relation either ritual evident\"                                  \n[4949] \"relation either ritual religion\"                                 \n[4950] \"relation either ritual sense\"                                    \n[4951] \"relation moral physical ritual\"                                  \n[4952] \"relation moral physical evident\"                                 \n[4953] \"relation moral physical religion\"                                \n[4954] \"relation moral ritual evident\"                                   \n[4955] \"relation moral ritual religion\"                                  \n[4956] \"relation moral ritual sense\"                                     \n[4957] \"relation moral evident religion\"                                 \n[4958] \"relation moral evident sense\"                                    \n[4959] \"relation moral evident take\"                                     \n[4960] \"relation may either moral physical\"                              \n[4961] \"relation may either moral ritual\"                                \n[4962] \"relation may either moral evident\"                               \n[4963] \"relation may either physical ritual\"                             \n[4964] \"relation may either physical evident\"                            \n[4965] \"relation may either physical religion\"                           \n[4966] \"relation may either ritual evident\"                              \n[4967] \"relation may either ritual religion\"                             \n[4968] \"relation may either ritual sense\"                                \n[4969] \"relation may moral physical ritual\"                              \n[4970] \"relation may moral physical evident\"                             \n[4971] \"relation may moral physical religion\"                            \n[4972] \"relation may moral ritual evident\"                               \n[4973] \"relation may moral ritual religion\"                              \n[4974] \"relation may moral ritual sense\"                                 \n[4975] \"relation may moral evident religion\"                             \n[4976] \"relation may moral evident sense\"                                \n[4977] \"relation may moral evident take\"                                 \n[4978] \"relation may physical ritual evident\"                            \n[4979] \"relation may physical ritual religion\"                           \n[4980] \"relation may physical ritual sense\"                              \n[4981] \"relation may physical evident religion\"                          \n[4982] \"relation may physical evident sense\"                             \n[4983] \"relation may physical evident take\"                              \n[4984] \"relation may physical religion sense\"                            \n[4985] \"relation may physical religion take\"                             \n[4986] \"relation may physical religion theologies\"                       \n[4987] \"relation either moral physical ritual\"                           \n[4988] \"relation either moral physical evident\"                          \n[4989] \"relation either moral physical religion\"                         \n[4990] \"relation either moral ritual evident\"                            \n[4991] \"relation either moral ritual religion\"                           \n[4992] \"relation either moral ritual sense\"                              \n[4993] \"relation either moral evident religion\"                          \n[4994] \"relation either moral evident sense\"                             \n[4995] \"relation either moral evident take\"                              \n[4996] \"relation either physical ritual evident\"                         \n[4997] \"relation either physical ritual religion\"                        \n[4998] \"relation either physical ritual sense\"                           \n[4999] \"relation either physical evident religion\"                       \n[5000] \"relation either physical evident sense\"                          \n[5001] \"relation either physical evident take\"                           \n[5002] \"relation either physical religion sense\"                         \n[5003] \"relation either physical religion take\"                          \n[5004] \"relation either physical religion theologies\"                    \n[5005] \"relation either ritual evident religion\"                         \n[5006] \"relation either ritual evident sense\"                            \n[5007] \"relation either ritual evident take\"                             \n[5008] \"relation either ritual religion sense\"                           \n[5009] \"relation either ritual religion take\"                            \n[5010] \"relation either ritual religion theologies\"                      \n[5011] \"relation either ritual sense take\"                               \n[5012] \"relation either ritual sense theologies\"                         \n[5013] \"relation either ritual sense philosophies\"                       \n[5014] \"relation moral physical ritual evident\"                          \n[5015] \"relation moral physical ritual religion\"                         \n[5016] \"relation moral physical ritual sense\"                            \n[5017] \"relation moral physical evident religion\"                        \n[5018] \"relation moral physical evident sense\"                           \n[5019] \"relation moral physical evident take\"                            \n[5020] \"relation moral physical religion sense\"                          \n[5021] \"relation moral physical religion take\"                           \n[5022] \"relation moral physical religion theologies\"                     \n[5023] \"relation moral ritual evident religion\"                          \n[5024] \"relation moral ritual evident sense\"                             \n[5025] \"relation moral ritual evident take\"                              \n[5026] \"relation moral ritual religion sense\"                            \n[5027] \"relation moral ritual religion take\"                             \n[5028] \"relation moral ritual religion theologies\"                       \n[5029] \"relation moral ritual sense take\"                                \n[5030] \"relation moral ritual sense theologies\"                          \n[5031] \"relation moral ritual sense philosophies\"                        \n[5032] \"relation moral evident religion sense\"                           \n[5033] \"relation moral evident religion take\"                            \n[5034] \"relation moral evident religion theologies\"                      \n[5035] \"relation moral evident sense take\"                               \n[5036] \"relation moral evident sense theologies\"                         \n[5037] \"relation moral evident sense philosophies\"                       \n[5038] \"relation moral evident take theologies\"                          \n[5039] \"relation moral evident take philosophies\"                        \n[5040] \"relation moral evident take ecclesiastical\"                      \n[5041] \"may either\"                                                      \n[5042] \"may moral\"                                                       \n[5043] \"may physical\"                                                    \n[5044] \"may either moral\"                                                \n[5045] \"may either physical\"                                             \n[5046] \"may either ritual\"                                               \n[5047] \"may moral physical\"                                              \n[5048] \"may moral ritual\"                                                \n[5049] \"may moral evident\"                                               \n[5050] \"may physical ritual\"                                             \n[5051] \"may physical evident\"                                            \n[5052] \"may physical religion\"                                           \n[5053] \"may either moral physical\"                                       \n[5054] \"may either moral ritual\"                                         \n[5055] \"may either moral evident\"                                        \n[5056] \"may either physical ritual\"                                      \n[5057] \"may either physical evident\"                                     \n[5058] \"may either physical religion\"                                    \n[5059] \"may either ritual evident\"                                       \n[5060] \"may either ritual religion\"                                      \n[5061] \"may either ritual sense\"                                         \n[5062] \"may moral physical ritual\"                                       \n[5063] \"may moral physical evident\"                                      \n[5064] \"may moral physical religion\"                                     \n[5065] \"may moral ritual evident\"                                        \n[5066] \"may moral ritual religion\"                                       \n[5067] \"may moral ritual sense\"                                          \n[5068] \"may moral evident religion\"                                      \n[5069] \"may moral evident sense\"                                         \n[5070] \"may moral evident take\"                                          \n[5071] \"may physical ritual evident\"                                     \n[5072] \"may physical ritual religion\"                                    \n[5073] \"may physical ritual sense\"                                       \n[5074] \"may physical evident religion\"                                   \n[5075] \"may physical evident sense\"                                      \n[5076] \"may physical evident take\"                                       \n[5077] \"may physical religion sense\"                                     \n[5078] \"may physical religion take\"                                      \n[5079] \"may physical religion theologies\"                                \n[5080] \"may either moral physical ritual\"                                \n[5081] \"may either moral physical evident\"                               \n[5082] \"may either moral physical religion\"                              \n[5083] \"may either moral ritual evident\"                                 \n[5084] \"may either moral ritual religion\"                                \n[5085] \"may either moral ritual sense\"                                   \n[5086] \"may either moral evident religion\"                               \n[5087] \"may either moral evident sense\"                                  \n[5088] \"may either moral evident take\"                                   \n[5089] \"may either physical ritual evident\"                              \n[5090] \"may either physical ritual religion\"                             \n[5091] \"may either physical ritual sense\"                                \n[5092] \"may either physical evident religion\"                            \n[5093] \"may either physical evident sense\"                               \n[5094] \"may either physical evident take\"                                \n[5095] \"may either physical religion sense\"                              \n[5096] \"may either physical religion take\"                               \n[5097] \"may either physical religion theologies\"                         \n[5098] \"may either ritual evident religion\"                              \n[5099] \"may either ritual evident sense\"                                 \n[5100] \"may either ritual evident take\"                                  \n[5101] \"may either ritual religion sense\"                                \n[5102] \"may either ritual religion take\"                                 \n[5103] \"may either ritual religion theologies\"                           \n[5104] \"may either ritual sense take\"                                    \n[5105] \"may either ritual sense theologies\"                              \n[5106] \"may either ritual sense philosophies\"                            \n[5107] \"may moral physical ritual evident\"                               \n[5108] \"may moral physical ritual religion\"                              \n[5109] \"may moral physical ritual sense\"                                 \n[5110] \"may moral physical evident religion\"                             \n[5111] \"may moral physical evident sense\"                                \n[5112] \"may moral physical evident take\"                                 \n[5113] \"may moral physical religion sense\"                               \n[5114] \"may moral physical religion take\"                                \n[5115] \"may moral physical religion theologies\"                          \n[5116] \"may moral ritual evident religion\"                               \n[5117] \"may moral ritual evident sense\"                                  \n[5118] \"may moral ritual evident take\"                                   \n[5119] \"may moral ritual religion sense\"                                 \n[5120] \"may moral ritual religion take\"                                  \n[5121] \"may moral ritual religion theologies\"                            \n[5122] \"may moral ritual sense take\"                                     \n[5123] \"may moral ritual sense theologies\"                               \n[5124] \"may moral ritual sense philosophies\"                             \n[5125] \"may moral evident religion sense\"                                \n[5126] \"may moral evident religion take\"                                 \n[5127] \"may moral evident religion theologies\"                           \n[5128] \"may moral evident sense take\"                                    \n[5129] \"may moral evident sense theologies\"                              \n[5130] \"may moral evident sense philosophies\"                            \n[5131] \"may moral evident take theologies\"                               \n[5132] \"may moral evident take philosophies\"                             \n[5133] \"may moral evident take ecclesiastical\"                           \n[5134] \"may physical ritual evident religion\"                            \n[5135] \"may physical ritual evident sense\"                               \n[5136] \"may physical ritual evident take\"                                \n[5137] \"may physical ritual religion sense\"                              \n[5138] \"may physical ritual religion take\"                               \n[5139] \"may physical ritual religion theologies\"                         \n[5140] \"may physical ritual sense take\"                                  \n[5141] \"may physical ritual sense theologies\"                            \n[5142] \"may physical ritual sense philosophies\"                          \n[5143] \"may physical evident religion sense\"                             \n[5144] \"may physical evident religion take\"                              \n[5145] \"may physical evident religion theologies\"                        \n[5146] \"may physical evident sense take\"                                 \n[5147] \"may physical evident sense theologies\"                           \n[5148] \"may physical evident sense philosophies\"                         \n[5149] \"may physical evident take theologies\"                            \n[5150] \"may physical evident take philosophies\"                          \n[5151] \"may physical evident take ecclesiastical\"                        \n[5152] \"may physical religion sense take\"                                \n[5153] \"may physical religion sense theologies\"                          \n[5154] \"may physical religion sense philosophies\"                        \n[5155] \"may physical religion take theologies\"                           \n[5156] \"may physical religion take philosophies\"                         \n[5157] \"may physical religion take ecclesiastical\"                       \n[5158] \"may physical religion theologies philosophies\"                   \n[5159] \"may physical religion theologies ecclesiastical\"                 \n[5160] \"may physical religion theologies organizations\"                  \n[5161] \"either moral\"                                                    \n[5162] \"either physical\"                                                 \n[5163] \"either ritual\"                                                   \n[5164] \"either moral physical\"                                           \n[5165] \"either moral ritual\"                                             \n[5166] \"either moral evident\"                                            \n[5167] \"either physical ritual\"                                          \n[5168] \"either physical evident\"                                         \n[5169] \"either physical religion\"                                        \n[5170] \"either ritual evident\"                                           \n[5171] \"either ritual religion\"                                          \n[5172] \"either ritual sense\"                                             \n[5173] \"either moral physical ritual\"                                    \n[5174] \"either moral physical evident\"                                   \n[5175] \"either moral physical religion\"                                  \n[5176] \"either moral ritual evident\"                                     \n[5177] \"either moral ritual religion\"                                    \n[5178] \"either moral ritual sense\"                                       \n[5179] \"either moral evident religion\"                                   \n[5180] \"either moral evident sense\"                                      \n[5181] \"either moral evident take\"                                       \n[5182] \"either physical ritual evident\"                                  \n[5183] \"either physical ritual religion\"                                 \n[5184] \"either physical ritual sense\"                                    \n[5185] \"either physical evident religion\"                                \n[5186] \"either physical evident sense\"                                   \n[5187] \"either physical evident take\"                                    \n[5188] \"either physical religion sense\"                                  \n[5189] \"either physical religion take\"                                   \n[5190] \"either physical religion theologies\"                             \n[5191] \"either ritual evident religion\"                                  \n[5192] \"either ritual evident sense\"                                     \n[5193] \"either ritual evident take\"                                      \n[5194] \"either ritual religion sense\"                                    \n[5195] \"either ritual religion take\"                                     \n[5196] \"either ritual religion theologies\"                               \n[5197] \"either ritual sense take\"                                        \n[5198] \"either ritual sense theologies\"                                  \n[5199] \"either ritual sense philosophies\"                                \n[5200] \"either moral physical ritual evident\"                            \n[5201] \"either moral physical ritual religion\"                           \n[5202] \"either moral physical ritual sense\"                              \n[5203] \"either moral physical evident religion\"                          \n[5204] \"either moral physical evident sense\"                             \n[5205] \"either moral physical evident take\"                              \n[5206] \"either moral physical religion sense\"                            \n[5207] \"either moral physical religion take\"                             \n[5208] \"either moral physical religion theologies\"                       \n[5209] \"either moral ritual evident religion\"                            \n[5210] \"either moral ritual evident sense\"                               \n[5211] \"either moral ritual evident take\"                                \n[5212] \"either moral ritual religion sense\"                              \n[5213] \"either moral ritual religion take\"                               \n[5214] \"either moral ritual religion theologies\"                         \n[5215] \"either moral ritual sense take\"                                  \n[5216] \"either moral ritual sense theologies\"                            \n[5217] \"either moral ritual sense philosophies\"                          \n[5218] \"either moral evident religion sense\"                             \n[5219] \"either moral evident religion take\"                              \n[5220] \"either moral evident religion theologies\"                        \n[5221] \"either moral evident sense take\"                                 \n[5222] \"either moral evident sense theologies\"                           \n[5223] \"either moral evident sense philosophies\"                         \n[5224] \"either moral evident take theologies\"                            \n[5225] \"either moral evident take philosophies\"                          \n[5226] \"either moral evident take ecclesiastical\"                        \n[5227] \"either physical ritual evident religion\"                         \n[5228] \"either physical ritual evident sense\"                            \n[5229] \"either physical ritual evident take\"                             \n[5230] \"either physical ritual religion sense\"                           \n[5231] \"either physical ritual religion take\"                            \n[5232] \"either physical ritual religion theologies\"                      \n[5233] \"either physical ritual sense take\"                               \n[5234] \"either physical ritual sense theologies\"                         \n[5235] \"either physical ritual sense philosophies\"                       \n[5236] \"either physical evident religion sense\"                          \n[5237] \"either physical evident religion take\"                           \n[5238] \"either physical evident religion theologies\"                     \n[5239] \"either physical evident sense take\"                              \n[5240] \"either physical evident sense theologies\"                        \n[5241] \"either physical evident sense philosophies\"                      \n[5242] \"either physical evident take theologies\"                         \n[5243] \"either physical evident take philosophies\"                       \n[5244] \"either physical evident take ecclesiastical\"                     \n[5245] \"either physical religion sense take\"                             \n[5246] \"either physical religion sense theologies\"                       \n[5247] \"either physical religion sense philosophies\"                     \n[5248] \"either physical religion take theologies\"                        \n[5249] \"either physical religion take philosophies\"                      \n[5250] \"either physical religion take ecclesiastical\"                    \n[5251] \"either physical religion theologies philosophies\"                \n[5252] \"either physical religion theologies ecclesiastical\"              \n[5253] \"either physical religion theologies organizations\"               \n[5254] \"either ritual evident religion sense\"                            \n[5255] \"either ritual evident religion take\"                             \n[5256] \"either ritual evident religion theologies\"                       \n[5257] \"either ritual evident sense take\"                                \n[5258] \"either ritual evident sense theologies\"                          \n[5259] \"either ritual evident sense philosophies\"                        \n[5260] \"either ritual evident take theologies\"                           \n[5261] \"either ritual evident take philosophies\"                         \n[5262] \"either ritual evident take ecclesiastical\"                       \n[5263] \"either ritual religion sense take\"                               \n[5264] \"either ritual religion sense theologies\"                         \n[5265] \"either ritual religion sense philosophies\"                       \n[5266] \"either ritual religion take theologies\"                          \n[5267] \"either ritual religion take philosophies\"                        \n[5268] \"either ritual religion take ecclesiastical\"                      \n[5269] \"either ritual religion theologies philosophies\"                  \n[5270] \"either ritual religion theologies ecclesiastical\"                \n[5271] \"either ritual religion theologies organizations\"                 \n[5272] \"either ritual sense take theologies\"                             \n[5273] \"either ritual sense take philosophies\"                           \n[5274] \"either ritual sense take ecclesiastical\"                         \n[5275] \"either ritual sense theologies philosophies\"                     \n[5276] \"either ritual sense theologies ecclesiastical\"                   \n[5277] \"either ritual sense theologies organizations\"                    \n[5278] \"either ritual sense philosophies ecclesiastical\"                 \n[5279] \"either ritual sense philosophies organizations\"                  \n[5280] \"either ritual sense philosophies may\"                            \n[5281] \"moral physical\"                                                  \n[5282] \"moral ritual\"                                                    \n[5283] \"moral evident\"                                                   \n[5284] \"moral physical ritual\"                                           \n[5285] \"moral physical evident\"                                          \n[5286] \"moral physical religion\"                                         \n[5287] \"moral ritual evident\"                                            \n[5288] \"moral ritual religion\"                                           \n[5289] \"moral ritual sense\"                                              \n[5290] \"moral evident religion\"                                          \n[5291] \"moral evident sense\"                                             \n[5292] \"moral evident take\"                                              \n[5293] \"moral physical ritual evident\"                                   \n[5294] \"moral physical ritual religion\"                                  \n[5295] \"moral physical ritual sense\"                                     \n[5296] \"moral physical evident religion\"                                 \n[5297] \"moral physical evident sense\"                                    \n[5298] \"moral physical evident take\"                                     \n[5299] \"moral physical religion sense\"                                   \n[5300] \"moral physical religion take\"                                    \n[5301] \"moral physical religion theologies\"                              \n[5302] \"moral ritual evident religion\"                                   \n[5303] \"moral ritual evident sense\"                                      \n[5304] \"moral ritual evident take\"                                       \n[5305] \"moral ritual religion sense\"                                     \n[5306] \"moral ritual religion take\"                                      \n[5307] \"moral ritual religion theologies\"                                \n[5308] \"moral ritual sense take\"                                         \n[5309] \"moral ritual sense theologies\"                                   \n[5310] \"moral ritual sense philosophies\"                                 \n[5311] \"moral evident religion sense\"                                    \n[5312] \"moral evident religion take\"                                     \n[5313] \"moral evident religion theologies\"                               \n[5314] \"moral evident sense take\"                                        \n[5315] \"moral evident sense theologies\"                                  \n[5316] \"moral evident sense philosophies\"                                \n[5317] \"moral evident take theologies\"                                   \n[5318] \"moral evident take philosophies\"                                 \n[5319] \"moral evident take ecclesiastical\"                               \n[5320] \"moral physical ritual evident religion\"                          \n[5321] \"moral physical ritual evident sense\"                             \n[5322] \"moral physical ritual evident take\"                              \n[5323] \"moral physical ritual religion sense\"                            \n[5324] \"moral physical ritual religion take\"                             \n[5325] \"moral physical ritual religion theologies\"                       \n[5326] \"moral physical ritual sense take\"                                \n[5327] \"moral physical ritual sense theologies\"                          \n[5328] \"moral physical ritual sense philosophies\"                        \n[5329] \"moral physical evident religion sense\"                           \n[5330] \"moral physical evident religion take\"                            \n[5331] \"moral physical evident religion theologies\"                      \n[5332] \"moral physical evident sense take\"                               \n[5333] \"moral physical evident sense theologies\"                         \n[5334] \"moral physical evident sense philosophies\"                       \n[5335] \"moral physical evident take theologies\"                          \n[5336] \"moral physical evident take philosophies\"                        \n[5337] \"moral physical evident take ecclesiastical\"                      \n[5338] \"moral physical religion sense take\"                              \n[5339] \"moral physical religion sense theologies\"                        \n[5340] \"moral physical religion sense philosophies\"                      \n[5341] \"moral physical religion take theologies\"                         \n[5342] \"moral physical religion take philosophies\"                       \n[5343] \"moral physical religion take ecclesiastical\"                     \n[5344] \"moral physical religion theologies philosophies\"                 \n[5345] \"moral physical religion theologies ecclesiastical\"               \n[5346] \"moral physical religion theologies organizations\"                \n[5347] \"moral ritual evident religion sense\"                             \n[5348] \"moral ritual evident religion take\"                              \n[5349] \"moral ritual evident religion theologies\"                        \n[5350] \"moral ritual evident sense take\"                                 \n[5351] \"moral ritual evident sense theologies\"                           \n[5352] \"moral ritual evident sense philosophies\"                         \n[5353] \"moral ritual evident take theologies\"                            \n[5354] \"moral ritual evident take philosophies\"                          \n[5355] \"moral ritual evident take ecclesiastical\"                        \n[5356] \"moral ritual religion sense take\"                                \n[5357] \"moral ritual religion sense theologies\"                          \n[5358] \"moral ritual religion sense philosophies\"                        \n[5359] \"moral ritual religion take theologies\"                           \n[5360] \"moral ritual religion take philosophies\"                         \n[5361] \"moral ritual religion take ecclesiastical\"                       \n[5362] \"moral ritual religion theologies philosophies\"                   \n[5363] \"moral ritual religion theologies ecclesiastical\"                 \n[5364] \"moral ritual religion theologies organizations\"                  \n[5365] \"moral ritual sense take theologies\"                              \n[5366] \"moral ritual sense take philosophies\"                            \n[5367] \"moral ritual sense take ecclesiastical\"                          \n[5368] \"moral ritual sense theologies philosophies\"                      \n[5369] \"moral ritual sense theologies ecclesiastical\"                    \n[5370] \"moral ritual sense theologies organizations\"                     \n[5371] \"moral ritual sense philosophies ecclesiastical\"                  \n[5372] \"moral ritual sense philosophies organizations\"                   \n[5373] \"moral ritual sense philosophies may\"                             \n[5374] \"moral evident religion sense take\"                               \n[5375] \"moral evident religion sense theologies\"                         \n[5376] \"moral evident religion sense philosophies\"                       \n[5377] \"moral evident religion take theologies\"                          \n[5378] \"moral evident religion take philosophies\"                        \n[5379] \"moral evident religion take ecclesiastical\"                      \n[5380] \"moral evident religion theologies philosophies\"                  \n[5381] \"moral evident religion theologies ecclesiastical\"                \n[5382] \"moral evident religion theologies organizations\"                 \n[5383] \"moral evident sense take theologies\"                             \n[5384] \"moral evident sense take philosophies\"                           \n[5385] \"moral evident sense take ecclesiastical\"                         \n[5386] \"moral evident sense theologies philosophies\"                     \n[5387] \"moral evident sense theologies ecclesiastical\"                   \n[5388] \"moral evident sense theologies organizations\"                    \n[5389] \"moral evident sense philosophies ecclesiastical\"                 \n[5390] \"moral evident sense philosophies organizations\"                  \n[5391] \"moral evident sense philosophies may\"                            \n[5392] \"moral evident take theologies philosophies\"                      \n[5393] \"moral evident take theologies ecclesiastical\"                    \n[5394] \"moral evident take theologies organizations\"                     \n[5395] \"moral evident take philosophies ecclesiastical\"                  \n[5396] \"moral evident take philosophies organizations\"                   \n[5397] \"moral evident take philosophies may\"                             \n[5398] \"moral evident take ecclesiastical organizations\"                 \n[5399] \"moral evident take ecclesiastical may\"                           \n[5400] \"moral evident take ecclesiastical secondarily\"                   \n[5401] \"physical ritual\"                                                 \n[5402] \"physical evident\"                                                \n[5403] \"physical religion\"                                               \n[5404] \"physical ritual evident\"                                         \n[5405] \"physical ritual religion\"                                        \n[5406] \"physical ritual sense\"                                           \n[5407] \"physical evident religion\"                                       \n[5408] \"physical evident sense\"                                          \n[5409] \"physical evident take\"                                           \n[5410] \"physical religion sense\"                                         \n[5411] \"physical religion take\"                                          \n[5412] \"physical religion theologies\"                                    \n[5413] \"physical ritual evident religion\"                                \n[5414] \"physical ritual evident sense\"                                   \n[5415] \"physical ritual evident take\"                                    \n[5416] \"physical ritual religion sense\"                                  \n[5417] \"physical ritual religion take\"                                   \n[5418] \"physical ritual religion theologies\"                             \n[5419] \"physical ritual sense take\"                                      \n[5420] \"physical ritual sense theologies\"                                \n[5421] \"physical ritual sense philosophies\"                              \n[5422] \"physical evident religion sense\"                                 \n[5423] \"physical evident religion take\"                                  \n[5424] \"physical evident religion theologies\"                            \n[5425] \"physical evident sense take\"                                     \n[5426] \"physical evident sense theologies\"                               \n[5427] \"physical evident sense philosophies\"                             \n[5428] \"physical evident take theologies\"                                \n[5429] \"physical evident take philosophies\"                              \n[5430] \"physical evident take ecclesiastical\"                            \n[5431] \"physical religion sense take\"                                    \n[5432] \"physical religion sense theologies\"                              \n[5433] \"physical religion sense philosophies\"                            \n[5434] \"physical religion take theologies\"                               \n[5435] \"physical religion take philosophies\"                             \n[5436] \"physical religion take ecclesiastical\"                           \n[5437] \"physical religion theologies philosophies\"                       \n[5438] \"physical religion theologies ecclesiastical\"                     \n[5439] \"physical religion theologies organizations\"                      \n[5440] \"physical ritual evident religion sense\"                          \n[5441] \"physical ritual evident religion take\"                           \n[5442] \"physical ritual evident religion theologies\"                     \n[5443] \"physical ritual evident sense take\"                              \n[5444] \"physical ritual evident sense theologies\"                        \n[5445] \"physical ritual evident sense philosophies\"                      \n[5446] \"physical ritual evident take theologies\"                         \n[5447] \"physical ritual evident take philosophies\"                       \n[5448] \"physical ritual evident take ecclesiastical\"                     \n[5449] \"physical ritual religion sense take\"                             \n[5450] \"physical ritual religion sense theologies\"                       \n[5451] \"physical ritual religion sense philosophies\"                     \n[5452] \"physical ritual religion take theologies\"                        \n[5453] \"physical ritual religion take philosophies\"                      \n[5454] \"physical ritual religion take ecclesiastical\"                    \n[5455] \"physical ritual religion theologies philosophies\"                \n[5456] \"physical ritual religion theologies ecclesiastical\"              \n[5457] \"physical ritual religion theologies organizations\"               \n[5458] \"physical ritual sense take theologies\"                           \n[5459] \"physical ritual sense take philosophies\"                         \n[5460] \"physical ritual sense take ecclesiastical\"                       \n[5461] \"physical ritual sense theologies philosophies\"                   \n[5462] \"physical ritual sense theologies ecclesiastical\"                 \n[5463] \"physical ritual sense theologies organizations\"                  \n[5464] \"physical ritual sense philosophies ecclesiastical\"               \n[5465] \"physical ritual sense philosophies organizations\"                \n[5466] \"physical ritual sense philosophies may\"                          \n[5467] \"physical evident religion sense take\"                            \n[5468] \"physical evident religion sense theologies\"                      \n[5469] \"physical evident religion sense philosophies\"                    \n[5470] \"physical evident religion take theologies\"                       \n[5471] \"physical evident religion take philosophies\"                     \n[5472] \"physical evident religion take ecclesiastical\"                   \n[5473] \"physical evident religion theologies philosophies\"               \n[5474] \"physical evident religion theologies ecclesiastical\"             \n[5475] \"physical evident religion theologies organizations\"              \n[5476] \"physical evident sense take theologies\"                          \n[5477] \"physical evident sense take philosophies\"                        \n[5478] \"physical evident sense take ecclesiastical\"                      \n[5479] \"physical evident sense theologies philosophies\"                  \n[5480] \"physical evident sense theologies ecclesiastical\"                \n[5481] \"physical evident sense theologies organizations\"                 \n[5482] \"physical evident sense philosophies ecclesiastical\"              \n[5483] \"physical evident sense philosophies organizations\"               \n[5484] \"physical evident sense philosophies may\"                         \n[5485] \"physical evident take theologies philosophies\"                   \n[5486] \"physical evident take theologies ecclesiastical\"                 \n[5487] \"physical evident take theologies organizations\"                  \n[5488] \"physical evident take philosophies ecclesiastical\"               \n[5489] \"physical evident take philosophies organizations\"                \n[5490] \"physical evident take philosophies may\"                          \n[5491] \"physical evident take ecclesiastical organizations\"              \n[5492] \"physical evident take ecclesiastical may\"                        \n[5493] \"physical evident take ecclesiastical secondarily\"                \n[5494] \"physical religion sense take theologies\"                         \n[5495] \"physical religion sense take philosophies\"                       \n[5496] \"physical religion sense take ecclesiastical\"                     \n[5497] \"physical religion sense theologies philosophies\"                 \n[5498] \"physical religion sense theologies ecclesiastical\"               \n[5499] \"physical religion sense theologies organizations\"                \n[5500] \"physical religion sense philosophies ecclesiastical\"             \n[5501] \"physical religion sense philosophies organizations\"              \n[5502] \"physical religion sense philosophies may\"                        \n[5503] \"physical religion take theologies philosophies\"                  \n[5504] \"physical religion take theologies ecclesiastical\"                \n[5505] \"physical religion take theologies organizations\"                 \n[5506] \"physical religion take philosophies ecclesiastical\"              \n[5507] \"physical religion take philosophies organizations\"               \n[5508] \"physical religion take philosophies may\"                         \n[5509] \"physical religion take ecclesiastical organizations\"             \n[5510] \"physical religion take ecclesiastical may\"                       \n[5511] \"physical religion take ecclesiastical secondarily\"               \n[5512] \"physical religion theologies philosophies ecclesiastical\"        \n[5513] \"physical religion theologies philosophies organizations\"         \n[5514] \"physical religion theologies philosophies may\"                   \n[5515] \"physical religion theologies ecclesiastical organizations\"       \n[5516] \"physical religion theologies ecclesiastical may\"                 \n[5517] \"physical religion theologies ecclesiastical secondarily\"         \n[5518] \"physical religion theologies organizations may\"                  \n[5519] \"physical religion theologies organizations secondarily\"          \n[5520] \"physical religion theologies organizations grow\"                 \n[5521] \"ritual evident\"                                                  \n[5522] \"ritual religion\"                                                 \n[5523] \"ritual sense\"                                                    \n[5524] \"ritual evident religion\"                                         \n[5525] \"ritual evident sense\"                                            \n[5526] \"ritual evident take\"                                             \n[5527] \"ritual religion sense\"                                           \n[5528] \"ritual religion take\"                                            \n[5529] \"ritual religion theologies\"                                      \n[5530] \"ritual sense take\"                                               \n[5531] \"ritual sense theologies\"                                         \n[5532] \"ritual sense philosophies\"                                       \n[5533] \"ritual evident religion sense\"                                   \n[5534] \"ritual evident religion take\"                                    \n[5535] \"ritual evident religion theologies\"                              \n[5536] \"ritual evident sense take\"                                       \n[5537] \"ritual evident sense theologies\"                                 \n[5538] \"ritual evident sense philosophies\"                               \n[5539] \"ritual evident take theologies\"                                  \n[5540] \"ritual evident take philosophies\"                                \n[5541] \"ritual evident take ecclesiastical\"                              \n[5542] \"ritual religion sense take\"                                      \n[5543] \"ritual religion sense theologies\"                                \n[5544] \"ritual religion sense philosophies\"                              \n[5545] \"ritual religion take theologies\"                                 \n[5546] \"ritual religion take philosophies\"                               \n[5547] \"ritual religion take ecclesiastical\"                             \n[5548] \"ritual religion theologies philosophies\"                         \n[5549] \"ritual religion theologies ecclesiastical\"                       \n[5550] \"ritual religion theologies organizations\"                        \n[5551] \"ritual sense take theologies\"                                    \n[5552] \"ritual sense take philosophies\"                                  \n[5553] \"ritual sense take ecclesiastical\"                                \n[5554] \"ritual sense theologies philosophies\"                            \n[5555] \"ritual sense theologies ecclesiastical\"                          \n[5556] \"ritual sense theologies organizations\"                           \n[5557] \"ritual sense philosophies ecclesiastical\"                        \n[5558] \"ritual sense philosophies organizations\"                         \n[5559] \"ritual sense philosophies may\"                                   \n[5560] \"ritual evident religion sense take\"                              \n[5561] \"ritual evident religion sense theologies\"                        \n[5562] \"ritual evident religion sense philosophies\"                      \n[5563] \"ritual evident religion take theologies\"                         \n[5564] \"ritual evident religion take philosophies\"                       \n[5565] \"ritual evident religion take ecclesiastical\"                     \n[5566] \"ritual evident religion theologies philosophies\"                 \n[5567] \"ritual evident religion theologies ecclesiastical\"               \n[5568] \"ritual evident religion theologies organizations\"                \n[5569] \"ritual evident sense take theologies\"                            \n[5570] \"ritual evident sense take philosophies\"                          \n[5571] \"ritual evident sense take ecclesiastical\"                        \n[5572] \"ritual evident sense theologies philosophies\"                    \n[5573] \"ritual evident sense theologies ecclesiastical\"                  \n[5574] \"ritual evident sense theologies organizations\"                   \n[5575] \"ritual evident sense philosophies ecclesiastical\"                \n[5576] \"ritual evident sense philosophies organizations\"                 \n[5577] \"ritual evident sense philosophies may\"                           \n[5578] \"ritual evident take theologies philosophies\"                     \n[5579] \"ritual evident take theologies ecclesiastical\"                   \n[5580] \"ritual evident take theologies organizations\"                    \n[5581] \"ritual evident take philosophies ecclesiastical\"                 \n[5582] \"ritual evident take philosophies organizations\"                  \n[5583] \"ritual evident take philosophies may\"                            \n[5584] \"ritual evident take ecclesiastical organizations\"                \n[5585] \"ritual evident take ecclesiastical may\"                          \n[5586] \"ritual evident take ecclesiastical secondarily\"                  \n[5587] \"ritual religion sense take theologies\"                           \n[5588] \"ritual religion sense take philosophies\"                         \n[5589] \"ritual religion sense take ecclesiastical\"                       \n[5590] \"ritual religion sense theologies philosophies\"                   \n[5591] \"ritual religion sense theologies ecclesiastical\"                 \n[5592] \"ritual religion sense theologies organizations\"                  \n[5593] \"ritual religion sense philosophies ecclesiastical\"               \n[5594] \"ritual religion sense philosophies organizations\"                \n[5595] \"ritual religion sense philosophies may\"                          \n[5596] \"ritual religion take theologies philosophies\"                    \n[5597] \"ritual religion take theologies ecclesiastical\"                  \n[5598] \"ritual religion take theologies organizations\"                   \n[5599] \"ritual religion take philosophies ecclesiastical\"                \n[5600] \"ritual religion take philosophies organizations\"                 \n[5601] \"ritual religion take philosophies may\"                           \n[5602] \"ritual religion take ecclesiastical organizations\"               \n[5603] \"ritual religion take ecclesiastical may\"                         \n[5604] \"ritual religion take ecclesiastical secondarily\"                 \n[5605] \"ritual religion theologies philosophies ecclesiastical\"          \n[5606] \"ritual religion theologies philosophies organizations\"           \n[5607] \"ritual religion theologies philosophies may\"                     \n[5608] \"ritual religion theologies ecclesiastical organizations\"         \n[5609] \"ritual religion theologies ecclesiastical may\"                   \n[5610] \"ritual religion theologies ecclesiastical secondarily\"           \n[5611] \"ritual religion theologies organizations may\"                    \n[5612] \"ritual religion theologies organizations secondarily\"            \n[5613] \"ritual religion theologies organizations grow\"                   \n[5614] \"ritual sense take theologies philosophies\"                       \n[5615] \"ritual sense take theologies ecclesiastical\"                     \n[5616] \"ritual sense take theologies organizations\"                      \n[5617] \"ritual sense take philosophies ecclesiastical\"                   \n[5618] \"ritual sense take philosophies organizations\"                    \n[5619] \"ritual sense take philosophies may\"                              \n[5620] \"ritual sense take ecclesiastical organizations\"                  \n[5621] \"ritual sense take ecclesiastical may\"                            \n[5622] \"ritual sense take ecclesiastical secondarily\"                    \n[5623] \"ritual sense theologies philosophies ecclesiastical\"             \n[5624] \"ritual sense theologies philosophies organizations\"              \n[5625] \"ritual sense theologies philosophies may\"                        \n[5626] \"ritual sense theologies ecclesiastical organizations\"            \n[5627] \"ritual sense theologies ecclesiastical may\"                      \n[5628] \"ritual sense theologies ecclesiastical secondarily\"              \n[5629] \"ritual sense theologies organizations may\"                       \n[5630] \"ritual sense theologies organizations secondarily\"               \n[5631] \"ritual sense theologies organizations grow\"                      \n[5632] \"ritual sense philosophies ecclesiastical organizations\"          \n[5633] \"ritual sense philosophies ecclesiastical may\"                    \n[5634] \"ritual sense philosophies ecclesiastical secondarily\"            \n[5635] \"ritual sense philosophies organizations may\"                     \n[5636] \"ritual sense philosophies organizations secondarily\"             \n[5637] \"ritual sense philosophies organizations grow\"                    \n[5638] \"ritual sense philosophies may secondarily\"                       \n[5639] \"ritual sense philosophies may grow\"                              \n[5640] \"evident religion\"                                                \n[5641] \"evident sense\"                                                   \n[5642] \"evident take\"                                                    \n[5643] \"evident religion sense\"                                          \n[5644] \"evident religion take\"                                           \n[5645] \"evident religion theologies\"                                     \n[5646] \"evident sense take\"                                              \n[5647] \"evident sense theologies\"                                        \n[5648] \"evident sense philosophies\"                                      \n[5649] \"evident take theologies\"                                         \n[5650] \"evident take philosophies\"                                       \n[5651] \"evident take ecclesiastical\"                                     \n[5652] \"evident religion sense take\"                                     \n[5653] \"evident religion sense theologies\"                               \n[5654] \"evident religion sense philosophies\"                             \n[5655] \"evident religion take theologies\"                                \n[5656] \"evident religion take philosophies\"                              \n[5657] \"evident religion take ecclesiastical\"                            \n[5658] \"evident religion theologies philosophies\"                        \n[5659] \"evident religion theologies ecclesiastical\"                      \n[5660] \"evident religion theologies organizations\"                       \n[5661] \"evident sense take theologies\"                                   \n[5662] \"evident sense take philosophies\"                                 \n[5663] \"evident sense take ecclesiastical\"                               \n[5664] \"evident sense theologies philosophies\"                           \n[5665] \"evident sense theologies ecclesiastical\"                         \n[5666] \"evident sense theologies organizations\"                          \n[5667] \"evident sense philosophies ecclesiastical\"                       \n[5668] \"evident sense philosophies organizations\"                        \n[5669] \"evident sense philosophies may\"                                  \n[5670] \"evident take theologies philosophies\"                            \n[5671] \"evident take theologies ecclesiastical\"                          \n[5672] \"evident take theologies organizations\"                           \n[5673] \"evident take philosophies ecclesiastical\"                        \n[5674] \"evident take philosophies organizations\"                         \n[5675] \"evident take philosophies may\"                                   \n[5676] \"evident take ecclesiastical organizations\"                       \n[5677] \"evident take ecclesiastical may\"                                 \n[5678] \"evident take ecclesiastical secondarily\"                         \n[5679] \"evident religion sense take theologies\"                          \n[5680] \"evident religion sense take philosophies\"                        \n[5681] \"evident religion sense take ecclesiastical\"                      \n[5682] \"evident religion sense theologies philosophies\"                  \n[5683] \"evident religion sense theologies ecclesiastical\"                \n[5684] \"evident religion sense theologies organizations\"                 \n[5685] \"evident religion sense philosophies ecclesiastical\"              \n[5686] \"evident religion sense philosophies organizations\"               \n[5687] \"evident religion sense philosophies may\"                         \n[5688] \"evident religion take theologies philosophies\"                   \n[5689] \"evident religion take theologies ecclesiastical\"                 \n[5690] \"evident religion take theologies organizations\"                  \n[5691] \"evident religion take philosophies ecclesiastical\"               \n[5692] \"evident religion take philosophies organizations\"                \n[5693] \"evident religion take philosophies may\"                          \n[5694] \"evident religion take ecclesiastical organizations\"              \n[5695] \"evident religion take ecclesiastical may\"                        \n[5696] \"evident religion take ecclesiastical secondarily\"                \n[5697] \"evident religion theologies philosophies ecclesiastical\"         \n[5698] \"evident religion theologies philosophies organizations\"          \n[5699] \"evident religion theologies philosophies may\"                    \n[5700] \"evident religion theologies ecclesiastical organizations\"        \n[5701] \"evident religion theologies ecclesiastical may\"                  \n[5702] \"evident religion theologies ecclesiastical secondarily\"          \n[5703] \"evident religion theologies organizations may\"                   \n[5704] \"evident religion theologies organizations secondarily\"           \n[5705] \"evident religion theologies organizations grow\"                  \n[5706] \"evident sense take theologies philosophies\"                      \n[5707] \"evident sense take theologies ecclesiastical\"                    \n[5708] \"evident sense take theologies organizations\"                     \n[5709] \"evident sense take philosophies ecclesiastical\"                  \n[5710] \"evident sense take philosophies organizations\"                   \n[5711] \"evident sense take philosophies may\"                             \n[5712] \"evident sense take ecclesiastical organizations\"                 \n[5713] \"evident sense take ecclesiastical may\"                           \n[5714] \"evident sense take ecclesiastical secondarily\"                   \n[5715] \"evident sense theologies philosophies ecclesiastical\"            \n[5716] \"evident sense theologies philosophies organizations\"             \n[5717] \"evident sense theologies philosophies may\"                       \n[5718] \"evident sense theologies ecclesiastical organizations\"           \n[5719] \"evident sense theologies ecclesiastical may\"                     \n[5720] \"evident sense theologies ecclesiastical secondarily\"             \n[5721] \"evident sense theologies organizations may\"                      \n[5722] \"evident sense theologies organizations secondarily\"              \n[5723] \"evident sense theologies organizations grow\"                     \n[5724] \"evident sense philosophies ecclesiastical organizations\"         \n[5725] \"evident sense philosophies ecclesiastical may\"                   \n[5726] \"evident sense philosophies ecclesiastical secondarily\"           \n[5727] \"evident sense philosophies organizations may\"                    \n[5728] \"evident sense philosophies organizations secondarily\"            \n[5729] \"evident sense philosophies organizations grow\"                   \n[5730] \"evident sense philosophies may secondarily\"                      \n[5731] \"evident sense philosophies may grow\"                             \n[5732] \"evident take theologies philosophies ecclesiastical\"             \n[5733] \"evident take theologies philosophies organizations\"              \n[5734] \"evident take theologies philosophies may\"                        \n[5735] \"evident take theologies ecclesiastical organizations\"            \n[5736] \"evident take theologies ecclesiastical may\"                      \n[5737] \"evident take theologies ecclesiastical secondarily\"              \n[5738] \"evident take theologies organizations may\"                       \n[5739] \"evident take theologies organizations secondarily\"               \n[5740] \"evident take theologies organizations grow\"                      \n[5741] \"evident take philosophies ecclesiastical organizations\"          \n[5742] \"evident take philosophies ecclesiastical may\"                    \n[5743] \"evident take philosophies ecclesiastical secondarily\"            \n[5744] \"evident take philosophies organizations may\"                     \n[5745] \"evident take philosophies organizations secondarily\"             \n[5746] \"evident take philosophies organizations grow\"                    \n[5747] \"evident take philosophies may secondarily\"                       \n[5748] \"evident take philosophies may grow\"                              \n[5749] \"evident take ecclesiastical organizations may\"                   \n[5750] \"evident take ecclesiastical organizations secondarily\"           \n[5751] \"evident take ecclesiastical organizations grow\"                  \n[5752] \"evident take ecclesiastical may secondarily\"                     \n[5753] \"evident take ecclesiastical may grow\"                            \n[5754] \"evident take ecclesiastical secondarily grow\"                    \n[5755] \"religion sense\"                                                  \n[5756] \"religion take\"                                                   \n[5757] \"religion theologies\"                                             \n[5758] \"religion sense take\"                                             \n[5759] \"religion sense theologies\"                                       \n[5760] \"religion sense philosophies\"                                     \n[5761] \"religion take theologies\"                                        \n[5762] \"religion take philosophies\"                                      \n[5763] \"religion take ecclesiastical\"                                    \n[5764] \"religion theologies philosophies\"                                \n[5765] \"religion theologies ecclesiastical\"                              \n[5766] \"religion theologies organizations\"                               \n[5767] \"religion sense take theologies\"                                  \n[5768] \"religion sense take philosophies\"                                \n[5769] \"religion sense take ecclesiastical\"                              \n[5770] \"religion sense theologies philosophies\"                          \n[5771] \"religion sense theologies ecclesiastical\"                        \n[5772] \"religion sense theologies organizations\"                         \n[5773] \"religion sense philosophies ecclesiastical\"                      \n[5774] \"religion sense philosophies organizations\"                       \n[5775] \"religion sense philosophies may\"                                 \n[5776] \"religion take theologies philosophies\"                           \n[5777] \"religion take theologies ecclesiastical\"                         \n[5778] \"religion take theologies organizations\"                          \n[5779] \"religion take philosophies ecclesiastical\"                       \n[5780] \"religion take philosophies organizations\"                        \n[5781] \"religion take philosophies may\"                                  \n[5782] \"religion take ecclesiastical organizations\"                      \n[5783] \"religion take ecclesiastical may\"                                \n[5784] \"religion take ecclesiastical secondarily\"                        \n[5785] \"religion theologies philosophies ecclesiastical\"                 \n[5786] \"religion theologies philosophies organizations\"                  \n[5787] \"religion theologies philosophies may\"                            \n[5788] \"religion theologies ecclesiastical organizations\"                \n[5789] \"religion theologies ecclesiastical may\"                          \n[5790] \"religion theologies ecclesiastical secondarily\"                  \n[5791] \"religion theologies organizations may\"                           \n[5792] \"religion theologies organizations secondarily\"                   \n[5793] \"religion theologies organizations grow\"                          \n[5794] \"religion sense take theologies philosophies\"                     \n[5795] \"religion sense take theologies ecclesiastical\"                   \n[5796] \"religion sense take theologies organizations\"                    \n[5797] \"religion sense take philosophies ecclesiastical\"                 \n[5798] \"religion sense take philosophies organizations\"                  \n[5799] \"religion sense take philosophies may\"                            \n[5800] \"religion sense take ecclesiastical organizations\"                \n[5801] \"religion sense take ecclesiastical may\"                          \n[5802] \"religion sense take ecclesiastical secondarily\"                  \n[5803] \"religion sense theologies philosophies ecclesiastical\"           \n[5804] \"religion sense theologies philosophies organizations\"            \n[5805] \"religion sense theologies philosophies may\"                      \n[5806] \"religion sense theologies ecclesiastical organizations\"          \n[5807] \"religion sense theologies ecclesiastical may\"                    \n[5808] \"religion sense theologies ecclesiastical secondarily\"            \n[5809] \"religion sense theologies organizations may\"                     \n[5810] \"religion sense theologies organizations secondarily\"             \n[5811] \"religion sense theologies organizations grow\"                    \n[5812] \"religion sense philosophies ecclesiastical organizations\"        \n[5813] \"religion sense philosophies ecclesiastical may\"                  \n[5814] \"religion sense philosophies ecclesiastical secondarily\"          \n[5815] \"religion sense philosophies organizations may\"                   \n[5816] \"religion sense philosophies organizations secondarily\"           \n[5817] \"religion sense philosophies organizations grow\"                  \n[5818] \"religion sense philosophies may secondarily\"                     \n[5819] \"religion sense philosophies may grow\"                            \n[5820] \"religion take theologies philosophies ecclesiastical\"            \n[5821] \"religion take theologies philosophies organizations\"             \n[5822] \"religion take theologies philosophies may\"                       \n[5823] \"religion take theologies ecclesiastical organizations\"           \n[5824] \"religion take theologies ecclesiastical may\"                     \n[5825] \"religion take theologies ecclesiastical secondarily\"             \n[5826] \"religion take theologies organizations may\"                      \n[5827] \"religion take theologies organizations secondarily\"              \n[5828] \"religion take theologies organizations grow\"                     \n[5829] \"religion take philosophies ecclesiastical organizations\"         \n[5830] \"religion take philosophies ecclesiastical may\"                   \n[5831] \"religion take philosophies ecclesiastical secondarily\"           \n[5832] \"religion take philosophies organizations may\"                    \n[5833] \"religion take philosophies organizations secondarily\"            \n[5834] \"religion take philosophies organizations grow\"                   \n[5835] \"religion take philosophies may secondarily\"                      \n[5836] \"religion take philosophies may grow\"                             \n[5837] \"religion take ecclesiastical organizations may\"                  \n[5838] \"religion take ecclesiastical organizations secondarily\"          \n[5839] \"religion take ecclesiastical organizations grow\"                 \n[5840] \"religion take ecclesiastical may secondarily\"                    \n[5841] \"religion take ecclesiastical may grow\"                           \n[5842] \"religion take ecclesiastical secondarily grow\"                   \n[5843] \"religion theologies philosophies ecclesiastical organizations\"   \n[5844] \"religion theologies philosophies ecclesiastical may\"             \n[5845] \"religion theologies philosophies ecclesiastical secondarily\"     \n[5846] \"religion theologies philosophies organizations may\"              \n[5847] \"religion theologies philosophies organizations secondarily\"      \n[5848] \"religion theologies philosophies organizations grow\"             \n[5849] \"religion theologies philosophies may secondarily\"                \n[5850] \"religion theologies philosophies may grow\"                       \n[5851] \"religion theologies ecclesiastical organizations may\"            \n[5852] \"religion theologies ecclesiastical organizations secondarily\"    \n[5853] \"religion theologies ecclesiastical organizations grow\"           \n[5854] \"religion theologies ecclesiastical may secondarily\"              \n[5855] \"religion theologies ecclesiastical may grow\"                     \n[5856] \"religion theologies ecclesiastical secondarily grow\"             \n[5857] \"religion theologies organizations may secondarily\"               \n[5858] \"religion theologies organizations may grow\"                      \n[5859] \"religion theologies organizations secondarily grow\"              \n[5860] \"sense take\"                                                      \n[5861] \"sense theologies\"                                                \n[5862] \"sense philosophies\"                                              \n[5863] \"sense take theologies\"                                           \n[5864] \"sense take philosophies\"                                         \n[5865] \"sense take ecclesiastical\"                                       \n[5866] \"sense theologies philosophies\"                                   \n[5867] \"sense theologies ecclesiastical\"                                 \n[5868] \"sense theologies organizations\"                                  \n[5869] \"sense philosophies ecclesiastical\"                               \n[5870] \"sense philosophies organizations\"                                \n[5871] \"sense philosophies may\"                                          \n[5872] \"sense take theologies philosophies\"                              \n[5873] \"sense take theologies ecclesiastical\"                            \n[5874] \"sense take theologies organizations\"                             \n[5875] \"sense take philosophies ecclesiastical\"                          \n[5876] \"sense take philosophies organizations\"                           \n[5877] \"sense take philosophies may\"                                     \n[5878] \"sense take ecclesiastical organizations\"                         \n[5879] \"sense take ecclesiastical may\"                                   \n[5880] \"sense take ecclesiastical secondarily\"                           \n[5881] \"sense theologies philosophies ecclesiastical\"                    \n[5882] \"sense theologies philosophies organizations\"                     \n[5883] \"sense theologies philosophies may\"                               \n[5884] \"sense theologies ecclesiastical organizations\"                   \n[5885] \"sense theologies ecclesiastical may\"                             \n[5886] \"sense theologies ecclesiastical secondarily\"                     \n[5887] \"sense theologies organizations may\"                              \n[5888] \"sense theologies organizations secondarily\"                      \n[5889] \"sense theologies organizations grow\"                             \n[5890] \"sense philosophies ecclesiastical organizations\"                 \n[5891] \"sense philosophies ecclesiastical may\"                           \n[5892] \"sense philosophies ecclesiastical secondarily\"                   \n[5893] \"sense philosophies organizations may\"                            \n[5894] \"sense philosophies organizations secondarily\"                    \n[5895] \"sense philosophies organizations grow\"                           \n[5896] \"sense philosophies may secondarily\"                              \n[5897] \"sense philosophies may grow\"                                     \n[5898] \"sense take theologies philosophies ecclesiastical\"               \n[5899] \"sense take theologies philosophies organizations\"                \n[5900] \"sense take theologies philosophies may\"                          \n[5901] \"sense take theologies ecclesiastical organizations\"              \n[5902] \"sense take theologies ecclesiastical may\"                        \n[5903] \"sense take theologies ecclesiastical secondarily\"                \n[5904] \"sense take theologies organizations may\"                         \n[5905] \"sense take theologies organizations secondarily\"                 \n[5906] \"sense take theologies organizations grow\"                        \n[5907] \"sense take philosophies ecclesiastical organizations\"            \n[5908] \"sense take philosophies ecclesiastical may\"                      \n[5909] \"sense take philosophies ecclesiastical secondarily\"              \n[5910] \"sense take philosophies organizations may\"                       \n[5911] \"sense take philosophies organizations secondarily\"               \n[5912] \"sense take philosophies organizations grow\"                      \n[5913] \"sense take philosophies may secondarily\"                         \n[5914] \"sense take philosophies may grow\"                                \n[5915] \"sense take ecclesiastical organizations may\"                     \n[5916] \"sense take ecclesiastical organizations secondarily\"             \n[5917] \"sense take ecclesiastical organizations grow\"                    \n[5918] \"sense take ecclesiastical may secondarily\"                       \n[5919] \"sense take ecclesiastical may grow\"                              \n[5920] \"sense take ecclesiastical secondarily grow\"                      \n[5921] \"sense theologies philosophies ecclesiastical organizations\"      \n[5922] \"sense theologies philosophies ecclesiastical may\"                \n[5923] \"sense theologies philosophies ecclesiastical secondarily\"        \n[5924] \"sense theologies philosophies organizations may\"                 \n[5925] \"sense theologies philosophies organizations secondarily\"         \n[5926] \"sense theologies philosophies organizations grow\"                \n[5927] \"sense theologies philosophies may secondarily\"                   \n[5928] \"sense theologies philosophies may grow\"                          \n[5929] \"sense theologies ecclesiastical organizations may\"               \n[5930] \"sense theologies ecclesiastical organizations secondarily\"       \n[5931] \"sense theologies ecclesiastical organizations grow\"              \n[5932] \"sense theologies ecclesiastical may secondarily\"                 \n[5933] \"sense theologies ecclesiastical may grow\"                        \n[5934] \"sense theologies ecclesiastical secondarily grow\"                \n[5935] \"sense theologies organizations may secondarily\"                  \n[5936] \"sense theologies organizations may grow\"                         \n[5937] \"sense theologies organizations secondarily grow\"                 \n[5938] \"sense philosophies ecclesiastical organizations may\"             \n[5939] \"sense philosophies ecclesiastical organizations secondarily\"     \n[5940] \"sense philosophies ecclesiastical organizations grow\"            \n[5941] \"sense philosophies ecclesiastical may secondarily\"               \n[5942] \"sense philosophies ecclesiastical may grow\"                      \n[5943] \"sense philosophies ecclesiastical secondarily grow\"              \n[5944] \"sense philosophies organizations may secondarily\"                \n[5945] \"sense philosophies organizations may grow\"                       \n[5946] \"sense philosophies organizations secondarily grow\"               \n[5947] \"sense philosophies may secondarily grow\"                         \n[5948] \"take theologies\"                                                 \n[5949] \"take philosophies\"                                               \n[5950] \"take ecclesiastical\"                                             \n[5951] \"take theologies philosophies\"                                    \n[5952] \"take theologies ecclesiastical\"                                  \n[5953] \"take theologies organizations\"                                   \n[5954] \"take philosophies ecclesiastical\"                                \n[5955] \"take philosophies organizations\"                                 \n[5956] \"take philosophies may\"                                           \n[5957] \"take ecclesiastical organizations\"                               \n[5958] \"take ecclesiastical may\"                                         \n[5959] \"take ecclesiastical secondarily\"                                 \n[5960] \"take theologies philosophies ecclesiastical\"                     \n[5961] \"take theologies philosophies organizations\"                      \n[5962] \"take theologies philosophies may\"                                \n[5963] \"take theologies ecclesiastical organizations\"                    \n[5964] \"take theologies ecclesiastical may\"                              \n[5965] \"take theologies ecclesiastical secondarily\"                      \n[5966] \"take theologies organizations may\"                               \n[5967] \"take theologies organizations secondarily\"                       \n[5968] \"take theologies organizations grow\"                              \n[5969] \"take philosophies ecclesiastical organizations\"                  \n[5970] \"take philosophies ecclesiastical may\"                            \n[5971] \"take philosophies ecclesiastical secondarily\"                    \n[5972] \"take philosophies organizations may\"                             \n[5973] \"take philosophies organizations secondarily\"                     \n[5974] \"take philosophies organizations grow\"                            \n[5975] \"take philosophies may secondarily\"                               \n[5976] \"take philosophies may grow\"                                      \n[5977] \"take ecclesiastical organizations may\"                           \n[5978] \"take ecclesiastical organizations secondarily\"                   \n[5979] \"take ecclesiastical organizations grow\"                          \n[5980] \"take ecclesiastical may secondarily\"                             \n[5981] \"take ecclesiastical may grow\"                                    \n[5982] \"take ecclesiastical secondarily grow\"                            \n[5983] \"take theologies philosophies ecclesiastical organizations\"       \n[5984] \"take theologies philosophies ecclesiastical may\"                 \n[5985] \"take theologies philosophies ecclesiastical secondarily\"         \n[5986] \"take theologies philosophies organizations may\"                  \n[5987] \"take theologies philosophies organizations secondarily\"          \n[5988] \"take theologies philosophies organizations grow\"                 \n[5989] \"take theologies philosophies may secondarily\"                    \n[5990] \"take theologies philosophies may grow\"                           \n[5991] \"take theologies ecclesiastical organizations may\"                \n[5992] \"take theologies ecclesiastical organizations secondarily\"        \n[5993] \"take theologies ecclesiastical organizations grow\"               \n[5994] \"take theologies ecclesiastical may secondarily\"                  \n[5995] \"take theologies ecclesiastical may grow\"                         \n[5996] \"take theologies ecclesiastical secondarily grow\"                 \n[5997] \"take theologies organizations may secondarily\"                   \n[5998] \"take theologies organizations may grow\"                          \n[5999] \"take theologies organizations secondarily grow\"                  \n[6000] \"take philosophies ecclesiastical organizations may\"              \n[6001] \"take philosophies ecclesiastical organizations secondarily\"      \n[6002] \"take philosophies ecclesiastical organizations grow\"             \n[6003] \"take philosophies ecclesiastical may secondarily\"                \n[6004] \"take philosophies ecclesiastical may grow\"                       \n[6005] \"take philosophies ecclesiastical secondarily grow\"               \n[6006] \"take philosophies organizations may secondarily\"                 \n[6007] \"take philosophies organizations may grow\"                        \n[6008] \"take philosophies organizations secondarily grow\"                \n[6009] \"take philosophies may secondarily grow\"                          \n[6010] \"take ecclesiastical organizations may secondarily\"               \n[6011] \"take ecclesiastical organizations may grow\"                      \n[6012] \"take ecclesiastical organizations secondarily grow\"              \n[6013] \"take ecclesiastical may secondarily grow\"                        \n[6014] \"theologies philosophies\"                                         \n[6015] \"theologies ecclesiastical\"                                       \n[6016] \"theologies organizations\"                                        \n[6017] \"theologies philosophies ecclesiastical\"                          \n[6018] \"theologies philosophies organizations\"                           \n[6019] \"theologies philosophies may\"                                     \n[6020] \"theologies ecclesiastical organizations\"                         \n[6021] \"theologies ecclesiastical may\"                                   \n[6022] \"theologies ecclesiastical secondarily\"                           \n[6023] \"theologies organizations may\"                                    \n[6024] \"theologies organizations secondarily\"                            \n[6025] \"theologies organizations grow\"                                   \n[6026] \"theologies philosophies ecclesiastical organizations\"            \n[6027] \"theologies philosophies ecclesiastical may\"                      \n[6028] \"theologies philosophies ecclesiastical secondarily\"              \n[6029] \"theologies philosophies organizations may\"                       \n[6030] \"theologies philosophies organizations secondarily\"               \n[6031] \"theologies philosophies organizations grow\"                      \n[6032] \"theologies philosophies may secondarily\"                         \n[6033] \"theologies philosophies may grow\"                                \n[6034] \"theologies ecclesiastical organizations may\"                     \n[6035] \"theologies ecclesiastical organizations secondarily\"             \n[6036] \"theologies ecclesiastical organizations grow\"                    \n[6037] \"theologies ecclesiastical may secondarily\"                       \n[6038] \"theologies ecclesiastical may grow\"                              \n[6039] \"theologies ecclesiastical secondarily grow\"                      \n[6040] \"theologies organizations may secondarily\"                        \n[6041] \"theologies organizations may grow\"                               \n[6042] \"theologies organizations secondarily grow\"                       \n[6043] \"theologies philosophies ecclesiastical organizations may\"        \n[6044] \"theologies philosophies ecclesiastical organizations secondarily\"\n[6045] \"theologies philosophies ecclesiastical organizations grow\"       \n[6046] \"theologies philosophies ecclesiastical may secondarily\"          \n[6047] \"theologies philosophies ecclesiastical may grow\"                 \n[6048] \"theologies philosophies ecclesiastical secondarily grow\"         \n[6049] \"theologies philosophies organizations may secondarily\"           \n[6050] \"theologies philosophies organizations may grow\"                  \n[6051] \"theologies philosophies organizations secondarily grow\"          \n[6052] \"theologies philosophies may secondarily grow\"                    \n[6053] \"theologies ecclesiastical organizations may secondarily\"         \n[6054] \"theologies ecclesiastical organizations may grow\"                \n[6055] \"theologies ecclesiastical organizations secondarily grow\"        \n[6056] \"theologies ecclesiastical may secondarily grow\"                  \n[6057] \"theologies organizations may secondarily grow\"                   \n[6058] \"philosophies ecclesiastical\"                                     \n[6059] \"philosophies organizations\"                                      \n[6060] \"philosophies may\"                                                \n[6061] \"philosophies ecclesiastical organizations\"                       \n[6062] \"philosophies ecclesiastical may\"                                 \n[6063] \"philosophies ecclesiastical secondarily\"                         \n[6064] \"philosophies organizations may\"                                  \n[6065] \"philosophies organizations secondarily\"                          \n[6066] \"philosophies organizations grow\"                                 \n[6067] \"philosophies may secondarily\"                                    \n[6068] \"philosophies may grow\"                                           \n[6069] \"philosophies ecclesiastical organizations may\"                   \n[6070] \"philosophies ecclesiastical organizations secondarily\"           \n[6071] \"philosophies ecclesiastical organizations grow\"                  \n[6072] \"philosophies ecclesiastical may secondarily\"                     \n[6073] \"philosophies ecclesiastical may grow\"                            \n[6074] \"philosophies ecclesiastical secondarily grow\"                    \n[6075] \"philosophies organizations may secondarily\"                      \n[6076] \"philosophies organizations may grow\"                             \n[6077] \"philosophies organizations secondarily grow\"                     \n[6078] \"philosophies may secondarily grow\"                               \n[6079] \"philosophies ecclesiastical organizations may secondarily\"       \n[6080] \"philosophies ecclesiastical organizations may grow\"              \n[6081] \"philosophies ecclesiastical organizations secondarily grow\"      \n[6082] \"philosophies ecclesiastical may secondarily grow\"                \n[6083] \"philosophies organizations may secondarily grow\"                 \n[6084] \"ecclesiastical organizations\"                                    \n[6085] \"ecclesiastical may\"                                              \n[6086] \"ecclesiastical secondarily\"                                      \n[6087] \"ecclesiastical organizations may\"                                \n[6088] \"ecclesiastical organizations secondarily\"                        \n[6089] \"ecclesiastical organizations grow\"                               \n[6090] \"ecclesiastical may secondarily\"                                  \n[6091] \"ecclesiastical may grow\"                                         \n[6092] \"ecclesiastical secondarily grow\"                                 \n[6093] \"ecclesiastical organizations may secondarily\"                    \n[6094] \"ecclesiastical organizations may grow\"                           \n[6095] \"ecclesiastical organizations secondarily grow\"                   \n[6096] \"ecclesiastical may secondarily grow\"                             \n[6097] \"ecclesiastical organizations may secondarily grow\"               \n[6098] \"organizations may\"                                               \n[6099] \"organizations secondarily\"                                       \n[6100] \"organizations grow\"                                              \n[6101] \"organizations may secondarily\"                                   \n[6102] \"organizations may grow\"                                          \n[6103] \"organizations secondarily grow\"                                  \n[6104] \"organizations may secondarily grow\"                              \n[6105] \"may secondarily\"                                                 \n[6106] \"may grow\"                                                        \n[6107] \"may secondarily grow\"                                            \n[6108] \"secondarily grow\""
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#sentence-and-paragraph-tokenizers",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#sentence-and-paragraph-tokenizers",
    "title": "Introduction to the tokenizers Package",
    "section": "Sentence and paragraph tokenizers",
    "text": "Sentence and paragraph tokenizers\nSometimes it is desirable to split texts into sentences or paragraphs prior to tokenizing into other forms.\n\ntokenize_sentences(james) \n\n[[1]]\n[1] \"The question thus becomes a verbal one again; and our knowledge of all these early stages of thought and feeling is in any case so conjectural and imperfect that farther discussion would not be worth while.\"                                               \n[2] \"Religion, therefore, as I now ask you arbitrarily to take it, shall mean for us _the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine_.\"\n[3] \"Since the relation may be either moral, physical, or ritual, it is evident that out of religion in the sense in which we take it, theologies, philosophies, and ecclesiastical organizations may secondarily grow.\"                                           \n\ntokenize_paragraphs(james)\n\n[[1]]\n[1] \"The question thus becomes a verbal one again; and our knowledge of all these early stages of thought and feeling is in any case so conjectural and imperfect that farther discussion would not be worth while.\"                                                                                                                                                                                                                                                                   \n[2] \"Religion, therefore, as I now ask you arbitrarily to take it, shall mean for us _the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine_. Since the relation may be either moral, physical, or ritual, it is evident that out of religion in the sense in which we take it, theologies, philosophies, and ecclesiastical organizations may secondarily grow. \""
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#text-chunking",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#text-chunking",
    "title": "Introduction to the tokenizers Package",
    "section": "Text chunking",
    "text": "Text chunking\nWhen one has a very long document, sometimes it is desirable to split the document into smaller chunks, each with the same length. This function chunks a document and gives it each of the chunks an ID to show their order. These chunks can then be further tokenized.\n\nchunks &lt;- chunk_text(mobydick, chunk_size = 100, doc_id = \"mobydick\")\nlength(chunks)\n\n[1] 2195\n\nchunks[5:6]\n\n$`mobydick-0005`\n[1] \"of a poor devil of a sub sub appears to have gone through the long vaticans and street stalls of the earth picking up whatever random allusions to whales he could anyways find in any book whatsoever sacred or profane therefore you must not in every case at least take the higgledy piggledy whale statements however authentic in these extracts for veritable gospel cetology far from it as touching the ancient authors generally as well as the poets here appearing these extracts are solely valuable or entertaining as affording a glancing bird's eye view of what has been promiscuously said\"\n\n$`mobydick-0006`\n[1] \"thought fancied and sung of leviathan by many nations and generations including our own so fare thee well poor devil of a sub sub whose commentator i am thou belongest to that hopeless sallow tribe which no wine of this world will ever warm and for whom even pale sherry would be too rosy strong but with whom one sometimes loves to sit and feel poor devilish too and grow convivial upon tears and say to them bluntly with full eyes and empty glasses and in not altogether unpleasant sadness give it up sub subs for by how much the\"\n\ntokenize_words(chunks[5:6])\n\n$`mobydick-0005`\n  [1] \"of\"            \"a\"             \"poor\"          \"devil\"        \n  [5] \"of\"            \"a\"             \"sub\"           \"sub\"          \n  [9] \"appears\"       \"to\"            \"have\"          \"gone\"         \n [13] \"through\"       \"the\"           \"long\"          \"vaticans\"     \n [17] \"and\"           \"street\"        \"stalls\"        \"of\"           \n [21] \"the\"           \"earth\"         \"picking\"       \"up\"           \n [25] \"whatever\"      \"random\"        \"allusions\"     \"to\"           \n [29] \"whales\"        \"he\"            \"could\"         \"anyways\"      \n [33] \"find\"          \"in\"            \"any\"           \"book\"         \n [37] \"whatsoever\"    \"sacred\"        \"or\"            \"profane\"      \n [41] \"therefore\"     \"you\"           \"must\"          \"not\"          \n [45] \"in\"            \"every\"         \"case\"          \"at\"           \n [49] \"least\"         \"take\"          \"the\"           \"higgledy\"     \n [53] \"piggledy\"      \"whale\"         \"statements\"    \"however\"      \n [57] \"authentic\"     \"in\"            \"these\"         \"extracts\"     \n [61] \"for\"           \"veritable\"     \"gospel\"        \"cetology\"     \n [65] \"far\"           \"from\"          \"it\"            \"as\"           \n [69] \"touching\"      \"the\"           \"ancient\"       \"authors\"      \n [73] \"generally\"     \"as\"            \"well\"          \"as\"           \n [77] \"the\"           \"poets\"         \"here\"          \"appearing\"    \n [81] \"these\"         \"extracts\"      \"are\"           \"solely\"       \n [85] \"valuable\"      \"or\"            \"entertaining\"  \"as\"           \n [89] \"affording\"     \"a\"             \"glancing\"      \"bird's\"       \n [93] \"eye\"           \"view\"          \"of\"            \"what\"         \n [97] \"has\"           \"been\"          \"promiscuously\" \"said\"         \n\n$`mobydick-0006`\n  [1] \"thought\"     \"fancied\"     \"and\"         \"sung\"        \"of\"         \n  [6] \"leviathan\"   \"by\"          \"many\"        \"nations\"     \"and\"        \n [11] \"generations\" \"including\"   \"our\"         \"own\"         \"so\"         \n [16] \"fare\"        \"thee\"        \"well\"        \"poor\"        \"devil\"      \n [21] \"of\"          \"a\"           \"sub\"         \"sub\"         \"whose\"      \n [26] \"commentator\" \"i\"           \"am\"          \"thou\"        \"belongest\"  \n [31] \"to\"          \"that\"        \"hopeless\"    \"sallow\"      \"tribe\"      \n [36] \"which\"       \"no\"          \"wine\"        \"of\"          \"this\"       \n [41] \"world\"       \"will\"        \"ever\"        \"warm\"        \"and\"        \n [46] \"for\"         \"whom\"        \"even\"        \"pale\"        \"sherry\"     \n [51] \"would\"       \"be\"          \"too\"         \"rosy\"        \"strong\"     \n [56] \"but\"         \"with\"        \"whom\"        \"one\"         \"sometimes\"  \n [61] \"loves\"       \"to\"          \"sit\"         \"and\"         \"feel\"       \n [66] \"poor\"        \"devilish\"    \"too\"         \"and\"         \"grow\"       \n [71] \"convivial\"   \"upon\"        \"tears\"       \"and\"         \"say\"        \n [76] \"to\"          \"them\"        \"bluntly\"     \"with\"        \"full\"       \n [81] \"eyes\"        \"and\"         \"empty\"       \"glasses\"     \"and\"        \n [86] \"in\"          \"not\"         \"altogether\"  \"unpleasant\"  \"sadness\"    \n [91] \"give\"        \"it\"          \"up\"          \"sub\"         \"subs\"       \n [96] \"for\"         \"by\"          \"how\"         \"much\"        \"the\""
  },
  {
    "objectID": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#counting-words-characters-sentences",
    "href": "R/Library/tokenizers/doc/introduction-to-tokenizers.html#counting-words-characters-sentences",
    "title": "Introduction to the tokenizers Package",
    "section": "Counting words, characters, sentences",
    "text": "Counting words, characters, sentences\nThe package also offers functions for counting words, characters, and sentences in a format which works nicely with the rest of the functions.\n\ncount_words(mobydick)\n\nmobydick \n  219415 \n\ncount_characters(mobydick)\n\nmobydick \n 1235185 \n\ncount_sentences(mobydick)\n\nmobydick \n   29076"
  },
  {
    "objectID": "R/Library/tokenizers/NEWS.html",
    "href": "R/Library/tokenizers/NEWS.html",
    "title": "tokenizers 0.3.0",
    "section": "",
    "text": "Remove the tokenize_tweets() function, which is no longer supported."
  },
  {
    "objectID": "R/Library/tokenizers/NEWS.html#features",
    "href": "R/Library/tokenizers/NEWS.html#features",
    "title": "tokenizers 0.3.0",
    "section": "Features",
    "text": "Features\n\nAdd the tokenize_ptb() function for Penn Treebank tokenizations ((jrnold?)) (#12).\nAdd a function chunk_text() to split long documents into pieces (#30).\nNew functions to count words, characters, and sentences without tokenization (#36).\nNew function tokenize_tweets() preserves usernames, hashtags, and URLS ((kbenoit?)) (#44).\nThe stopwords() function has been removed in favor of using the stopwords package (#46).\nThe package now complies with the basic recommendations of the Text Interchange Format. All tokenization functions are now methods. This enables them to take corpus inputs as either TIF-compliant named character vectors, named lists, or data frames. All outputs are still named lists of tokens, but these can be easily coerced to data frames of tokens using the tif package. (#49)\nAdd a new vignette “The Text Interchange Formats and the tokenizers Package” (#49)."
  },
  {
    "objectID": "R/Library/tokenizers/NEWS.html#bug-fixes-and-performance-improvements",
    "href": "R/Library/tokenizers/NEWS.html#bug-fixes-and-performance-improvements",
    "title": "tokenizers 0.3.0",
    "section": "Bug fixes and performance improvements",
    "text": "Bug fixes and performance improvements\n\ntokenize_skip_ngrams has been improved to generate unigrams and bigrams, according to the skip definition (#24).\nC++98 has replaced the C++11 code used for n-gram generation, widening the range of compilers tokenizers supports ((ironholds?)) (#26).\ntokenize_skip_ngrams now supports stopwords (#31).\nIf tokenisers fail to generate tokens for a particular entry, they return NA consistently (#33).\nKeyboard interrupt checks have been added to Rcpp-backed functions to enable users to terminate them before completion (#37).\ntokenize_words() gains arguments to preserve or strip punctuation and numbers (#48).\ntokenize_skip_ngrams() and tokenize_ngrams() to return properly marked UTF8 strings on Windows ((patperry?)) (#58).\ntokenize_tweets() now removes stopwords prior to stripping punctuation, making its behavior more consistent with tokenize_words() (#76)."
  },
  {
    "objectID": "instructors.html#materials",
    "href": "instructors.html#materials",
    "title": "Instructors",
    "section": "Materials",
    "text": "Materials\n\nSlide decks\n\n\nExercises"
  },
  {
    "objectID": "R/Library/tokenizers/doc/tif-and-tokenizers.html",
    "href": "R/Library/tokenizers/doc/tif-and-tokenizers.html",
    "title": "The Text Interchange Formats and the tokenizers Package",
    "section": "",
    "text": "The Text Interchange Formats are a set of standards defined at an rOpenSci sponsored meeting in London in 2017. The formats allow R text analysis packages to target defined inputs and outputs for corpora, tokens, and document-term matrices. By adhering to these recommendations, R packages can buy into an interoperable ecosystem.\nThe TIF recommendations are still a draft, but the tokenizers package implements its recommendation to accept both of the corpora formats and to output one of its recommended tokens formats.\nConsider these two recommended forms of a corpus. One (corpus_c) is a named character vector; the other (corpus_d) is a data frame. They both include a document ID and the full text for each item. The data frame format obviously allows for the use of other metadata fields besides the document ID, whereas the other format does not. Using the coercion functions in the tif package, one could switch back and forth between these formats. Tokenizers also supports a corpus formatted as a named list where each element is a character vector of length one (corpus_l), though this is not a part of the draft TIF standards.\n\n# Named list\n(corpus_l &lt;- list(man_comes_around = \"There's a man goin' 'round takin' names\",\n                  wont_back_down = \"Well I won't back down, no I won't back down\",\n                  bird_on_a_wire = \"Like a bird on a wire\"))\n#&gt; $man_comes_around\n#&gt; [1] \"There's a man goin' 'round takin' names\"\n#&gt; \n#&gt; $wont_back_down\n#&gt; [1] \"Well I won't back down, no I won't back down\"\n#&gt; \n#&gt; $bird_on_a_wire\n#&gt; [1] \"Like a bird on a wire\"\n\n# Named character vector\n(corpus_c &lt;- unlist(corpus_l))\n#&gt;                               man_comes_around \n#&gt;      \"There's a man goin' 'round takin' names\" \n#&gt;                                 wont_back_down \n#&gt; \"Well I won't back down, no I won't back down\" \n#&gt;                                 bird_on_a_wire \n#&gt;                        \"Like a bird on a wire\"\n\n# Data frame\n(corpus_d &lt;- data.frame(doc_id = names(corpus_c), text = unname(corpus_c),\n                        stringsAsFactors = FALSE))\n#&gt;             doc_id                                         text\n#&gt; 1 man_comes_around      There's a man goin' 'round takin' names\n#&gt; 2   wont_back_down Well I won't back down, no I won't back down\n#&gt; 3   bird_on_a_wire                        Like a bird on a wire\n\nAll of the tokenizers in this package can accept any of those formats and will return an identical output for each.\n\nlibrary(tokenizers)\n\ntokens_l &lt;- tokenize_ngrams(corpus_l, n = 2)\ntokens_c &lt;- tokenize_ngrams(corpus_c, n = 2)\ntokens_d &lt;- tokenize_ngrams(corpus_c, n = 2)\n\n# Are all these identical?\nall(identical(tokens_l, tokens_c),\n    identical(tokens_c, tokens_d),\n    identical(tokens_l, tokens_d))\n#&gt; [1] TRUE\n\nThe output of all of the tokenizers is a named list, where each element of the list corresponds to a document in the corpus. The names of the list are the document IDs, and the elements are character vectors containing the tokens.\n\ntokens_l\n#&gt; $man_comes_around\n#&gt; [1] \"there's a\"   \"a man\"       \"man goin\"    \"goin round\"  \"round takin\"\n#&gt; [6] \"takin names\"\n#&gt; \n#&gt; $wont_back_down\n#&gt; [1] \"well i\"     \"i won't\"    \"won't back\" \"back down\"  \"down no\"   \n#&gt; [6] \"no i\"       \"i won't\"    \"won't back\" \"back down\" \n#&gt; \n#&gt; $bird_on_a_wire\n#&gt; [1] \"like a\"  \"a bird\"  \"bird on\" \"on a\"    \"a wire\"\n\nThis format can be coerced to a data frame of document IDs and tokens, one row per token, using the coercion functions in the tif package. That tokens data frame would look like this.\n\n#&gt;              doc_id       token\n#&gt; 1  man_comes_around   there's a\n#&gt; 2  man_comes_around       a man\n#&gt; 3  man_comes_around    man goin\n#&gt; 4  man_comes_around  goin round\n#&gt; 5  man_comes_around round takin\n#&gt; 6  man_comes_around takin names\n#&gt; 7    wont_back_down      well i\n#&gt; 8    wont_back_down     i won't\n#&gt; 9    wont_back_down  won't back\n#&gt; 10   wont_back_down   back down"
  },
  {
    "objectID": "R/Library/qtalrkit/NEWS.html",
    "href": "R/Library/qtalrkit/NEWS.html",
    "title": "qtalrkit 0.9.4",
    "section": "",
    "text": "qtalrkit 0.9.4\n\nA fix to get_compressed_data() to avoid the infinite loop that was caused by the confirmation prompt.\n\n\n\nqtalrkit 0.9.3\n\nupdates create_data_origin() to use base R. It also adds functionality to optionally return the data origin as a data frame and to overwrite the file if it already exists.\n\n\n\nqtalrkit 0.9.2\n\nAdds curate_enntt_data() function to curate data from the Europarl Corpus of Native, Non-Native, and Translated Text (ENNTT).\nUpdates recipes\n\n\n\nqtalrkit 0.9.1\n\nChanges mirror to “https://gutenberg.pglaf.org/” for get_gutenberg_works() function\nOrganizes the output to include the LCC classification as part of the data written to disk for the get_gutenberg_works() function\n\n\n\nqtalrkit 0.9.0\n\nUpdated version number to reflect that the package is now in beta\nAdded curate_swda_data() function to curate data from the Switchboard Dialog Act Corpus\n\n\n\nqtalrkit 0.0.4.0\n\nAdds get_gutenberg_works() function to import data from Project Gutenberg\n\n\n\nqtalrkit 0.0.3.400\n\nFixes warnings on calc_assoc_metrics()\nUpdates Date on DESCRIPTION file\nAdds test-add_pkg_to_bib.R\n\n\n\nqtalrkit 0.0.3.000\n\nAdds calc_assoc_metrics to calculate (pmi, dice, G) for a given type bigram\n\n\n\nqtalrkit 0.0.3.210\n\nRemoves calc_dispersion_metrics() function and replaces it with calc_type_metric() which includes frequency and dispersion metrics.\n\n\n\nqtalrkit 0.0.3.200\n\nFixes bug in get_compressed_data() that caused the function to create dot file copies of the original files\n\n\n\nqtalrkit 0.0.3.100\n\nAdds idf measure to calc_dispersion_metrics()\n\n\n\nqtalrkit 0.0.3.000\n\nAdds calc_dispersion_metrics() function to calculate dispersion metrics\n\n\n\nqtalrkit 0.0.2.000\n\nAdded get_talkbank_data() function to import data from TalkBank\nAdded internal confirm_permissions() function to confirm that users are aware of the permissions required to use data\nUpdated get_*() functions to use confirm_permissions() internally\nChanged get_outliers() to find_outliers() to be more consistent with other functions\n\n\n\nqtalrkit 0.0.1.9400\n\nUpdated create_data_dictionary() to provide default scaffold structure for data dictionary, in lieu of OpenAI model. This scaffold is to be updated manually by the user.\n\n\n\nqtalrkit 0.0.1.9300\n\nAdded create_data_origin() function. This creates a .csv file to scaffold a data origin file\n\n\n\nqtalrkit 0.0.1.9200\n\nAdds project template for RStudio: “Minimal Reproducible Project”\n\n\n\nqtalrkit 0.0.1.9100\n\nAdjusted create_data_dictionary() to produce results more in line with the QTALR textbook\n\n\n\nqtalrkit 0.0.1.9000\n\nAdded get_outliers() function\nAdded Instructor Guide\n\n\n\nqtalrkit 0.0.1.0000\n\nAdded R tutorial 0\n\n\n\nqtalrkit 0.0.0.9000\n\nAdded a NEWS.md file to track changes to the package."
  },
  {
    "objectID": "R/Library/here/demo-project/analysis/report.html",
    "href": "R/Library/here/demo-project/analysis/report.html",
    "title": "Palmer penguins",
    "section": "",
    "text": "lm(flipper_length_mm ~ body_mass_g + species, penguins)\n\n\nCall:\nlm(formula = flipper_length_mm ~ body_mass_g + species, data = penguins)\n\nCoefficients:\n     (Intercept)       body_mass_g  speciesChinstrap     speciesGentoo  \n       1.589e+02         8.402e-03         5.597e+00         1.568e+01"
  },
  {
    "objectID": "R/Library/here/demo-project/analysis/report.html#linear-model",
    "href": "R/Library/here/demo-project/analysis/report.html#linear-model",
    "title": "Palmer penguins",
    "section": "",
    "text": "lm(flipper_length_mm ~ body_mass_g + species, penguins)\n\n\nCall:\nlm(formula = flipper_length_mm ~ body_mass_g + species, data = penguins)\n\nCoefficients:\n     (Intercept)       body_mass_g  speciesChinstrap     speciesGentoo  \n       1.589e+02         8.402e-03         5.597e+00         1.568e+01"
  },
  {
    "objectID": "R/Library/here/doc/here.html",
    "href": "R/Library/here/doc/here.html",
    "title": "here",
    "section": "",
    "text": "The here package enables easy file referencing by using the top-level directory of a file project to easily build file paths. This is in contrast to using setwd(), which is fragile and dependent on the way you order your files on your computer. Read more about project-oriented workflows:"
  },
  {
    "objectID": "R/Library/here/doc/here.html#basic-functionality",
    "href": "R/Library/here/doc/here.html#basic-functionality",
    "title": "here",
    "section": "Basic functionality",
    "text": "Basic functionality\nFor demonstration, this article uses a data analysis project that lives in /Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project on my machine. This is the project root. The path will most likely be different on your machine, the here package helps deal with this situation.\nThe project has the following structure:\n\n#&gt; /Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\n#&gt; ├── analysis\n#&gt; │   ├── report.Rmd\n#&gt; │   ├── report.html\n#&gt; │   └── report_cache\n#&gt; │       └── html\n#&gt; │           ├── __packages\n#&gt; │           ├── setup_0c1d92767c6b3fa18b692c9308b5a9ee.RData\n#&gt; │           ├── setup_0c1d92767c6b3fa18b692c9308b5a9ee.rdb\n#&gt; │           ├── setup_0c1d92767c6b3fa18b692c9308b5a9ee.rdx\n#&gt; │           ├── unnamed-chunk-1_54733d1c24c19e45f2d9528df7ddd3cd.RData\n#&gt; │           ├── unnamed-chunk-1_54733d1c24c19e45f2d9528df7ddd3cd.rdb\n#&gt; │           └── unnamed-chunk-1_54733d1c24c19e45f2d9528df7ddd3cd.rdx\n#&gt; ├── data\n#&gt; │   └── penguins.csv\n#&gt; ├── demo-project.Rproj\n#&gt; └── prepare\n#&gt;     └── penguins.R\n\nYou can review the project on GitHub and also download a copy.\nTo start working on this project in RStudio, open the demo-project.Rproj file. This ensures that the working directory is set to /Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project, the project root. Opening only the .R or the .Rmd file may be insufficient!\nOther development environments may have a different notion of a project. Either way, it is important that the working directory is set to the project root or a subdirectory of that path. You can check with:\n\nsetwd(project_path)\n\n\ngetwd()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\n\n(See vignette(\"rmarkdown\") for an example where the working directory is set to a subdirectory on start.)\n\nDeclare the location of the current script\nThe intended use is to add a call to here::i_am() at the beginning of your script or in the first chunk of your rmarkdown report.1 This achieves the following:\n\nThe location of the current script or report within the project is declared\nThe project root is initialized, consistent with the location of the current script or report\nAn informative message is emitted.2\n\nThe first argument to here::i_am() should be the path to the current file, relative to the project root. The penguins.R script uses:\n\nhere::i_am(\"prepare/penguins.R\")\n\nhere::i_am() displays the top-level directory of the current project. Because the project has a prepare/ directory in its root that contains penguins.R, it is correctly inferred as the project root.\nAfter here::i_am(), insert library(here) to make the here() function available:3\n\nlibrary(here)\n\nThe top-level directory is also returned from the here() function:\n\nhere()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\n\nOne important distinction from the working directory is that this remains stable even if the working directory is changed:\n\nsetwd(\"analysis\")\ngetwd()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/analysis\"\nhere()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\nsetwd(\"..\")\n\n(I suggest to steer clear from ever changing the working directory. This may not always be feasible, in particular if the working directory is changed by code that you do not control.)\n\n\nUse project-relative paths\nYou can build a path relative to the top-level directory in order to build the full path to a file:\n\nhere(\"data\", \"penguins.csv\")\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/data/penguins.csv\"\nreadr::read_csv(\n  here(\"data\", \"penguins.csv\"),\n  col_types = list(.default = readr::col_guess()),\n  n_max = 3\n)\n#&gt; # A tibble: 3 × 8\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt; 3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt; # ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\nThis works regardless of where the associated source file lives inside your project. With here(), the path will always be relative to the top-level project directory.\nhere() works very similarly to file.path() or fs::path(), you can pass path components or entire subpaths:\n\nhere(\"data/penguins.csv\")\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/data/penguins.csv\"\n\nAs seen above, here() returns absolute paths (starting with /, &lt;drive letter&gt;:\\ or \\\\). This makes it safe to pass these paths to other functions, even if the working directory is changed along the way.\nAs of version 1.0.0, absolute paths passed to here() are returned unchanged. This means that you can safely use both absolute and project-relative paths in here().\n\ndata_path &lt;- here(\"data\")\nhere(data_path)\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/data\"\nhere(data_path, \"penguins.csv\")\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project/data/penguins.csv\"\n\n\n\nSituation report\nThe dr_here() function explains the reasoning behind choosing the project root:\n\ndr_here()\n\nThe show_reason argument can be set to FALSE to reduce the output to one line:\n\ndr_here(show_reason = FALSE)\n\n\n\nWhat if the working directory is wrong?\nThe declaration of the active file via here::i_am() also protects against accidentally running the script from a working directory outside of your project. The example below calls here::i_am() from the temporary directory, which is clearly outside our project:\n\nwithr::with_dir(tempdir(), {\n  print(getwd())\n  here::i_am(\"prepare/penguins.R\")\n})\n#&gt; [1] \"/private/var/folders/gm/psh723xs3s50nqry92xw3_l80000gp/T/RtmpgdwJcI\"\n#&gt; Error: Could not find associated project in working directory or any parent directory.\n#&gt; - Path in project: prepare/penguins.R\n#&gt; - Current working directory: /private/var/folders/gm/psh723xs3s50nqry92xw3_l80000gp/T/RtmpgdwJcI\n#&gt; Please open the project associated with this file and try again.\n\nThis can also happen when a file has been renamed or moved without updating the here::i_am() call. In the future, a helper function will assist with installing and updating suitably formatted here::i_am() calls in your scripts and reports."
  },
  {
    "objectID": "R/Library/here/doc/here.html#extra-safety",
    "href": "R/Library/here/doc/here.html#extra-safety",
    "title": "here",
    "section": "Extra safety",
    "text": "Extra safety\n\nConflicts with other packages\nOther packages also export a here() function. Loading these packages after loading here masks our here() function:\n\nlibrary(plyr)\n#&gt; Error in library(plyr): there is no package called 'plyr'\nhere()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\n\nOne way to work around this problem is to use here::here():\n\nhere::here()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\n\nThe conflicted package offers an alternative: it detects that here() is exported from more than one package and allows you to use neither until you indicate a preference.\n\nlibrary(conflicted)\nhere()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\n\nconflicted::conflict_prefer(\"here\", \"here\")\nhere()\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\n\n\n\nSpecify a unique identifier\nTo eliminate potential confusion, here::i_am() accepts a uuid argument. The idea is that each script and report calls here::i_am() very early (in the first 100 lines) with a universally unique identifier. Even if a file location is reused across projects (e.g. two projects contain a “prepare/data.R” file), the files can be identified correctly if the uuid argument in the here::i_am() call is different.\nIf a uuid argument is passed to here::i_am():\n\na project root that contains a file at the specified file path is searched\nthe first 100 lines of that file are read\nfor the correct file, the here::i_am() call that passes this very uuid is among those 100 lines, and will be matched\nfor the wrong file, uuid is not found in the text\n\nUse uuid::UUIDgenerate() to create universally unique identifiers:\n\nuuid::UUIDgenerate()\n#&gt; [1] \"ef09bc5c-847e-41af-bb9f-a8cc4fe947d1\"\n\nEnsure that the uuid arguments are actually unique across your files! In the future, a helper function will assist with installing and updating suitably formatted here::i_am() calls in your scripts and reports."
  },
  {
    "objectID": "R/Library/here/doc/here.html#beyond-here",
    "href": "R/Library/here/doc/here.html#beyond-here",
    "title": "here",
    "section": "Beyond here",
    "text": "Beyond here\n\nChange project root\nIt is advisable to start a fresh R session as often as possible, especially before focusing on another project. There still may be legitimate cases when it is desirable to reset the project root.\nTo start, let’s create a temporary project for demonstration:\n\ntemp_project_path &lt;- tempfile()\ndir.create(temp_project_path)\nscripts_path &lt;- file.path(temp_project_path, \"scripts\")\ndir.create(scripts_path)\nscript_path &lt;- file.path(scripts_path, \"script.R\")\nwriteLines(\n  c(\n    'here::i_am(\"scripts/script.R\")',\n    'print(\"Hello, world!\")'\n  ),\n  script_path\n)\nfs::dir_tree(temp_project_path)\n#&gt; /var/folders/gm/psh723xs3s50nqry92xw3_l80000gp/T//RtmpgdwJcI/file106515fbd3898\n#&gt; └── scripts\n#&gt;     └── script.R\nwriteLines(readLines(script_path))\n#&gt; here::i_am(\"scripts/script.R\")\n#&gt; print(\"Hello, world!\")\n\nThe script.R file contains a call to here::i_am() to declare its location. Running it from the current working directory will fail:\n\nsource(script_path, echo = TRUE)\n#&gt; \n#&gt; &gt; here::i_am(\"scripts/script.R\")\n#&gt; Error: Could not find associated project in working directory or any parent directory.\n#&gt; - Path in project: scripts/script.R\n#&gt; - Current working directory: /Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\n#&gt; Please open the project associated with this file and try again.\n\nTo reset the project root mid-session, change the working directory with setwd(). Now, the subsequent call to here::i_am() from within script.R works:\n\nsetwd(temp_project_path)\n\n\nsource(script_path, echo = TRUE)\n#&gt; \n#&gt; &gt; here::i_am(\"scripts/script.R\")\n#&gt; \n#&gt; &gt; print(\"Hello, world!\")\n#&gt; [1] \"Hello, world!\"\n\nTo reiterate: a fresh session is almost always the better, cleaner, safer, and more robust solution. Use this approach only as a last resort.\n\n\nUnder the hood: rprojroot\nThe here package has a very simple and restricted interface, by design. The underlying logic is provided by the much more powerful rprojroot package. If the default behavior of here does not suit your workflow for one reason or another, the rprojroot package may be a better alternative. It is also recommended to import rprojroot and not here from other packages.\nThe following example shows how to find an RStudio project starting from a directory:\n\nlibrary(rprojroot)\nfind_root(is_rstudio_project, file.path(project_path, \"analysis\"))\n#&gt; [1] \"/Users/francojc/Documents/Academic/Research/Projects/1Active/qtalr/resources/R/Library/here/demo-project\"\n\nArbitrary criteria can be defined. See vignette(\"rprojroot\", package = \"rprojroot\") for an introduction."
  },
  {
    "objectID": "R/Library/here/doc/here.html#footnotes",
    "href": "R/Library/here/doc/here.html#footnotes",
    "title": "here",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrior to version 1.0.0, it was recommended to attach the here package via library(here). This still works, but is no longer the recommended approach.↩︎\nlibrary(here) no longer emits an informative message if here::i_am() has been called before.↩︎\nlibrary(here) emits a message that may be confusing if followed by the message from here::i_am().↩︎"
  },
  {
    "objectID": "R/Library/urltools/doc/urltools.html",
    "href": "R/Library/urltools/doc/urltools.html",
    "title": "{{< var site.title >}}",
    "section": "",
    "text": "URLs are treated, by base R, as nothing more than components of a data retrieval process: they exist to create connections to retrieve datasets. This is an essential feature for the language to have, but it also means that URL handlers are designed for situations where URLs get you to the data - not situations where URLs are the data.\nThere is no support for encoding or decoding URLs en-masse, and no support for parsing and interpreting them. urltools provides this support!\n\n\nBase R provides two functions - URLdecode and URLencode - for taking percentage-encoded URLs and turning them into regular strings, or vice versa. As discussed, these are primarily designed to enable connections, and so they have several inherent limitations, including a lack of vectorisation, that make them unsuitable for large datasets.\nNot only are they not vectorised, they also have several particularly idiosyncratic bugs and limitations: URLdecode, for example, breaks if the decoded value is out of range:\n\nURLdecode(\"test%gIL\")\nError in rawToChar(out) : embedded nul in string: '\\0L'\nIn addition: Warning message:\nIn URLdecode(\"%gIL\") : out-of-range values treated as 0 in coercion to raw\n\nURLencode, on the other hand, encodes slashes on its most strict setting - without paying attention to where those slashes are: if we attempt to URLencode an entire URL, we get:\n\nURLencode(\"https://en.wikipedia.org/wiki/Article\", reserved = TRUE)\n[1] \"https%3a%2f%2fen.wikipedia.org%2fwiki%2fArticle\"\n\nThat’s a completely unusable URL (or ewRL, if you will).\nurltools replaces both functions with url_decode and url_encode respectively:\n\nlibrary(urltools)\nurl_decode(\"test%gIL\")\n[1] \"test\"\nurl_encode(\"https://en.wikipedia.org/wiki/Article\")\n[1] \"https://en.wikipedia.org%2fwiki%2fArticle\"\n\nAs you can see, url_decode simply excludes out-of-range characters from consideration, while url_encode detects characters that make up part of the URLs scheme, and leaves them unencoded. Both are extremely fast; with urltools, you can decode a vector of 1,000,000 URLs in 0.9 seconds.\nAlongside these, we have functions for encoding and decoding the ‘punycode’ format of URLs - ones that are designed to be internationalised and have unicode characters in them. These also take one argument, a vector of URLs, and can be found at puny_encode and puny_decode respectively.\n\n\n\nOnce you’ve got your nicely decoded (or encoded) URLs, it’s time to do something with them - and, most of the time, you won’t actually care about most of the URL. You’ll want to look at the scheme, or the domain, or the path, but not the entire thing as one string.\nThe solution is url_parse, which takes a URL and breaks it out into its RfC 3986 components: scheme, domain, port, path, query string and fragment identifier. This is, again, fully vectorised, and can happily be run over hundreds of thousands of URLs, rapidly processing them. The results are provided as a data.frame, since most people use data.frames to store data.\n\n&gt; parsed_address &lt;- url_parse(\"https://en.wikipedia.org/wiki/Article\")\n&gt; str(parsed_address)\n'data.frame':   1 obs. of  6 variables:\n $ scheme   : chr \"https\"\n $ domain   : chr \"en.wikipedia.org\"\n $ port     : chr NA\n $ path     : chr \"wiki/Article\"\n $ parameter: chr NA\n $ fragment : chr NA                         \n\nWe can also perform the opposite of this operation with url_compose:\n\n&gt; url_compose(parsed_address)\n[1] \"https://en.wikipedia.org/wiki/article\"\n\n\n\n\nWith the inclusion of a URL parser, we suddenly have the opportunity for lubridate-style component getting and setting. Syntax is identical to that of lubridate, but uses URL components as function names.\n\nurl &lt;- \"https://en.wikipedia.org/wiki/Article\"\nscheme(url)\n\"https\"\nscheme(url) &lt;- \"ftp\"\nurl\n\"ftp://en.wikipedia.org/wiki/Article\"\n\nFields that can be extracted or set are scheme, domain, port, path, parameters and fragment.\n\n\n\nOnce we’ve extracted a domain from a URL with domain or url_parse, we can identify which bit is the domain name, and which bit is the suffix:\n\n&gt; url &lt;- \"https://en.wikipedia.org/wiki/Article\"\n&gt; domain_name &lt;- domain(url)\n&gt; domain_name\n[1] \"en.wikipedia.org\"\n&gt; str(suffix_extract(domain_name))\n'data.frame':   1 obs. of  4 variables:\n $ host     : chr \"en.wikipedia.org\"\n $ subdomain: chr \"en\"\n $ domain   : chr \"wikipedia\"\n $ suffix      : chr \"org\"\n\nThis relies on an internal database of public suffixes, accessible at suffix_dataset - we recognise, though, that this dataset may get a bit out of date, so you can also pass the results of the suffix_refresh function, which retrieves an updated dataset, to suffix_extract:\n\ndomain_name &lt;- domain(\"https://en.wikipedia.org/wiki/Article\")\nupdated_suffixes &lt;- suffix_refresh()\nsuffix_extract(domain_name, updated_suffixes)\n\nWe can do the same thing with top-level domains, with precisely the same setup, except the functions and datasets are tld_refresh, tld_extract and tld_dataset.\nIn the other direction we have host_extract, which retrieves, well, the host! If the URL has subdomains, it’ll be the lowest-level subdomain. If it doesn’t, it’ll be the actual domain name, without the suffixes:\n\ndomain_name &lt;- domain(\"https://en.wikipedia.org/wiki/Article\")\nhost_extract(domain_name)\n\n\n\n\nOnce a URL is parsed, it’s sometimes useful to get the value associated with a particular query parameter. As an example, take the URL http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json. What pageID is being used? What is the export format? We can find out with param_get.\n\n&gt; str(param_get(urls = \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json\",\n                     parameter_names = c(\"pageid\",\"export\")))\n'data.frame':   1 obs. of  2 variables:\n $ pageid: chr \"1023\"\n $ export: chr \"json\"\n\nThis isn’t the only function for query manipulation; we can also dynamically modify the values a particular parameter might have, or strip them out entirely.\nTo modify the values, we use param_set:\n\nurl &lt;- \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json\"\nurl &lt;- param_set(url, key = \"pageid\", value = \"12\")\nurl\n# [1] \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=12&export=json\"\n\nAs you can see this works pretty well; it even works in situations where the URL doesn’t have a query yet:\n\nurl &lt;- \"http://en.wikipedia.org/wiki/api.php\"\nurl &lt;- param_set(url, key = \"pageid\", value = \"12\")\nurl\n# [1] \"http://en.wikipedia.org/wiki/api.php?pageid=12\"\n\nOn the other hand we might have a parameter we just don’t want any more - that can be handled with param_remove, which can take multiple parameters as well as multiple URLs:\n\nurl &lt;- \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json\"\nurl &lt;- param_remove(url, keys = c(\"action\",\"export\"))\nurl\n# [1] \"http://en.wikipedia.org/wiki/api.php?pageid=1023\"\n\n\n\n\nIf you have ideas for other URL handlers that would make your data processing easier, the best approach is to either request it or add it!"
  },
  {
    "objectID": "R/Library/urltools/doc/urltools.html#elegant-url-handling-with-urltools",
    "href": "R/Library/urltools/doc/urltools.html#elegant-url-handling-with-urltools",
    "title": "{{< var site.title >}}",
    "section": "",
    "text": "URLs are treated, by base R, as nothing more than components of a data retrieval process: they exist to create connections to retrieve datasets. This is an essential feature for the language to have, but it also means that URL handlers are designed for situations where URLs get you to the data - not situations where URLs are the data.\nThere is no support for encoding or decoding URLs en-masse, and no support for parsing and interpreting them. urltools provides this support!\n\n\nBase R provides two functions - URLdecode and URLencode - for taking percentage-encoded URLs and turning them into regular strings, or vice versa. As discussed, these are primarily designed to enable connections, and so they have several inherent limitations, including a lack of vectorisation, that make them unsuitable for large datasets.\nNot only are they not vectorised, they also have several particularly idiosyncratic bugs and limitations: URLdecode, for example, breaks if the decoded value is out of range:\n\nURLdecode(\"test%gIL\")\nError in rawToChar(out) : embedded nul in string: '\\0L'\nIn addition: Warning message:\nIn URLdecode(\"%gIL\") : out-of-range values treated as 0 in coercion to raw\n\nURLencode, on the other hand, encodes slashes on its most strict setting - without paying attention to where those slashes are: if we attempt to URLencode an entire URL, we get:\n\nURLencode(\"https://en.wikipedia.org/wiki/Article\", reserved = TRUE)\n[1] \"https%3a%2f%2fen.wikipedia.org%2fwiki%2fArticle\"\n\nThat’s a completely unusable URL (or ewRL, if you will).\nurltools replaces both functions with url_decode and url_encode respectively:\n\nlibrary(urltools)\nurl_decode(\"test%gIL\")\n[1] \"test\"\nurl_encode(\"https://en.wikipedia.org/wiki/Article\")\n[1] \"https://en.wikipedia.org%2fwiki%2fArticle\"\n\nAs you can see, url_decode simply excludes out-of-range characters from consideration, while url_encode detects characters that make up part of the URLs scheme, and leaves them unencoded. Both are extremely fast; with urltools, you can decode a vector of 1,000,000 URLs in 0.9 seconds.\nAlongside these, we have functions for encoding and decoding the ‘punycode’ format of URLs - ones that are designed to be internationalised and have unicode characters in them. These also take one argument, a vector of URLs, and can be found at puny_encode and puny_decode respectively.\n\n\n\nOnce you’ve got your nicely decoded (or encoded) URLs, it’s time to do something with them - and, most of the time, you won’t actually care about most of the URL. You’ll want to look at the scheme, or the domain, or the path, but not the entire thing as one string.\nThe solution is url_parse, which takes a URL and breaks it out into its RfC 3986 components: scheme, domain, port, path, query string and fragment identifier. This is, again, fully vectorised, and can happily be run over hundreds of thousands of URLs, rapidly processing them. The results are provided as a data.frame, since most people use data.frames to store data.\n\n&gt; parsed_address &lt;- url_parse(\"https://en.wikipedia.org/wiki/Article\")\n&gt; str(parsed_address)\n'data.frame':   1 obs. of  6 variables:\n $ scheme   : chr \"https\"\n $ domain   : chr \"en.wikipedia.org\"\n $ port     : chr NA\n $ path     : chr \"wiki/Article\"\n $ parameter: chr NA\n $ fragment : chr NA                         \n\nWe can also perform the opposite of this operation with url_compose:\n\n&gt; url_compose(parsed_address)\n[1] \"https://en.wikipedia.org/wiki/article\"\n\n\n\n\nWith the inclusion of a URL parser, we suddenly have the opportunity for lubridate-style component getting and setting. Syntax is identical to that of lubridate, but uses URL components as function names.\n\nurl &lt;- \"https://en.wikipedia.org/wiki/Article\"\nscheme(url)\n\"https\"\nscheme(url) &lt;- \"ftp\"\nurl\n\"ftp://en.wikipedia.org/wiki/Article\"\n\nFields that can be extracted or set are scheme, domain, port, path, parameters and fragment.\n\n\n\nOnce we’ve extracted a domain from a URL with domain or url_parse, we can identify which bit is the domain name, and which bit is the suffix:\n\n&gt; url &lt;- \"https://en.wikipedia.org/wiki/Article\"\n&gt; domain_name &lt;- domain(url)\n&gt; domain_name\n[1] \"en.wikipedia.org\"\n&gt; str(suffix_extract(domain_name))\n'data.frame':   1 obs. of  4 variables:\n $ host     : chr \"en.wikipedia.org\"\n $ subdomain: chr \"en\"\n $ domain   : chr \"wikipedia\"\n $ suffix      : chr \"org\"\n\nThis relies on an internal database of public suffixes, accessible at suffix_dataset - we recognise, though, that this dataset may get a bit out of date, so you can also pass the results of the suffix_refresh function, which retrieves an updated dataset, to suffix_extract:\n\ndomain_name &lt;- domain(\"https://en.wikipedia.org/wiki/Article\")\nupdated_suffixes &lt;- suffix_refresh()\nsuffix_extract(domain_name, updated_suffixes)\n\nWe can do the same thing with top-level domains, with precisely the same setup, except the functions and datasets are tld_refresh, tld_extract and tld_dataset.\nIn the other direction we have host_extract, which retrieves, well, the host! If the URL has subdomains, it’ll be the lowest-level subdomain. If it doesn’t, it’ll be the actual domain name, without the suffixes:\n\ndomain_name &lt;- domain(\"https://en.wikipedia.org/wiki/Article\")\nhost_extract(domain_name)\n\n\n\n\nOnce a URL is parsed, it’s sometimes useful to get the value associated with a particular query parameter. As an example, take the URL http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json. What pageID is being used? What is the export format? We can find out with param_get.\n\n&gt; str(param_get(urls = \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json\",\n                     parameter_names = c(\"pageid\",\"export\")))\n'data.frame':   1 obs. of  2 variables:\n $ pageid: chr \"1023\"\n $ export: chr \"json\"\n\nThis isn’t the only function for query manipulation; we can also dynamically modify the values a particular parameter might have, or strip them out entirely.\nTo modify the values, we use param_set:\n\nurl &lt;- \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json\"\nurl &lt;- param_set(url, key = \"pageid\", value = \"12\")\nurl\n# [1] \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=12&export=json\"\n\nAs you can see this works pretty well; it even works in situations where the URL doesn’t have a query yet:\n\nurl &lt;- \"http://en.wikipedia.org/wiki/api.php\"\nurl &lt;- param_set(url, key = \"pageid\", value = \"12\")\nurl\n# [1] \"http://en.wikipedia.org/wiki/api.php?pageid=12\"\n\nOn the other hand we might have a parameter we just don’t want any more - that can be handled with param_remove, which can take multiple parameters as well as multiple URLs:\n\nurl &lt;- \"http://en.wikipedia.org/wiki/api.php?action=parse&pageid=1023&export=json\"\nurl &lt;- param_remove(url, keys = c(\"action\",\"export\"))\nurl\n# [1] \"http://en.wikipedia.org/wiki/api.php?pageid=1023\"\n\n\n\n\nIf you have ideas for other URL handlers that would make your data processing easier, the best approach is to either request it or add it!"
  },
  {
    "objectID": "R/Library/triebeard/doc/rcpp_radix.html",
    "href": "R/Library/triebeard/doc/rcpp_radix.html",
    "title": "Radix trees in Rcpp",
    "section": "",
    "text": "A radix tree is a data structure optimised for storing key-value pairs in a way optimised for searching. This makes them very, very good for efficiently matching data against keys, and retrieving the values associated with those keys.\ntriebeard provides an implementation of radix trees for Rcpp (and also for use directly in R). To start using radix trees in your Rcpp development, simply modify your C++ file to include at the top:\n//[[Rcpp::depends(triebeard)]]\n#include &lt;radix.h&gt;"
  },
  {
    "objectID": "R/Library/triebeard/doc/rcpp_radix.html#constructing-trees",
    "href": "R/Library/triebeard/doc/rcpp_radix.html#constructing-trees",
    "title": "Radix trees in Rcpp",
    "section": "Constructing trees",
    "text": "Constructing trees\nTrees are constructed using the syntax:\n\nradix_tree&lt;type1, type2&gt; radix;\n\nWhere type represents the type of the keys (for example, std::string) and type2 the type of the values.\nRadix trees can have any scalar type as keys, although strings are most typical; they can also have any scalar type for values. Once you’ve constructed a tree, new entries can be added in a very R-like way: radix[new_key] = new_value;. Entries can also be removed, with radix.erase(key)."
  },
  {
    "objectID": "R/Library/triebeard/doc/rcpp_radix.html#matching-against-trees",
    "href": "R/Library/triebeard/doc/rcpp_radix.html#matching-against-trees",
    "title": "Radix trees in Rcpp",
    "section": "Matching against trees",
    "text": "Matching against trees\nWe then move on to the fun bit: matching! As mentioned, radix trees are really good for matching arbitrary values against keys (well, keys of the same type) and retrieving the associated values.\nThere are three types of supported matching; longest, prefix, and greedy. Longest does exactly what it says on the tin: it finds the key-value pair where the longest initial part of the key matches the arbitrary value:\n\nradix_tree&lt;std::string, std::string&gt; radix;\nradix[\"turnin\"] = \"entry the first\";\nradix[\"turin\"] = \"entry the second\";\n\nradix_tree&lt;std::string, std::string&gt;::iterator it;\n\nit = radix.longest_match(\"turing\");\n\nif(it = radix.end()){\n  printf(\"No match was found :(\");\n} else {\n  std::string result = \"Key of longest match: \" + it-&gt;first + \" , value of longest match: \" + it-&gt;second;\n}\n\nPrefix matching provides all trie entries where the value-to-match is a prefix of the key:\n\nradix_tree&lt;std::string, std::string&gt; radix;\nradix[\"turnin\"] = \"entry the first\";\nradix[\"turin\"] = \"entry the second\";\n\nstd::vector&lt;radix_tree&lt;std::string, std::string&gt;::iterator&gt; vec;\nstd::vector&lt;radix_tree&lt;std::string, std::string&gt;::iterator&gt;::iterator it;\n\nit = radix.prefix_match(\"tur\");\n\nif(it == vec.end()){\n  printf(\"No match was found :(\");\n} else {\n  for (it = vec.begin(); it != vec.end(); ++it) {\n    std::string result = \"Key of a prefix match: \" + it-&gt;first + \" , value of a prefix match: \" + it-&gt;second;\n  }\n}\n\nGreedy matching matches very, very fuzzily (a value of ‘bring’, for example, will match ‘blind’, ‘bind’ and ‘binary’) and, syntactically, looks exactly the same as prefix-matching, albeit with radix.greedy_match() instead of radix.prefix_match().\n\nOther trie things\nIf you have ideas for other trie-like structures, or functions that would be useful with these tries, the best approach is to either request it or add it!"
  },
  {
    "objectID": "R/Library/tidytext/NEWS.html",
    "href": "R/Library/tidytext/NEWS.html",
    "title": "tidytext 0.4.2",
    "section": "",
    "text": "tidytext 0.4.2\n\nAdded alt text to figures in vignettes and README (#233)\nUpdate vignette for quanteda::dfm() v4 (#242)\n\n\n\ntidytext 0.4.1\n\nFixed bug for FREX stm tidier (#228)\n\n\n\ntidytext 0.4.0\n\nhunspell is now a suggested dependency, thanks to (MichaelChirico?) (#221)\nAdded stm() tidiers for high FREX and lift words (#223)\nRemoved tweet-specific tokenizers because of changes in upstream dependencies (#227)\n\n\n\ntidytext 0.3.4\n\nUpdated the tidy method for a quanteda dfm because of the upcoming release of Matrix (#218)\n\n\n\ntidytext 0.3.3\n\nscale_x/y_reordered() now uses a function labels as its main input (#200)\nFixed how to_lower is passed to underlying tokenization function for character shingles (#208)\nAdded support for tidying STM models that use content, thanks to (jonathanvoelkle?) (#209)\n\n\n\ntidytext 0.3.2\n\nUpdate testing for rlang change + testthat 3e\n\n\n\ntidytext 0.3.1\n\nCheck for installation of stopwords more gracefully\nUpdate tidiers and casters for new version of quanteda\n\n\n\ntidytext 0.3.0\n\nUse vdiffr conditionally\nBug fix/breaking change for collapse argument to unnest_functions(). This argument now takes either NULL (do not collapse text across rows for tokenizing) or a character vector of variables (use said variables to collapse text across rows for tokenizing). This fixes a long-standing bug and provides more consistent behavior, but does change results for many situations (such as n-gram tokenization).\n\n\n\ntidytext 0.2.6\n\nMove one vignette to pkgdown site, because of dependency removal\nMove all CI from Travis to GH actions\n\n\n\ntidytext 0.2.5\n\nreorder_within() now handles multiple variables, thanks to (tmastny?) (#170)\nMove stopwords to Suggests so tidytext can be installed on older versions of R\nPass to_lower argument to other tokenizing functions, for more consistent behavior (#175)\nAdd glance() method for stm’s estimated regressions, thanks to (vincentarelbundock?) (#176)\n\n\n\ntidytext 0.2.4\n\nUpdate tidying test for new tibble release (inner names for columns)\nDeprecate SE versions of main functions (have long been replaced by tidy eval semantics)\nImprove error handling throughout\n\n\n\ntidytext 0.2.3\n\nWrapper tokenization functions for n-grams, characters, sentences, tweets, and more, thanks to (ColinFay?) (#137).\nSimplify get_sentiments() thanks to (jennybc?) (#151).\nFix flaky tests for corpus tidiers.\n\n\n\ntidytext 0.2.2\n\nAccess NRC lexicon via textdata package\n\n\n\ntidytext 0.2.1\n\nFix bug in augment() function for stm topic model.\nWarn when tf-idf is negative, thanks to (EmilHvitfeldt?) (#112).\nSwitch from importing broom to importing generics, for lighter dependencies (#133).\nAdd functions for reordering factors (such as for ggplot2 bar plots) thanks to (tmastny?) (#110).\nUpdate to tibble() where appropriate, thanks to (luisdza?) (#136).\nClarify documentation about impact of lowercase conversion on URLs (#139).\nChange how sentiment lexicons are accessed from package (remove NRC lexicon entirely, access AFINN and Loughran lexicons via textdata package so they are no longer included in this package).\n\n\n\ntidytext 0.2.0\n\nImprovements to documentation (#117)\nFix for NSE thanks to (lepennec?) (#122).\nTidier for estimated regressions from stm package thanks to (jefferickson?) (#115).\nTidier for correlated topic model from topicmodels package (#123).\n\n\n\ntidytext 0.1.9\n\nUpdates to documentation (#109) thanks to Emil Hvitfeldt.\nAdd new tokenizers for tweets, Penn Treebank to unnest_tokens().\nBetter error message (#111) and code styling.\nDeclare dependency for tests.\n\n\n\ntidytext 0.1.8\n\nUpdates to documentation (#102), README, and vignettes.\nAdd tokenizing by character shingles thanks to Kanishka Misra (#105).\nFix tests for skip grams thanks to Lincoln Mullen (#106).\n\n\n\ntidytext 0.1.7\n\nUpdated more docs/tests so package can build on R-oldrel. (Still trying!)\n\n\n\ntidytext 0.1.6\n\nunnest_tokens can now unnest a data frame with a list column (which formerly threw the error unnest_tokens expects all columns of input to be atomic vectors (not lists)). The unnested result repeats the objects within each list. (It’s still not possible when collapse = TRUE, in which tokens can span multiple lines).\nAdd get_tidy_stopwords() to obtain stopword lexicons in multiple languages in a tidy format.\nAdd a dataset nma_words of negators, modals, and adverbs that affect sentiment analysis (#55).\nUpdated various vignettes/docs/tests so package can build on R-oldrel.\n\n\n\ntidytext 0.1.5\n\nChange how NA values are handled in unnest_tokens so they no longer cause other columns to become NA (#82).\nUpdate tidiers and casters to align with quanteda v1.0 (#87).\nHandle input/output object classes (such as data.table) consistently (#88).\n\n\n\ntidytext 0.1.4\n\nFix tidier for quanteda dictionary for correct class (#71).\nAdd a pkgdown site.\nConvert NSE from underscored function to tidyeval (unnest_tokens, bind_tf_idf, all sparse casters) (#67, #74).\nAdded tidiers for topic models from the stm package (#51).\n\n\n\ntidytext 0.1.3\n\nget_sentiments now works regardless of whether tidytext has been loaded or not (#50).\nunnest_tokens now supports data.table objects (#37).\nFixed to_lower parameter in unnest_tokens to work properly for all tokenizing options.\nUpdated tidy.corpus, glance.corpus, tests, and vignette for changes to quanteda API\nRemoved the deprecated pair_count function, which is now in the in-development widyr package\nAdded tidiers for LDA models from the mallet package\nAdded the Loughran and McDonald dictionary of sentiment words specific to financial reports\nunnest_tokens preserves custom attributes of data frames and data.tables\n\n\n\ntidytext 0.1.2\n\nUpdated DESCRIPTION to require purrr &gt;= 0.1.1.\nFixed cast_sparse, cast_dtm, and other sparse casters to ignore groups in the input (#19)\nChanged unnest_tokens so that it no longer uses tidyr’s unnest, but rather a custom version that removes some overhead. In some experiments, this sped up unnest_tokens on large inputs by about 40%. This also moves tidyr from Imports to Suggests for now.\nunnest_tokens now checks that there are no list columns in the input, and raises an error if present (since those cannot be unnested).\nAdded a format argument to unnest_tokens so that it can process html, xml, latex or man pages using the hunspell package, though only when token = \"words\".\nAdded a get_sentiments function that takes the name of a lexicon (“nrc”, “bing”, or “sentiment”) and returns just that sentiment data frame (#25)\n\n\n\ntidytext 0.1.1\n\nAdded documentation for n-grams, skip n-grams, and regex\nAdded codecov and appveyor\nAdded tidiers for LDA objects from topicmodels and a vignette on topic modeling\nAdded function to calculate tf-idf of a tidy text dataset and a tf-idf vignette\nFixed a bug when tidying by line/sentence/paragraph/regex and there are multiple non-text columns\nFixed a bug when unnesting using n-grams and skip n-grams (entire text was not being collapsed)\nAdded ability to pass a (custom tokenizing) function to token. Also added a collapse argument that makes the choice whether to combine lines before tokenizing explicit.\nChanged tidy.dictionary to return a tbl_df rather than a data.frame\nUpdated cast_sparse to work with dplyr 0.5.0\nDeprecated the pair_count function, which has been moved to pairwise_count in the widyr package. This will be removed entirely in a future version.\n\n\n\ntidytext 0.1.0\n\nInitial release for text mining using tidy tools"
  },
  {
    "objectID": "R/Library/tidytext/doc/tidying_casting.html",
    "href": "R/Library/tidytext/doc/tidying_casting.html",
    "title": "Converting to and from Document-Term Matrix and Corpus objects",
    "section": "",
    "text": "Tidying document-term matrices\nMany existing text mining datasets are in the form of a DocumentTermMatrix class (from the tm package). For example, consider the corpus of 2246 Associated Press articles from the topicmodels package:\n\nlibrary(tm)\ndata(\"AssociatedPress\", package = \"topicmodels\")\nAssociatedPress\n\nIf we want to analyze this with tidy tools, we need to turn it into a one-term-per-document-per-row data frame first. The tidy function does this. (For more on the tidy verb, see the broom package).\n\nlibrary(dplyr)\nlibrary(tidytext)\n\nap_td &lt;- tidy(AssociatedPress)\n\nJust as shown in this vignette, having the text in this format is convenient for analysis with the tidytext package. For example, you can perform sentiment analysis on these newspaper articles.\n\nap_sentiments &lt;- ap_td %&gt;%\n  inner_join(get_sentiments(\"bing\"), join_by(term == word))\n\nap_sentiments\n\nWe can find the most negative documents:\n\nlibrary(tidyr)\n\nap_sentiments %&gt;%\n  count(document, sentiment, wt = count) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(sentiment = positive - negative) %&gt;%\n  arrange(sentiment)\n\nOr visualize which words contributed to positive and negative sentiment:\n\nlibrary(ggplot2)\n\nap_sentiments %&gt;%\n  count(sentiment, term, wt = count) %&gt;%\n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  mutate(term = reorder(term, n)) %&gt;%\n  ggplot(aes(n, term, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(vars(sentiment), scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\", y = NULL)\n\nNote that a tidier is also available for the dfm class from the quanteda package:\n\nlibrary(methods)\n\ndata(\"data_corpus_inaugural\", package = \"quanteda\")\nd &lt;- quanteda::tokens(data_corpus_inaugural) %&gt;%\n  quanteda::dfm()\n\nd\n\ntidy(d)\n\n\n\nCasting tidy text data into a DocumentTermMatrix\nSome existing text mining tools or algorithms work only on sparse document-term matrices. Therefore, tidytext provides cast_ verbs for converting from a tidy form to these matrices.\n\nap_td\n\n# cast into a Document-Term Matrix\nap_td %&gt;%\n  cast_dtm(document, term, count)\n\n# cast into a Term-Document Matrix\nap_td %&gt;%\n  cast_tdm(term, document, count)\n\n# cast into quanteda's dfm\nap_td %&gt;%\n  cast_dfm(term, document, count)\n\n\n# cast into a Matrix object\nm &lt;- ap_td %&gt;%\n  cast_sparse(document, term, count)\nclass(m)\ndim(m)\n\nThis allows for easy reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications.\n\n\nTidying corpus data\nYou can also tidy Corpus objects from the tm package. For example, consider a Corpus containing 20 documents, one for each\n\nreut21578 &lt;- system.file(\"texts\", \"crude\", package = \"tm\")\nreuters &lt;- VCorpus(DirSource(reut21578),\n                   readerControl = list(reader = readReut21578XMLasPlain))\n\nreuters\n\nThe tidy verb creates a table with one row per document:\n\nreuters_td &lt;- tidy(reuters)\nreuters_td\n\nSimilarly, you can tidy a corpus object from the quanteda package:\n\nlibrary(quanteda)\n\ndata(\"data_corpus_inaugural\")\n\ndata_corpus_inaugural\n\ninaug_td &lt;- tidy(data_corpus_inaugural)\ninaug_td\n\nThis lets us work with tidy tools like unnest_tokens to analyze the text alongside the metadata.\n\ninaug_words &lt;- inaug_td %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\ninaug_words\n\nWe could then, for example, see how the appearance of a word changes over time:\n\ninaug_freq &lt;- inaug_words %&gt;%\n  count(Year, word) %&gt;%\n  complete(Year, word, fill = list(n = 0)) %&gt;%\n  group_by(Year) %&gt;%\n  mutate(year_total = sum(n), percent = n / year_total) %&gt;%\n  ungroup()\n\ninaug_freq\n\nFor example, we can use the broom package to perform logistic regression on each word.\n\nlibrary(broom)\nmodels &lt;- inaug_freq %&gt;%\n  group_by(word) %&gt;%\n  filter(sum(n) &gt; 50) %&gt;%\n  group_modify(\n    ~ tidy(glm(cbind(n, year_total - n) ~ Year, ., family = \"binomial\"))\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"Year\")\n\nmodels\n\nmodels %&gt;%\n  filter(term == \"Year\") %&gt;%\n  arrange(desc(abs(estimate)))\n\nYou can show these models as a volcano plot, which compares the effect size with the significance:\n\nlibrary(ggplot2)\n\nmodels %&gt;%\n  mutate(adjusted.p.value = p.adjust(p.value)) %&gt;%\n  ggplot(aes(estimate, adjusted.p.value)) +\n  geom_point() +\n  scale_y_log10() +\n  geom_text(aes(label = word), vjust = 1, hjust = 1, check_overlap = TRUE) +\n  labs(x = \"Estimated change over time\", y = \"Adjusted p-value\")\n\nWe can also use the ggplot2 package to display the top 6 terms that have changed in frequency over time.\n\nlibrary(scales)\n\nmodels %&gt;%\n  slice_max(abs(estimate), n = 6) %&gt;%\n  inner_join(inaug_freq) %&gt;%\n  ggplot(aes(Year, percent)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(word)) +\n  scale_y_continuous(labels = percent_format()) +\n  labs(y = \"Frequency of word in speech\")"
  },
  {
    "objectID": "R/Library/janeaustenr/NEWS.html",
    "href": "R/Library/janeaustenr/NEWS.html",
    "title": "janeaustenr 1.0.0",
    "section": "",
    "text": "janeaustenr 1.0.0\n\nUse suggested packages more strictly\n\n\n\njaneaustenr 0.1.5\n\nFixed encoding for Mansfield Park\nAdded package to calls to data objects, since they are lazy-loaded and not in the namespace\n\n\n\njaneaustenr 0.1.4\n\nActually fixed factor order in austen_books function to align with publication order\n\n\n\njaneaustenr 0.1.3\n\nAttempted to fix factor order in austen_books function to align with publication order (made an error in this)\nAdded unit test to check output of austen_books\n\n\n\njaneaustenr 0.1.2\n\nMoved dplyr to Suggests; change implementation of austen_books to use base functions thanks to Jeroen Ooms\n\n\n\njaneaustenr 0.1.1\n\nAdded a NEWS.md file to track changes to the package.\nAdded some details on usage differences between novels to README\nReplaced all data files with new versions to solve problem of formatting change at 10000 lines\n\n\n\njaneaustenr 0.1.0\n\nInitial release of full texts of Jane Austen’s 6 completed, published novels"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Getting started",
    "section": "Overview",
    "text": "Overview\nResources available for the textbook include:\n\n\n\nTable 1: Resources Kit for the textbook\n\n\n\n\n\n\n\n\n\n\nResource\nDescription\nLocation\n\n\n\n\nLessons\nA series of interactive R programming lessons using {swirl} that accompany the textbook.\nGitHub\n\n\nRecipes\nA collection of coding demonstrations and examples that illustrate how to perform specific tasks in the text analysis workflow.\nResources Kit\n\n\nLabs\nA series of hands-on exercises that guide you through the process of conducting text analysis research.\nGitHub\n\n\n\n\n\n\nIn addition to these main resources, the Resources Kit also includes the following:\n\n\n\nTable 2: Additional resources in the Resources Kit\n\n\n\n\n\n\n\n\n\n\nResource\nDescription\nLocation\n\n\n\n\nGuides\nA collection of guides that provide additional information and instructions on how to use the resources available in the kit.\nResources Kit\n\n\nInstructors\nInformation for instructors who are using the textbook in their courses.\nResources Kit"
  },
  {
    "objectID": "index.html#steps-to-get-started",
    "href": "index.html#steps-to-get-started",
    "title": "Getting started",
    "section": "Steps to get started",
    "text": "Steps to get started\nTo get started with the textbook and the resources available in the kit, follow these steps:\n\nChoose and set up your preferred R environment. See the Setting up an R environment guide for a description of the possible setups and instructions on how to set up your R environment.\nInstall the key packages used in the textbook. These include {tidyverse}, {tinytex}, {swirl}, and {qtkit}. See the Installing and managing R packages guide for instructions on how to install these (and other) packages.\nLoad the interactive R programming lessons in your R environment. See the Working with the interactive R programming lessons guide for instructions on how to do this.\n\nWith these steps completed, you will be ready to start working with the textbook and the resources available in the kit. Refer back to this site for additional information and resources as needed."
  },
  {
    "objectID": "recipes/recipe-01/index.html",
    "href": "recipes/recipe-01/index.html",
    "title": "01. Academic writing with Quarto",
    "section": "",
    "text": "Skills\n\nNumbered sections\nTable of contents\nCross-referencing tables and figures\nIn-line citations and references list"
  },
  {
    "objectID": "recipes/recipe-01/index.html#concepts-and-strategies",
    "href": "recipes/recipe-01/index.html#concepts-and-strategies",
    "title": "01. Academic writing with Quarto",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\nFor many of the style components that we use in Quarto, there is a part that is addressed in the front-matter section and a part that is addressed in the prose section and/ or code block sections.\nTo refresh our memory, the front-matter is fenced by three dashes (---) and is where we set the document attributes. The prose section is where we write the text of the document. The code block section is where we write the code that will be executed and is fenced by three backticks (```) and the name of the code interpreter {r} (R for us).\n---\n1title: \"My document title\"\n2format: pdf\n---\n\n3This is the prose section.\n\n4```{r}\n#| label: example-code-block\n1 + 1\n```\n\n1\n\nThe title of the document\n\n2\n\nThe format of the document\n\n3\n\nThe prose section\n\n4\n\nThe code block section\n\n\nWith this in mind let’s look at each of these elements in turn.\n\nNumbered sections\nTo number sections in Quarto, we use the number_sections key with the value yes. This is set in the front-matter section, nested under the value for the document type to be rendered. For example, to number sections in a PDF document, we would set the number-sections key to true in the front-matter section as follows:\n---\n1title: \"My document title\"\n2format:\n3  pdf:\n4    number-sections: true\n---\n\n1\n\nThe title of the document\n\n2\n\nThe format of the document\n\n3\n\nThe type of document to be rendered, note the identation\n\n4\n\nThe key-value pair to number sections in the PDF document, again note the identation\n\n\nHeaders in the prose section are then numbered automatically. For example, the following markdown:\n# Section\n\n## Subsection\n\n### Subsubsection\n\n#### Subsubsubsection\n\n##### Subsubsubsubsection\nwould render as:\n\n\n\n\n\n\n\n\n\nWe can also control the depth of the numbering by setting the number-depth key in the front-matter section. For example, to number sections and subsections, but not subsubsections, we would set the number-depth key to 2 as follows:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n1    number-depth: 2\n---\n\n1\n\nThe key-value pair to control the depth of the numbering\n\n\nNow the first and second headers are numbered and formated but third and subsequent headers are only formatted.\nIf for some reason you want to turn off numbering for a specific header, you can add {.unnumbered} to the end of the header. For example, the following markdown:\n# Section {.unnumbered}\nThis is particularly useful in academic writing when we want to add a reference, materials, or other section that is not numbered at the end of the document.\n\n\n\n\n\n\n Warning\nNote that if you have a header that is unnumbered, the next header will be numbered as if the unnumbered header did not exist. This can have unexpected results if you have children of an unnumbered header.\n\n\n\n\n\nTable of contents\nFor longer documents including a table of contents can be a useful way to help readers navigate the document. To include a table of contents in Quarto, we use the toc key with the value true. Again, in the front-matter section, nested under the format value, as seen below:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n1    toc: true\n---\n\n1\n\nThe key-value pair to include a table of contents in the PDF document\n\n\n\n\n\n\n\n\n Tip\nFor PDF and Word document outputs, the table of contents will be automatically generated and placed at the beginning of the document. For HTML documents, the table of contents will be placed in the sidebar by default.\n\n\n\nIf if our headers are numbered, they will appeared numbered in the table of contents. If we unnnumbered a header, it will not appear with a section number. As with section numbering, we can also control the depth of the table of contents by setting the toc-depth key in the front-matter section. For example, to include sections and subsections, but not subsubsections, we would set the toc-depth key to 2 as follows:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    toc: true\n1    toc-depth: 2\n---\n\n1\n\nThe key-value pair to control the depth of the table of contents\n\n\nAnd as with section numbering we can avoid listing a header in the table of contents by adding {.unlisted} to the end of the header.\n\n\nCross-referencing tables and figures\nAnother key element in academic writing are using cross-references to tables and figures. This allows us to refer to a table or figure by number without having to manually update the number if we add or remove a table or figure.\nIn this case, we will not need to add anything to the front-matter section. Instead, we will modify keys in the code block section of a code-generated table or figure.\nTo cross-reference a table or figure, we need to add a prefix to the label key’s value. The prefix, either tbl- or fig-, indicates whether the label is for a table or figure. Additionally, table or figure captions can be added with the tbl-cap or fig-cap keys, respectively.\nLet’s look at a basic figure that we can cross-reference. The following code block will generate a very simple scatterplot.\n```{r}\n1#| label: fig-scatterplot\n2#| fig-cap: \"A scatterplot\"\n\nplot(x = 1:10, y = 1:10)\n```\n\n3In @fig-scatterplot we see a scatterplot. ....\n\n1\n\nThe label for the figure. Includes fig- as a prefix.\n\n2\n\nThe caption for the figure.\n\n3\n\nThe in-line reference to the figure. Uses the @ symbol followed by the label value.\n\n\n\n\n\n\n\n\n\nplot(1:10, 1:10)\n\n\n\n\n\n\n\nFigure 1: A scatterplot\n\n\n\n\n\nIn Figure 1 we see a scatterplot. …\n\n\n\nFor tables generated by R, the process is very similar to that of figures. The only difference is that we use the tbl- prefix on the label value and the tbl-cap key instead of the fig-cap key for the caption.\nWe can also create tables using markdown syntax. In this case, the format is a little different. Consider Table Table 1, for example.\n| Column 1 | Column 2 | Column 3 |\n|:---------|:---------|:---------|\n| A        | B        | C        |\n| D        | E        | F        |\n\n: A simple table {#tbl-table-1}\n\n\n\n\n\n\n\n\n\nTable 1: A simple table\n\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\n\n\n\n\n\n\n\n\n\nIn-line citations and references list\nThe last element we will cover here is adding citations and a references list to a Quarto document. To add citations we need three things:\n\nA bibliography file\nA reference to the bibliography file in the front-matter section\nA citation in the prose section which is contained in the bibliography file\n\nThe bibliography file is a plain text file that contains the citations that we want to use in our document. The file requires the extension .bib and is formatted using the BibTeX format. BibTeX is a reference syntax that is commonly used in academia.\nLet’s take a look at a sample file, bibliography.bib, that contains a single reference.\n@Manual{R-dplyr,\n  title = {dplyr: A Grammar of Data Manipulation},\n  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},\n  year = {2023},\n  note = {R package version 1.1.4},\n  url = {https://dplyr.tidyverse.org},\n}\nIn this file, we can see that the reference is for a manual entry with @Manual. The type of entry will change what fields are relevant and/ or required. In this entry, we have the cite key R-dplyr, the title, the authors, the year, a note, and a URL. Other entries, and entry types will have different fields.\nYou can find BibTeX formatted references almost everywhere you can find scholarly work. For example, Google Scholar, Web of Science, and Scopus all provide BibTeX formatted references. Additionally, many journals provide BibTeX formatted references for the articles they publish.\n\n\n\n\n\n\n Dive deeper\nManaging your references can be a challenge if you begin to amass a large number of them. There are a number of tools that can help you manage your references. For example, Zotero is a free, open-source reference manager that can help you organize your references and generate BibTeX formatted references.\nZotero also has a browser extension that allows you to easily add references to your Zotero library from your browser.\nFurthermore, Zotero can be connected to RStudio to facilitate the incorporation of BibTeX formatted references in a Quarto document. See the RStudio documentation for more information.\n\n\n\nIn the front-matter of our Quarto document, we need to add a reference to the bibliography file. This is done using the bibliography key. For example, if our bibliography file is called bibliography.bib and is located in the same directory as our Quarto document, we would add the following to the front-matter section:\n---\ntitle: \"My document title\"\nformat: pdf\n1bibliography: bibliography.bib\n---\n\n1\n\nThe key-value pair to include a path to the file which contains the BibTeX formatted references.\n\n\nWith the bibliography file and the reference to the bibliography file in the front-matter section, we can now add citations to our document. To do this, we use the @ symbol followed by the citation key in the prose section. For example, to cite the R-dplyr reference from the bibliography.bib file, we would add @R-dplyr to the prose section as follows:\nThis is a citation to @R-dplyr.\nThe citation will appear as below in the rendered document.\n\n\n\n\n\n\nThis is a citation to Wickham et al. (2023).\n\n\n\nAnd automatically, on rendering the document, a references list will be added to the end of the document. For this reason if you have citations in your document, it is a good idea to include a header section # References at the end of your document.\n\n\n\n\n\n\n Tip\nThere are a number of ways of having your inline citations appear. For example, in parentheses, with multiple citations, only with the year, adding a page number, etc.. For more information on how to format your citations, see the Quarto documentation."
  },
  {
    "objectID": "recipes/recipe-01/index.html#check-your-understanding",
    "href": "recipes/recipe-01/index.html#check-your-understanding",
    "title": "01. Academic writing with Quarto",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nConsider the following front-matter sections, A and B.\n\n\n\n\n\n\nA\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n    number-depth: 3\n    toc: false\n---\n\n\n\n\n\n\n\n\n\nB\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n    toc: true\n    toc-depth: 2\n---\n\n\n\nChoose whether the following statements are true or false.\n\nTRUEFALSE Section numbering will be included in the PDF output for both A and B.\nTRUEFALSE Section numbering will be applied to the first three levels of headers in the PDF output for both A and B.\nTRUEFALSE A table of contents will be included in the PDF output for both A and B.\nTRUEFALSE A table of contents will be included in the PDF output for B, but will only include the first two levels of headers.\n\nNow respond to the following questions.\n\n@tbl-scatterplot@fig-scatterplot@scatterplot will cross-reference a figure with the label fig-scatterplot.\n is the front-matter key to include a path to the file which contains the BibTeX formatted references."
  },
  {
    "objectID": "recipes/recipe-01/index.html#lab-preparation",
    "href": "recipes/recipe-01/index.html#lab-preparation",
    "title": "01. Academic writing with Quarto",
    "section": "Lab preparation",
    "text": "Lab preparation\nThis rounds out our introduction to academic writing in Quarto. In Lab 1 you will have an opportunity to practice these concepts by doing an article summary which includes some of these features using Quarto.\nIn preparation for Lab 1, ensure that you are prepared to do the following:\n\nEdit the front-matter section of a Quarto document to render:\n\na PDF document or a Word document\na document with numbered sections\na document with a table of contents\na document with a path to a bibliography file\n\nAdd an inline citation to the prose section of a Quarto document\n\nAlso, since you will do an article summary, you should be prepared with:\n\nan article of interest related to text analysis that you have read or at least skimmed for the following:\n\nthe research question\nthe data used\nthe methods used\nthe results/ findings of the study\n\na BibTeX formatted reference for the article\n\n\n\n\n\n\n\nIf you do not find an article of interest, you can use Bychkovska and Lee (2017)."
  },
  {
    "objectID": "recipes/recipe-07/index.html",
    "href": "recipes/recipe-07/index.html",
    "title": "07. Transforming and documenting data",
    "section": "",
    "text": "Skills\n\nText normalization and tokenization\nCreating new variables by splitting, merging, and recoding existing variables\nAugmenting data with additional variables from other sources or resources\nIn this recipe, we will employ a variety of tools and techniques to accomplish these tasks. Let’s load the packages we will need for this recipe. Let’s load the packages we will need for this recipe.\n# Load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(qtalrkit)\nIn Lab 7, we will apply what we have learned in this recipe to a new dataset."
  },
  {
    "objectID": "recipes/recipe-07/index.html#concepts-and-strategies",
    "href": "recipes/recipe-07/index.html#concepts-and-strategies",
    "title": "07. Transforming and documenting data",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\n\nCurated datasets are often project-neutral. That is, they are not necessarily designed to answer a specific research question. Rather, they are designed to be flexible enough to be used in a variety of projects. This is a good thing, but it also means that we will likely need to transform the data to bring it more in line with our research goals. This can include normalizing text, modifying the unit of observation and/ or adding additional attributes to the data.\n\nIn this recipe, we will explore a practical example of transforming data. We will start with a curated dataset and transform it to reflect a specific research goal. The dataset we will use is the MASC dataset (Ide et al. 2008). This dataset contains a collection of words from a variety of genres and modalities of American English.\n\n\n\n\n\n\n Tip\nThe MASC dataset is a curated version of the original data. This data is relatively project-neutral.\nIf you would like to acquire the original data and curate it for use in this recipe, you can do so by running the following code:\n# Acquire the original data\nqtalrkit::get_compressed_data(\n  url = \"..\",\n  target_dir = \"data/original/masc/\"\n)\n\n# Curate the data\n\n# ... write a function and add it to the package\n\n\n\nAs a starting point, I will assume that the curated dataset is available in the data/derived/masc/ directory, as seen below.\ndata/\n├── analysis/\n├── derived/\n│   ├── masc_curated_dd.csv\n│   ├── masc/\n│   │   ├── masc_curated.csv\n├── original/\n│   ├── masc_do.csv\n│   ├── masc/\n│   │   ├── ...\nThe first step is to inspect the data dictionary file. This file contains information about the variables in the dataset. It is also a good idea to review the data origin file, which contains information about the original data source.\nLooking at the data dictionary, in Table 1.\n\n\n\n\nTable 1: Data dictionary for the MASC dataset\n\n\n\n\n\n\nvariable\nname\ndescription\nvariable_type\n\n\n\n\nfile\nFile\nID number of the source file\ncharacter\n\n\nref\nReference\nReference number within the source file\ninteger\n\n\nbase\nBase\nBase form of the word (lemma)\ncharacter\n\n\nmsd\nMSD\nPart-of-speech tag (PENN tagset)\ncharacter\n\n\nstring\nString\nText content of the word\ncharacter\n\n\ntitle\nTitle\nTitle of the source file\ncharacter\n\n\nsource\nSource\nName of the source\ncharacter\n\n\ndate\nDate\nDate of the source file (if available)\ncharacter\n\n\nclass\nClass\nClassification of the source. Modality and genre\ncharacter\n\n\ndomain\nDomain\nDomain or topic of the source\ncharacter\n\n\n\n\n\n\n\n\n\n\nLet’s read in the data and take a glimpse at it.\n\n# Read the data\nmasc_curated &lt;- read_csv(\"data/derived/masc/masc_curated.csv\")\n\n# Preview\nglimpse(masc_curated)\n\nRows: 591,097\nColumns: 10\n$ file   &lt;chr&gt; \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"…\n$ ref    &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ base   &lt;chr&gt; \"december\", \"1998\", \"your\", \"contribution\", \"to\", \"goodwill\", \"…\n$ msd    &lt;chr&gt; \"NNP\", \"CD\", \"PRP$\", \"NN\", \"TO\", \"NNP\", \"MD\", \"VB\", \"JJR\", \"IN\"…\n$ string &lt;chr&gt; \"December\", \"1998\", \"Your\", \"contribution\", \"to\", \"Goodwill\", \"…\n$ title  &lt;chr&gt; \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"…\n$ source &lt;chr&gt; \"ICIC Corpus of Philanthropic Fundraising Discourse\", \"ICIC Cor…\n$ date   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ class  &lt;chr&gt; \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", …\n$ domain &lt;chr&gt; \"philanthropic fundraising discourse\", \"philanthropic fundraisi…\n\n\nWe may also want to do a summary overview of the dataset with {skimr}. This will give us a sense of the data types and the number of missing values.\n── Data Summary ───────────────────────\n                           Values\nName                       masc_curated\nNumber of rows             591097\nNumber of columns          10\n_______________________\nColumn type frequency:\n  character                9\n  numeric                  1\n________________________\nGroup variables            None\n\n── Variable type: character ───────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 file                  0         1       3  40     0      392          0\n2 base                  4         1.00    1  99     0    28010          0\n3 msd                   0         1       1   8     0       60          0\n4 string               25         1.00    1  99     0    39474          0\n5 title                 0         1       3 203     0      373          0\n6 source             5732         0.990   3 139     0      348          0\n7 date              94002         0.841   4  17     0       62          0\n8 class                 0         1       5   5     0       18          0\n9 domain            18165         0.969   4  35     0       21          0\n\n── Variable type: numeric ─────────────\n  skim_variable n_missing complete_rate  mean    sd p0 p25  p50  p75  p100 hist\n1 ref                   0             1 3854. 4633.  0 549 2033 5455 24519 ▇▂▁▁▁\nIn summary, the dataset contains 591,097 observations and 10 variables. The unit of observation is the word. The variable names are somewhat opaque, but the data dictionary provides some context that will help us understand the data.\nNow we want to consider how we plan to use this data in our analysis. Let’s assume that we want to use this data to explore lexical variation in the MASC dataset across modalities and genres. We will want to transform the data to reflect this goal.\nIn Table 2, we see an idealized version of the dataset we would like to have.\n\n\n\n\nTable 2: Idealized version of the MASC dataset\n\n\n\n\n\n\nvariable\nname\ntype\ndescription\n\n\n\n\ndoc_id\nDocument ID\nnumeric\nA unique identifier for each document\n\n\nmodality\nModality\ncharacter\nThe modality of the document (e.g., spoken, written)\n\n\ngenre\nGenre\ncharacter\nThe genre of the document (e.g., blog, newspaper)\n\n\nterm_num\nTerm number\nnumeric\nThe position of the term in the document\n\n\nterm\nTerm\ncharacter\nThe word\n\n\nlemma\nLemma\ncharacter\nThe lemma of the word\n\n\npos\nPart-of-speech\ncharacter\nThe part-of-speech tag of the word\n\n\n\n\n\n\n\n\n\n\nOf note, in this recipe we will derive a single transformed dataset. In other projects, you may want to generate various datasets with different units of observations. It all depends on your research question and the research aim that you are adopting.\n\n\nTransforming data\nTo get from the curated dataset to the idealized dataset, we will need to perform a number of transformations. Some of these transformations will be relatively straightforward, while others will require more work. Let’s start with the easy ones.\n\nLet’s drop the variables that we will not use and at the same time rename the variables to make them more intuitive.\n\nWe will use the select() function to drop or rename variables.\n\n# Drop and rename variables\nmasc_df &lt;-\n  masc_curated |&gt;\n  select(\n    doc_id = file,\n    term_num = ref,\n    term = string,\n    lemma = base,\n    pos = msd,\n    mod_gen = class\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 6\n   doc_id    term_num term         lemma        pos   mod_gen\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  \n 1 110CYL067        0 December     december     NNP   WR LT  \n 2 110CYL067        1 1998         1998         CD    WR LT  \n 3 110CYL067        2 Your         your         PRP$  WR LT  \n 4 110CYL067        3 contribution contribution NN    WR LT  \n 5 110CYL067        4 to           to           TO    WR LT  \n 6 110CYL067        5 Goodwill     goodwill     NNP   WR LT  \n 7 110CYL067        6 will         will         MD    WR LT  \n 8 110CYL067        7 mean         mean         VB    WR LT  \n 9 110CYL067        8 more         more         JJR   WR LT  \n10 110CYL067        9 than         than         IN    WR LT  \n# ℹ 591,087 more rows\n\n\nThat’s a good start on the structure.\n\nNext, we will split the mod_gen variable into two variables: modality and genre.\n\nWe have a variable mod_gen that contains two pieces of information: modality and genre (e.g., WR LT). The information appears to separated by a space. We can make sure this is the case by tabulating the values. The count() function will count the number of occurrences of each value in a variable, and as a side effect it will summarize the values of the variable so we can see if there are any unexpected values.\n\n# Tabulate mod_gen\nmasc_df |&gt;\n  count(mod_gen) |&gt;\n  arrange(-n)\n\n# A tibble: 18 × 2\n   mod_gen     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 SP TR   71630\n 2 WR EM   62036\n 3 WR FC   38608\n 4 WR ES   34938\n 5 WR FT   34373\n 6 WR BL   33278\n 7 WR JO   33042\n 8 WR JK   32420\n 9 WR NP   31225\n10 SP MS   29879\n11 WR NF   29531\n12 WR TW   28128\n13 WR GV   27848\n14 WR TG   27624\n15 WR LT   26468\n16 SP FF   23871\n17 WR TC   19419\n18 SP TP    6779\n\n\nLooks good, our values are separated by a space. We can use the separate_wider_delim() function from {tidyr} to split the variable into two variables. We will use the delim argument to specify the delimiter and the names argument to specify the names of the new variables.\n\n# Split mod_gen into modality and genre\nmasc_df &lt;-\n  masc_df |&gt;\n  separate_wider_delim(\n    cols = mod_gen,\n    delim = \" \",\n    names = c(\"modality\", \"genre\")\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id    term_num term         lemma        pos   modality genre\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1 110CYL067        0 December     december     NNP   WR       LT   \n 2 110CYL067        1 1998         1998         CD    WR       LT   \n 3 110CYL067        2 Your         your         PRP$  WR       LT   \n 4 110CYL067        3 contribution contribution NN    WR       LT   \n 5 110CYL067        4 to           to           TO    WR       LT   \n 6 110CYL067        5 Goodwill     goodwill     NNP   WR       LT   \n 7 110CYL067        6 will         will         MD    WR       LT   \n 8 110CYL067        7 mean         mean         VB    WR       LT   \n 9 110CYL067        8 more         more         JJR   WR       LT   \n10 110CYL067        9 than         than         IN    WR       LT   \n# ℹ 591,087 more rows\n\n\n\nCreate a document id variable.\n\nNow that we have the variables we want, we can turn our attention to the values of the variables. Let’s start with the doc_id variable. This may a good variable to use as the document id. If we take a look at the values, however, we can see that the values are not very informative.\nLet’s use the distinct() function to only show the unique values of the variable. We will also chain a slice_sample() function to randomly select a sample of the values. This will give us a sense of the values in the variable.\n\n# Preview doc_id\nmasc_df |&gt;\n  distinct(doc_id) |&gt;\n  slice_sample(n = 10)\n\n\n\n# A tibble: 10 × 1\n   doc_id             \n   &lt;chr&gt;              \n 1 JurassicParkIV-INT \n 2 111367             \n 3 NYTnewswire6       \n 4 sw2014-ms98-a-trans\n 5 52713              \n 6 new_clients        \n 7 cable_spool_fort   \n 8 jokes10            \n 9 wsj_2465           \n10 wsj_0158           \n\n\nYou can run this code various times to get a different sample of values.\nSince the doc_id variable is not informative, let’s replace the variable’s values with numeric values. In the end, we want a digit for each unique document and we want the words in each document to be grouped together.\nTo do this we will need to group the data by doc_id and then generate a new number for each group. We can achieve this by passing the data grouped by doc_id (group_by()) to the mutate() function and then using the cur_group_id() function to generate a number for each group.\n\n# Recode doc_id\nmasc_df &lt;-\n  masc_df |&gt;\n  group_by(doc_id) |&gt;\n  mutate(doc_id = cur_group_id()) |&gt;\n  ungroup()\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre\n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1      1        0 December     december     NNP   WR       LT   \n 2      1        1 1998         1998         CD    WR       LT   \n 3      1        2 Your         your         PRP$  WR       LT   \n 4      1        3 contribution contribution NN    WR       LT   \n 5      1        4 to           to           TO    WR       LT   \n 6      1        5 Goodwill     goodwill     NNP   WR       LT   \n 7      1        6 will         will         MD    WR       LT   \n 8      1        7 mean         mean         VB    WR       LT   \n 9      1        8 more         more         JJR   WR       LT   \n10      1        9 than         than         IN    WR       LT   \n# ℹ 591,087 more rows\n\n\nTo check, we can again apply the count() function.\n\n# Check\nmasc_df |&gt;\n  count(doc_id) |&gt;\n  arrange(-n)\n\n# A tibble: 392 × 2\n   doc_id     n\n    &lt;int&gt; &lt;int&gt;\n 1    158 24520\n 2    300 22261\n 3    112 18459\n 4    113 17986\n 5    215 17302\n 6    312 14752\n 7    311 13376\n 8    200 13138\n 9    217 11753\n10    186 10665\n# ℹ 382 more rows\n\n\nWe have 392 unique documents in the dataset. We also can see that the word lengths vary quite a bit. That’s something we will need to keep in mind as we move forward into the analysis.\n\nCheck the values of the pos variable.\n\nThe pos variable contains the part-of-speech tags for each word. The PENN Treebank tagset is used. Let’s take a look at the values to get familiar with them, and also to see if there are any unexpected values.\nLet’s use the slice_sample() function to randomly select a sample of the values. This will give us a sense of the values in the variable.\n# Preview pos\nmasc_df |&gt;\n  slice_sample(n = 10)\n\n\n# A tibble: 10 × 7\n   doc_id term_num term          lemma         pos   modality genre\n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1    303     2511 proliferation proliferation NN    WR       TC   \n 2     76     5245 And           and           CC    WR       FT   \n 3    300    17170 DAVY          davy          NNP   SP       MS   \n 4     80     5341 ”             ”             NN    WR       FT   \n 5    171      900 .             .             .     WR       TG   \n 6    166     2588 out           out           RP    WR       BL   \n 7     67       58 organization  organization  NN    WR       LT   \n 8    216     2944 include       include       VB    WR       TG   \n 9    234     1304 donation      donation      NN    WR       LT   \n10    231     3539 say           say           VB    WR       NF   \n\n\nAfter running this code a few times, we can see that the many of the values are as expected. There are, however, some unexpected values. In particular, some punctuation and symbols are tagged as nouns.\nWe can get a better appreciation for the unexpected values by filtering the data to only show non alpha-numeric values (^\\\\W+$) in the term column and then tabulating the values by term and pos.\n\n# Filter and tabulate\nmasc_df |&gt;\n  filter(str_detect(term, \"^\\\\W+$\")) |&gt;\n  count(term, pos) |&gt;\n  arrange(-n) |&gt;\n  print(n = 20)\n\n# A tibble: 152 × 3\n   term  pos       n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 \",\"   ,     27112\n 2 \".\"   .     26256\n 3 \"\\\"\"  ''     5495\n 4 \":\"   :      4938\n 5 \"?\"   .      3002\n 6 \")\"   )      2447\n 7 \"(\"   (      2363\n 8 \"-\"   :      1778\n 9 \"!\"   .      1747\n10 \"/\"   NN     1494\n11 \"’\"   NN     1319\n12 \"-\"   -      1213\n13 \"”\"   NN     1076\n14 \"“\"   NN     1061\n15 \"]\"   NN     1003\n16 \"[\"   NN     1001\n17 \";\"   :       991\n18 \"--\"  :       772\n19 \"&gt;\"   NN      752\n20 \"...\" ...     716\n# ℹ 132 more rows\n\n\nAs we can see from the sample above and from the PENN tagset documentation, most punctuation is tagged as the punctuation itself. For example, the period is tagged as . and the comma is tagged as ,. Let’s edit the data to reflect this.\nLet’s look at the code, and then we will discuss it.\n\n# Recode\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(pos = case_when(\n    str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),\n    TRUE ~ pos\n  ))\n\n# Check\nmasc_df |&gt;\n  filter(str_detect(term, \"^\\\\W+$\")) |&gt; # preview\n  count(term, pos) |&gt;\n  arrange(-n) |&gt;\n  print(n = 20)\n\n# A tibble: 127 × 3\n   term  pos       n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 \",\"   \",\"   27113\n 2 \".\"   \".\"   26257\n 3 \"\\\"\"  \"\\\"\"   5502\n 4 \":\"   \":\"    4939\n 5 \"?\"   \"?\"    3002\n 6 \"-\"   \"-\"    2994\n 7 \")\"   \")\"    2447\n 8 \"(\"   \"(\"    2363\n 9 \"!\"   \"!\"    1747\n10 \"/\"   \"/\"    1495\n11 \"’\"   \"’\"    1325\n12 \"”\"   \"”\"    1092\n13 \"“\"   \"“\"    1078\n14 \"]\"   \"]\"    1003\n15 \"[\"   \"[\"    1001\n16 \";\"   \";\"     993\n17 \"--\"  \"-\"     772\n18 \"&gt;\"   \"&gt;\"     753\n19 \"...\" \".\"     747\n20 \"'\"   \"'\"     741\n# ℹ 107 more rows\n\n\nThe case_when() function allows us to specify a series of conditions and values. The first condition is that the term variable contains only non alpha-numeric characters. If it does, then we want to replace the value of the pos variable with the first character of the term variable, str_sub(term, start = 1, end = 1). If the condition is not met, then we want to keep the original value of the pos variable, TRUE ~ pos.\nWe can see that our code worked by filtering the data to only show non alpha-numeric values (^\\\\W+$) in the term column and then tabulating the values by term and pos.\nFor completeness, I will also recode the lemma values for these values as well as the lemma can some times be multiple punctuation marks (e.g. !!!!!, ---, etc.) for these terms.\n\n# Recode\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(lemma = case_when(\n    str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),\n    TRUE ~ lemma\n  ))\n\n# Check\nmasc_df |&gt;\n  filter(str_detect(term, \"^\\\\W+$\")) |&gt; # preview\n  count(term, lemma) |&gt;\n  arrange(-n) |&gt;\n  print(n = 20)\n\n# A tibble: 127 × 3\n   term  lemma     n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 \",\"   \",\"   27113\n 2 \".\"   \".\"   26257\n 3 \"\\\"\"  \"\\\"\"   5502\n 4 \":\"   \":\"    4939\n 5 \"?\"   \"?\"    3002\n 6 \"-\"   \"-\"    2994\n 7 \")\"   \")\"    2447\n 8 \"(\"   \"(\"    2363\n 9 \"!\"   \"!\"    1747\n10 \"/\"   \"/\"    1495\n11 \"’\"   \"’\"    1325\n12 \"”\"   \"”\"    1092\n13 \"“\"   \"“\"    1078\n14 \"]\"   \"]\"    1003\n15 \"[\"   \"[\"    1001\n16 \";\"   \";\"     993\n17 \"--\"  \"-\"     772\n18 \"&gt;\"   \"&gt;\"     753\n19 \"...\" \".\"     747\n20 \"'\"   \"'\"     741\n# ℹ 107 more rows\n\n\n\nCheck the values of the modality variable.\n\nThe modality variable contains the modality tags for each document. Let’s take a look at the values.\nLet’s tabulate the values with count().\n\n# Tabulate modality\nmasc_df |&gt;\n  count(modality)\n\n# A tibble: 2 × 2\n  modality      n\n  &lt;chr&gt;     &lt;int&gt;\n1 SP       132159\n2 WR       458938\n\n\nWe see that the values are SP and WR, which stand for spoken and written, respectively. To make this a bit more transparent, we can recode these values to Spoken and Written. We will use the case_when() function to do this.\n\n# Recode modality\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(\n    modality = case_when(\n      modality == \"SP\" ~ \"Spoken\",\n      modality == \"WR\" ~ \"Written\"\n    )\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre\n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1      1        0 December     december     NNP   Written  LT   \n 2      1        1 1998         1998         CD    Written  LT   \n 3      1        2 Your         your         PRP$  Written  LT   \n 4      1        3 contribution contribution NN    Written  LT   \n 5      1        4 to           to           TO    Written  LT   \n 6      1        5 Goodwill     goodwill     NNP   Written  LT   \n 7      1        6 will         will         MD    Written  LT   \n 8      1        7 mean         mean         VB    Written  LT   \n 9      1        8 more         more         JJR   Written  LT   \n10      1        9 than         than         IN    Written  LT   \n# ℹ 591,087 more rows\n\n\n\nCheck the values of the genre variable.\n\nLet’s look at the values of the genre variable.\n\n# Tabulate genre\nmasc_df |&gt;\n  count(genre) |&gt;\n  print(n = Inf)\n\n# A tibble: 18 × 2\n   genre     n\n   &lt;chr&gt; &lt;int&gt;\n 1 BL    33278\n 2 EM    62036\n 3 ES    34938\n 4 FC    38608\n 5 FF    23871\n 6 FT    34373\n 7 GV    27848\n 8 JK    32420\n 9 JO    33042\n10 LT    26468\n11 MS    29879\n12 NF    29531\n13 NP    31225\n14 TC    19419\n15 TG    27624\n16 TP     6779\n17 TR    71630\n18 TW    28128\n\n\nThese genre labels are definitely cryptic. The data dictionary does not list these labels and their more verbose descriptions. However, looking at the original data’s README, we can find the file (resource-headers.xml) that lists these genre labels.\n1. 'BL' for blog\n2. 'NP' is newspaper\n3. 'EM' is email\n4. 'ES' is essay\n5. 'FT' is fictlets\n6. 'FC' is fiction\n7. 'GV' is government\n8. 'JK' is jokes\n9. 'JO' is journal\n10. 'LT' is letters\n11. 'MS' is movie script\n12. 'NF' is non-fiction\n13. 'FF' is face-to-face\n14. 'TC' is technical\n15. 'TG' is travel guide\n16. 'TP' is telephone\n17. 'TR' is transcript\n18. 'TW' is twitter\nNow we can again use the case_when() function. This time we will see if genre is equal to one of the genre labels and if it is, then we will replace the value with the more verbose description.\n\n# Recode genre\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(\n    genre = case_when(\n      genre == \"BL\" ~ \"Blog\",\n      genre == \"NP\" ~ \"Newspaper\",\n      genre == \"EM\" ~ \"Email\",\n      genre == \"ES\" ~ \"Essay\",\n      genre == \"FT\" ~ \"Fictlets\",\n      genre == \"FC\" ~ \"Fiction\",\n      genre == \"GV\" ~ \"Government\",\n      genre == \"JK\" ~ \"Jokes\",\n      genre == \"JO\" ~ \"Journal\",\n      genre == \"LT\" ~ \"Letters\",\n      genre == \"MS\" ~ \"Movie script\",\n      genre == \"NF\" ~ \"Non-fiction\",\n      genre == \"FF\" ~ \"Face-to-face\",\n      genre == \"TC\" ~ \"Technical\",\n      genre == \"TG\" ~ \"Travel guide\",\n      genre == \"TP\" ~ \"Telephone\",\n      genre == \"TR\" ~ \"Transcript\",\n      genre == \"TW\" ~ \"Twitter\"\n    )\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre  \n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  \n 1      1        0 December     december     NNP   Written  Letters\n 2      1        1 1998         1998         CD    Written  Letters\n 3      1        2 Your         your         PRP$  Written  Letters\n 4      1        3 contribution contribution NN    Written  Letters\n 5      1        4 to           to           TO    Written  Letters\n 6      1        5 Goodwill     goodwill     NNP   Written  Letters\n 7      1        6 will         will         MD    Written  Letters\n 8      1        7 mean         mean         VB    Written  Letters\n 9      1        8 more         more         JJR   Written  Letters\n10      1        9 than         than         IN    Written  Letters\n# ℹ 591,087 more rows\n\n\nDuring the process of transformation and afterwards, it is a good idea to tabulate and/ or visualize the dataset. This provides us an opportunity to get to know the dataset better and also may help us identify inconsistencies that we would like to address in the transformation, or at least be aware of as we move towards analysis.\n\n# How many documents are in each modality?\nmasc_df |&gt;\n  distinct(doc_id, modality) |&gt;\n  count(modality) |&gt;\n  arrange(-n)\n\n# A tibble: 2 × 2\n  modality     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Written    371\n2 Spoken      21\n\n# How many documents are in each genre?\nmasc_df |&gt;\n  distinct(doc_id, genre) |&gt;\n  count(genre) |&gt;\n  arrange(-n)\n\n# A tibble: 18 × 2\n   genre            n\n   &lt;chr&gt;        &lt;int&gt;\n 1 Email          174\n 2 Newspaper       54\n 3 Letters         49\n 4 Blog            21\n 5 Jokes           16\n 6 Journal         12\n 7 Essay            8\n 8 Fiction          7\n 9 Travel guide     7\n10 Face-to-face     6\n11 Movie script     6\n12 Technical        6\n13 Fictlets         5\n14 Government       5\n15 Non-fiction      5\n16 Telephone        5\n17 Transcript       4\n18 Twitter          2\n\n# What is the averge length of documents (in words)?\nmasc_df |&gt;\n  group_by(doc_id) |&gt;\n  summarize(n = n()) |&gt;\n  summarize(\n    mean = mean(n),\n    median = median(n),\n    min = min(n),\n    max = max(n)\n  )\n\n# A tibble: 1 × 4\n   mean median   min   max\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 1508.   418.    45 24520\n\nmasc_df |&gt;\n  group_by(doc_id) |&gt;\n  summarize(n = n()) |&gt;\n  ggplot(aes(x = n)) +\n  geom_density()\n\n\n\n\n\n\n\n# What is the distribution of the length of documents by modality?\nmasc_df |&gt;\n  group_by(doc_id, modality) |&gt;\n  summarize(n = n()) |&gt;\n  ggplot(aes(x = n, fill = modality)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n# What is the distribution of the length of documents by genre?\nmasc_df |&gt;\n  group_by(doc_id, modality, genre) |&gt;\n  summarize(n = n()) |&gt;\n  ggplot(aes(x = genre, y = n)) +\n  geom_boxplot() +\n  facet_wrap(~ modality, scales = \"free_x\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nOnce we are satisfied with the structure and values of the dataset, we can save it to a file. We will use the write_csv() function from {readr} to do this.\n\n# Save the data\nwrite_csv(masc_df, \"data/derived/masc/masc_transformed.csv\")\n\nThe structure of the data/ directory in our project should now look like this:\ndata/\n├── analysis/\n├── derived/\n│   ├── masc_curated_dd.csv\n│   ├── masc/\n│   │   ├── masc_curated.csv\n│   │   ├── masc_transformed.csv\n├── original/\n\n\nDocumenting data\nThe last step is to document the process and the resulting dataset(s). In this particular case we only derived one transformed dataset. The documentation steps are the same as in the curation step. We will organize and document the process file (often a .qmd file) and then create a data dictionary for each of the transformed datasets. The create_data_dictionary() function can come in handy for scaffolding the data dictionary file.\n\n# Create a data dictionary\ncreate_data_dictionary(\n  data = masc_df,\n  file_path = \"data/derived/masc/masc_transformed_dd.csv\"\n)"
  },
  {
    "objectID": "recipes/recipe-07/index.html#summary",
    "href": "recipes/recipe-07/index.html#summary",
    "title": "07. Transforming and documenting data",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we have looked at an example of transforming a curated dataset. This recipe included operations such as:\n\nText normalization\nVariable recoding\nSplitting variables\n\nIn other projects, the transformation steps will inevitably differ, but these strategies are commonly necessary in almost any project.\nJust as with other steps in the data preparation process, it is important to document the transformation steps. This will help you and others understand the process and the resulting dataset(s)."
  },
  {
    "objectID": "recipes/recipe-07/index.html#check-your-understanding",
    "href": "recipes/recipe-07/index.html#check-your-understanding",
    "title": "07. Transforming and documenting data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nWhich function would you use to remove duplicate rows in a dataset? group_by()mutate()distinct()filter()\nTRUEFALSE The str_c() function from {stringr} is used to separate strings rather than combine them.\nTRUEFALSE The count() function from {dplyr} is used to tabulate the values of a variable.\nIf you want to recode the age of learners into categories such as “child”, “teen”, and “adult” based on their age, which function should you use? mutate()case_when()unite()separate_wider_delim()\nTo normalize text by removing leading and trailing whitespace, you use the () function from {stringr}.\nTo normalize text by converting all characters to lowercase, you use the () function from {stringr}."
  },
  {
    "objectID": "recipes/recipe-07/index.html#lab-preparation",
    "href": "recipes/recipe-07/index.html#lab-preparation",
    "title": "07. Transforming and documenting data",
    "section": "Lab preparation",
    "text": "Lab preparation\n\nIn preparation for Lab 7, review and ensure you are comfortable with the following:\n\nVector, data frame, and list data structures\nSubsetting and filtering data structures with and without regular expressions\nReshaping datasets by rows and columns\n\nIn this lab, we will practice these skills and expand our knowledge of data preparation by transforming and documenting data with Tidyverse packages such as {dplyr}, {tidyr}, and {stringr}.\nYou will have a choice of a dataset to transform. Before you start the lab, you should consider which dataset you would like to use, what the idealized structure the transformed dataset will take, and what strategies you will likely employ to transform the dataset. You should also consider the information you need to document the data transformation process."
  },
  {
    "objectID": "recipes/recipe-09/index.html",
    "href": "recipes/recipe-09/index.html",
    "title": "09. Building predictive models",
    "section": "",
    "text": "Skills\n\nHow to identify variables of interest\nHow to inspect datasets\nHow to interrogate datasets and iteratively develop a model to improve performance\nHow to interpret results of predictive models\nThe workflow for building a predictive model is shown in Table 1. Note that Step 6 includes an optional step to iterate on the model to improve performance. This is optional because it is not always necessary to iterate on the model. However, it is often the case that the first model you build is not the best model. So it is good to be prepared to iterate on the model.\nLet’s get started by loading some of the key packages we will use in this recipe.\nlibrary(tidymodels) # for modeling\nlibrary(textrecipes) # for text preprocessing\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation\nlibrary(stringr) # for string manipulation\nlibrary(tidytext) # for text manipulation\nlibrary(ggplot2) # for visualization\nlibrary(janitor) # for tabyl()\n\ntidymodels_prefer() # avoid function name conflicts"
  },
  {
    "objectID": "recipes/recipe-09/index.html#concepts-and-strategies",
    "href": "recipes/recipe-09/index.html#concepts-and-strategies",
    "title": "09. Building predictive models",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\n\nWe will use the ACTIV-ES corpus to build a predictive model that can classify text as one of three dialects of Spanish: Argentinian, Mexican, or Spanish. We will frame this as a supervised learning problem, where we have a set of texts that have been labeled with the dialect of Spanish that they are written in. In contrast to the classification task in the chapter, which was binary, this is a multiclass classification task, where we are trying to classify each document as one of three classes.\n\nLet’s preview the structure of the ACTIVES dataset.\n\naes_df\n\n# # A tibble: 430 × 3\n#    doc_id variety   text                                                        \n#     &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;                                                       \n#  1 199500 Argentina No está , señora . Aquí tampoco . No aparece , señora . ¿ D…\n#  2 184782 Argentina ALGUIEN AL TELÉFONO . LA ANGUSTIA . Ah , no , no , no mi hi…\n#  3  47823 Argentina Habrá que cumplir su última voluntad , ¿ el medallón ? Lo v…\n#  4 282622 Argentina Sucedió en Hualfin Esta es la historia de tres generaciones…\n#  5  62433 Argentina 10 secuestros en 10 días ! Y no hay el menor índice . Bueno…\n#  6  70250 Argentina Y preguntada que fue sí reconocen el cadáver exhumado ... y…\n#  7  71897 Argentina ¡ Jeremías ! ¡ Jeremías ! ¡ No dejés parir a tu mujer ! Sei…\n#  8 333883 Argentina Usted . Usted que frecuenta el éxito como una costumbre más…\n#  9 333954 Argentina Miles de campanas nos traen , a través de los siglos , el t…\n# 10 175243 Argentina Y ? Enseguida viene , fue al baño . Bueno , pero la mesa la…\n# # ℹ 420 more rows\n\n\nThis dataset contains 430 documents, each of which is labeled with the variety of Spanish that it is written in and the text of the document. A document ID is also included, which we will be able to use to index the documents. The variety vector is a factor. As this will be the outcome variable in our predictive model, this is good as most predictive models require classification variables to be factors.\nLet’s get a sense of the distribution of the variety variable.\n\naes_df |&gt;\n  tabyl(variety) |&gt;\n  adorn_pct_formatting(digits = 1)\n\n   variety   n percent\n Argentina 128   29.8%\n    Mexico 119   27.7%\n     Spain 183   42.6%\n\n\nWe can see that the dataset is somewhat balanced, with Peninsular Spanish comprising the larger portion of the texts.\n\n\nAnalysis\nAt this point we can start to approach building a predictive model that can distinguish between the Spanish varieties using the text. We will first start by applying steps 1 and 2 of the workflow. We will then apply steps 3-5 iteratively to build, evaluate, and improve the model, as necessary, before applying it to the test data to assess and interpret the results.\nLet’s go ahead and perform steps 1 and 2. To split the data into training and testing sets, we will use {rsample}. The initial_split() function, sets up the splits and we use variety as the stratification variable to ensure that the training and testing sets are representative of the distribution of the outcome variable. As this process is random, we will set the seed for reproducibility. Finally, we call the training() and testing() functions to extract the training and testing sets.\n\n# Set the seed for reproducibility\nset.seed(1234)\n\n# Split the data into training and testing sets\naes_split &lt;-\n  initial_split(\n    data = aes_df,\n    prop = 0.8,\n    strata = variety\n  )\n\naes_train &lt;- training(aes_split) # extract the training set\naes_test &lt;- testing(aes_split) # extract the testing set\n\nWe will then set the base recipe which formally identifies the relationship between the predictor and outcome variables. The recipe() function from {recipes} is used to create a recipe.\n\n# Set the base recipe\naes_base_rec &lt;-\n  recipe(\n    formula = variety ~ text,\n    data = aes_train\n  )\n\nWe now have steps 1 and 2 of the workflow completed. We have identified the variables of interest and split the data into training and testing sets.\nOne more thing we will do here is to set up the cross-validation folds. Every time we fit a model to the training data, we will want to evaluate the model’s performance on the training data. However, we don’t want to do this in a way that is biased –testing the model on the same data that it was trained on! For this reason, we will use cross-validation to split the training data into multiple training and validation sets which represent different splits of the training data.\nWe will use the vfold_cv() function from {rsample} to set up the cross-validation folds. We will use 10 folds, which is a common number of folds to use. We will also use the strata argument to ensure that the folds are representative of the distribution of the outcome variable.\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Set up the cross-validation folds\ncv_folds &lt;-\n  vfold_cv(\n    data = aes_train,\n    v = 10,\n    strata = variety\n  )\n\nWith these common steps completed, we can now apply and reapply steps 3-5 of the workflow to build, evaluate, and improve the model.\n\nApproach 1\nIn our first approach, let’s start simple by using words as features and apply a logistic regression model. We won’t be completely naive, however, as we are familiar with the undo influence of the most frequent words. To address this, we will apply a term frequency-inverse document frequency (TF-IDF) transformation to the text in order to downweight the influence of the most frequent words and promote words that are more indicative of each class. Furthermore, we know that we will want to use a regularized regression model to avoid overfitting to particular words.\nTo get started, we will use {textrecipes} to add steps to our aes_base_rec recipe to preprocess the text. We will use the step_tokenize() function to tokenize the text. This tokenization process will likely result in a very large number of terms, most of which will not be informative and will add computational overhead. We will want to restrict the number of terms with the step_tokenfilter() function. However, it is not clear how many terms we should restrict the tokens to. For now, we will start with 1,000 tokens, but we will likely want to revisit this later. We will also use the step_tfidf() function to apply a TF-IDF transformation to the text setting smooth_idf to FALSE.\n\n# Add preprocessing steps to the recipe\naes_rec &lt;-\n  aes_base_rec |&gt;\n  step_tokenize(text) |&gt;\n  step_tokenfilter(text, max_tokens = 1000) |&gt;\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Preview the recipe\naes_rec\n\nTo implement the recipe and to preview the text preprocessing steps we apply the prep() and bake() functions.\n\naes_bake &lt;-\n  aes_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# Preview\ndim(aes_bake)\n\n[1]  343 1001\n\naes_bake[1:5, 1:5]\n\n# A tibble: 5 × 5\n  variety   tfidf_text_1 tfidf_text_10 tfidf_text_15 tfidf_text_2\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Argentina     0.000206     0.000599       0.000402      0      \n2 Argentina     0            0              0             0      \n3 Argentina     0            0.0000887      0             0      \n4 Argentina     0.00437      0              0.00142       0.00381\n5 Argentina     0            0.00155        0             0.00124\n\n\nWe now have a recipe that will tokenize the text, restrict the tokens to the most common 1,000 tokens, and create a TF-IDF matrix.\nIt is not a bad idea to inspect the features at this point to make sure that the preprocessing steps have been applied correctly and to gauge what this feature selection looks like so that when it comes time to interpret the model, we have a sense of what the model is doing.\nAs TF-IDF is going to be the main feature in our model, let’s visualize the top 20 terms by class. To do this, we will use {dplyr} to get the median TF-IDF score for each word by class, convert the data from wide to long format using the pivot_longer() function, and then use {ggplot2} to visualize the data.\n\n# Sum the term frequencies by class\nclass_freq_wide &lt;-\n  aes_bake |&gt;\n  group_by(variety) |&gt;\n  summarize(\n    across(\n      starts_with(\"tfidf_\"),\n      median\n    )\n  ) |&gt;\n  ungroup()\n\n# Convert the data from wide to long format\nclass_freq_long &lt;-\n  class_freq_wide |&gt;\n  pivot_longer(\n    cols = starts_with(\"tfidf_\"),\n    names_to = \"term\",\n    values_to = \"tfidf\"\n  ) |&gt;\n  mutate(term = str_remove(term, \"tfidf_text_\"))\n\n# Visualize the top 20 terms by class\nclass_freq_long |&gt;\n  slice_max(n = 20, order_by = tfidf, by = variety) |&gt;\n  mutate(term = reorder_within(term, tfidf, variety)) |&gt;\n  ggplot(aes(x = term, y = tfidf)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~variety, scales = \"free_y\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 1: Top 20 terms by class\n\n\n\n\n\nIn Figure 1, we see the words that are most indicative of each language variety. If you are familiar with Spanish, you can probably detect some variety-specific terms. For example, “vos” is a pronoun used in Argentinian Spanish and “os” is a pronoun used in Peninsular Spanish. There is also some overlap between the varieties, such as “tienes” and “te”.\nAnother point to note is the difference in magnitude of the TF-IDF scores between the Argentinian and other varieties. This suggests that the Argentinian variety is more distinct from the other varieties than the other varieties are from each other. Among the most distinctive terms are verbal forms that are specific to Argentinian Spanish, such as “tenés” and “sos”.\nNow let’s create a model specification. We will use the multinom_reg() function from {parsnip} to create a multinomial logistic regression model, as we have multiple classes in our prediction task. We will use the “glmnet” engine, which will allow us to apply regularization to the model, arbitrarily set to 0.01. We will use the set_engine() function to set the engine and the set_mode() function to set the mode to “classification”.\n\n# Create a model specification\naes_spec &lt;-\n  multinom_reg(\n    penalty = 0.01,\n    mixture = 1\n  ) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nTo combine the recipe and the model specification, we will use {workflows}. We will use the workflow() function and pass add_recipe(aes_rec) and add_model(aes_spec) as arguments to add the recipe and the model specification to the workflow.\n\n# Create a workflow\naes_wf &lt;-\n  workflow() |&gt;\n  add_recipe(aes_rec) |&gt;\n  add_model(aes_spec)\n\nWe can now use the cross-validation folds that we set up earlier. We will use the fit_resamples() function to fit the model to the training data using the cross-validation folds.\n\n# Fit the model to the training data\naes_train_fit &lt;-\n  aes_wf |&gt;\n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nWe can now evaluate the model’s performance on the training data. We will use the collect_metrics() function to collect the metrics from the cross-validation folds.\n\n# Evaluate the model's performance on the training data\naes_train_fit |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.804    10  0.0210 Preprocessor1_Model1\n2 brier_class multiclass 0.147    10  0.0130 Preprocessor1_Model1\n3 roc_auc     hand_till  0.923    10  0.0109 Preprocessor1_Model1\n\n\nWe can see that the model has a mean accuracy of 80.4% and ROC-AUC of 14.7%. That pretty good for a first pass. To get a sense of how good (or bad) it is, let’s compare it to a baseline model.\nA baseline model is the simplest model that we can use to compare the performance of our model to. A common baseline model is a model that always predicts the most frequent class. In our case, this is Peninsular Spanish, which accounts for 42.6% of the data. So it is clear that our model is doing much better than a baseline model which will have an accuracy score of 42.6%.\nWe can visualize the correct and incorrect predictions using a confusion matrix. We will use the conf_mat_resampled() function from {yardstick} to create the confusion matrix and the autoplot() function from {ggfortify} to visualize it.\n\naes_train_fit |&gt;\n  conf_mat_resampled(tidy = FALSE) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nFigure 2: Confusion matrix for the model in Approach 1\n\n\n\n\n\nThe left-downward diagonal of the confusion matrix represents the average number of documents correctly predicted for the aggregated model across the cross-validation folds. Other cells represent the average number of documents incorrectly predicted for each class-class combination. You can read these by using the row label to identify the predicted class and the column label to identify the actual class. So, for example, the model predicted Mexico \\(n\\) times when the actual class was Argentina. And so on.\n\n\nApproach 2\nIn our first approach we applied a TF-IDF transformation to the text and used a regularized multinomial logistic regression model. We also restricted the tokens to the 1,000 most frequent tokens and arbitrarily set the regularization parameter to 0.01. This resulted in an aggregate accuracy score of 80.4% on the training data. This is a good start, but see if we can do better.\nIn this second approach, let’s try to improve the model by applying a more principled approach to feature and hyperparameter selection.\nTo do this we will ‘tune’ the max_tokens and penalty hyperparameters in our recipe and model specifications, respectively. We need to update our recipe and model specification to include placeholders for these parameters replacing the previous values with tune(). We will also need to update our workflow to include the updated recipe and model specification.\n\n# Update the recipe\naes_rec &lt;-\n  aes_base_rec |&gt;\n  step_tokenize(text) |&gt;\n  step_tokenfilter(text, max_tokens = tune()) |&gt; # adds placeholder\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Update the model specification\naes_spec &lt;-\n  multinom_reg(\n    penalty = tune(), # adds placeholder\n    mixture = 1\n  ) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nWe can now create a workflow that includes the recipe and the model specification.\n\n# Create a workflow\naes_wf &lt;-\n  workflow() |&gt;\n  add_recipe(aes_rec) |&gt;\n  add_model(aes_spec)\n\nNow we set up the range of values for both the max_tokens and penalty hyperparameters. The grid_regular() function from {dials} will allow us to specify a grid of values for each hyperparameter.\n\n# Set the hyperparameter grid\naes_grid &lt;-\n  grid_regular(\n    max_tokens(range = c(250, 2000)),\n    penalty(range = c(-3, -1)),\n    levels = c(max_tokens = 5, penalty = 10)\n  )\n\naes_grid\n\n# A tibble: 50 × 2\n   max_tokens penalty\n        &lt;int&gt;   &lt;dbl&gt;\n 1        250 0.001  \n 2        687 0.001  \n 3       1125 0.001  \n 4       1562 0.001  \n 5       2000 0.001  \n 6        250 0.00167\n 7        687 0.00167\n 8       1125 0.00167\n 9       1562 0.00167\n10       2000 0.00167\n# ℹ 40 more rows\n\n\nThe range = argument specifies the range of values to include in the grid. For max_tokens, this is straightforward. For penalty, we are specifying the range of values on the log scale. So the range of values is 0.001 to 0.1. The levels argument specifies the number of values to include in the grid. In this case, we will include 5 values for max_tokens and 10 values for penalty. This will result in 50 combinations of hyperparameter values.\nWe will then pass our aes_wf workflow to the tune_grid() function with the grid values we specified to tune the hyperparameters.\n\n# Tune the hyperparameters\naes_tune &lt;-\n  aes_wf |&gt;\n  tune_grid(\n    resamples = cv_folds,\n    grid = aes_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nThe aes_grid object is a tibble which contains the grid all the combinations of hyperparameter values. In this case, there are 50 combinations. That means we are going to fit 50 models to the training data! This is a lot of models, but it is worth it to get a more robust estimate of the model’s performance.\nWe can use the collect_metrics() function to collect the metrics from the cross-validation folds for each of our tuning parameters, but this will result in a lot of output. Instead, we can use the autoplot() function to visualize the metrics.\n\n# Plot the collected metrics\n\naes_tune |&gt; autoplot()\n\n\n\n\n\n\n\nFigure 3: Metrics for model tuning in Approach 2\n\n\n\n\n\nWe see some variation across the folds in the accuracy and ROC-AUC scores. This will help us make a more informed decision about which hyperparameters to use.\nThe metric to use to select the best model is something to consider.Accuracy is an important measure, but does not tell the whole story. In particular, accuracy does not tell us how well the model is doing for each class –only the overall correct and incorrect predictions. To get a better sense of how the model is doing across the classes, we can pay attention to the ROC-AUC score. The ROC-AUC score is a measure of the area under the receiver operating characteristic (ROC) curve. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) for each class at different probability thresholds. This measure is useful because it is not affected by class imbalance.\nLet’s select the best model based on the ROC-AUC score.\n\n# Get the best model\naes_tune_best &lt;-\n  aes_tune |&gt;\n  select_best(metric = \"roc_auc\")\n\naes_tune_best\n\n# A tibble: 1 × 3\n  penalty max_tokens .config              \n    &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1  0.0215       1562 Preprocessor4_Model07\n\n\nWe can now update our workflow with the best hyperparameters.\n\n# Update the workflow\naes_wf &lt;-\n  aes_wf |&gt;\n  finalize_workflow(aes_tune_best)\n\naes_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.0215443469003188\n  mixture = 1\n\nComputational engine: glmnet \n\n\nWe can now see that the updated workflow will replace the tune() placeholders with the best hyperparameters we selected.\nLet’s again perform a resampled fit on the training data using our new tuned model and then compare our results with the previous, abritrarily tuned model.\n\n# Fit the model to the training data\naes_train_fit &lt;-\n  aes_wf |&gt;\n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Evaluate the model's performance on the training data\naes_train_fit |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.819    10  0.0135 Preprocessor1_Model1\n2 brier_class multiclass 0.130    10  0.0101 Preprocessor1_Model1\n3 roc_auc     hand_till  0.938    10  0.0104 Preprocessor1_Model1\n\n\nThe accuracy score has improved just a bit, from an aggregate score of 80.4% to 81.9%.\nIn all likelihood, we would want to continue to iterate on this model, applying different feature selection and engineering procedures, different models, and different hyperparameters –I will consider some suggestions in the next section. However, for the sake of time, we will stop here and train our final model on the training data and then apply the model to the test data to assess and interpret the results.\n\nInterpreting the model\nAt this stage we are ready to interpret the model. We first fit the model to the training data, then apply the model to the test data, and evaluate the model’s performance on the test data. Finally, we will dig into the model to interpret the importance of the features to help us understand what the model is doing and what it can tell us about words that are indicative, or not, of each variety.\nLet’s fit our final model to the training data and evaluate it on the testing data using the last_fit() function which takes our updated workflow and the original split we created earlier which is stored in aes_split.\n\n# Fit the final model\naes_final_fit &lt;- last_fit(aes_wf, aes_split)\n\nWe can now collect the performance metrics from the testing data.\n\n# Get the performance metrics\naes_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass     0.736 Preprocessor1_Model1\n2 roc_auc     hand_till      0.872 Preprocessor1_Model1\n3 brier_class multiclass     0.180 Preprocessor1_Model1\n\n\nThe accuracy of this model on the test data is 73.6%. This is lower than the accuracy on the training data. Should we be surprised? Not really. The model was trained on the training data, so it is not surprising that it would perform better on the training data than the test data, despite the fact that we used cross-validation to evaluate the model on the training data. This is a good reminder that the model is not perfect and that we should not expect it to be.\nWhat does the ‘kap’ metric mean? The Kappa statistic is a measure of agreement between the predicted and actual classes. It is a measure of agreement that is corrected for the possibility that some correct prediction may have occurred by chance. The kappa statistic ranges from 0 to 1, with 0 indicating no agreement above chance and 1 indicating perfect agreement. In this case, the kappa statistic is 87.2%, which indicates that there is a moderate amount of agreement between the predicted and actual classes.\nLet’s explore if there is a difference in performance across the classes. To do this, we will use the conf_mat() function from {yardstick} to create the confusion matrix and the autoplot() function from {ggfortify} to visualize it.\n\naes_final_fit |&gt;\n  collect_predictions() |&gt;\n  conf_mat(truth = variety, estimate = .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nFigure 4: Confusion matrix for the model in Approach 2\n\n\n\n\n\nWe can see that the model is doing a good job of predicting Peninsular Spanish, but is not doing as well with the other varieties. This is not surprising given that Peninsular Spanish is the most frequent class in the data. This is a good reminder that accuracy is not the only metric to consider when evaluating a model. We can get a better sense of how the model is doing across the classes by looking at the ROC-AUC score.\n\n# Get the ROC-AUC score\naes_final_fit |&gt;\n  collect_predictions() |&gt;\n  roc_curve(truth = variety, .pred_Argentina:.pred_Spain) |&gt;\n  autoplot()\n\n\n\n\n\n\n\nFigure 5: ROC plot for the final fitted model\n\n\n\n\n\nTaken together, we have a decent model that can predict the variety of Spanish that a text is written in. We can also see that although prediction accuracy appears higher for Peninsular Spanish, the ROC-AUC curves suggest that the model is doing a better job of predicting the other varieties based on the features.\nThere is still room for improvement –as we recognized earlier. However, it is important that we do not start to use the testing data to improve the model. The testing data should only be used to evaluate the model. If we start to use the testing data to improve the model, we will no longer have an unbiased estimate of the model’s performance.\nLet’s now dig into our model’s features to explore what words are driving the model’s predictions. The approach to do this will depend on the model. In this case, we used a multinomial logistic regression model, which is a linear model. This means that we can interpret the model’s coefficients to understand the importance of the features. Coefficients that are positive indicate that the feature is associated with the reference class and coefficients that are negative indicate that the feature is associated with the non-reference class. For classification tasks with two classes, this is straightforward to interpret.\nThe issue here, however, is that we have more than two classes (i.e., Argentina, Mexico, and Spain). In these cases, the coefficients estimates for each class need to be extracted and standardized to be compared across classes.\nWe can do this using the extract_fit_parsnip() function from {parsnip}. This will extract the model object from the workflow object. The tidy() function from {broom} will then organize the coefficients (log-odds) for each predictor terms for each outcome class. We can then use the filter() function from {dplyr} to remove the intercept term and the mutate() function from {dplyr} to remove the “tfidf_text_” prefix from the term names so that they are more legible.\n\n# Get the coefficients\naes_coefs &lt;-\n  aes_final_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy() |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(term = str_remove(term, \"tfidf_text_\"))\n\nslice_sample(aes_coefs, n = 10)\n\n# A tibble: 10 × 4\n   class     term      estimate penalty\n   &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1 Mexico    vengó            0  0.0215\n 2 Mexico    tú               0  0.0215\n 3 Mexico    tin              0  0.0215\n 4 Mexico    papel            0  0.0215\n 5 Spain     están            0  0.0215\n 6 Argentina perdido          0  0.0215\n 7 Argentina caballero        0  0.0215\n 8 Spain     señora           0  0.0215\n 9 Argentina casar            0  0.0215\n10 Spain     serás            0  0.0215\n\n\nNow to standardize the log-odds coefficients so that they are comparable across the classes, we will use the scale() function from base R to transform the coeffients such that each class has a mean of 0 and a standard deviation of 1. scale() returns a matrix, so we will use the as.vector() function to convert the matrix to a vector.\n\naes_coefs_z &lt;-\n  aes_coefs |&gt;\n  group_by(class) |&gt;\n  mutate(z_score = as.vector(scale(estimate))) |&gt;\n  ungroup()\n\nslice_sample(aes_coefs_z, n = 10)\n\n# A tibble: 10 × 5\n   class     term     estimate penalty z_score\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Argentina iba             0  0.0215 -0.0857\n 2 Spain     pudo            0  0.0215 -0.0320\n 3 Spain     pará            0  0.0215 -0.0320\n 4 Argentina dicho           0  0.0215 -0.0857\n 5 Argentina sirve           0  0.0215 -0.0857\n 6 Mexico    cuesta          0  0.0215 -0.0787\n 7 Spain     demonios        0  0.0215 -0.0320\n 8 Argentina punto           0  0.0215 -0.0857\n 9 Mexico    también         0  0.0215 -0.0787\n10 Spain     cabeza          0  0.0215 -0.0320\n\n\nFinally, let’s visualize the top 25 terms by class. Note that we are using the reorder_within() and scale_x_reordered() functions from {tidytext} to reorder the terms in such a way that our facets allow for distinct terms on the x-axis for each class. Then the coord_flip() function from {ggplot2} is used to flip the axes for easier reading.\n\naes_coefs_z |&gt;\n  mutate(term = reorder_within(term, z_score, class)) |&gt;\n  slice_max(n = 25, order_by = z_score, by = class) |&gt;\n  ggplot(aes(x = term, y = z_score)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~class, scales = \"free_y\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 6: Top 25 terms by class\n\n\n\n\n\nWe can assess the distinct features for each class and also gauge the magnitude of the estimates. We should be cautious, however, as these terms are derived from our model that only performs moderately well.\n\n\n\nOther approaches\nAs we have seen, there are many decisions to make when building a predictive model. We have only scratched the surface of the options available. In this section, I will briefly consider some other approaches that may be of interest.\nFeatures:\nWe used words in this recipe and in the chapter classification task. This is merely in order to keep the focus on the process of building a predictive model. There are many other features that could be used. For example, we could use n-grams, character n-grams, or word embeddings. {textrecipes} provides many options for text preprocessing and feature engineering.\nLet’s look at how we can derive other linguistic units using {textrecipes}. First, let’s set up a simple dataset and base recipe.\n\ndf &lt;- tibble(\n  outcome = factor(c(\"a\", \"a\", \"b\", \"b\")),\n  date = as.Date(c(\"2020-01-01\", \"2021-06-14\", \"2020-11-05\", \"2023-12-25\")),\n  text = c(\n    \"This is a fantastic sentence.\",\n    \"This is another great sentence.\",\n    \"This is a third, boring sentence.\",\n    \"This is a fourth and final sentence.\"\n  )\n)\n\nbase_rec &lt;- recipe(outcome ~ text, data = df)\n\nNow, say instead of words, we were interested in deriving word \\(n\\)-grams as our terms. We again use the step_tokenize() function in our recipe. This time, however, we add a value to the token = argument. In this case, we will use “ngrams”. {textrecipes} uses the tokenization engine from {tokenizers}, so the types of tokenization available are the same as those available (see help(tokenizers) for more information).\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\", # word n-grams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"this is a\"            \"is a fantastic\"       \"a fantastic sentence\"\n\n[[2]]\n[1] \"this is another\"        \"is another great\"       \"another great sentence\"\n\n[[3]]\n[1] \"this is a\"             \"is a third\"            \"a third boring\"       \n[4] \"third boring sentence\"\n\n[[4]]\n[1] \"this is a\"          \"is a fourth\"        \"a fourth and\"      \n[4] \"fourth and final\"   \"and final sentence\"\n\n\nBy default tokens = \"ngrams\" produces trigrams.\nAnother option is to use character n-grams. This is useful when we want to capture information about the morphology of the words. For character n-grams, we can use “character_shingle”.\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"character_shingle\" # character n-grams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afa\" \"fan\" \"ant\" \"nta\" \"tas\" \"ast\"\n[13] \"sti\" \"tic\" \"ics\" \"cse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[2]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"san\" \"ano\" \"not\" \"oth\" \"the\" \"her\" \"erg\"\n[13] \"rgr\" \"gre\" \"rea\" \"eat\" \"ats\" \"tse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[3]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"sat\" \"ath\" \"thi\" \"hir\" \"ird\" \"rdb\" \"dbo\"\n[13] \"bor\" \"ori\" \"rin\" \"ing\" \"ngs\" \"gse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[4]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afo\" \"fou\" \"our\" \"urt\" \"rth\" \"tha\"\n[13] \"han\" \"and\" \"ndf\" \"dfi\" \"fin\" \"ina\" \"nal\" \"als\" \"lse\" \"sen\" \"ent\" \"nte\"\n[25] \"ten\" \"enc\" \"nce\"\n\n\nBy default tokens = \"character_shingle\" also produces trigrams.\nNow, say we want to change the number of words in each n-gram or character n-gram. We can do this using the options = argument. This is where we pass tokenizer-specific options. For example, to change the number of words in each n-gram, we can use the n = argument.\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2) # word bigrams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"this is\"            \"is a\"               \"a fantastic\"       \n[4] \"fantastic sentence\"\n\n[[2]]\n[1] \"this is\"        \"is another\"     \"another great\"  \"great sentence\"\n\n[[3]]\n[1] \"this is\"         \"is a\"            \"a third\"         \"third boring\"   \n[5] \"boring sentence\"\n\n[[4]]\n[1] \"this is\"        \"is a\"           \"a fourth\"       \"fourth and\"    \n[5] \"and final\"      \"final sentence\"\n\n\nIf you would like to calculate multiple \\(n\\)-gram windows, you can pass the n_min = argument.\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2, n_min = 1) # word unigrams and bigrams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"this\"               \"this is\"            \"is\"                \n[4] \"is a\"               \"a\"                  \"a fantastic\"       \n[7] \"fantastic\"          \"fantastic sentence\" \"sentence\"          \n\n[[2]]\n[1] \"this\"           \"this is\"        \"is\"             \"is another\"    \n[5] \"another\"        \"another great\"  \"great\"          \"great sentence\"\n[9] \"sentence\"      \n\n[[3]]\n [1] \"this\"            \"this is\"         \"is\"              \"is a\"           \n [5] \"a\"               \"a third\"         \"third\"           \"third boring\"   \n [9] \"boring\"          \"boring sentence\" \"sentence\"       \n\n[[4]]\n [1] \"this\"           \"this is\"        \"is\"             \"is a\"          \n [5] \"a\"              \"a fourth\"       \"fourth\"         \"fourth and\"    \n [9] \"and\"            \"and final\"      \"final\"          \"final sentence\"\n[13] \"sentence\"      \n\n\nNames and values of the arguments that options = will take will depend on the type of tokenization specified.\nWe could also use metadata, such as the year the text was written, the author, the genre, etc. In these cases, will will update our base recipe to include the metadata as predictors and then we can use the necessary preprocessing steps to prepare the metadata for modeling using functions from the recipes() package (i.e., `step_normalize(), step_dummy(), etc.).\n\nbase_rec &lt;- recipe(outcome ~ date + text, data = df) # add date\n\nbase_rec |&gt;\n  step_tokenize(text) |&gt;\n  step_date(date, features = c(\"year\")) |&gt; # extract the year\n  prep() |&gt;\n  juice()\n\n# A tibble: 4 × 4\n  date             text outcome date_year\n  &lt;date&gt;      &lt;tknlist&gt; &lt;fct&gt;       &lt;int&gt;\n1 2020-01-01 [5 tokens] a            2020\n2 2021-06-14 [5 tokens] a            2021\n3 2020-11-05 [6 tokens] b            2020\n4 2023-12-25 [7 tokens] b            2023\n\n\nWe could also use other features derived from the text, such as word length, syntactic complexity, sentiment, readability, etc. A number of stylistic features are available using the step_textfeature() function, some 26 (see ?count_functions). However, it is also possible to derive your own features working with the original dataset and then adding the features\n\ndf &lt;-\n  df |&gt;\n  left_join(\n    # Calculate word count and average word length\n    df |&gt;\n      unnest_tokens(word, text, drop = FALSE) |&gt;\n      group_by(text) |&gt;\n      summarize(\n        word_count = n(),\n        avg_word_length = mean(nchar(word))\n      )\n  )\n\nrecipe(\n  outcome ~ ., # use all variables\n  data = df\n) |&gt;\n  step_tokenize(text) |&gt;\n  step_tf(text) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 4 × 16\n  date       word_count avg_word_length outcome tf_text_a tf_text_and\n  &lt;date&gt;          &lt;int&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;int&gt;       &lt;int&gt;\n1 2020-01-01          5            4.8  a               1           0\n2 2021-06-14          5            5.2  a               0           0\n3 2020-11-05          6            4.33 b               1           0\n4 2023-12-25          7            4.14 b               1           1\n# ℹ 10 more variables: tf_text_another &lt;int&gt;, tf_text_boring &lt;int&gt;,\n#   tf_text_fantastic &lt;int&gt;, tf_text_final &lt;int&gt;, tf_text_fourth &lt;int&gt;,\n#   tf_text_great &lt;int&gt;, tf_text_is &lt;int&gt;, tf_text_sentence &lt;int&gt;,\n#   tf_text_third &lt;int&gt;, tf_text_this &lt;int&gt;\n\n\nModels:\nA big advantage to using the {tidymodels} approach to modeling is that it allows us to easily try different models. We have used a multinomial logistic regression model in this recipe, but we could also try other models, such as a random forest model, a support vector machine, or a neural network. We can do this by simply changing the model specification in our workflow.\nFor example, we could use a random forest model. We would first need to update our model specification to use the rand_forest() function from {parsnip} to create a random forest model. We would also need to update the engine to use {ranger}, which is a fast implementation of random forest models. Finally, we would need to update the mode to “classification”.\n\n# Create a model specification\naes_spec &lt;-\n  # Random Forest\n  rand_forest(\n    mtry = 10,\n    trees = 1000\n  ) |&gt;\n  set_engine(\"ranger\") |&gt; # use the ranger engine\n  set_mode(\"classification\")\n\nIt is important to understand that different models have different hyperparameters. As we say with the logistic_reg() and multinom_reg() models, we can tune the penalty hyperparameter. However, this is not the case for all models. For example, the rand_forest() model does not have a penalty hyperparameter. Instead, it has a mtry hyperparameter, which is the number of variables to consider at each split. We can tune this hyperparameter in the same way that we tuned the penalty hyperparameter using tune(), grid_regular(), and tune_grid().\nOther models to consider for text classification include Naive Bayes, Support Vector Machines, and Neural Networks. The {tidymodels} framework supports all of these models.\nA last point to consider is whether we will want to be able to interpret the features that drive the model’s performance. If so, we will want to use a model that allows us to interpret the features. For example, we could use a linear model, such as a logistic regression model, or a tree-based model, such as a random forest model. However, we would not be able to interpret the features of a neural network model.\nFurthermore, the methods we use to interpret the features will depend on the model. For example, we can interpret the features of a linear model by looking at the coefficients. However, we cannot interpret the features of a random forest model in the same way. Instead, we can use the vip() function from {vip} to visualize the importance of the features."
  },
  {
    "objectID": "recipes/recipe-09/index.html#summary",
    "href": "recipes/recipe-09/index.html#summary",
    "title": "09. Building predictive models",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we’ve covered the foundational skills needed to construct a predictive (classification) model using the tidymodels framework. We examined the key steps in predictive modeling: identifying data, dividing it into training and test sets, preprocessing, iterative model training, and result interpretation.\nWe used a dataset of Spanish texts from three different varieties to demonstrate the process iterating over two approaches. In the first approach, we used a multinomial logistic regression model with TF-IDF features. In the second approach, we tuned the hyperparameters of the model and the preprocessing steps to improve the model’s performance. We also touched upon alternative methods, like incorporating other features such as n-grams and experimenting with other models such as random forests, which may prove useful in text classification tasks.\nWith the matierals in this chapter you should now have an understanding of how to build and understand a text classification model in R, equipped with insights to further develop your predictive analysis projects."
  },
  {
    "objectID": "recipes/recipe-09/index.html#check-your-understanding",
    "href": "recipes/recipe-09/index.html#check-your-understanding",
    "title": "09. Building predictive models",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE There are two basic types of prediction models: regression and classification.\nWhat is the purpose of splitting data into training and testing sets? To make computation fasterTo avoid overfitting the modelTo decrease the size of the datasetTo make the model simpler\nWhat is the purpose of cross-validation? To make computation fasterTo avoid overfitting the modelTo evaluate the model’s performanceTo make the model simpler\nWhich of the following models would not be appropriate for a classification task? Logistic regressionRandom forestSupport vector machineLinear regression\nIterative improvement in modeling involves: Changing the modelChanging the hyperparametersChanging the preprocessing stepsAll of the above\nTRUEFALSE Feature importance measures are uniform across models."
  },
  {
    "objectID": "recipes/recipe-09/index.html#lab-preparation",
    "href": "recipes/recipe-09/index.html#lab-preparation",
    "title": "09. Building predictive models",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn preparation for Lab 9, review and ensure that you are familiar with the following concepts:\n\nBuilding feature engineering pipelines with {recipes}\nBuilding model specifications with {parsnip}\nIterative model training, evaluation, and improvement with {workflows}, {tune}, and {yardstick}\n\nIn this lab, you will have an opportunity to apply these concepts to a new dataset and classification task. You should consider the dataset and the task in be performed in the lab and think about how you might approach the task from a feature engineering and model selection perspective. You will be asked to submit you code and a brief reflection on your approach and the results."
  },
  {
    "objectID": "recipes/recipe-09/index.html#references",
    "href": "recipes/recipe-09/index.html#references",
    "title": "09. Building predictive models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "recipes/recipe-05/index.html",
    "href": "recipes/recipe-05/index.html",
    "title": "05. Collecting and documenting data",
    "section": "",
    "text": "Skills\n\nFinding data sources\nData collection strategies\nData documentation"
  },
  {
    "objectID": "recipes/recipe-05/index.html#concepts-and-strategies",
    "href": "recipes/recipe-05/index.html#concepts-and-strategies",
    "title": "05. Collecting and documenting data",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nFinding data sources\nTo find data sources, it is best to have a research question in mind. This will help to narrow the search for data sources. However, finding data sources can also be a good way to generate research questions. In either case, it takes some sleuthing to find data sources that will work for your research question. In addition, to the data source itself, you will also need to consider the permissions and licensing of the data source. It is best to consider these early in the process to avoid surprises later. Finally, you will also need to consider the data format and how it will be used in the analysis. It can be the case that a data source seems ideal, but the data format is not conducive to the analysis you would like to do.\n\n\n\n\n\n\n Tip\nConsult the Identifying data and data sources guide for some ideas on where to find data sources.\n\n\n\nIn this recipe, we will consider some hypothetical research aimed at exploring potential similarities and differences in the lexical, syntactic, and/ or stylistic features between American and English literature during the mid 19th century.\n\n\n\n\n\n\n Dive deeper\nIf you are interested in understanding a literary analysis perspective to text analysis, I highly recommend Matthew Jockers’ book Text Analysis with R for Students of Literature (Jockers 2014). This book is a great resource for understanding how to apply text analysis to literary analysis.\n\n\n\nProject Gutenberg is a great source of data for this research question. Project Gutenberg is a volunteer effort to digitize and archive cultural works. The great majority of the works in the Project Gutenberg database are in the public domain in the United States. This means that the works can be freely used and shared.\nFurthermore, {gutenbergr} provides an API for accessing the Project Gutenberg database. This means that we can use R to access the Project Gutenberg database and download the text and metadata for the works we are interested in. {gutenbergr} also provides a number of data frames that can help us to identify the works we are interested in.\n\n\nData collection strategy\nLet’s now turn to the data collection strategy. There are a number of data collection strategies that can be used to acquire data for a text analysis project. In the chapter, we covered manual and programmatic downloads and APIs. Here we will use an R package which will provide an API for accessing the data source.\n\n\n\n\n\n\n Dive deeper\nIf you are interested in learning about another data collection strategy, web scraping, I suggest you look at the Web scraping with R guide.\n\n\n\nWe will load {dplyr}, {readr}, and {gutenbergr} to prepare for the data collection process.\n\n# Load packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(gutenbergr)\n\nThe main workhorse of {gutenbergr} is the gutenberg_download(). It’s only required argument is the id(s) used by Project Gutenberg to index all of the works in their database. This function will then download the text of the work(s) and return a data frame with the gutenberg id and the text of the work(s).\nSo how do we find the gutenberg ids? The manual method is to go to the Project Gutenberg website and search for the work you are interested in. For example, let’s say we are interested in the work “A Tale of Two Cities” by Charles Dickens. We can search for this work on the Project Gutenberg website and then click on the link to the work. The url for this work is: https://www.gutenberg.org/ebooks/98. The gutenberg id is the number at the end of the url, in this case 98.\nThis will work for individual works, but why wouldn’t we just download the text from the Project Gutenberg website? For the works on Project Gutenberg this would be perfectly fine. We can share the text with others as the license for the works on Project Gutenberg are in the public domain.\nHowever, what if are interested in downloading multiple works? As the number of works increases, the time it takes to manually download each work increases. Furthermore, {gutenbergr} provides a number of additional attributes that can be downloaded and organized along side the text. Finally, the results of the gutenberg_download() function are returned as a data frame which can be easily manipulated and analyzed in R.\nIn our data acquisition plan, we want to collect works from a number of authors. So it will be best to leverage {gutenbergr} to download the works we are interested in. To do this we need to know the gutenberg ids for the works we are interested in.\nConveniently, {gutenbergr} also includes a number of data frames that contain meta data for the works in the Project Gutenberg database. These data frames include meta data for works in the Project Gutenberg database (gutenberg_metadata), authors (gutenberg_authors), and subjects (gutenberg_subjects).\nLet’s take a look at the structure of these data frames.\n\nglimpse(gutenberg_metadata)\n\nRows: 72,569\nColumns: 8\n$ gutenberg_id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;int&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_authors)\n\nRows: 23,980\nColumns: 7\n$ gutenberg_author_id &lt;int&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;int&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;int&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_subjects)\n\nRows: 231,741\nColumns: 3\n$ gutenberg_id &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\nFrom this overview, we can see that there are 72,569 works in the Project Gutenberg database. We can also see that there are 23,980 authors and 231,741 subjects.\nAs we dicussed, each work in the Project Gutenberg database has a gutenberg id. The gutenberg_id appears in the gutenberg_metadata and also in the gutenberg_subjects data frame. This common attribute means that a work with a particular gutenberg id can be linked to the subject(s) associated with that work. Another important attribute is is the gutenberg_author_id which links the work to the author(s) of that work. Yes, the author name is in the gutenberg_metadata data frame, but the gutenberg_author_id can be used to link the work to the gutenberg_authors data frame which contains additional information about authors.\n\n\n\n\n\n\n Tip\n{gutenbergr} is periodically updated. To check to see when each data frame was last updated run:\nattr(gutenberg_metadata, \"date_updated\")\n\n\n\nLet’s now describe a few more attributes that will be useful for our data acquisition plan. In the gutenberg_subjects data frame, we have subject_type and subject. The subject_type is the type of subject classification system used to classify the work. If you tabulate this column, you will see that there are two types of subject classification systems used: Library of Congress Classification (lcc) and Library of Congress Subject Headings (lcsh). The subject column contains the subject code for the work. For lsch the subject code is a descriptive character string and for lcc the subject code is an id as a character string that is a combination of letters (and numbers) that the Library of Congress uses to classify works.\nFor our data acquistion plan, we will use the lcc subject classification system to select works from the Library of Congress Classification for English Literature (PR) and American Literature (PS).\nIn the gutenberg_authors data frame, we have the birthdate and deathdate attributes. These attributes will be useful for filtering the authors that lived during the mid 19th century.\nWith this overview of {gutenbergr} and the data frames that it contains, we can now begin to develop our data acquisition plan.\n\nSelect the authors that lived during the mid 19th century from the gutenberg_authors data frame.\nSelect the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the gutenberg_subjects data frame.\nSelect works from gutenberg_metadata that are associated with the authors and subjects selected in steps 1 and 2.\nDownload the text and metadata for the works selected in step 3 using the gutenberg_download() function.\nWrite the data to disk in an appropriate format.\n\n\n\nData collection\nLet’s take each of these steps in turn. First, we need to select the authors that lived during the mid 19th century from the gutenberg_authors data frame. To do this we will use the filter() function. We will pass the gutenberg_authors data frame to the filter() function and then use the birthdate column to select the authors that were born after 1800 and died before 1880 –this year is chosen as the mid 19th century is generally considered to be the period from 1830 to 1870. We will then assign the result to the variable name authors.\n\nauthors &lt;-\n  gutenberg_authors |&gt;\n  filter(\n    birthdate &gt; 1800,\n    deathdate &lt; 1880\n  )\n\nThat’s it! We now have a data frame with the authors that lived during the mid 19th century, some 787 authors in total. This will span all subjects and languages, so this isn’t the final number of authors we will be working with.\nThe next step is to select the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the gutenberg_subjects data frame. To do this we will use the filter() function again. We will pass the gutenberg_subjects data frame to the filter() function and then use the subject_type and subject columns to select the works that are associated with the Library of Congress Classification for English Literature (PR) and American Literature (PS). We will then assign the result to the variable name subjects.\n\nsubjects &lt;-\n  gutenberg_subjects |&gt;\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n\nNow, we have a data frame with the subjects that we are interested in. Let’s inspect this data frame to see how many works we have for each subject.\n\nsubjects |&gt;\n  count(subject)\n\n# A tibble: 2 × 2\n  subject     n\n  &lt;chr&gt;   &lt;int&gt;\n1 PR       9926\n2 PS      10953\n\n\nThe next step is to subset the gutenberg_metadata data frame to select works from the authors and subjects selected in the previous steps. Again, we will use filter() to do this. We will pass the gutenberg_metadata data frame to the filter() function and then use the gutenberg_author_id and gutenberg_id columns to select the works that are associated with the authors and subjects selected in the previous steps. We will then assign the result to the variable name works.\n\nworks &lt;-\n  gutenberg_metadata |&gt;\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n\nworks\n\n# A tibble: 1,014 × 8\n   gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n          &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n 1           33 The Sca… Hawth…                  28 en       \"Harvard Classics/…\n 2           46 A Chris… Dicke…                  37 en       \"Children's Litera…\n 3           71 On the … Thore…                  54 en       \"\"                 \n 4           77 The Hou… Hawth…                  28 en       \"Best Books Ever L…\n 5           98 A Tale … Dicke…                  37 en       \"Historical Fictio…\n 6          205 Walden,… Thore…                  54 en       \"\"                 \n 7          258 Poems b… Gordo…                 145 en       \"\"                 \n 8          271 Black B… Sewel…                 154 en       \"Best Books Ever L…\n 9          292 Beauty … Taylo…                 167 en       \"\"                 \n10          394 Cranford Gaske…                 220 en       \"\"                 \n# ℹ 1,004 more rows\n# ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\n\nFiltering the gutenberg_metadata data frame by the authors and subjects selected in the previous steps, we now have a data frame with 1,014 works. This is the final number of works we will be working with so we can now download the text and metadata for these works using the gutenberg_download() function.\nA few things to note about the gutenberg_download() function. First, it is vectorized, that is, it can take a single value or multiple values for the argument gutenberg_id. This is good as we will be passing a vector of gutenberg ids to the function. A small fraction of the works on Project Gutenberg are not in the public domain and therefore cannot be downloaded, this is documented in the rights column. Furthermore, not all of the works have text available, as seen in the has_text column. Finally, the gutenberg_download() function returns a data frame with the gutenberg id and the text of the work(s) –but we can also select additional attributes to be returned by passing a character vector of the attribute names to the argument meta_fields. The column names of the gutenberg_metadata data frame contains the available attributes.\nWith this in mind, let’s do a quick test before we download all of the works. Let’s select the first 5 works from the works data frame that fit our criteria and then download the text and metadata for these works using the gutenberg_download() function. We will then assign the result to the variable name works_sample.\n\nworks_sample &lt;-\n  works |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  slice_head(n = 5) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  )\n\nworks_sample\n\n# A tibble: 34,385 × 6\n   gutenberg_id text        title author gutenberg_author_id gutenberg_bookshelf\n          &lt;int&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;              \n 1           33 \"The Scarl… The … Hawth…                  28 Harvard Classics/M…\n 2           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 3           33 \"by Nathan… The … Hawth…                  28 Harvard Classics/M…\n 4           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 5           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 6           33 \"Contents\"  The … Hawth…                  28 Harvard Classics/M…\n 7           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 8           33 \" THE CUST… The … Hawth…                  28 Harvard Classics/M…\n 9           33 \" THE SCAR… The … Hawth…                  28 Harvard Classics/M…\n10           33 \" I. THE P… The … Hawth…                  28 Harvard Classics/M…\n# ℹ 34,375 more rows\n\n\nLet’s inspect the works_sample data frame. First, from the output we can see that all of our meta data attributes were returned. Second, we can see that the text column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded, even blank lines. To make sure that we have the correct number of works, we can use the count() function to count the number of works by gutenberg_id.\n\nworks_sample |&gt;\n  count(gutenberg_id)\n\n# A tibble: 4 × 2\n  gutenberg_id     n\n         &lt;int&gt; &lt;int&gt;\n1           33  8212\n2          258 11050\n3          271  5997\n4          292  9126\n\n\nYes, we have 5 works and we can see how many lines are in each of these works.\nWe could now run this code on the entire works data frame and then write the data to disk like so:\nworks |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |&gt;\n  write_csv(file = \"data/original/gutenberg/works.csv\")\nThis would accomplish the primary goal of our data acquisition plan.\nHowever, there is some key functionality that we are missing if we would like to make this code more reproducible-friendly. First, we are not checking to see if the data already exists on disk. If we already have run this code in our script, we likely do not want to run it again. Second, we may want to use this code again with different parameters, for example, we may want to retrieve different subject codes, or different time periods, or other languages.\nAll three of these additional features can be accomplished with writing a custom function. Let’s take a look at the code we have written so far and see how we can turn this into a custom function.\n# Get authors within years\nauthors &lt;-\n  gutenberg_authors |&gt;\n  filter(\n    birthdate &gt; 1800,\n    deathdate &lt; 1880\n  )\n# Get LCC subjects\nsubjects &lt;-\n  gutenberg_subjects |&gt;\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n# Get works based on authors and subjects\nworks &lt;-\n  gutenberg_metadata |&gt;\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n# Download works\nworks |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |&gt;\n  write_csv(file = \"data/original/gutenberg/works.csv\")\n\nBuild the custom function\n\nNameArgumentsCode: commentsCode: packagesCode: data checkCode: authorsCode: subjectCode: worksCode: downloadCode: write\n\n\nLet’s start to create our function by creating a name and calling the function() function. We will name our function get_gutenberg_works().\nget_gutenberg_works &lt;- function() {\n\n}\n\n\nNow, we need to think of the arguments that we would like to pass to our function so they can be used to customize the data acquisition process. First, we want to check to see if the data already exists on disk. To do this we will need to pass the path to the data file to our function. We will name this argument target_file.\nget_gutenberg_works &lt;- function(target_file) {\n\n}\nNext, we want to pass the subject code that the works should be associated with. We will name this argument lcc_subject.\nget_gutenberg_works &lt;- function(target_file, lcc_subject) {\n\n}\nFinally, we want to pass the birth year and death year that the authors should be associated with. We will name these arguments birth_year and death_year.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n\n}\n\n\nWe now turn to the code. I like to start by creating comments to describe the steps inside the function before adding code.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe have some packages we want to make sure are installed and loaded. We will use the {pacman} package to do this. We will use the p_load() function to install and load the packages. We will pass the character vector of package names to the p_load() function.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe need to create the code to check if the data exists. We will use an if statement to do this. If the data does exist, we will print a message to the console that the data already exists and stop the function. If the data does not exist, we will create the directory structure and continue with the data acquisition process. I will use {fs} (Hester, Wickham, and Csárdi 2024) in this code so I will load the library at the top of the function.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nLet’s now add the code to get the authors within the years. We will now use the birth_year and death_year arguments to filter the gutenberg_authors data frame.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nUsing the lcc_subject argument, we will now filter the gutenberg_subjects data frame.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe will use the authors and subjects data frames to filter the gutenberg_metadata data frame as before.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe will now use the works data frame to download the text and metadata for the works using the gutenberg_download() function and assign it to results.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results &lt;-\n    works |&gt;\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |&gt;\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n}\n\n\nFinally, we will write the results data frame to disk using the write_csv() function and the target_file argument.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results &lt;-\n    works |&gt;\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |&gt;\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n  write_csv(results, file = target_file)\n}\n\n\n\n\n\nUsing the custom function\nWe now have a function, get_gutenberg_works(), that we can use to acquire works from Project Gutenberg for a given LCC code for authors that lived during a given time period. We now have a flexible function that we can use to acquire data.\nWe can add this function to the script in which we use it, or we can add it to a separate script and source it into any script in which we want to use it.\n# Source function\nsource(\"get_gutenberg_works.R\")\n\n# Get works for PR and PS for authors born between 1800 and 1880\nget_gutenberg_works(\n  target_file = \"data/original/gutenberg/works.csv\",\n  lcc_subject = c(\"PR\", \"PS\"),\n  birth_year = 1800,\n  death_year = 1880\n)\nAnother option is to add this function to your own package. This is a great option if you plan to use this function in multiple projects or share it with others. Since I have already created a package for this book, {qtkit}, I’ve added this function, with some additional functionality, to the package.\n\n# Load package\nlibrary(qtalrkit)\n\n# Get works for fiction for authors born between 1870 and 1920\nget_gutenberg_works(\n  target_dir = \"data/original/gutenberg/\",\n  lcc_subject = \"PZ\",\n  birth_year = 1870,\n  death_year = 1920\n)\nThis modified function will create a directory structure for the data file if it does not already exist. It will also create a file name for the data file based on the arguments passed to the function.\n\n\n\nData documentation\nFinding data sources and collecting data are important steps in the acquisition process. However, it is also important to document the data collection process. This is important so that you, and others, can reproduce the data collection process.\nIn data acquisition, the documentation is includes the code, code comments, and prose in the process file used to acquire the data and also a data origin file. The data origin file is a text file that describes the data source and the data collection process.\nThe {qtkit} package includes a function, create_data_origin(), that can be used to scaffold a data origin file. This simply takes a file path and creates a data origin file in CSV format.\nattribute,description\nResource name,The name of the resource.\nData source,\"URL, DOI, etc.\"\nData sampling frame,\"Language, language variety, modality, genre, etc.\"\nData collection date(s),The dates the data was collected.\nData format,\".txt, .csv, .xml, .html, etc.\"\nData schema,\"Relationships between data elements: files, folders, etc.\"\nLicense,\"CC BY, CC BY-SA, etc.\"\nAttribution,Citation information.\nThe you edit this file and ensure that it contains all of the information needed to document the data. Make sure that this file is near the data file so that it is easy to find.\ndata\n  ├── analysis/\n  ├── derived/\n  └── original/\n      ├── works_do.csv\n      └── gutenberg/\n          ├── works_pr.csv\n          └── works_ps.csv"
  },
  {
    "objectID": "recipes/recipe-05/index.html#summary",
    "href": "recipes/recipe-05/index.html#summary",
    "title": "05. Collecting and documenting data",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we have covered acquiring data for a text analysis project. We used the {gutenbergr} (Johnston and Robinson 2023) to acquire works from Project Gutenberg. After exploring the resources available, we established an acquisition plan. We then used R to implement our plan. To make our code more reproducible-friendly, we wrote a custom function to acquire the data. Finally, we discussed the importance of documenting the data collection process and introduced the data origin file."
  },
  {
    "objectID": "recipes/recipe-05/index.html#check-your-understanding",
    "href": "recipes/recipe-05/index.html#check-your-understanding",
    "title": "05. Collecting and documenting data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nIn the chapter and in this recipe, strategies for acquiring data were discussed. Which of the following was not discussed as a strategy for acquiring data? Direct downloadProgrammatic downloadAPIsWeb scraping\nIn this recipe, we used {gutenbergr} to acquire works from Project Gutenberg. What is the name of the function that we used to acquire the actual text? gutenberg_metatagutenberg_get()gutenberg_search()gutenberg_download()\nTrueFalse A custom function is only really necessary if you are writting an R package.\nWhen writing a custom function, what is the first step? Write the codeWrite the commentsLoad the packagesCreate the function arguments\nWhat does it mean when we say that a function is ‘vectorized’ in R? The function returns a vectorThe function can take a vector as an argumentThe function can take a vector and operates on each element of the vector\nWhich Tidyverse package allows us to apply non-vectorized functions to vectors? dplyrstringrreadrpurrr"
  },
  {
    "objectID": "recipes/recipe-05/index.html#lab-preparation",
    "href": "recipes/recipe-05/index.html#lab-preparation",
    "title": "05. Collecting and documenting data",
    "section": "Lab preparation",
    "text": "Lab preparation\nBefore beginning Lab 5, make sure you are comfortable with the following:\n\nReading and subsetting data in R\nWriting data in R\nThe project structure of reproducible projects\n\nThe additional skills covered in this lab are:\n\nIdentifying data sources\nAcquiring data through manual and programmatic downloads and APIs\nCreating a data acquisition plan\nDocumenting the data collection process\nWriting a custom function\nDocumenting the data source with a data origin file\n\nYou will have a choice of data source to acquire data from. Before you start the lab, you should consider which data source you would like to use, what strategy you will use to acquire the data, and what data you will acquire. You should also consider the information you need to document the data collection process.\nConsult the Identifying data and data sources guide for some ideas on where to find data sources."
  },
  {
    "objectID": "recipes/recipe-03/index.html",
    "href": "recipes/recipe-03/index.html",
    "title": "03. Descriptive assessment of datasets",
    "section": "",
    "text": "Skills\n\nSummary overviews of datasets with {skimr}\nSummary statistics with {dplyr}\nCreating Quarto tables with {knitr}\nCreating Quarto plots with {ggplot2}"
  },
  {
    "objectID": "recipes/recipe-03/index.html#concepts-and-strategies",
    "href": "recipes/recipe-03/index.html#concepts-and-strategies",
    "title": "03. Descriptive assessment of datasets",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\nIn this Recipe, we will use the PassiveBrownFam dataset from {corpora} (Evert 2023). This dataset contains information on the passive voice usage in the Brown family of corpora. The dataset contains 11 variables and 2,449 observations.\nI have assigned this dataset to the object brown_fam_df and have made minor modifications to the variable names to improve the readability of the dataset.\n\n\n# Load packages\nlibrary(dplyr)\n\n# Read the dataset from {corpora}\nbrown_fam_df &lt;-\n  corpora::PassiveBrownFam |&gt; # reference the dataset\n  as_tibble() # convert to a tibble\n\n# Rename variables\nbrown_fam_df &lt;-\n  brown_fam_df |&gt; # pass the original dataset\n  rename( # rename variables: new_name = old_name\n    lang_variety = lang,\n    num_words = n.words,\n    active_verbs = act,\n    passive_verbs = pass,\n    total_verbs = verbs,\n    percent_passive = p.pass\n  )\n\n# Preview\nglimpse(brown_fam_df)\n\nRows: 2,499\nColumns: 11\n$ id              &lt;chr&gt; \"brown_A01\", \"brown_A02\", \"brown_A03\", \"brown_A04\", \"b…\n$ corpus          &lt;fct&gt; Brown, Brown, Brown, Brown, Brown, Brown, Brown, Brown…\n$ section         &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, …\n$ genre           &lt;fct&gt; press reportage, press reportage, press reportage, pre…\n$ period          &lt;fct&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, …\n$ lang_variety    &lt;fct&gt; AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE,…\n$ num_words       &lt;int&gt; 2080, 2116, 2051, 2095, 2111, 2102, 2099, 2069, 2058, …\n$ active_verbs    &lt;int&gt; 164, 154, 135, 128, 170, 166, 165, 163, 153, 169, 132,…\n$ passive_verbs   &lt;int&gt; 40, 25, 34, 25, 32, 21, 31, 19, 39, 23, 17, 10, 15, 26…\n$ total_verbs     &lt;int&gt; 204, 179, 169, 153, 202, 187, 196, 182, 192, 192, 149,…\n$ percent_passive &lt;dbl&gt; 19.61, 13.97, 20.12, 16.34, 15.84, 11.23, 15.82, 10.44…\n\n\nYou can learn more about these variables by reading the dataset documentation with ?corpora::PassiveBrownFam.\n\nStatistical overviews\nUnderstanding our data is of utmost importance before, during, and after analysis. After we get to know our data by inspecting the data origin, dictionary, and structure, we then move to summarizing the data.\nA statistical overview of the data is a good place to start as it gives us a sense of all of the variables and variable types in the dataset. We can use {skimr} to create a statistical overview of the data, using the very convienent skim() function.\nLet’s create a statistical overview of the brown_fam_df dataset.\n\n# Load packages\nlibrary(skimr)\n\n# Create a statistical overview of the `brown_fam_df` dataset\nskim(brown_fam_df)\n\n\n# ── Data Summary ────────────────────────\n#                            Values      \n# Name                       brown_fam_df\n# Number of rows             2499        \n# Number of columns          11          \n# _______________________                \n# Column type frequency:                 \n#   character                1           \n#   factor                   5           \n#   numeric                  5           \n# ________________________               \n# Group variables            None        \n# \n# ── Variable type: character ────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate min max empty n_unique whitespace\n# 1 id                    0             1   7   9     0     2499          0\n# \n# ── Variable type: factor ───────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate ordered n_unique\n# 1 corpus                0             1 FALSE          5\n# 2 section               0             1 FALSE         15\n# 3 genre                 0             1 FALSE         15\n# 4 period                0             1 FALSE          3\n# 5 lang_variety          0             1 FALSE          2\n#   top_counts                            \n# 1 BLO: 500, Bro: 500, LOB: 500, FLO: 500\n# 2 J: 400, G: 381, F: 228, A: 220        \n# 3 lea: 400, bel: 381, pop: 228, pre: 220\n# 4 196: 1000, 199: 999, 193: 500         \n# 5 BrE: 1500, AmE: 999                   \n# \n# ── Variable type: numeric ──────────────────────────────────────────────────────\n#   skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50\n# 1 num_words               0             1 2165.  97.8  1406     2127    2163  \n# 2 active_verbs            0             1  179.  56.6    39      139     170  \n# 3 passive_verbs           0             1   25.7 12.9     2       16      23  \n# 4 total_verbs             0             1  204.  49.1    66      170     196  \n# 5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1\n#      p75   p100 hist \n# 1 2200   4397   ▁▇▁▁▁\n# 2  214    551   ▃▇▂▁▁\n# 3   32     86   ▆▇▂▁▁\n# 4  234    571   ▃▇▂▁▁\n# 5   18.2   67.7 ▇▅▁▁▁\n\n\nThe output of the skim() function contains a lot of information but it essentially has two parts: a summary of the dataset and a summary of each variable in the dataset. The summary of each of the variables, however, is grouped by variable type. Remember, each of our variables in a data frame is a vector and each vector has a type.\nWe have already learned about different types of vectors in R, including character, numeric, and logical. In this dataset, we are presented with a new type of vector: a factor. A factor is essentially a character vector that contains a set of discrete values, or levels. Factors can be ordered or unordered and can contain levels that are not present in the data.\nNow, looking at each of the variable types, we can see that we have 1 character variable, 5 factor variables, and 5 numeric variables. Each of these variable types assume a different set of summary statistics. For example, we can calculate the mean of a numeric variable but not of a character variable. Or, we can count the number of unique values in a character variable but not in a numeric variable.\nFor all variables, skim() will also provide the number of missing values and the percent of non-missing values.\nInspecting the entire dataset is a good place to start but at some point we often want focus in on a set of variables. We can add the yank() function to extract the statistical overview of a set of variables by their variable types.\nLet’s extract the statistical overview of the numeric variables in the brown_fam_df dataset.\n\n# Extract the statistical overview of the numeric variables\nbrown_fam_df |&gt;\n  skim() |&gt;\n  yank(\"numeric\")\n\n── Variable type: numeric ─────────────────────────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50    p75   p100 hist\n1 num_words               0             1 2165.  97.8  1406     2127    2163   2200   4397   ▁▇▁▁▁\n2 active_verbs            0             1  179.  56.6    39      139     170    214    551   ▃▇▂▁▁\n3 passive_verbs           0             1   25.7 12.9     2       16      23     32     86   ▆▇▂▁▁\n4 total_verbs             0             1  204.  49.1    66      170     196    234    571   ▃▇▂▁▁\n5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1   18.2   67.7 ▇▅▁▁▁\n\n\nSummary statistics of particular variables\nThese summary statistics are useful but for a preliminary and interactive use, but it is oftent the case that we will want to focus in on a particular variable or set of variables and their potential relationships to other variables.\nWe can use {dplyr} to calculate summary statistics for a particular variable or set of variables. We can use the group_by() function to group the data by a particular variable or variables. Then we can use the summarize() function to calculate summary statistics for the grouped data.\nFor example, let’s calculate the mean and median of the percent_passive variable in the brown_fam_df dataset grouped by the lang_variety variable.\n\n# Mean and median of `percent_passive` grouped by `lang_variety`\nbrown_fam_df |&gt;\n  group_by(lang_variety) |&gt;\n  summarize(\n    mean_percent_passive = mean(percent_passive),\n    median_percent_passive = median(percent_passive)\n  )\n\n# A tibble: 2 × 3\n  lang_variety mean_percent_passive median_percent_passive\n  &lt;fct&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;\n1 AmE                          12.9                   11.0\n2 BrE                          14.8                   13.3\n\n\nThe result is a 2x3 data frame which includes both the mean and median of the percent_passive variable for each of the two levels of the lang_variety variable.\nThe group_by() function can also be used to group by multiple variables. For example, let’s calculate the mean and median of the percent_passive variable in the brown_fam_df dataset grouped by the lang_variety and genre variables.\n\n# Mean and median of `percent_passive` grouped by\n# `lang_variety` and `genre`\nbrown_fam_df |&gt;\n  group_by(lang_variety, genre) |&gt;\n  summarize(\n    mean_percent_passive = mean(percent_passive),\n    median_percent_passive = median(percent_passive)\n  )\n\n# A tibble: 30 × 4\n# Groups:   lang_variety [2]\n   lang_variety genre            mean_percent_passive median_percent_passive\n   &lt;fct&gt;        &lt;fct&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;\n 1 AmE          press reportage                 11.5                   11.0 \n 2 AmE          press editorial                 10.6                   10.1 \n 3 AmE          press reviews                    9.54                   9.77\n 4 AmE          religion                        14.3                   14.3 \n 5 AmE          skills / hobbies                14.9                   13.9 \n 6 AmE          popular lore                    14.0                   12.7 \n 7 AmE          belles lettres                  12.0                   11.7 \n 8 AmE          miscellaneous                   23.5                   23.3 \n 9 AmE          learned                         21.3                   18.3 \n10 AmE          general fiction                  6.22                   5.89\n# ℹ 20 more rows\n\n\nFor numeric variables, such as percent_passive, there are a number of summary statistics that we can calculate. We’ve seen the R functions for mean and median but we can also calculate the standard deviation (sd()), variance (var()), minimum (min()), maximum (max()), interquartile range (IQR()), median absolute deviation (mad()), and quantiles (quantile()). All these calculations make sense for numeric variables but not for character variables.\nFor character variables, and factors, the summary statistics are more limited. We can calculate the number of observations (n()) and/ or the number of unique values (n_distinct()). Let’s now summarize the number of observations n() grouped by the genre variable in the brown_fam_df dataset.\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  group_by(genre) |&gt;\n  summarize(\n    n = n(),\n  )\n\n# A tibble: 15 × 2\n   genre                n\n   &lt;fct&gt;            &lt;int&gt;\n 1 press reportage    220\n 2 press editorial    135\n 3 press reviews       85\n 4 religion            85\n 5 skills / hobbies   186\n 6 popular lore       228\n 7 belles lettres     381\n 8 miscellaneous      150\n 9 learned            400\n10 general fiction    145\n11 detective          120\n12 science fiction     30\n13 adventure          144\n14 romance            145\n15 humour              45\n\n\nJust as before, we can add multiple grouping variables to group_by(). Let’s add lang_variety to the grouping and calculate the number of observations n() grouped by the genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  )\n\n# A tibble: 30 × 3\n# Groups:   genre [15]\n   genre            lang_variety     n\n   &lt;fct&gt;            &lt;fct&gt;        &lt;int&gt;\n 1 press reportage  AmE             88\n 2 press reportage  BrE            132\n 3 press editorial  AmE             54\n 4 press editorial  BrE             81\n 5 press reviews    AmE             34\n 6 press reviews    BrE             51\n 7 religion         AmE             34\n 8 religion         BrE             51\n 9 skills / hobbies AmE             72\n10 skills / hobbies BrE            114\n# ℹ 20 more rows\n\n\n\n\n\n\n\n\n Tip\nThe result of calculating the number of observations for a character or factor variable is known as a frequency table. Grouping two or more categorical variables is known as a cross-tabulation or a contingency table.\n\n\n\nNow, we can also pipe the results of a group_by() and summarize() to another function. This can be to say sort, select, or filter the results. It can also be to perform another summary function. It is important, however, to remember that the result of a group_by() produces a grouped data frame. Subsequent functions will be applied to the grouped data frame. This can lead to unexpected results if the original grouping is not relevant for the subsequent function. To avoid this, we can use the ungroup() function to remove the grouping after the relevant grouped summary statistics have been calculated.\nLet’s return to calculating the number of observations n() grouped by the genre and lang_variety variables in the brown_fam_df dataset. But let’s add another summary which uses the n variable to calculate the mean and median number of observations.\nIf we do not use the ungroup() function, the mean and median will be calculated for each genre collapsed across lang_variety.\n\n# Mean and median of `n` grouped by `genre`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  ) |&gt;\n  summarize(\n    mean_n = mean(n),\n    median_n = median(n)\n  )\n\n# A tibble: 15 × 3\n   genre            mean_n median_n\n   &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1 press reportage   110      110  \n 2 press editorial    67.5     67.5\n 3 press reviews      42.5     42.5\n 4 religion           42.5     42.5\n 5 skills / hobbies   93       93  \n 6 popular lore      114      114  \n 7 belles lettres    190.     190. \n 8 miscellaneous      75       75  \n 9 learned           200      200  \n10 general fiction    72.5     72.5\n11 detective          60       60  \n12 science fiction    15       15  \n13 adventure          72       72  \n14 romance            72.5     72.5\n15 humour             22.5     22.5\n\n\nTherefore we see that we have a mean and median calculated for the number of documents in the corpus for each of the 15 genres.\nIf we use the ungroup() function, the mean and median will be calculated for all genres. Note we will use the ungroup() function between these summaries to clear the grouping before calculating the mean and median.\n\n# Number of observations for each `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  ) |&gt;\n  ungroup() |&gt;\n  summarize(\n    mean_n = mean(n),\n    median_n = median(n)\n  )\n\n# A tibble: 1 × 2\n  mean_n median_n\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   83.3       72\n\n\nNow we see that we have a mean and median calculated across all genres.\n\nBefore we leave this section, let’s look some other ways to create frequency and contingency tables for character and factor variables. A shortcut to calculate a frequency table for a character or factor variable is to use the count() function from {dplyr}.\nLet’s calculate the number of observations grouped by the genre variable in the brown_fam_df dataset.\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  count(genre)\n\n# A tibble: 15 × 2\n   genre                n\n   &lt;fct&gt;            &lt;int&gt;\n 1 press reportage    220\n 2 press editorial    135\n 3 press reviews       85\n 4 religion            85\n 5 skills / hobbies   186\n 6 popular lore       228\n 7 belles lettres     381\n 8 miscellaneous      150\n 9 learned            400\n10 general fiction    145\n11 detective          120\n12 science fiction     30\n13 adventure          144\n14 romance            145\n15 humour              45\n\n\nWe can also add multiple grouping variables to count() and create contingency tables.\nLet’s add lang_variety to the grouping and create a cross-tabulation for genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  count(genre, lang_variety)\n\n# A tibble: 30 × 3\n   genre            lang_variety     n\n   &lt;fct&gt;            &lt;fct&gt;        &lt;int&gt;\n 1 press reportage  AmE             88\n 2 press reportage  BrE            132\n 3 press editorial  AmE             54\n 4 press editorial  BrE             81\n 5 press reviews    AmE             34\n 6 press reviews    BrE             51\n 7 religion         AmE             34\n 8 religion         BrE             51\n 9 skills / hobbies AmE             72\n10 skills / hobbies BrE            114\n# ℹ 20 more rows\n\n\nNote that the results of count() are not grouped so we do not need to use the ungroup() function before calculating subsequent summary statistics.\nAnother way to create frequency and contingency tables is to use the tabyl() function from {janitor} (Firke 2023). Let’s create a frequency table for the genre variable in the brown_fam_df dataset.\n\n# Load packages\nlibrary(janitor)\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  tabyl(genre)\n\n            genre   n percent\n  press reportage 220  0.0880\n  press editorial 135  0.0540\n    press reviews  85  0.0340\n         religion  85  0.0340\n skills / hobbies 186  0.0744\n     popular lore 228  0.0912\n   belles lettres 381  0.1525\n    miscellaneous 150  0.0600\n          learned 400  0.1601\n  general fiction 145  0.0580\n        detective 120  0.0480\n  science fiction  30  0.0120\n        adventure 144  0.0576\n          romance 145  0.0580\n           humour  45  0.0180\n\n\nIn addition to providing frequency counts, the tabyl() function also provides the percent of observations for each level of the variable. And, we can add up to three grouping variables to tabyl() as well.\nLet’s add lang_variety to the grouping and create a contingency table for the genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  tabyl(genre, lang_variety)\n\n            genre AmE BrE\n  press reportage  88 132\n  press editorial  54  81\n    press reviews  34  51\n         religion  34  51\n skills / hobbies  72 114\n     popular lore  96 132\n   belles lettres 150 231\n    miscellaneous  60  90\n          learned 160 240\n  general fiction  58  87\n        detective  48  72\n  science fiction  12  18\n        adventure  57  87\n          romance  58  87\n           humour  18  27\n\n\nThe results do not include the percent of observations for each level of the variable as it is not clear how to calculate the percent of observations for each level of the variable when there are multiple grouping variables. We must specify if we want to calculate the percent of observations by row or by column.\n\n\n\n\n\n\n Dive deeper\n{janitor} includes a variety of adorn_*() functions to add additional information to the results of tabyl(), including percentages, frequencies, and totals. Feel free to explore these functions on your own. We will return to this topic again later in the course.\n\n\n\n\n\nCreating Quarto tables\nSummarizing the data is not only useful for our understanding of the data as part of our analysis but also for communicating the data in reports, manuscripts, and presentations.\nOne way to communicate summary statistics is with tables. In Quarto, we can use {knitr} (Xie 2024) in combination with code block options to produce formatted tables which we can cross-reference in our prose sections.\nLet’s create an object from the cross-tabulation for the genre and lang_variety variables in the brown_fam_df dataset to work with.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbf_genre_lang_ct &lt;-\n  brown_fam_df |&gt;\n  tabyl(genre, lang_variety)\n\nTo create a table in Quarto, we use the kable() function. The kable() function takes a data frame (or matrix) as an argument. The format argument will be derived from the Quarto document format (‘html’, ‘pdf’, etc.).\n\n# Load packages\nlibrary(knitr)\n\n# Create a table in Quarto\nkable(bf_genre_lang_ct)\n\n\n\n\ngenre\nAmE\nBrE\n\n\n\n\npress reportage\n88\n132\n\n\npress editorial\n54\n81\n\n\npress reviews\n34\n51\n\n\nreligion\n34\n51\n\n\nskills / hobbies\n72\n114\n\n\npopular lore\n96\n132\n\n\nbelles lettres\n150\n231\n\n\nmiscellaneous\n60\n90\n\n\nlearned\n160\n240\n\n\ngeneral fiction\n58\n87\n\n\ndetective\n48\n72\n\n\nscience fiction\n12\n18\n\n\nadventure\n57\n87\n\n\nromance\n58\n87\n\n\nhumour\n18\n27\n\n\n\n\n\nTo add a caption to the table and to enable cross-referencing, we use the code block options label and tbl-cap. The label option takes a label prefixed with tbl- to create a cross-reference to the table. The tbl-cap option takes a caption for the table, in quotation marks.\n#| label: tbl-brown-genre-lang-ct\n#| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"\n\n# Create a table in Quarto\nkable(bf_genre_lang_ct)\nNow we can cross-reference the table with the @tbl-brown-genre-lang-ct syntax. So the following Quarto document will produce the following prose with a cross-reference to the formatted table output.\n\nAs we see in @tbl-brown-genre-lang-ct, the distribution of `genre` is similar across `lang_variety`.\n\n```{r}\n#| label: tbl-brown-genre-lang-ct\n#| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"\n\n# Print cross-tabulation\nkable(bf_genre_lang_ct)\n```\n\nAs we see in Table 1, the distribution of genre is similar across lang_variety.\n\n\n\n\nTable 1: Cross-tabulation of genre and lang_variety\n\n\n\n\n\n\ngenre\nAmE\nBrE\n\n\n\n\npress reportage\n88\n132\n\n\npress editorial\n54\n81\n\n\npress reviews\n34\n51\n\n\nreligion\n34\n51\n\n\nskills / hobbies\n72\n114\n\n\npopular lore\n96\n132\n\n\nbelles lettres\n150\n231\n\n\nmiscellaneous\n60\n90\n\n\nlearned\n160\n240\n\n\ngeneral fiction\n58\n87\n\n\ndetective\n48\n72\n\n\nscience fiction\n12\n18\n\n\nadventure\n57\n87\n\n\nromance\n58\n87\n\n\nhumour\n18\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\n{kableExtra} (Zhu 2024) provides additional functionality for formatting tables in Quarto.\n\n\n\n\n\nCreating Quarto plots\nWhere tables are useful for communicating summary statistics for numeric and character variables, plots are useful for communicating relationships between variables especially when one or more of the variables is numeric. Furthermore, for complex relationships, plots can be more effective than tables.\nIn Quarto, we can use {ggplot2} (Wickham et al. 2024) in combination with code block options to produce formatted plots which we can cross-reference in our prose sections.\nLet’s see this in action with a simple histogram of the percent_passive variable in the brown_fam_df dataset. The Quarto document will produce the following prose with a cross-reference to the formatted plot output.\nAs we see in @fig-brown-fam-percent-passive-hist, the distribution of `percent_passive` is skewed to the right.\n\n```{r}\n#| label: fig-brown-fam-percent-passive-hist\n#| fig-cap: \"Histogram of `percent_passive`\"\n\n# Create a histogram in Quarto\nggplot(brown_fam_df) +\n  geom_histogram(aes(x = percent_passive))\n```\n\nAs we see in Figure 1, the distribution of percent_passive is skewed to the right.\n\n\n\n\n\n\n\n\nFigure 1: Histogram of percent_passive\n\n\n\n\n\n\n{ggplot2} implements the ‘Grammar of Graphics’ approach to creating plots. This approach is based on the idea that plots can be broken down into components, or layers, and that each layer can be manipulated independently.\nThe main components are data, aesthetics, and geometries. Data is the data frame that contains the variables to be plotted. Aesthetics are the variables that will be mapped to the x-axis, y-axis (as well as color, shape, size, etc.). Geometries are the visual elements that will be used to represent the data, such as points, lines, bars, etc..\nAs discussed in the R lesson “Visual Summaries”, the aes() function is used to map variables to aesthetics and can be added to the ggplot() function or to the geom_*() function depending on whether the aesthetic is mapped to all geometries or to a specific geometry, respectively.\nTake a look at the following stages of the earlier plot in each of the tabs below.\n\nStages\n\nDataAestheticsGeometries\n\n\nThe data layer does not produce a plot but it is the foundation of the plot.\n\n# Data layer\nggplot(brown_fam_df)\n\n\n\n\n\n\n\n\n\n\nThe aesthetics layer does not produce a plot but it maps the variables to the aesthetics to be used in the plot.\n\n# Aesthetics layer\nggplot(brown_fam_df, aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\nThe geometries layer produces the plot connecting the data and aesthetics layers in the particular way specified by the geometries, in this case a histogram.\n\n# Geometries layer\nggplot(brown_fam_df, aes(x = percent_passive)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right plot\nJust as with tables, the type of summary we choose to communicate with a plot depends on the type of variables we are working with and the relationships between those variables.\nBelow I’ve included a few examples of plots that can be used to communicate different types of variables and relationships.\n\n\nSingle numeric variable\n\nHistogramDensity plot\n\n\n\n# Histogram\nggplot(brown_fam_df) +\n  geom_histogram(aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\n\n# Density plot\nggplot(brown_fam_df) +\n  geom_density(aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumeric and categorical variables\n\nDensity plotBoxplotViolin plot\n\n\n\n# Density plot\nggplot(brown_fam_df) +\n  geom_density(\n    aes(\n      x = percent_passive,\n      fill = lang_variety\n    ),\n    alpha = 0.5 # adds transparency\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Boxplot\nggplot(brown_fam_df) +\n  geom_boxplot(\n    aes(\n      x = lang_variety,\n      y = percent_passive\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Violin plot\nggplot(brown_fam_df) +\n  geom_violin(\n    aes(\n      x = lang_variety,\n      y = percent_passive\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numeric variables\n\nScatterplotScatterplot with regression line\n\n\n\n# Scatterplot\nggplot(brown_fam_df) +\n  geom_point(\n    aes(\n      x = active_verbs,\n      y = passive_verbs\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Scatterplot with regression line\nggplot(\n  brown_fam_df,\n  aes(\n    x = active_verbs,\n    y = passive_verbs\n  )\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther variable combinations\nIn these examples, we have only looked at the most common variable combinations for one and two variable plots. There are more sophisticated plots that can be used for other variable combinations using {ggplot2}. For now, we will leave these for another time."
  },
  {
    "objectID": "recipes/recipe-03/index.html#check-your-understanding",
    "href": "recipes/recipe-03/index.html#check-your-understanding",
    "title": "03. Descriptive assessment of datasets",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nA factor is a character vector augmented to include information about the discrete values, or levels, of the vector. TRUEFALSE\nWhat is the difference between a frequency table and a contingency table? A frequency table is a cross-tabulation of two or more categorical variables.A contingency table is a cross-tabulation of two or more categorical variables.\nThe skimrdplyrggplot2knitr package is used to create formatted tables in R.\nTo add a geometry layer, such as geom_histogram(), to a ggplot object the |&gt; operator is used. TRUEFALSE\nTo visualize the relationship between two numeric variables, a histogramdensity plotboxplotviolin plotscatterplot is often used.\nWhen the aes() function is added to the ggplot() function, the aesthetic is mapped to all geometries. TRUEFALSE"
  },
  {
    "objectID": "recipes/recipe-03/index.html#lab-preparation",
    "href": "recipes/recipe-03/index.html#lab-preparation",
    "title": "03. Descriptive assessment of datasets",
    "section": "Lab preparation",
    "text": "Lab preparation\n\nBefore beginning Lab 3, learners should be comfortable with the skills and knowledge developed in the previous recipes and labs. In this lab, you will have a chance to use these skills and those introduced in this Recipe to provide a descriptive assessment of a dataset that includes statistics, tables, and plots using Quarto and R.\nThe additional skills and knowledge you will need to complete Lab 3 include:\n\nSummarizing data with {skimr}\nSummarizing data with {dplyr}\nCreating Quarto tables with {knitr}\nCreating Quarto plots with {ggplot2}"
  },
  {
    "objectID": "recipes/recipe-10/index.html",
    "href": "recipes/recipe-10/index.html",
    "title": "10. Building inference models",
    "section": "",
    "text": "Skills\n\nidentify and map the hypothesis statement to the appropriate response and explanatory variables\nemploy simulation-based methods for statistical inference\ninterpret and evaluate the results of inferential models\nStatistical inference is the most structured approach to data analysis. It is the process of using data to draw conclusions about a population, therefore the underlying data and the process to conduct the analysis must be rigorous and exploration is limited and iteration is avoided. The workflow for building inference-based models can be seen in Table 1.\nBefore we begin, let’s load the packages we will use in this recipe.\nlibrary(readr) # for reading data\nlibrary(kableExtra) # for table formatting\nlibrary(dplyr) # for data wrangling\nlibrary(skimr) # for data summaries\nlibrary(janitor) # for data cleaning/ tablulations\nlibrary(ggplot2) # for data visualizations\nlibrary(infer) # for statistical inference"
  },
  {
    "objectID": "recipes/recipe-10/index.html#concepts-and-strategies",
    "href": "recipes/recipe-10/index.html#concepts-and-strategies",
    "title": "10. Building inference models",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\nIn the area of Second Language Acquisition and Teaching, the relationship between learner proficiency and particular linguistic variables has been a topic of interest for decades. In this section, we will explore the relationship between placement scores and lexical features of learner sample writing. The goal is to determine the extent to which a simplified set of lexical features can be employed as a diagnostic tool for assessing overall learner proficiency.\nThe background here comes from a study by Crossley et al. (2010), who investigated the relationship between lexical features and learner proficiency. The authors used a corpus of learner writing samples which were assessed and scored by human raters. Then a set of lexical features were extracted from the writing samples and subjected to a series of statistical analyses. The results suggested three key variables which were highly correlated with the human ratings of learner proficiency: lexical diversity, word hypernymy values, and content word frequency.\nIn this sample study, I will suggest that the variables lexical diversity and content word frequency actually represent a single underlying construct, which I will call “lexical sophistication”. Lexical diversity aims to gauge the range of vocabulary used by the learner. In the context of L2 writing, it is less likely that demonstrated range of unique vocabulary is what constitutes proficiency –as a lexically diverse text in which the writer uses primarily spoken register vocabulary is unlikely to be considered in academic writing contexts. Instead, rather it is more likely that the ability to use more sophisticated vocabulary is what is being measured. On the other hand, content word frequency purports to gauge the degree to which more infrequent words are used in L2 writing. However, I would argue that this is also a measure of lexical sophistication. In other words, the ability to use more sophisticated vocabulary inherently taps into the use of more infrequent words.\nThe goal of this study is to determine the extent to which this construct can be used as a proxy for lexical diversity and content word frequency, and thus as a diagnostic tool for assessing learner proficiency. The research statement is as follows:\n\nThe lexical sophistication of learner writing is positively correlated with learner proficiency.\n\nOperationalizing this statement requires a few steps. First, we need to identify the variables which will be used to represent the construct of lexical sophistication.\nIn addition, ideally the variables used to represent lexical sophistication should be easy to extract from learner writing samples, if this is to be used as a diagnostic tool. Plausible linguistic variables to consider in this study are the following:\n\nNumber of syllables per word\nNumber of morphemes per word\n\nIn addtion, to these variables, we will also consider word frequency estimates to maintain consistency with the original study. I will also consider the number of characters per word, although not strictly linguistic in nature, this is a variable which is easy to extract from learner writing samples and is likely to be correlated with the number of syllables and/ or morphemes per word.\nSecond, we need to identify the variables which will be used to represent learner proficiency. In this study, we will use the placement scores of the learners as a proxy for proficiency. The placement scores are based on the results of a placement test which was administered to the learners prior to the writing samples being collected.\nThe hypothesis statement is as follows:\n\nLearner proficiency as measured by placement scores is positively correlated with lexical sophistication as measured by the number of syllables per word, number of morphemes per word, and word frequency estimates.\n\n\n\nAnalysis\nThe dataset used in this study is is transformed version of the Pittsburgh English Language Institute Corpus (PELIC). Writing samples and placement scores were extracted from the corpus for learners in the English for Academic Purposes (EAP) program. The tokenized writing samples were filtered for content words (i.e. nouns, verbs, adjectives). Subsequently, lexical features were joined from the English Lexicon Project (ELP) database by word form.\nThe data dictionary for the dataset is as follows:\n\n\n\n\nTable 2: Data dictionary for the transformed PELIC/ ELP dataset\n\n\n\n\n\n\nVariable\nName\nType\nDescription\n\n\n\n\nid\nID\ncategorical\nUnique identifier for each learner\n\n\nplacement\nPlacement\nnumeric\nNumerical value indicating score on the placement test for each learner (0-100)\n\n\nchars\nCharacters\nnumeric\nMean number of characters per word in the text sample\n\n\nsylls\nSyllables\nnumeric\nMean number of syllables per word in the text sample\n\n\nmorphs\nMorphemes\nnumeric\nMean number of morphemes per word in the text sample\n\n\nfreq\nFrequency\nnumeric\nMean frequency of occurrence per word in the text sample\n\n\n\n\n\n\n\n\n\n\n\nIdentify\nIn Table 2, we can see the variables which will be used to represent lexical sophistication and learner proficiency. The explanatory variables which will be used to represent lexical sophistication are chars, sylls, morphs, and freq. The response variable which will be used to represent learner proficiency is placement.\nLet’s read in the dataset.\n\npelic &lt;- read_csv(\"data/derived/pelic/pelic_transformed.csv\")\n\npelic\n\n# A tibble: 276 × 6\n   id    placement chars sylls morphs     freq\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 aa0          80  6.01  1.90   1.66  498076.\n 2 aa1          66  5.96  1.85   1.53  657678.\n 3 ab8          40  5.81  1.81   1.52  375383.\n 4 ac3          65  5.98  1.81   1.52  419318.\n 5 ad4          72  6.26  1.93   1.69  345658.\n 6 ad9          56  5.82  1.72   1.57  375747.\n 7 ae0          61  4.99  1.53   1.35 1087512.\n 8 ae2          81  6.32  1.98   1.69  403618.\n 9 ae4          69  5.23  1.68   1.45  788536.\n10 ae9          71  5.64  1.75   1.47  586251.\n# ℹ 266 more rows\n\n\nThe dataset contains 276 observations for our variables of interest. Let’s now map the hypothesis statement to the appropriate response and explanatory variables.\nplacement ~ chars + sylls + morphs + freq\nWe will specify the relationship between the response and explanatory variables using the formula notation in the interrogation phase. The explanatory variables will be used in an additive model using multiple linear regression.\n\n\nInspect\nFirst step is to get a statistical overview of the dataset. We can use the skim() function from {skimr} to get a statistical summary of the dataset.\npelic |&gt;\n  skim()\n\n\n── Data Summary ────────────────────────\n                           Values\nName                       pelic \nNumber of rows             276   \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  character                1     \n  numeric                  5     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 id                    0             1   3   3     0      276          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate      mean         sd        p0       p25\n1 placement             0             1     59.2      12.7       32        48   \n2 chars                 0             1      5.79      0.473      3.75      5.50\n3 sylls                 0             1      1.79      0.169      1.25      1.68\n4 morphs                0             1      1.53      0.127      1.12      1.44\n5 freq                  0             1 556751.   160046.    165138.   453424.  \n        p50       p75       p100 hist \n1     59        69         88    ▃▆▇▆▃\n2      5.79      6.08       8.07 ▁▂▇▁▁\n3      1.78      1.88       2.66 ▁▇▆▁▁\n4      1.53      1.62       1.97 ▁▅▇▃▁\n5 544026.   631784.   1379696.   ▂▇▂▁▁\n\n\nWe are looking for any missing values or other anomalies in the dataset. We can see that there are no missing values in the dataset.\nLet’s now inspect the variables of interest to get a sense of the distributions of the variables. There are really three things we are looking to find out:\n\nWhat is the distribution of the variables individually?\nWhat is the relationship between the explanatory variables and the response variable?\nWhat is the relationship (if any) between the variables?\n\nSince we are working with numeric variables, we would create a histogram or density plot for each individual variable. Then a scatterplot for the relationship between the explanatory variables and the response variable. Finally, a correlation matrix for the relationship between the variables.\nI would like to introduce a shortcut to create all three types of visualizations in one plot. The ggpairs() function from {GGally} creates a matrix of plots for all combinations of variables in the dataset. We can specify the type of plot to create for each combination of variables. Let’s create a matrix of plots for the variables of interest.\n\n# Load the GGally package\nlibrary(GGally)\n\n# Create the matrix of plots\npelic |&gt;\n  select(placement, chars, sylls, morphs, freq) |&gt;\n  ggpairs()\n\n\n\n\n\n\n\nFigure 1: Matrix of plots for the variables of interest\n\n\n\n\n\nThe plot in Figure 1 shows a lot of information. Let’s break it down.\nThe diagonal plots show the distribution of each variable. In these plots we are looking for any outliers or skewness in the distributions. We can see that on the whole the distributions are fairly normal. Our simulation-based inference methods do not require the data to be normally distributed, but highly skewed distributions can compress the range of the data and thus affect the results.\nThe lower triangle plots show the scatterplots for the relationship between the explanatory variables and the response variable and in the upper triangle we see the correlation coefficients for the relationship between the variables.\nLet’s focus on the upper triangle, specifically the first row of statistics reported. The first row shows the correlation between the response variable and the explanatory variables. All variables show a positive correlation with the response variable, except for frequency, which is negative. This is what we predicted in the hypothesis statement. The strengths of these correlations are fairly weak, especially for frequency. We will let our model determine the strength of the relationship and whether it is statistically significant, but it’s worth noting.\nNow let’s focus on the upper triangle for the second, third, and fourth row of correlation measures. These show the correlation between the explanatory variables themselves. The variables chars, sylls, and morphs are highly intercorrelated. This is not surprising since the number of characters in a word is related to the number of syllables and morphemes. Yet if we consider all three in our model we run the risk of multicollinearity. So we need to decide which of these variables to include in our model.\nOne way to do this is assess the theoretical importance of each variable. In this case, we might consider the number of syllables and morphemes to be more important than the number of characters. Another perspective is to see which of the remaining two variables is least correlated with freq with the hopes of capturing non-overlapping variance in the response variable. In this case, sylls is less correlated with freq than morphs. So we will include sylls and freq in our model.\nIf we were to use the response variable placement as our reason for selecting the explanatory variables, we would be committing the logical fallacy of circular reasoning –in essence, tailoring the model to the data.\nSo lets select sylls and freq as our explanatory variables for our final model.\n\npelic &lt;-\n  pelic |&gt;\n  select(placement, sylls, freq)\n\nA last thing to consider before we enter into the model building phase, is to address the fact that the sylls and freq variables are on very distinct scales. In regression modeling, this can cause problems both for fitting the model and for interpreting the model.\nWe can address this by normalizing the variables. This will transform the variables to have a mean of zero and a standard deviation of one. This is known as a z-score. Z-score normalization does not change the distribution nor the relationship between the variables, but it does make the variables more comparable.\n\n# Function to get z-score\nget_z_score &lt;- function(x) {\n  (x - mean(x)) / sd(x)\n}\n\n# Normalize the variables\npelic &lt;-\n  pelic |&gt;\n  mutate(\n    sylls_z = get_z_score(sylls),\n    freq_z = get_z_score(freq)\n  )\n\n\n\nInterrogate\nNow we will analyze the data using {infer}. {infer} is a framework for conducting statistical inference using simulation-based methods. The steps for using {infer} are as follows:\n\nSpecify the model relationships\nCalculate the model statistics (fit)\nSimulate the null distribution\nCalculate the \\(p\\)-value\nSimulate model statistics (fit)\nCalculate the confidence interval\nCalculate the effect size\n\nStep 1. We will use the specify() function to add the formula notation to specify the relationship between the response and explanatory variables.\n\n# Specify the model\npelic_spec &lt;-\n  pelic |&gt;\n  specify(placement ~ sylls_z + freq_z)\n\npelic_spec\n\nResponse: placement (numeric)\nExplanatory: sylls_z (numeric), freq_z (numeric)\n# A tibble: 276 × 3\n   placement sylls_z freq_z\n       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1        80   0.691 -0.367\n 2        66   0.381  0.631\n 3        40   0.157 -1.13 \n 4        65   0.123 -0.859\n 5        72   0.822 -1.32 \n 6        56  -0.378 -1.13 \n 7        61  -1.49   3.32 \n 8        81   1.15  -0.957\n 9        69  -0.624  1.45 \n10        71  -0.189  0.184\n# ℹ 266 more rows\n\n\nStep 2. We will use the fit() function to calculate the model statistics.\n\n# Calculate the model statistics\npelic_obs_fit &lt;-\n  pelic_spec |&gt;\n  fit()\n\npelic_obs_fit\n\n# A tibble: 3 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept   59.2  \n2 sylls_z      2.91 \n3 freq_z       0.617\n\n\nWe now have the calculated model statistics. The model statistics in a linear regression model are the intercept and the slopes for the explanatory variables. The intercept is the predicted value of the response variable when all explanatory variables are zero. The slopes are the predicted change in the response variable for a one unit change in the explanatory variable.\nStep 3. At this point we need to use hypothesize() and to use ‘independence’ as our null hypothesis. The hypothesize() function takes the model object pelic_spec and the type of null hypothesis we are assuming, in this case ‘independence’. Then we will pass this to generate() to create a null distribution. The generate() function takes the model object, the number of simulations, and the type of simulation as arguments. The type of simulation for models with multiple independent variables is permute.\n\n# Create the null distribution\npelic_null_fit &lt;-\n  pelic_spec |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 10000, type = \"permute\") |&gt;\n  fit()\n\n\n\n\n\n\n\n Tip\nThe larger the number of reps the more reliable the results will be. Note, that the generate() function will take a while to run for large reps and depending on your computer’s memory, it may crash. If this happens, try reducing the number of reps.\n\n\n\nStep 4. The get_p_value() function takes the model object and the null distribution object as arguments. We choose a “two-sided” test because we are interested in whether the explanatory variables are positively or negatively correlated with the response variable.\n\n# Calculate the p-value\npelic_p_value &lt;-\n  get_pvalue(\n    x = pelic_null_fit,\n    obs_stat = pelic_obs_fit,\n    direction = \"two-sided\"\n  )\n\npelic_p_value\n\n# A tibble: 3 × 2\n  term      p_value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 freq_z     0.445 \n2 intercept  1     \n3 sylls_z    0.0012\n\n\nOK. So results suggest that syllables is a significant predictor of placement scores, but frequency is not. Let’s now move to the interpretation phase.\n\n\nInterpret\nOur signficant \\(p\\)-value suggests that the explanatory variable sylls_z is a significant predictor of the response variable placement. However, a \\(p\\)-value is an arbitrary threshold. We need to consider the likelihood of the observed test statistic is different from zero. We can do this by calculating the confidence interval.\nStep 5. We will need to simulate the model statistics again, but this time we will use the generate() function with the type “bootstrap” to simulate the model statistics with replacement giving us a distribution of model statistics.\n\n# Create the bootstrap distribution\npelic_bootstrap_fit &lt;-\n  pelic_spec |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  fit()\n\nStep 6. Calculate the confidence interval.\nWe get the confidence interval by using the get_ci() function. The get_ci() function takes the bootstrapped model object and the observed statistics as the estimates to calculate the confidence interval. We will use the default confidence level of 95%.\n\n# Calculate the confidence interval\npelic_obs_ci &lt;-\n  pelic_bootstrap_fit |&gt;\n  get_ci(point_estimate = pelic_obs_fit)\n\npelic_obs_ci\n\n# A tibble: 3 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 freq_z      -0.906     2.21\n2 intercept   57.7      60.7 \n3 sylls_z      1.26      4.81\n\n\nThe confidence interval underscores the results of the \\(p\\)-value. The confidence interval does not include zero for syllables, but does for frequency. Comparing the confidence interval to the actual observed statistics, we can see how close the observed statistics are to the confidence interval margins.\n\npelic_obs_fit\n\n# A tibble: 3 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept   59.2  \n2 sylls_z      2.91 \n3 freq_z       0.617\n\n\nThe observed statistic for sylls_z is nicely within the confidence interval.\nStep 7. Calculate effect size\nWhether a explanatory variable is significant or not is one thing, but gauging the magnitude of the relationship is another. Looking at the observed fit of the model, we can see that the coefficient for syllables is 2.91. But what does this mean? It means the the model predicts that for an increase of our syllable measure by one unit, the placement score will increase by 2.91 units.\nNow the placement score is still in the original scale, so this means that we are dealing with 2.91 score points. For the syllables, however, we have standardized the variable, so we are dealing with standard deviations –less straightforward to interpret. The good news is we can back-transform the standardized variable to the original scale by multiplying by the standard deviation and adding the mean from the original variable.\n\n# Back-transform the standardized variable\n(2.91 * sd(pelic$sylls)) + mean(pelic$sylls)\n\n[1] 2.28\n\n\nTherefore a mean increase of 1 syllable is associated with a 2.28 point increase in placement score.\nAnother helpful way to interpret the results is to consider how much of the variance in the dependent variable is explained by the independent variable. \\(R^2\\) is a typical measure of effect size. To calculate \\(R^2\\) we need the correlation coefficient (\\(r\\)), which can be calculated by dividing the coefficient by the standard deviation of the response variable. Then we square the correlation coefficient to get \\(R^2\\).\n\n# Correlation coefficient and R^2\npelic_obs_fit |&gt;\n  mutate(\n    r = estimate / sd(pelic_spec$placement),\n    r2 = r^2\n  )\n\n# A tibble: 3 × 4\n  term      estimate      r       r2\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept   59.2   4.64   21.6    \n2 sylls_z      2.91  0.229   0.0524 \n3 freq_z       0.617 0.0485  0.00235\n\n\nWe can see her that the \\(R^2\\) is 0.052, which means that the number of syllables explains 5.2% of the variance in placement scores. This means that syllables don’t explain all that much in the variation in placement scores. But it is a significant relationship."
  },
  {
    "objectID": "recipes/recipe-10/index.html#summary",
    "href": "recipes/recipe-10/index.html#summary",
    "title": "10. Building inference models",
    "section": "Summary",
    "text": "Summary\nIn summary, this programming tutorial has provided a comprehensive guide to building inference-based models using the infer package in R. We have explored the concept of lexical sophistication as a diagnostic tool for assessing learner proficiency in Second Language Acquisition and Teaching. Through a detailed workflow involving identifying variables, inspecting data distributions, interrogating the dataset with statistical procedures, and interpreting results, we have demonstrated the importance of careful variable selection and normalization in regression modeling. Our analysis revealed that while syllables per word significantly predict learner placement scores, content word frequency does not. The effect size, indicated by an \\(R^2\\) of 0.052, suggests that lexical sophistication, as measured by syllable count, accounts for a small but significant portion of the variance in learner proficiency. This study underscores the nuanced relationship between linguistic features and language learning outcomes, and highlights the potential of statistical modeling in educational research."
  },
  {
    "objectID": "recipes/recipe-10/index.html#check-your-understanding",
    "href": "recipes/recipe-10/index.html#check-your-understanding",
    "title": "10. Building inference models",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\nTRUEFALSE Simulation-based inference is a method that can be used to approximate traditional inferential statistics without the need for theoretical assumptions about the data.\nWhat is the primary goal of using {infer} for simulation-based inference? To visualize data distributionsTo perform regression analysisTo create resampling simulations for hypothesis testing and constructing confidence intervalsTo fit predictive models\nHow does simulation-based inference contribute to understanding population parameters? It allows us to make probabilistic statements about population parameters based on simulated sampling distributionsIt provides exact calculations of population parametersIt eliminates the need for sample dataIt guarantees more accurate results than traditional methods\nWhich of the following is not a typical step in simulation-based inference? Defining a model of interestGenerating resamples or permutationsCalculating summary statisticsTraining a machine learning classifier\nIn the context of Second Language Acquisition research, why might simulation-based inference be particularly useful? Because it can handle very large datasetsBecause it allows for the exploration of hypotheses under different assumptions and conditionsBecause it simplifies the data collection processBecause it directly measures the language ability of individuals\nTRUEFALSE The results from simulation-based inference are always deterministic and do not vary between simulations."
  },
  {
    "objectID": "recipes/recipe-10/index.html#lab-preparation",
    "href": "recipes/recipe-10/index.html#lab-preparation",
    "title": "10. Building inference models",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn preparation for Lab 10, ensure that you are comfortable with the following key concepts related to simulation-based inference:\n\nUnderstanding the principles of statistical inference and how simulations can be used to approximate traditional inferential statistics.\nUtilizing {infer} in R to create resampling simulations for hypothesis testing and constructing confidence intervals.\nInterpreting the results of simulation-based inference focusing on what the simulated distributions imply about the population parameters.\n\nIn this lab, you will be challenged to apply these core ideas to a new dataset of your choosing. Reflect on the nature of the data and the hypotheses you might test using simulation-based methods. Consider how you would design your simulation study to address a particular hypothesis. You will be expected to submit your code along with a concise reflection on your methodology and the insights gained from your analysis."
  },
  {
    "objectID": "guides/guide-02/index.html",
    "href": "guides/guide-02/index.html",
    "title": "02. Installing and managing R packages",
    "section": "",
    "text": "Outcomes\n\nRecognize the difference between the R interpreter and interfaces to the R interpreter.\nInstall R packages using the RStudio IDE interface and the R console.\nManage R packages in R sessions and the R environment."
  },
  {
    "objectID": "guides/guide-02/index.html#r-ide",
    "href": "guides/guide-02/index.html#r-ide",
    "title": "02. Installing and managing R packages",
    "section": "R != IDE",
    "text": "R != IDE\nAs you begin your journey into R programming, it is key to understand an important distinction that can often be overlooked by many a clever student; the difference between R and RStudio (or any other integrated development environment (IDE) or editor).\nWhen you install R on your computing environment, what you are in fact installing is an R interpreter. That is, as R is a programming “language”, we need software to make sense of the R code we write and execute. The interpreter is the engine that we send commands to and from which the results are sent back. To send commands to the R interpreter, we can use many various interfaces ranging from black and white screens with a flashing cursor at the prompt to sophisticated graphical user interfaces (GUI), such as RStudio or Visual Studio Code.\nWhen you open up RStudio, you are opening up an IDE that is designed to make working with R easier. It provides a console to interact with the R interpreter, a script editor to write and run R code, and many other features to help you write, debug, and share your R code.\n\n\n\n\n\n\nFigure 1: RStudio on clean start\n\n\n\nRStudio is not R. It is a tool that helps you work with R. You can use R without RStudio, but you cannot use RStudio without R. Keep this distinction in mind as you continue your journey into R programming.\nRStudio has a number of keyboard shortcuts that can be used to speed up your workflow. You can find a list of them here.\nFor starters, here are the ones I use the most to work with Quarto and R:\n\nFor Quarto document elements\n\n\n\n\n\n\nDescription\nShortcut\n\n\n\n\nRender Quarto documents\n\n\n\nAdd a code block to a Quarto document\n\n\n\n\n\n\n\nSymbols used in keyboard shortcuts\n\n\n\n\n\n\nSymbol\nKey\n\n\n\n\nShiftShift\nShift key\n\n\nCtrlCtrl\nControl key\n\n\nCommandCommand\nCommand key\n(Mac only)\n\n\nAltAlt\nAlt key\n\n\nOptionOption\nOption key\n(Mac only)\n\n\nEscEsc\nEscape key\n\n\nTabTab\nTab key\n\n\nEnterEnter\nEnter key\n\n\n\n\nFor R code elements\n\n\n\n\n\n\nDescription\nShortcut\n\n\n\n\nTo invoke code completion when typing R code\n\n\n\nRun current line or selection from the Editor in the Console\n\n\n\nTo comment or uncomment a line or selection so that it is or is not run as R code\n\n\n\nTo insert the &lt;- operator to assign code output to a variable\n\n\n\nTo insert a |&gt; operator to pipe the output of one operation to the input of the next\n\n\n\nTo reformat R code so that indentation is more legible"
  },
  {
    "objectID": "guides/guide-02/index.html#install-packages",
    "href": "guides/guide-02/index.html#install-packages",
    "title": "02. Installing and managing R packages",
    "section": "Install packages",
    "text": "Install packages\nAnother key principle in programming is that there is often more than one way to get something done. For package installation, there are two primary methods: using a GUI, such as the windows and panes in the RStudio IDE interface or using the R console. We will cover both methods here as getting comfortable with both will make you a more versatile R programmer.\nIn RStudio, you can install packages using the Packages pane. This pane is located in the bottom right corner of interface. If you don’t see it, you can open it by clicking on the “Packages” tab in the bottom right corner of the RStudio interface. You can also open it by going to the “Tools” menu and selecting “Install Packages…”.\nFrom the “Install Packages” dialog, you can search for packages by name. As you type a package name, the list of available packages will be filtered. Click install to install the package. You can also install multiple packages at once by separating the package names with a space.\n\n\n\n\n\n\nFigure 2: RStudio Packages pane and Install Packages dialog\n\n\n\nFrom the R console, you can install packages using the install.packages() function. This function takes the name of the package you want to install as an argument. For example, to install {dplyr}, you would run:\ninstall.packages(\"dplyr\")\nYou can also install multiple packages at once by passing a vector of package names to the install.packages() function. For example, to install {dplyr} and {ggplot2} packages, you would run:\ninstall.packages(c(\"dplyr\", \"ggplot2\"))\nIn either case, you will need to select a CRAN mirror the first time you install a package in a new session. At this point, it does not matter which mirror you choose – 1 is usually a good choice.\n\n\n\n\n\n\n Dive deeper\nYou can set a default CRAN mirror in your .Rprofile file. This file is located in your home directory (~/.Rprofile). If the file does not exist, you can create it. Add the following line to set a default CRAN mirror:\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n\n\nBoth methods will install the package and its dependencies from CRAN. If you want to install a package from GitHub (and/ or CRAN), you can use {pak}. Once installed, {pak} provides the pak::pak() function, which can install packages from CRAN or GitHub. To install a package from CRAN, you can use the package name as an argument. To install a package from GitHub, you can use the user/repo format as an argument.\nFor example, let’s install the {stringr} package from CRAN and the {qtkit} package from GitHub:\npak::pak(\"stringr\")\npak::pak(\"qtalr/qtkit\")"
  },
  {
    "objectID": "guides/guide-02/index.html#using-packages",
    "href": "guides/guide-02/index.html#using-packages",
    "title": "02. Installing and managing R packages",
    "section": "Using packages",
    "text": "Using packages\n\nR Sessions\nTo understand how to use packages for programming with R, you need to understand how R sessions work. Every time you open an R session, you start a new R session. This session starts with a clean environment. No packages or variables are available at this point –other than the base R functions and variables.\nWe can see this by running the search() function in the R console.\n\n\n\n\n\n\nFigure 3: R session search path on clean start\n\n\n\n\nAttach packages\nTo make a package available for use in an R session, you need to “attach it” to the search path. This will persist until you close this R session, or “detach” the package manually. You can attach a package to the search path using the library() function. For example, to attach the dplyr package to the search path, you would run:\nlibrary(dplyr)\nYou can see what packages are currently attached by running search().\n\n\n\n\n\n\nFigure 4: R session search path with {dplyr} attached\n\n\n\nIn RStudio, you can see the checkboxes next to the packages in the Packages pane. This is a visual representation of packages attached to the search path.\nSome R session gotchas:\n\nIf you attach a package in one session, it will not be available in another session automatically. Each session is independent.\nRunning an R script or rendering a literate document (e.g., R Markdown, Quarto) will start a new R session, and close it when the script or document is done running.\nTherefore, packages you attach in the R console are not available in other scripts or documents, and vice versa.\nTo make a package available in a script or document, you need to attach it in that script or document. This is a good thing, as it makes your scripts and documents self-contained and reproducible.\n\n\n\nDetach packages\nIf you “quit” an R session (q()), the packages you attached will be detached automatically. You can also detach a package manually by running detach(\"package:package_name\"). For example, to detach {dplyr}, you would run:\ndetach(\"package:dplyr\")\n\n\n\nManaging packages\nIn addition to installing packages, you may need to manage them. This will include listing packages that have newer versions available, updating packages, and removing packages.\nIn RStudio, you can see which packages have newer versions available in the Packages pane. The “Updates” tab will list packages that have newer versions available. You can update packages by selecting the checkboxes or some or all packages and clicking the “Update” button.\nAt the R Console, the old.packages() function will list packages that have newer versions available. You can then decide which packages to update, or just update all packages by running:\nupdate.packages(ask = FALSE)\nThis will update all packages without asking for confirmation. It also is of note that this function updates packages installed by any method. That is, there is not a {pak} version of this function.\nTo remove a package in RSudio, you can click the  button next to the package in the Packages pane. In the R console, you can remove a package using the remove.packages() function. For example, to remove the {dplyr} package, you would run:\nremove.packages(\"dplyr\")\n\n\n\n\n\n\n Dive deeper\nIf you are working on a project and want to ensure that the package versions are consistent across all collaborators, you can use {renv}. {renv} is a package that helps you create and manage project-specific R environments. You can use {renv} to create a project-specific library of packages, and ensure that all collaborators are using the same versions of packages. To use {renv}, you will need to install it first:\ninstall.packages(\"renv\")\nThen, you can use the following functions to manage packages in your project:\n\nrenv::init(): Initialize a new project with {renv}.\nrenv::install(\"package_name\"): Install a package in the project library.\nrenv::snapshot(): Snapshot the project library.\nrenv::restore(): Restore the project library to a previous snapshot.\nrenv::status(): Show the status of the project library."
  },
  {
    "objectID": "guides/guide-02/index.html#summary",
    "href": "guides/guide-02/index.html#summary",
    "title": "02. Installing and managing R packages",
    "section": "Summary",
    "text": "Summary\nIn this guide, we covered how to install and manage R packages. We discussed two primary methods for installing packages: using the RStudio IDE interface and using the R console. We also covered how to attach and detach packages in an R session, and how to manage packages by listing, updating, and removing them. Finally, we introduced {renv} as a way to manage project-specific R environments."
  },
  {
    "objectID": "guides/guide-04/index.html",
    "href": "guides/guide-04/index.html",
    "title": "04. Setting up Git and GitHub",
    "section": "",
    "text": "Outcomes\n\nRecognize the purpose of Git and GitHub\nEstablish a working Git and GitHub environment\nRecognize the basic Git and GitHub workflow for managing a project"
  },
  {
    "objectID": "guides/guide-04/index.html#introduction",
    "href": "guides/guide-04/index.html#introduction",
    "title": "04. Setting up Git and GitHub",
    "section": "Introduction",
    "text": "Introduction\nThis textbook places a heavy emphasis on reproducible research. The most important thing you can do to ensure reproducibility is to use a version control system and a hosting service. For most people, the best option is Git and GitHub."
  },
  {
    "objectID": "guides/guide-04/index.html#what-is-git-github-and-why-should-i-use-it",
    "href": "guides/guide-04/index.html#what-is-git-github-and-why-should-i-use-it",
    "title": "04. Setting up Git and GitHub",
    "section": "What is Git, Github? And why should I use it?",
    "text": "What is Git, Github? And why should I use it?\nGit is a version control system. It allows you to track changes to files and folders over time. It also allows you to collaborate with others on projects. Think of it as MS Word’s “Track Changes” feature on steroids.\nGit is a command line tool, but there are also GUIs to interact with Git in a more user-friendly way. Git is a great tool for managing projects. It is especially useful for managing projects that involve multiple people.\nGitHub is a web-based hosting service for Git repositories. It allows you to store your Git repositories in the cloud. It also allows you to collaborate with others on projects by sharing projects. GitHub is a great place to store and share your code. It is also a great place to find code that others have shared.\nCombining Git with GitHub allows you to store your Git managed project repositories in the cloud. This means that you can access your repositories from anywhere. It also means that you can collaborate with others on projects. You can also use GitHub to share your code with others. This is especially useful for making your projects reproducible."
  },
  {
    "objectID": "guides/guide-04/index.html#how-do-i-set-up-git-and-github",
    "href": "guides/guide-04/index.html#how-do-i-set-up-git-and-github",
    "title": "04. Setting up Git and GitHub",
    "section": "How do I set up Git and GitHub?",
    "text": "How do I set up Git and GitHub?\n\nInstall and setup Git\nThe process for installation and setup will differ based on what operating system you are using. If you are using a Windows machine, you will likely need to install Git. If you are using a Mac or Linux machine, you will likely already have Git installed.\nWindows users can install Git by downloading the installer from https://git-scm.com/downloads. Once you have downloaded the installer, you will need to run it. You will need to follow the instructions in the installer to complete the installation.\n\n\nTerminal pane in RStudio\n\nMac and Linux users can verify that Git is installed by opening a terminal window and typing git --version. If Git is installed, you will see a version number. If Git is not installed, you will see an error message. If you need to install Git, you can do so following these instructions:\n\nMac users can install Xcode Command Line Tools by running xcode-select --install in the terminal. This will install Git along with other tools that may be useful for development.\nLinux users can install Git using the package manager for their distribution. For example, Ubuntu users can install Git by running sudo apt-get install git in the terminal.\n\nOnce you have Git installed, you will need to set up your Git configuration. Most of the defaults will be fine for now, but you will need to at least set your name and email address, this will be the same address you use to create your GitHub account, so choose accordingly.\nYou can do this by opening a terminal window and entering the following commands (changing the name and email address to your own):\ngit config --global user.name \"Your Name\"; \\\ngit config --global user.email \"your.email@email.edu\"\nAlternatively, you can use {usethis}(Wickham et al. 2024) in R to set up Git in your environment. Open RStudio, install and/ or load {usethis}, and run the following code in the console:\nuse_git_config(user.name = \"Jerid Francom\", user.email = \"francojc@wfu.edu\")\n\n\n\n\n\n\nFigure 1: Console pane in RStudio\n\n\n\n\n\nHow do I set up GitHub?\nTo set up GitHub, you will need to create an account. You can do this at [github.com(https://github.com). Be sure to use the email address you used in the Git setup!\nThe service is free and there are extra features available for students and educators. Once you have created an account, you will be able to create repositories. You can also create organizations and teams. You can use these to collaborate with others on projects."
  },
  {
    "objectID": "guides/guide-04/index.html#understanding-the-basic-git-and-github-workflow",
    "href": "guides/guide-04/index.html#understanding-the-basic-git-and-github-workflow",
    "title": "04. Setting up Git and GitHub",
    "section": "Understanding the basic Git and GitHub workflow",
    "text": "Understanding the basic Git and GitHub workflow\nOnce your Git installation and Github account are set up, a number of options for working with and managing your project are available to you. At this point, let’s focus on one typical scenario that you will encounter early on in this textbook.\n\n\n\n\n\n\nFigure 2: Visualizing a common scenario using Git and GitHub\n\n\n\nSome definitions are in order.\nFirst, let’s define some key nouns:\n\nRepository: This is a collection of folders (directories) and files in a project that are managed by Git.\nRemote Repository: This is the repository that is stored on GitHub. It can be your repository or a repository owned by someone else. These repositories can be public, accessible by anyone, or private, accessible only by those with permission.\nLocal Repository: This is the repository that is stored on your local machine. This is where you will make changes to your project. The local repository may be a copy of a remote repository or a new repository that you create that is not stored on GitHub (yet, or ever).\n\nNow, some key verbs:\n\nClone: This is the process of copying a remote repository to your local machine.\nEdit: This is the process of making changes to the files in your local repository.\n\nThere are many other actions that you can perform with Git and GitHub, but these are sufficient to get you started with this textbook.\n\n\n\n\n\n\n Tip\nBryan and Hester (2020) is an excellent reference resource for all things Git and GitHub for R users."
  },
  {
    "objectID": "guides/guide-04/index.html#how-do-i-manage-my-project-with-git-and-github",
    "href": "guides/guide-04/index.html#how-do-i-manage-my-project-with-git-and-github",
    "title": "04. Setting up Git and GitHub",
    "section": "How do I manage my project with Git and GitHub?",
    "text": "How do I manage my project with Git and GitHub?\nLet’s describe the step-wise process in Scenario A, visualized in the diagram above. This scenario involves cloning a remote repository to your local machine and then making changes to the files in the local repository. Conveniently, this is the process you will follow when working on a lab assignment in this textbook.\n\nNavigate to the repository on GitHub\nClick on the ‘Code’ button and copy the clone URL (https) to your clipboard.\nOpen RStudio\nFrom the menu, select File &gt; New Project &gt; Version Control &gt; Git\nPaste the URL of the repository into the ‘Repository URL’ field\nChoose the directory location for the project\nClick ‘Create Project’\n\nRStudio will then clone the repository to your local machine and open the project as a new RStudio project. You can now make changes to the files in the project."
  },
  {
    "objectID": "guides/guide-04/index.html#summary",
    "href": "guides/guide-04/index.html#summary",
    "title": "04. Setting up Git and GitHub",
    "section": "Summary",
    "text": "Summary\nIn this guide, we covered the basics of setting up Git and GitHub. We also covered the basics of using Git and GitHub to manage a project. We discussed the purpose of Git and GitHub, and why you should use them. We also discussed how to install and set up Git, how to set up GitHub, and the basic Git and GitHub workflow for managing a project.\nWe will return to Git and GitHub later in the textbook to cover more functionality and use cases."
  },
  {
    "objectID": "guides/guide-01/index.html",
    "href": "guides/guide-01/index.html",
    "title": "01. Setting up an R environment",
    "section": "",
    "text": "Choosing to work with R locally means that you will install R and an integrated development environment (IDE) on your local computer. This approach offers the following advantages:\n\nFast and responsive performance\nNo (inherent) reliance on internet connectivity\nIncreased flexibility to customize your environment\n\nThe main disadvantages of working locally are:\n\nyou will need to install R and an IDE on your local computer,\nmanage your own software environment, and\nmanage your own backups and version control for collaborative projects.\n\nRunning and managing R locally can sometimes be a challenge for new users, compared to some other methds. However, there are important advantages to this approach. Remember there are a number of resources available to help you get started and troubleshoot any issues you may encounter.\nTo get started, install R from CRAN. You can download the latest version of R for your operating system here. Once you have installed R, you will need to install an IDE or editor. For complete beginners, I recommend RStudio, a free and open-source IDE for R. RStudio provides a number of features that make it easier to work with R. If you have experience with programming and/ or are looking for a more customizable editor, you may prefer to use Visual Studio Code. Setting up VS Code for R can be found here.\n\n\n\nAn alternative to running R locally is to work with R in the cloud. This is known as a remote environment. There are a number of cloud-based options for working with R, including Posit Cloud, Google Colab, and Microsoft Azure. These options provide an R environment that you can access from any computer with an internet connection.\nRemote environments provide an environment where you can create, edit, and run R projects from anywhere with internet access. They offers several advantages:\n\nNo need to install R or an IDE/ editor locally\nAccess your projects from any device\nCollaborate with others in real-time\nEasily share your work\n\nSome of the drawbacks of working in the cloud include\n\nReliance on stable internet connection\nPotential latency and performance issues\nSomewhat limited customization options compared to a local setup\n\nTo get started with Posit Cloud, you will need to create an account. You can sign up for a free account here. Once you have created an account, you will see a list of spaces. By default you will have your personal workspace, but you can also join or be invited to other spaces. Instructors may create spaces for their courses which can provide pre-configured environments for students.\n\n\n\n\n\n\nFigure 1: Posit Cloud interface\n\n\n\nVisit the Guide documentation to learn more about the features of Posit Cloud.\n\n\n\nIf you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE/ editor on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nIn addtion to the advantages of working with R locally, using Docker with Rocker offers several benefits:\n\nSafe and isolated environment from the host system\nReproducible and portable environments\nSimplified dependency management using {pak}\n\nThe drawbacks to using Docker with Rocker include:\n\nLearning curve for setting up and managing Docker containers\nIncreased memory and resource requirements for the host machine\nIncreased complexity in managing Git/ GitHub credentials\n\nTo start using Docker with a Rocker image, follow these steps:\n\nInstall Docker on your local machine (pay special attention to the installation instructions for your operating system)\nPull the desired Rocker image from Docker Hub\n\n\n\n\n\n\n\nFigure 2: Pull rocker/rstudio image from Docker Hub\n\n\n\n\nRun a container using the pulled image\nNote: you will need to specify the following options before running the container.\n\n\ncontainer name: no spaces\nport mapping: 8787 on the host to 8787 on the container\nenvironment variables: PASSWORD to set the password for RStudio, ROOT to allow root access.\n\n\n\n\n\n\n\nFigure 3: Run a container using the rocker/rstudio image\n\n\n\nOptional: you can also mount a volume to share files between the host and container.\n\nAccess RStudio in your browser at http://localhost:8787 and log in with username rstudio and the password you set"
  },
  {
    "objectID": "guides/guide-01/index.html#environment-setups",
    "href": "guides/guide-01/index.html#environment-setups",
    "title": "01. Setting up an R environment",
    "section": "",
    "text": "Choosing to work with R locally means that you will install R and an integrated development environment (IDE) on your local computer. This approach offers the following advantages:\n\nFast and responsive performance\nNo (inherent) reliance on internet connectivity\nIncreased flexibility to customize your environment\n\nThe main disadvantages of working locally are:\n\nyou will need to install R and an IDE on your local computer,\nmanage your own software environment, and\nmanage your own backups and version control for collaborative projects.\n\nRunning and managing R locally can sometimes be a challenge for new users, compared to some other methds. However, there are important advantages to this approach. Remember there are a number of resources available to help you get started and troubleshoot any issues you may encounter.\nTo get started, install R from CRAN. You can download the latest version of R for your operating system here. Once you have installed R, you will need to install an IDE or editor. For complete beginners, I recommend RStudio, a free and open-source IDE for R. RStudio provides a number of features that make it easier to work with R. If you have experience with programming and/ or are looking for a more customizable editor, you may prefer to use Visual Studio Code. Setting up VS Code for R can be found here.\n\n\n\nAn alternative to running R locally is to work with R in the cloud. This is known as a remote environment. There are a number of cloud-based options for working with R, including Posit Cloud, Google Colab, and Microsoft Azure. These options provide an R environment that you can access from any computer with an internet connection.\nRemote environments provide an environment where you can create, edit, and run R projects from anywhere with internet access. They offers several advantages:\n\nNo need to install R or an IDE/ editor locally\nAccess your projects from any device\nCollaborate with others in real-time\nEasily share your work\n\nSome of the drawbacks of working in the cloud include\n\nReliance on stable internet connection\nPotential latency and performance issues\nSomewhat limited customization options compared to a local setup\n\nTo get started with Posit Cloud, you will need to create an account. You can sign up for a free account here. Once you have created an account, you will see a list of spaces. By default you will have your personal workspace, but you can also join or be invited to other spaces. Instructors may create spaces for their courses which can provide pre-configured environments for students.\n\n\n\n\n\n\nFigure 1: Posit Cloud interface\n\n\n\nVisit the Guide documentation to learn more about the features of Posit Cloud.\n\n\n\nIf you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE/ editor on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nIn addtion to the advantages of working with R locally, using Docker with Rocker offers several benefits:\n\nSafe and isolated environment from the host system\nReproducible and portable environments\nSimplified dependency management using {pak}\n\nThe drawbacks to using Docker with Rocker include:\n\nLearning curve for setting up and managing Docker containers\nIncreased memory and resource requirements for the host machine\nIncreased complexity in managing Git/ GitHub credentials\n\nTo start using Docker with a Rocker image, follow these steps:\n\nInstall Docker on your local machine (pay special attention to the installation instructions for your operating system)\nPull the desired Rocker image from Docker Hub\n\n\n\n\n\n\n\nFigure 2: Pull rocker/rstudio image from Docker Hub\n\n\n\n\nRun a container using the pulled image\nNote: you will need to specify the following options before running the container.\n\n\ncontainer name: no spaces\nport mapping: 8787 on the host to 8787 on the container\nenvironment variables: PASSWORD to set the password for RStudio, ROOT to allow root access.\n\n\n\n\n\n\n\nFigure 3: Run a container using the rocker/rstudio image\n\n\n\nOptional: you can also mount a volume to share files between the host and container.\n\nAccess RStudio in your browser at http://localhost:8787 and log in with username rstudio and the password you set"
  },
  {
    "objectID": "guides/guide-01/index.html#summary",
    "href": "guides/guide-01/index.html#summary",
    "title": "01. Setting up an R environment",
    "section": "Summary",
    "text": "Summary\nIn this guide, we have discussed strategies for working with R. All three options offer unique advantages. In Table 1, we summarize some of the characteristics, benefits, and drawbacks of each option.\n\n\n\nTable 1: Comparison of different environments for working with R and RStudio\n\n\n\n\n\n\n\n\n\n\n\nEnvironment\nCharacteristics\nBenefits\nDrawbacks\n\n\n\n\nLocal (Computer)\n- R/RStudio installed locally- Project files stored on local machine- Accessible without internet connection- Full control over software version and environment\n- Fast and responsive performance- No reliance on internet connectivity- Ability to work offline- Complete control over software version and environment\n- Limited collaboration options- Difficulty in sharing projects with others- Potential compatibility issues with different operating systems\n\n\nRemote (Cloud)\n- R/RStudio accessed via web browser- Project files stored on cloud server- Accessible from any device with internet connection- Easy collaboration with others- Automatic backups and version control\n- No need for local installation or setup- Easy access from anywhere- Seamless collaboration with teammates- Backup and version control provided by the cloud service\n- Reliance on stable internet connection- Potential latency and performance issues- Limited customization options compared to a local setup\n\n\nVirtual (Docker)\n- R/RStudio environment encapsulated in a Docker container- Project files stored locally or on the cloud- Consistent environment across different machines\n- Reproducible and portable environment- Easy setup and sharing of the container- Flexibility to run on different operating systems- Isolation from host system dependencies\n- Learning curve for setting up and managing Docker containers- Increased memory and resource requirements- Potential compatibility issues with certain packages or libraries\n\n\n\n\n\n\nGive them a try and see which one works best for your needs! Remember, you can always switch between different environments as your needs change."
  },
  {
    "objectID": "guides/guide-01/index.html#references",
    "href": "guides/guide-01/index.html#references",
    "title": "01. Setting up an R environment",
    "section": "References",
    "text": "References"
  }
]