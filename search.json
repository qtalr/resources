[
  {
    "objectID": "guides/guide-01/index.html",
    "href": "guides/guide-01/index.html",
    "title": "1. Setting up an R environment",
    "section": "",
    "text": "Choosing to work with R locally means that you will install R and an IDE on your local computer. This approach offers the following advantages:\n\nFast and responsive performance\nNo reliance on internet connectivity\nFlexibility to customize your environment\n\nThe main disadvantages of working locally are:\n\nyou will need to install R and an IDE on your local computer,\nmanage your own software environment, and\nmanage your own backups and version control for collaborative projects.\n\nThis can be a challenge for new users, but there are a number of resources available to help you get started and troubleshoot any issues you may encounter.\nTo get started, install R from CRAN. You can download the latest version of R for your operating system here. Once you have installed R, you will need to install an IDE. For complete beginners, I recommend RStudio, a free and open-source IDE for R. RStudio provides a number of features that make it easier to work with R. If you are new to R, but have experience with other programming languages, you may prefer to use a more general-purpose IDE such as VS Code.\n\n\n\nYou can also choose to work with R in the cloud, a remote environment. There are a number of cloud-based options for working with R, including Posit Cloud and Microsoft Azure. These options provide a pre-configured R environment that you can access from any computer with an internet connection.\nPosit Cloud provides an environment where you can create, edit, and run R projects from anywhere with internet access. It offers several advantages:\n\nNo need to install R or RStudio locally\nAccess your projects from any device\nCollaborate with others in real-time\nEasily share your work\n\nSome of the drawbacks of working in the cloud include:\n\nReliance on stable internet connection\nPotential latency and performance issues\nLimited customization options compared to a local setup\n\nTo get started with Posit Cloud, you will need to create an account. You can sign up for a free account here. Once you have created an account, you will see a list of spaces. By default you will have your personal workspace, but you can also join or be invited to other spaces.\nVisit the Guide documentation to learn more about the features of Posit Cloud.\n\n\n\nIf you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Virtual environments are a good option if you want to ensure that everyone in your research group is working with the same computing environment. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nUsing Docker with Rocker offers several benefits:\n\nReproducible environments\nSimplified dependency management\nEasy deployment and scaling\n\nThe drawbacks to using Docker with Rocker include:\n\nLearning curve for setting up and managing Docker containers\nIncreased memory and resource requirements\nPotential compatibility issues with certain packages or libraries\n\nTo start using Docker with Rocker, follow these steps:\n\nInstall Docker on your local machine\nPull the desired Rocker image from Docker Hub\ndocker pull rocker/rstudio\nRun a container using the pulled image\ndocker run -d -p 8787:8787 -e PASSWORD=your_password --name rstudio_container rocker/rstudio\nAccess RStudio in your browser at http://localhost:8787 and log in with username rstudio and the password you set"
  },
  {
    "objectID": "guides/guide-01/index.html#environment-setups",
    "href": "guides/guide-01/index.html#environment-setups",
    "title": "1. Setting up an R environment",
    "section": "",
    "text": "Choosing to work with R locally means that you will install R and an IDE on your local computer. This approach offers the following advantages:\n\nFast and responsive performance\nNo reliance on internet connectivity\nFlexibility to customize your environment\n\nThe main disadvantages of working locally are:\n\nyou will need to install R and an IDE on your local computer,\nmanage your own software environment, and\nmanage your own backups and version control for collaborative projects.\n\nThis can be a challenge for new users, but there are a number of resources available to help you get started and troubleshoot any issues you may encounter.\nTo get started, install R from CRAN. You can download the latest version of R for your operating system here. Once you have installed R, you will need to install an IDE. For complete beginners, I recommend RStudio, a free and open-source IDE for R. RStudio provides a number of features that make it easier to work with R. If you are new to R, but have experience with other programming languages, you may prefer to use a more general-purpose IDE such as VS Code.\n\n\n\nYou can also choose to work with R in the cloud, a remote environment. There are a number of cloud-based options for working with R, including Posit Cloud and Microsoft Azure. These options provide a pre-configured R environment that you can access from any computer with an internet connection.\nPosit Cloud provides an environment where you can create, edit, and run R projects from anywhere with internet access. It offers several advantages:\n\nNo need to install R or RStudio locally\nAccess your projects from any device\nCollaborate with others in real-time\nEasily share your work\n\nSome of the drawbacks of working in the cloud include:\n\nReliance on stable internet connection\nPotential latency and performance issues\nLimited customization options compared to a local setup\n\nTo get started with Posit Cloud, you will need to create an account. You can sign up for a free account here. Once you have created an account, you will see a list of spaces. By default you will have your personal workspace, but you can also join or be invited to other spaces.\nVisit the Guide documentation to learn more about the features of Posit Cloud.\n\n\n\nIf you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Virtual environments are a good option if you want to ensure that everyone in your research group is working with the same computing environment. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nUsing Docker with Rocker offers several benefits:\n\nReproducible environments\nSimplified dependency management\nEasy deployment and scaling\n\nThe drawbacks to using Docker with Rocker include:\n\nLearning curve for setting up and managing Docker containers\nIncreased memory and resource requirements\nPotential compatibility issues with certain packages or libraries\n\nTo start using Docker with Rocker, follow these steps:\n\nInstall Docker on your local machine\nPull the desired Rocker image from Docker Hub\ndocker pull rocker/rstudio\nRun a container using the pulled image\ndocker run -d -p 8787:8787 -e PASSWORD=your_password --name rstudio_container rocker/rstudio\nAccess RStudio in your browser at http://localhost:8787 and log in with username rstudio and the password you set"
  },
  {
    "objectID": "guides/guide-01/index.html#summary",
    "href": "guides/guide-01/index.html#summary",
    "title": "1. Setting up an R environment",
    "section": "Summary",
    "text": "Summary\nIn this guide, we have discussed strategies for working with R. All three options offer unique advantages. In Table 1, we summarize some of the characteristics, benefits, and drawbacks of each option.\n\n\n\nTable 1: Comparison of different environments for working with R and RStudio\n\n\n\n\n\n\n\n\n\n\n\nEnvironment\nCharacteristics\nBenefits\nDrawbacks\n\n\n\n\nLocal (Computer)\n- R/RStudio installed locally- Project files stored on local machine- Accessible without internet connection- Full control over software version and environment\n- Fast and responsive performance- No reliance on internet connectivity- Ability to work offline- Complete control over software version and environment\n- Limited collaboration options- Difficulty in sharing projects with others- Potential compatibility issues with different operating systems\n\n\nRemote (Cloud)\n- R/RStudio accessed via web browser- Project files stored on cloud server- Accessible from any device with internet connection- Easy collaboration with others- Automatic backups and version control\n- No need for local installation or setup- Easy access from anywhere- Seamless collaboration with teammates- Backup and version control provided by the cloud service\n- Reliance on stable internet connection- Potential latency and performance issues- Limited customization options compared to a local setup\n\n\nVirtual (Docker)\n- R/RStudio environment encapsulated in a Docker container- Project files stored locally or on the cloud- Consistent environment across different machines\n- Reproducible and portable environment- Easy setup and sharing of the container- Flexibility to run on different operating systems- Isolation from host system dependencies\n- Learning curve for setting up and managing Docker containers- Increased memory and resource requirements- Potential compatibility issues with certain packages or libraries\n\n\n\n\n\n\nGive them a try and see which one works best for your needs! Remember, you can always switch between different environments as your needs change."
  },
  {
    "objectID": "guides/guide-01/index.html#references",
    "href": "guides/guide-01/index.html#references",
    "title": "1. Setting up an R environment",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "guides/guide-04/index.html",
    "href": "guides/guide-04/index.html",
    "title": "Guide 04",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "guides/guide-02/index.html",
    "href": "guides/guide-02/index.html",
    "title": "Guide 02",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "recipes/recipe-10/index.html",
    "href": "recipes/recipe-10/index.html",
    "title": "10. Recipe",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "recipes/recipe-03/index.html",
    "href": "recipes/recipe-03/index.html",
    "title": "03. Descriptive assessment of datasets",
    "section": "",
    "text": "Skills\n\nSummary overviews of datasets with skimr\nSummary statistics with dplyr\nCreating Quarto tables with knitr\nCreating Quarto plots with ggplot2"
  },
  {
    "objectID": "recipes/recipe-03/index.html#concepts-and-strategies",
    "href": "recipes/recipe-03/index.html#concepts-and-strategies",
    "title": "03. Descriptive assessment of datasets",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\nIn this Recipe, we will use the PassiveBrownFam dataset from the corpora package (Evert 2023). This dataset contains information on the passive voice usage in the Brown family of corpora. The dataset contains 11 variables and 2,449 observations.\nI have assigned this dataset to the object brown_fam_df and have made minor modifications to the variable names to improve the readability of the dataset.\n\n\n# Load packages\nlibrary(dplyr)\n\n# Read the dataset from the `corpora` package\nbrown_fam_df &lt;-\n  corpora::PassiveBrownFam |&gt; # reference the dataset\n  as_tibble() # convert to a tibble\n\n# Rename variables\nbrown_fam_df &lt;-\n  brown_fam_df |&gt; # pass the original dataset\n  rename( # rename variables: new_name = old_name\n    lang_variety = lang,\n    num_words = n.words,\n    active_verbs = act,\n    passive_verbs = pass,\n    total_verbs = verbs,\n    percent_passive = p.pass\n  )\n\n# Preview\nglimpse(brown_fam_df)\n\n&gt; Rows: 2,499\n&gt; Columns: 11\n&gt; $ id              &lt;chr&gt; \"brown_A01\", \"brown_A02\", \"brown_A03\", \"brown_A04\", \"b…\n&gt; $ corpus          &lt;fct&gt; Brown, Brown, Brown, Brown, Brown, Brown, Brown, Brown…\n&gt; $ section         &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, …\n&gt; $ genre           &lt;fct&gt; press reportage, press reportage, press reportage, pre…\n&gt; $ period          &lt;fct&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, …\n&gt; $ lang_variety    &lt;fct&gt; AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE,…\n&gt; $ num_words       &lt;int&gt; 2080, 2116, 2051, 2095, 2111, 2102, 2099, 2069, 2058, …\n&gt; $ active_verbs    &lt;int&gt; 164, 154, 135, 128, 170, 166, 165, 163, 153, 169, 132,…\n&gt; $ passive_verbs   &lt;int&gt; 40, 25, 34, 25, 32, 21, 31, 19, 39, 23, 17, 10, 15, 26…\n&gt; $ total_verbs     &lt;int&gt; 204, 179, 169, 153, 202, 187, 196, 182, 192, 192, 149,…\n&gt; $ percent_passive &lt;dbl&gt; 19.61, 13.97, 20.12, 16.34, 15.84, 11.23, 15.82, 10.44…\n\n\nYou can learn more about these variables by reading the dataset documentation with ?corpora::PassiveBrownFam.\n\nStatistical overviews\nUnderstanding our data is of utmost importance before, during, and after analysis. After we get to know our data by inspecting the data origin, dictionary, and structure, we then move to summarizing the data.\nA statistical overview of the data is a good place to start as it gives us a sense of all of the variables and variable types in the dataset. We can use the skimr package to create a statistical overview of the data, using the very convienent skim() function.\nLet’s create a statistical overview of the brown_fam_df dataset.\n\n# Load packages\nlibrary(skimr)\n\n# Create a statistical overview of the `brown_fam_df` dataset\nskim(brown_fam_df)\n\n\n# ── Data Summary ────────────────────────\n#                            Values      \n# Name                       brown_fam_df\n# Number of rows             2499        \n# Number of columns          11          \n# _______________________                \n# Column type frequency:                 \n#   character                1           \n#   factor                   5           \n#   numeric                  5           \n# ________________________               \n# Group variables            None        \n# \n# ── Variable type: character ────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate min max empty n_unique whitespace\n# 1 id                    0             1   7   9     0     2499          0\n# \n# ── Variable type: factor ───────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate ordered n_unique\n# 1 corpus                0             1 FALSE          5\n# 2 section               0             1 FALSE         15\n# 3 genre                 0             1 FALSE         15\n# 4 period                0             1 FALSE          3\n# 5 lang_variety          0             1 FALSE          2\n#   top_counts                            \n# 1 BLO: 500, Bro: 500, LOB: 500, FLO: 500\n# 2 J: 400, G: 381, F: 228, A: 220        \n# 3 lea: 400, bel: 381, pop: 228, pre: 220\n# 4 196: 1000, 199: 999, 193: 500         \n# 5 BrE: 1500, AmE: 999                   \n# \n# ── Variable type: numeric ──────────────────────────────────────────────────────\n#   skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50\n# 1 num_words               0             1 2165.  97.8  1406     2127    2163  \n# 2 active_verbs            0             1  179.  56.6    39      139     170  \n# 3 passive_verbs           0             1   25.7 12.9     2       16      23  \n# 4 total_verbs             0             1  204.  49.1    66      170     196  \n# 5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1\n#      p75   p100 hist \n# 1 2200   4397   ▁▇▁▁▁\n# 2  214    551   ▃▇▂▁▁\n# 3   32     86   ▆▇▂▁▁\n# 4  234    571   ▃▇▂▁▁\n# 5   18.2   67.7 ▇▅▁▁▁\n\n\nThe output of the skim() function contains a lot of information but it essentially has two parts: a summary of the dataset and a summary of each variable in the dataset. The summary of each of the variables, however, is grouped by variable type. Remember, each of our variables in a data frame is a vector and each vector has a type.\nWe have already learned about different types of vectors in R, including character, numeric, and logical. In this dataset, we are presented with a new type of vector: a factor. A factor is essentially a character vector that contains a set of discrete values, or levels. Factors can be ordered or unordered and can contain levels that are not present in the data.\nNow, looking at each of the variable types, we can see that we have 1 character variable, 5 factor variables, and 5 numeric variables. Each of these variable types assume a different set of summary statistics. For example, we can calculate the mean of a numeric variable but not of a character variable. Or, we can count the number of unique values in a character variable but not in a numeric variable.\nFor all variables, skim() will also provide the number of missing values and the percent of non-missing values.\nInspecting the entire dataset is a good place to start but at some point we often want focus in on a set of variables. We can add the yank() function to extract the statistical overview of a set of variables by their variable types.\nLet’s extract the statistical overview of the numeric variables in the brown_fam_df dataset.\n\n# Extract the statistical overview of the numeric variables\nbrown_fam_df |&gt;\n  skim() |&gt;\n  yank(\"numeric\")\n\n── Variable type: numeric ─────────────────────────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50    p75   p100 hist\n1 num_words               0             1 2165.  97.8  1406     2127    2163   2200   4397   ▁▇▁▁▁\n2 active_verbs            0             1  179.  56.6    39      139     170    214    551   ▃▇▂▁▁\n3 passive_verbs           0             1   25.7 12.9     2       16      23     32     86   ▆▇▂▁▁\n4 total_verbs             0             1  204.  49.1    66      170     196    234    571   ▃▇▂▁▁\n5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1   18.2   67.7 ▇▅▁▁▁\n\n\nSummary statistics of particular variables\nThese summary statistics are useful but for a preliminary and interactive use, but it is oftent the case that we will want to focus in on a particular variable or set of variables and their potential relationships to other variables.\nWe can use the dplyr package to calculate summary statistics for a particular variable or set of variables. We can use the group_by() function to group the data by a particular variable or variables. Then we can use the summarize() function to calculate summary statistics for the grouped data.\nFor example, let’s calculate the mean and median of the percent_passive variable in the brown_fam_df dataset grouped by the lang_variety variable.\n\n# Mean and median of `percent_passive` grouped by `lang_variety`\nbrown_fam_df |&gt;\n  group_by(lang_variety) |&gt;\n  summarize(\n    mean_percent_passive = mean(percent_passive),\n    median_percent_passive = median(percent_passive)\n  )\n\n&gt; # A tibble: 2 × 3\n&gt;   lang_variety mean_percent_passive median_percent_passive\n&gt;   &lt;fct&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;\n&gt; 1 AmE                          12.9                   11.0\n&gt; 2 BrE                          14.8                   13.3\n\n\nThe result is a 2x3 data frame which includes both the mean and median of the percent_passive variable for each of the two levels of the lang_variety variable.\nThe group_by() function can also be used to group by multiple variables. For example, let’s calculate the mean and median of the percent_passive variable in the brown_fam_df dataset grouped by the lang_variety and genre variables.\n\n# Mean and median of `percent_passive` grouped by\n# `lang_variety` and `genre`\nbrown_fam_df |&gt;\n  group_by(lang_variety, genre) |&gt;\n  summarize(\n    mean_percent_passive = mean(percent_passive),\n    median_percent_passive = median(percent_passive)\n  )\n\n&gt; # A tibble: 30 × 4\n&gt; # Groups:   lang_variety [2]\n&gt;    lang_variety genre            mean_percent_passive median_percent_passive\n&gt;    &lt;fct&gt;        &lt;fct&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;\n&gt;  1 AmE          press reportage                 11.5                   11.0 \n&gt;  2 AmE          press editorial                 10.6                   10.1 \n&gt;  3 AmE          press reviews                    9.54                   9.77\n&gt;  4 AmE          religion                        14.3                   14.3 \n&gt;  5 AmE          skills / hobbies                14.9                   13.9 \n&gt;  6 AmE          popular lore                    14.0                   12.7 \n&gt;  7 AmE          belles lettres                  12.0                   11.7 \n&gt;  8 AmE          miscellaneous                   23.5                   23.3 \n&gt;  9 AmE          learned                         21.3                   18.3 \n&gt; 10 AmE          general fiction                  6.22                   5.89\n&gt; # ℹ 20 more rows\n\n\nFor numeric variables, such as percent_passive, there are a number of summary statistics that we can calculate. We’ve seen the R functions for mean and median but we can also calculate the standard deviation (sd()), variance (var()), minimum (min()), maximum (max()), interquartile range (IQR()), median absolute deviation (mad()), and quantiles (quantile()). All these calculations make sense for numeric variables but not for character variables.\nFor character variables, and factors, the summary statistics are more limited. We can calculate the number of observations (n()) and/ or the number of unique values (n_distinct()). Let’s now summarize the number of observations n() grouped by the genre variable in the brown_fam_df dataset.\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  group_by(genre) |&gt;\n  summarize(\n    n = n(),\n  )\n\n&gt; # A tibble: 15 × 2\n&gt;    genre                n\n&gt;    &lt;fct&gt;            &lt;int&gt;\n&gt;  1 press reportage    220\n&gt;  2 press editorial    135\n&gt;  3 press reviews       85\n&gt;  4 religion            85\n&gt;  5 skills / hobbies   186\n&gt;  6 popular lore       228\n&gt;  7 belles lettres     381\n&gt;  8 miscellaneous      150\n&gt;  9 learned            400\n&gt; 10 general fiction    145\n&gt; 11 detective          120\n&gt; 12 science fiction     30\n&gt; 13 adventure          144\n&gt; 14 romance            145\n&gt; 15 humour              45\n\n\nJust as before, we can add multiple grouping variables to group_by(). Let’s add lang_variety to the grouping and calculate the number of observations n() grouped by the genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  )\n\n&gt; # A tibble: 30 × 3\n&gt; # Groups:   genre [15]\n&gt;    genre            lang_variety     n\n&gt;    &lt;fct&gt;            &lt;fct&gt;        &lt;int&gt;\n&gt;  1 press reportage  AmE             88\n&gt;  2 press reportage  BrE            132\n&gt;  3 press editorial  AmE             54\n&gt;  4 press editorial  BrE             81\n&gt;  5 press reviews    AmE             34\n&gt;  6 press reviews    BrE             51\n&gt;  7 religion         AmE             34\n&gt;  8 religion         BrE             51\n&gt;  9 skills / hobbies AmE             72\n&gt; 10 skills / hobbies BrE            114\n&gt; # ℹ 20 more rows\n\n\n\n\n\n\n\n\n Tip\nThe result of calculating the number of observations for a character or factor variable is known as a frequency table. Grouping two or more categorical variables is known as a cross-tabulation or a contingency table.\n\n\n\nNow, we can also pipe the results of a group_by() and summarize() to another function. This can be to say sort, select, or filter the results. It can also be to perform another summary function. It is important, however, to remember that the result of a group_by() produces a grouped data frame. Subsequent functions will be applied to the grouped data frame. This can lead to unexpected results if the original grouping is not relevant for the subsequent function. To avoid this, we can use the ungroup() function to remove the grouping after the relevant grouped summary statistics have been calculated.\nLet’s return to calculating the number of observations n() grouped by the genre and lang_variety variables in the brown_fam_df dataset. But let’s add another summary which uses the n variable to calculate the mean and median number of observations.\nIf we do not use the ungroup() function, the mean and median will be calculated for each genre collapsed across lang_variety.\n\n# Mean and median of `n` grouped by `genre`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  ) |&gt;\n  summarize(\n    mean_n = mean(n),\n    median_n = median(n)\n  )\n\n&gt; # A tibble: 15 × 3\n&gt;    genre            mean_n median_n\n&gt;    &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n&gt;  1 press reportage   110      110  \n&gt;  2 press editorial    67.5     67.5\n&gt;  3 press reviews      42.5     42.5\n&gt;  4 religion           42.5     42.5\n&gt;  5 skills / hobbies   93       93  \n&gt;  6 popular lore      114      114  \n&gt;  7 belles lettres    190.     190. \n&gt;  8 miscellaneous      75       75  \n&gt;  9 learned           200      200  \n&gt; 10 general fiction    72.5     72.5\n&gt; 11 detective          60       60  \n&gt; 12 science fiction    15       15  \n&gt; 13 adventure          72       72  \n&gt; 14 romance            72.5     72.5\n&gt; 15 humour             22.5     22.5\n\n\nTherefore we see that we have a mean and median calculated for the number of documents in the corpus for each of the 15 genres.\nIf we use the ungroup() function, the mean and median will be calculated for all genres. Note we will use the ungroup() function between these summaries to clear the grouping before calculating the mean and median.\n\n# Number of observations for each `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  ) |&gt;\n  ungroup() |&gt;\n  summarize(\n    mean_n = mean(n),\n    median_n = median(n)\n  )\n\n&gt; # A tibble: 1 × 2\n&gt;   mean_n median_n\n&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n&gt; 1   83.3       72\n\n\nNow we see that we have a mean and median calculated across all genres.\n\nBefore we leave this section, let’s look some other ways to create frequency and contingency tables for character and factor variables. A shortcut to calculate a frequency table for a character or factor variable is to use the count() function from the dplyr package.\nLet’s calculate the number of observations grouped by the genre variable in the brown_fam_df dataset.\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  count(genre)\n\n&gt; # A tibble: 15 × 2\n&gt;    genre                n\n&gt;    &lt;fct&gt;            &lt;int&gt;\n&gt;  1 press reportage    220\n&gt;  2 press editorial    135\n&gt;  3 press reviews       85\n&gt;  4 religion            85\n&gt;  5 skills / hobbies   186\n&gt;  6 popular lore       228\n&gt;  7 belles lettres     381\n&gt;  8 miscellaneous      150\n&gt;  9 learned            400\n&gt; 10 general fiction    145\n&gt; 11 detective          120\n&gt; 12 science fiction     30\n&gt; 13 adventure          144\n&gt; 14 romance            145\n&gt; 15 humour              45\n\n\nWe can also add multiple grouping variables to count() and create contingency tables.\nLet’s add lang_variety to the grouping and create a cross-tabulation for genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  count(genre, lang_variety)\n\n&gt; # A tibble: 30 × 3\n&gt;    genre            lang_variety     n\n&gt;    &lt;fct&gt;            &lt;fct&gt;        &lt;int&gt;\n&gt;  1 press reportage  AmE             88\n&gt;  2 press reportage  BrE            132\n&gt;  3 press editorial  AmE             54\n&gt;  4 press editorial  BrE             81\n&gt;  5 press reviews    AmE             34\n&gt;  6 press reviews    BrE             51\n&gt;  7 religion         AmE             34\n&gt;  8 religion         BrE             51\n&gt;  9 skills / hobbies AmE             72\n&gt; 10 skills / hobbies BrE            114\n&gt; # ℹ 20 more rows\n\n\nNote that the results of count() are not grouped so we do not need to use the ungroup() function before calculating subsequent summary statistics.\nAnother way to create frequency and contingency tables is to use the tabyl() function from the janitor package (Firke 2023). Let’s create a frequency table for the genre variable in the brown_fam_df dataset.\n\n# Load packages\nlibrary(janitor)\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  tabyl(genre)\n\n&gt;             genre   n percent\n&gt;   press reportage 220  0.0880\n&gt;   press editorial 135  0.0540\n&gt;     press reviews  85  0.0340\n&gt;          religion  85  0.0340\n&gt;  skills / hobbies 186  0.0744\n&gt;      popular lore 228  0.0912\n&gt;    belles lettres 381  0.1525\n&gt;     miscellaneous 150  0.0600\n&gt;           learned 400  0.1601\n&gt;   general fiction 145  0.0580\n&gt;         detective 120  0.0480\n&gt;   science fiction  30  0.0120\n&gt;         adventure 144  0.0576\n&gt;           romance 145  0.0580\n&gt;            humour  45  0.0180\n\n\nIn addition to providing frequency counts, the tabyl() function also provides the percent of observations for each level of the variable. And, we can add up to three grouping variables to tabyl() as well.\nLet’s add lang_variety to the grouping and create a contingency table for the genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  tabyl(genre, lang_variety)\n\n&gt;             genre AmE BrE\n&gt;   press reportage  88 132\n&gt;   press editorial  54  81\n&gt;     press reviews  34  51\n&gt;          religion  34  51\n&gt;  skills / hobbies  72 114\n&gt;      popular lore  96 132\n&gt;    belles lettres 150 231\n&gt;     miscellaneous  60  90\n&gt;           learned 160 240\n&gt;   general fiction  58  87\n&gt;         detective  48  72\n&gt;   science fiction  12  18\n&gt;         adventure  57  87\n&gt;           romance  58  87\n&gt;            humour  18  27\n\n\nThe results do not include the percent of observations for each level of the variable as it is not clear how to calculate the percent of observations for each level of the variable when there are multiple grouping variables. We must specify if we want to calculate the percent of observations by row or by column.\n\n\n\n\n\n\n Dive deeper\nThe janitor package includes a variety of adorn_*() functions to add additional information to the results of tabyl(), including percentages, frequencies, and totals. Feel free to explore these functions on your own. We will return to this topic again later in the course.\n\n\n\n\n\nCreating Quarto tables\nSummarizing the data is not only useful for our understanding of the data as part of our analysis but also for communicating the data in reports, manuscripts, and presentations.\nOne way to communicate summary statistics is with tables. In Quarto, we can use the knitr package (Xie 2023) in combination with code block options to produce formatted tables which we can cross-reference in our prose sections.\nLet’s create an object from the cross-tabulation for the genre and lang_variety variables in the brown_fam_df dataset to work with.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbf_genre_lang_ct &lt;-\n  brown_fam_df |&gt;\n  tabyl(genre, lang_variety)\n\nTo create a table in Quarto, we use the kable() function. The kable() function takes a data frame (or matrix) as an argument. The format argument will be derived from the Quarto document format (‘html’, ‘pdf’, etc.).\n\n# Load packages\nlibrary(knitr)\n\n# Create a table in Quarto\nkable(bf_genre_lang_ct)\n\n\n\n\ngenre\nAmE\nBrE\n\n\n\n\npress reportage\n88\n132\n\n\npress editorial\n54\n81\n\n\npress reviews\n34\n51\n\n\nreligion\n34\n51\n\n\nskills / hobbies\n72\n114\n\n\npopular lore\n96\n132\n\n\nbelles lettres\n150\n231\n\n\nmiscellaneous\n60\n90\n\n\nlearned\n160\n240\n\n\ngeneral fiction\n58\n87\n\n\ndetective\n48\n72\n\n\nscience fiction\n12\n18\n\n\nadventure\n57\n87\n\n\nromance\n58\n87\n\n\nhumour\n18\n27\n\n\n\n\n\nTo add a caption to the table and to enable cross-referencing, we use the code block options label and tbl-cap. The label option takes a label prefixed with tbl- to create a cross-reference to the table. The tbl-cap option takes a caption for the table, in quotation marks.\n#| label: tbl-brown-genre-lang-ct\n#| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"\n\n# Create a table in Quarto\nkable(bf_genre_lang_ct)\nNow we can cross-reference the table with the @tbl-brown-genre-lang-ct syntax. So the following Quarto document will produce the following prose with a cross-reference to the formatted table output.\n\nAs we see in @tbl-brown-genre-lang-ct, the distribution of `genre` is similar across `lang_variety`.\n\n```{r}\n#| label: tbl-brown-genre-lang-ct\n#| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"\n\n# Print cross-tabulation\nkable(bf_genre_lang_ct)\n```\n\nAs we see in Table 1, the distribution of genre is similar across lang_variety.\n\n\n\n\nTable 1: Cross-tabulation of genre and lang_variety\n\n\n\n\n\n\ngenre\nAmE\nBrE\n\n\n\n\npress reportage\n88\n132\n\n\npress editorial\n54\n81\n\n\npress reviews\n34\n51\n\n\nreligion\n34\n51\n\n\nskills / hobbies\n72\n114\n\n\npopular lore\n96\n132\n\n\nbelles lettres\n150\n231\n\n\nmiscellaneous\n60\n90\n\n\nlearned\n160\n240\n\n\ngeneral fiction\n58\n87\n\n\ndetective\n48\n72\n\n\nscience fiction\n12\n18\n\n\nadventure\n57\n87\n\n\nromance\n58\n87\n\n\nhumour\n18\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\nThe kableExtra package (Zhu 2024) provides additional functionality for formatting tables in Quarto.\n\n\n\n\n\nCreating Quarto plots\nWhere tables are useful for communicating summary statistics for numeric and character variables, plots are useful for communicating relationships between variables especially when one or more of the variables is numeric. Furthermore, for complex relationships, plots can be more effective than tables.\nIn Quarto, we can use the ggplot2 package (Wickham et al. 2024) in combination with code block options to produce formatted plots which we can cross-reference in our prose sections.\nLet’s see this in action with a simple histogram of the percent_passive variable in the brown_fam_df dataset. The Quarto document will produce the following prose with a cross-reference to the formatted plot output.\nAs we see in @fig-brown-fam-percent-passive-hist, the distribution of `percent_passive` is skewed to the right.\n\n```{r}\n#| label: fig-brown-fam-percent-passive-hist\n#| fig-cap: \"Histogram of `percent_passive`\"\n\n# Create a histogram in Quarto\nggplot(brown_fam_df) +\n  geom_histogram(aes(x = percent_passive))\n```\n\nAs we see in Figure 1, the distribution of percent_passive is skewed to the right.\n\n\n\n\n\n\n\n\nFigure 1: Histogram of percent_passive\n\n\n\n\n\n\nThe ggplot2 package implements the ‘Grammar of Graphics’ approach to creating plots. This approach is based on the idea that plots can be broken down into components, or layers, and that each layer can be manipulated independently.\nThe main components are data, aesthetics, and geometries. Data is the data frame that contains the variables to be plotted. Aesthetics are the variables that will be mapped to the x-axis, y-axis (as well as color, shape, size, etc.). Geometries are the visual elements that will be used to represent the data, such as points, lines, bars, etc..\nAs discussed in the R lesson “Visual Summaries”, the aes() function is used to map variables to aesthetics and can be added to the ggplot() function or to the geom_*() function depending on whether the aesthetic is mapped to all geometries or to a specific geometry, respectively.\nTake a look at the following stages of the earlier plot in each of the tabs below.\n\nStages\n\nDataAestheticsGeometries\n\n\nThe data layer does not produce a plot but it is the foundation of the plot.\n\n# Data layer\nggplot(brown_fam_df)\n\n\n\n\n\n\n\n\n\n\nThe aesthetics layer does not produce a plot but it maps the variables to the aesthetics to be used in the plot.\n\n# Aesthetics layer\nggplot(brown_fam_df, aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\nThe geometries layer produces the plot connecting the data and aesthetics layers in the particular way specified by the geometries, in this case a histogram.\n\n# Geometries layer\nggplot(brown_fam_df, aes(x = percent_passive)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right plot\nJust as with tables, the type of summary we choose to communicate with a plot depends on the type of variables we are working with and the relationships between those variables.\nBelow I’ve included a few examples of plots that can be used to communicate different types of variables and relationships.\n\n\nSingle numeric variable\n\nHistogramDensity plot\n\n\n\n# Histogram\nggplot(brown_fam_df) +\n  geom_histogram(aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\n\n# Density plot\nggplot(brown_fam_df) +\n  geom_density(aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumeric and categorical variables\n\nDensity plotBoxplotViolin plot\n\n\n\n# Density plot\nggplot(brown_fam_df) +\n  geom_density(\n    aes(\n      x = percent_passive,\n      fill = lang_variety\n    ),\n    alpha = 0.5 # adds transparency\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Boxplot\nggplot(brown_fam_df) +\n  geom_boxplot(\n    aes(\n      x = lang_variety,\n      y = percent_passive\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Violin plot\nggplot(brown_fam_df) +\n  geom_violin(\n    aes(\n      x = lang_variety,\n      y = percent_passive\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numeric variables\n\nScatterplotScatterplot with regression line\n\n\n\n# Scatterplot\nggplot(brown_fam_df) +\n  geom_point(\n    aes(\n      x = active_verbs,\n      y = passive_verbs\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Scatterplot with regression line\nggplot(\n  brown_fam_df,\n  aes(\n    x = active_verbs,\n    y = passive_verbs\n  )\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther variable combinations\nIn these examples, we have only looked at the most common variable combinations for one and two variable plots. There are more sophisticated plots that can be used for other variable combinations using ggplot2. For now, we will leave these for another time."
  },
  {
    "objectID": "recipes/recipe-03/index.html#check-your-understanding",
    "href": "recipes/recipe-03/index.html#check-your-understanding",
    "title": "03. Descriptive assessment of datasets",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nA factor is a character vector augmented to include information about the discrete values, or levels, of the vector. TRUEFALSE\nWhat is the difference between a frequency table and a contingency table? A frequency table is a cross-tabulation of two or more categorical variables.A contingency table is a cross-tabulation of two or more categorical variables.\nThe skimrdplyrggplot2knitr package is used to create formatted tables in R.\nTo add a geometry layer, such as geom_histogram(), to a ggplot object the |&gt; operator is used. TRUEFALSE\nTo visualize the relationship between two numeric variables, a histogramdensity plotboxplotviolin plotscatterplot is often used.\nWhen the aes() function is added to the ggplot() function, the aesthetic is mapped to all geometries. TRUEFALSE"
  },
  {
    "objectID": "recipes/recipe-03/index.html#lab-preparation",
    "href": "recipes/recipe-03/index.html#lab-preparation",
    "title": "03. Descriptive assessment of datasets",
    "section": "Lab preparation",
    "text": "Lab preparation\n\nBefore beginning Lab 3, learners should be comfortable with the skills and knowledge developed in the previous recipes and labs. In this lab, you will have a chance to use these skills and those introduced in this Recipe to provide a descriptive assessment of a dataset that includes statistics, tables, and plots using Quarto and R.\nThe additional skills and knowledge you will need to complete Lab 3 include:\n\nSummarizing data with skimr\nSummarizing data with dplyr\nCreating Quarto tables with knitr\nCreating Quarto plots with ggplot2"
  },
  {
    "objectID": "recipes/recipe-05/index.html",
    "href": "recipes/recipe-05/index.html",
    "title": "05. Collecting and documenting data",
    "section": "",
    "text": "Skills\n\nFinding data sources\nData collection strategies\nData documentation"
  },
  {
    "objectID": "recipes/recipe-05/index.html#concepts-and-strategies",
    "href": "recipes/recipe-05/index.html#concepts-and-strategies",
    "title": "05. Collecting and documenting data",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nFinding data sources\nTo find data sources, it is best to have a research question in mind. This will help to narrow the search for data sources. However, finding data sources can also be a good way to generate research questions. In either case, it takes some sleuthing to find data sources that will work for your research question. In addition, to the data source itself, you will also need to consider the permissions and licensing of the data source. It is best to consider these early in the process to avoid surprises later. Finally, you will also need to consider the data format and how it will be used in the analysis. It can be the case that a data source seems ideal, but the data format is not conducive to the analysis you would like to do.\n\n\n\n\n\n\n Tip\nConsult the Identifying data and data sources guide for some ideas on where to find data sources.\n\n\n\nIn this recipe, we will consider some hypothetical reseach aimed at exploring potential similarities and differences in the lexical, syntactic, and/ or stylistic features between American and English literature during the mid 19th century.\n\n\n\n\n\n\n Dive deeper\nIf you are interested in understanding a literary analysis perspective to text analysis, I highly recommend Matthew Jockers’ book Text Analysis with R for Students of Literature (Jockers 2014). This book is a great resource for understanding how to apply text analysis to literary analysis.\n\n\n\nProject Gutenberg is a great source of data for this research question. Project Gutenberg is a volunteer effort to digitize and archive cultural works. The great majority of the works in the Project Gutenberg database are in the public domain in the United States. This means that the works can be freely used and shared.\nFurthermore, {gutenbergr} provides an API for accessing the Project Gutenberg database. This means that we can use R to access the Project Gutenberg database and download the text and metadata for the works we are interested in. {gutenbergr} also provides a number of data frames that can help us to identify the works we are interested in.\n\n\nData collection strategy\nLet’s now turn to the data collection strategy. There are a number of data collection strategies that can be used to acquire data for a text analysis project. In the chapter, we covered manual and programmatic downloads and APIs. Here we will use an R package which will provide an API for accessing the data source.\n\n\n\n\n\n\n Dive deeper\nIf you are interested in learning about another data collection strategy, web scraping, I suggest you look at the Web scraping with R guide.\n\n\n\nWe will load {dplyr}, {readr}, and {gutenbergr} to prepare for the data collection process.\n\n# Load packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(gutenbergr)\n\nThe main workhorse of {gutenbergr} is the gutenberg_download(). It’s only required argument is the id(s) used by Project Gutenberg to index all of the works in their database. This function will then download the text of the work(s) and return a data frame with the gutenberg id and the text of the work(s).\nSo how do we find the gutenberg ids? The manual method is to go to the Project Gutenberg website and search for the work you are interested in. For example, let’s say we are interested in the work “A Tale of Two Cities” by Charles Dickens. We can search for this work on the Project Gutenberg website and then click on the link to the work. The url for this work is: https://www.gutenberg.org/ebooks/98. The gutenberg id is the number at the end of the url, in this case 98.\nThis will work for individual works, but why wouldn’t we just download the text from the Project Gutenberg website? For the works on Project Gutenberg this would be perfectly fine. We can share the text with others as the license for the works on Project Gutenberg are in the public domain.\nHowever, what if are interested in downloading multiple works? As the number of works increases, the time it takes to manually download each work increases. Furthermore, {gutenbergr} provides a number of additional attributes that can be downloaded and organized along side the text. Finally, the results of the gutenberg_download() function are returned as a data frame which can be easily manipulated and analyzed in R.\nIn our data acquisition plan, we want to collect works from a number of authors. So it will be best to leverge {gutenbergr} to download the works we are interested in. To do this we need to know the gutenberg ids for the works we are interested in.\nConvienently, {gutenbergr} also includes a number of data frames that contain meta data for the works in the Project Gutenberg database. These data frames include meta data for works in the Project Gutenberg database (gutenberg_metadata), authors (gutenberg_authors), and subjects (gutenberg_subjects).\nLet’s take a look at the structure of these data frames.\n\nglimpse(gutenberg_metadata)\n\n&gt; Rows: 72,569\n&gt; Columns: 8\n&gt; $ gutenberg_id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n&gt; $ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n&gt; $ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n&gt; $ gutenberg_author_id &lt;int&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, …\n&gt; $ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n&gt; $ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n&gt; $ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n&gt; $ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_authors)\n\n&gt; Rows: 23,980\n&gt; Columns: 7\n&gt; $ gutenberg_author_id &lt;int&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n&gt; $ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n&gt; $ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n&gt; $ birthdate           &lt;int&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n&gt; $ deathdate           &lt;int&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n&gt; $ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n&gt; $ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_subjects)\n\n&gt; Rows: 231,741\n&gt; Columns: 3\n&gt; $ gutenberg_id &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n&gt; $ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n&gt; $ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\nFrom this overvew, we can see that there are 72,569 works in the Project Gutenberg database. We can also see that there are 23,980 authors and 231,741 subjects.\nAs we dicussed, each work in the Project Gutenberg database has a gutenberg id. The gutenberg_id appears in the gutenberg_metadata and also in the gutenberg_subjects data frame. This common attribute means that a work with a particular gutenberg id can be linked to the subject(s) associated with that work. Another important attribute is is the gutenberg_author_id which links the work to the author(s) of that work. Yes, the author name is in the gutenberg_metadata data frame, but the gutenberg_author_id can be used to link the work to the gutenberg_authors data frame which contains additional information about authors.\n\n\n\n\n\n\n Tip\n{gutenbergr} is periodically updated. To check to see when each data frame was last updated run:\nattr(gutenberg_metadata, \"date_updated\")\n\n\n\nLet’s now describe a few more attributes that will be useful for our data acquisition plan. In the gutenberg_subjects data frame, we have subject_type and subject. The subject_type is the type of subject classification system used to classify the work. If you tabulate this column, you will see that there are two types of subject classification systems used: Library of Congress Classification (lcc) and Library of Congress Subject Headings (lcsh). The subject column contains the subject code for the work. For lsch the subject code is a descriptive character string and for lcc the subject code is an id as a character string that is a combination of letters (and numbers) that the Library of Congress uses to classify works.\nFor our data acquistion plan, we will use the lcc subject classification system to select works from the Library of Congress Classification for English Literature (PR) and American Literature (PS).\nIn the gutenberg_authors data frame, we have the birthdate and deathdate attributes. These attributes will be useful for filtering the authors that lived during the mid 19th century.\nWith this overview of {gutenbergr} and the data frames that it contains, we can now begin to develop our data acquisition plan.\n\nSelect the authors that lived during the mid 19th century from the gutenberg_authors data frame.\nSelect the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the gutenberg_subjects data frame.\nSelect works from gutenberg_metadata that are associated with the authors and subjects selected in steps 1 and 2.\nDownload the text and metadata for the works selected in step 3 using the gutenberg_download() function.\nWrite the data to disk in an appropriate format.\n\n\n\nData collection\nLet’s take each of these steps in turn. First, we need to select the authors that lived during the mid 19th century from the gutenberg_authors data frame. To do this we will use the filter() function. We will pass the gutenberg_authors data frame to the filter() function and then use the birthdate column to select the authors that were born after 1800 and died before 1880 –this year is chosen as the mid 19th century is generally considered to be the period from 1830 to 1870. We will then assign the result to the variable name authors.\n\nauthors &lt;-\n  gutenberg_authors |&gt;\n  filter(\n    birthdate &gt; 1800,\n    deathdate &lt; 1880\n  )\n\nThat’s it! We now have a data frame with the authors that lived during the mid 19th century, some 787 authors in total. This will span all subjects and languages, so this isn’t the final number of authors we will be working with.\nThe next step is to select the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the gutenberg_subjects data frame. To do this we will use the filter() function again. We will pass the gutenberg_subjects data frame to the filter() function and then use the subject_type and subject columns to select the works that are associated with the Library of Congress Classification for English Literature (PR) and American Literature (PS). We will then assign the result to the variable name subjects.\n\nsubjects &lt;-\n  gutenberg_subjects |&gt;\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n\nNow, we have a data frame with the subjects that we are interested in. Let’s inspect this data frame to see how many works we have for each subject.\n\nsubjects |&gt;\n  count(subject)\n\n&gt; # A tibble: 2 × 2\n&gt;   subject     n\n&gt;   &lt;chr&gt;   &lt;int&gt;\n&gt; 1 PR       9926\n&gt; 2 PS      10953\n\n\nThe next step is to subset the gutenberg_metadata data frame to select works from the authors and subjects selected in the previous steps. Again, we will use filter() to do this. We will pass the gutenberg_metadata data frame to the filter() function and then use the gutenberg_author_id and gutenberg_id columns to select the works that are associated with the authors and subjects selected in the previous steps. We will then assign the result to the variable name works.\n\nworks &lt;-\n  gutenberg_metadata |&gt;\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n\nworks\n\n&gt; # A tibble: 1,014 × 8\n&gt;    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n&gt;           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n&gt;  1           33 The Sca… Hawth…                  28 en       \"Harvard Classics/…\n&gt;  2           46 A Chris… Dicke…                  37 en       \"Children's Litera…\n&gt;  3           71 On the … Thore…                  54 en       \"\"                 \n&gt;  4           77 The Hou… Hawth…                  28 en       \"Best Books Ever L…\n&gt;  5           98 A Tale … Dicke…                  37 en       \"Historical Fictio…\n&gt;  6          205 Walden,… Thore…                  54 en       \"\"                 \n&gt;  7          258 Poems b… Gordo…                 145 en       \"\"                 \n&gt;  8          271 Black B… Sewel…                 154 en       \"Best Books Ever L…\n&gt;  9          292 Beauty … Taylo…                 167 en       \"\"                 \n&gt; 10          394 Cranford Gaske…                 220 en       \"\"                 \n&gt; # ℹ 1,004 more rows\n&gt; # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\n\nFiltering the gutenberg_metadata data frame by the authors and subjects selected in the previous steps, we now have a data frame with 1,014 works. This is the final number of works we will be working with so we can now download the text and metadata for these works using the gutenberg_download() function.\nA few things to note about the gutenberg_download() function. First, it is vectorized, that is, it can take a single value or multiple values for the argument gutenberg_id. This is good as we will be passing a vector of gutenberg ids to the function. A small fraction of the works on Project Gutenberg are not in the public domain and therefore cannot be downloaded, this is documented in the rights column. Furthermore, not all of the works have text available, as seen in the has_text column. Finally, the gutenberg_download() function returns a data frame with the gutenberg id and the text of the work(s) –but we can also select additional attributes to be returned by passing a character vector of the attribute names to the argument meta_fields. The column names of the gutenberg_metadata data frame contains the available attributes.\nWith this in mind, let’s do a quick test before we download all of the works. Let’s select the first 5 works from the works data frame that fit our criteria and then download the text and metadata for these works using the gutenberg_download() function. We will then assign the result to the variable name works_sample.\n\nworks_sample &lt;-\n  works |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  slice_head(n = 5) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  )\n\nworks_sample\n\n&gt; # A tibble: 34,385 × 6\n&gt;    gutenberg_id text        title author gutenberg_author_id gutenberg_bookshelf\n&gt;           &lt;int&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;              \n&gt;  1           33 \"The Scarl… The … Hawth…                  28 Harvard Classics/M…\n&gt;  2           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n&gt;  3           33 \"by Nathan… The … Hawth…                  28 Harvard Classics/M…\n&gt;  4           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n&gt;  5           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n&gt;  6           33 \"Contents\"  The … Hawth…                  28 Harvard Classics/M…\n&gt;  7           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n&gt;  8           33 \" THE CUST… The … Hawth…                  28 Harvard Classics/M…\n&gt;  9           33 \" THE SCAR… The … Hawth…                  28 Harvard Classics/M…\n&gt; 10           33 \" I. THE P… The … Hawth…                  28 Harvard Classics/M…\n&gt; # ℹ 34,375 more rows\n\n\nLet’s inspect the works_sample data frame. First, from the output we can see that all of our meta data attributes were returned. Second, we can see that the text column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded, even blank lines. To make sure that we have the correct number of works, we can use the count() function to count the number of works by gutenberg_id.\n\nworks_sample |&gt;\n  count(gutenberg_id)\n\n&gt; # A tibble: 4 × 2\n&gt;   gutenberg_id     n\n&gt;          &lt;int&gt; &lt;int&gt;\n&gt; 1           33  8212\n&gt; 2          258 11050\n&gt; 3          271  5997\n&gt; 4          292  9126\n\n\nYes, we have 5 works and we can see how many lines are in each of these works.\nWe could now run this code on the entire works data frame and then write the data to disk like so:\nworks |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |&gt;\n  write_csv(file = \"data/original/gutenberg/works.csv\")\nThis would accomplish the primary goal of our data acquisition plan.\nHowever, there is some key functionality that we are missing if we would like to make this code more reproducible-friendly. First, we are not checking to see if the data already exists on disk. If we already have run this code in our script, we likely do not want to run it again. Second, we may want to use this code again with different parameters, for example, we may want to retrieve different subject codes, or different time periods, or other languages.\nAll three of these additional features can be accomplished with writing a custom function. Let’s take a look at the code we have written so far and see how we can turn this into a custom function.\n# Get authors within years\nauthors &lt;-\n  gutenberg_authors |&gt;\n  filter(\n    birthdate &gt; 1800,\n    deathdate &lt; 1880\n  )\n# Get LCC subjects\nsubjects &lt;-\n  gutenberg_subjects |&gt;\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n# Get works based on authors and subjects\nworks &lt;-\n  gutenberg_metadata |&gt;\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n# Download works\nworks |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |&gt;\n  write_csv(file = \"data/original/gutenberg/works.csv\")\n\nBuild the custom function\n\nNameArgumentsCode: commentsCode: packagesCode: data checkCode: authorsCode: subjectCode: worksCode: downloadCode: write\n\n\nLet’s start to create our function by creating a name and calling the function() function. We will name our function get_gutenberg_works().\nget_gutenberg_works &lt;- function() {\n\n}\n\n\nNow, we need to think of the arguments that we would like to pass to our function so they can be used to customize the data acquisition process. First, we want to check to see if the data already exists on disk. To do this we will need to pass the path to the data file to our function. We will name this argument target_file.\nget_gutenberg_works &lt;- function(target_file) {\n\n}\nNext, we want to pass the subject code that the works should be associated with. We will name this argument lcc_subject.\nget_gutenberg_works &lt;- function(target_file, lcc_subject) {\n\n}\nFinally, we want to pass the birth year and death year that the authors should be associated with. We will name these arguments birth_year and death_year.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n\n}\n\n\nWe now turn to the code. I like to start by creating comments to describe the steps inside the function before adding code.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe have some packages we want to make sure are installed and loaded. We will use the {pacman} package to do this. We will use the p_load() function to install and load the packages. We will pass the character vector of package names to the p_load() function.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe need to create the code to check if the data exists. We will use an if statement to do this. If the data does exist, we will print a message to the console that the data already exists and stop the function. If the data does not exist, we will create the directory structure and continue with the data acquisition process. I will use the fs package (Hester, Wickham, and Csárdi 2023) in this code so I will load the library at the top of the function.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nLet’s now add the code to get the authors within the years. We will now use the birth_year and death_year arguments to filter the gutenberg_authors data frame.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nUsing the lcc_subject argument, we will now filter the gutenberg_subjects data frame.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe will use the authors and subjects data frames to filter the gutenberg_metadata data frame as before.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe will now use the works data frame to download the text and metadata for the works using the gutenberg_download() function and assign it to results.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results &lt;-\n    works |&gt;\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |&gt;\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n}\n\n\nFinally, we will write the results data frame to disk using the write_csv() function and the target_file argument.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results &lt;-\n    works |&gt;\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |&gt;\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n  write_csv(results, file = target_file)\n}\n\n\n\n\n\nUsing the custom function\nWe now have a function, get_gutenberg_works(), that we can use to acquire works from Project Gutenberg for a given LCC code for authors that lived during a given time period. We now have a flexible function that we can use to acquire data.\nWe can add this function to the script in which we use it, or we can add it to a separate script and source it into any script in which we want to use it.\n# Source function\nsource(\"get_gutenberg_works.R\")\n\n# Get works for PR and PS for authors born between 1800 and 1880\nget_gutenberg_works(\n  target_file = \"data/original/gutenberg/works.csv\",\n  lcc_subject = c(\"PR\", \"PS\"),\n  birth_year = 1800,\n  death_year = 1880\n)\nAnother option is to add this function to your own package. This is a great option if you plan to use this function in multiple projects or share it with others. Since I have already created a package for this book, {qtkit}, I’ve added this function, with some additional functionality, to the package.\n\n# Load package\nlibrary(qtalrkit)\n\n# Get works for fiction for authors born between 1870 and 1920\nget_gutenberg_works(\n  target_dir = \"data/original/gutenberg/\",\n  lcc_subject = \"PZ\",\n  birth_year = 1870,\n  death_year = 1920\n)\nThis modified function will create a directory structure for the data file if it does not already exist. It will also create a file name for the data file based on the arguments passed to the function.\n\n\n\nData documentation\nFinding data sources and collecting data are important steps in the acquisition process. However, it is also important to document the data collection process. This is important so that you, and others, can reproduce the data collection process.\nIn data acquisition, the documentation is includes the code, code comments, and prose in the process file used to acquire the data and also a data origin file. The data origin file is a text file that describes the data source and the data collection process.\nThe {qtkit} package includes a function, create_data_origin(), that can be used to scaffold a data origin file. This simply takes a file path and creates a data origin file in CSV format.\nattribute,description\nResource name,The name of the resource.\nData source,\"URL, DOI, etc.\"\nData sampling frame,\"Language, language variety, modality, genre, etc.\"\nData collection date(s),The dates the data was collected.\nData format,\".txt, .csv, .xml, .html, etc.\"\nData schema,\"Relationships between data elements: files, folders, etc.\"\nLicense,\"CC BY, CC BY-SA, etc.\"\nAttribution,Citation information.\nThe you edit this file and ensure that it contains all of the information needed to document the data. Make sure that this file is near the data file so that it is easy to find.\ndata\n  ├── analysis/\n  ├── derived/\n  └── original/\n      ├── works_do.csv\n      └── gutenberg/\n          ├── works_pr.csv\n          └── works_ps.csv"
  },
  {
    "objectID": "recipes/recipe-05/index.html#summary",
    "href": "recipes/recipe-05/index.html#summary",
    "title": "05. Collecting and documenting data",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we have covered acquiring data for a text analysis project. We used the {gutenbergr} (Johnston and Robinson 2023) to acquire works from Project Gutenberg. After exploring the resources available, we established an acquisition plan. We then used R to implement our plan. To make our code more reproducible-friendly, we wrote a custom function to acquire the data. Finally, we discussed the importance of documenting the data collection process and introduced the data origin file."
  },
  {
    "objectID": "recipes/recipe-05/index.html#check-your-understanding",
    "href": "recipes/recipe-05/index.html#check-your-understanding",
    "title": "05. Collecting and documenting data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nIn the chapter and in this recipe, strategies for acquiring data were discussed. Which of the following was not discussed as a strategy for acquiring data? Direct downloadProgrammatic downloadAPIsWeb scraping\nIn this recipe, we used {gutenbergr} to acquire works from Project Gutenberg. What is the name of the function that we used to acquire the actual text? gutenberg_metatagutenberg_get()gutenberg_search()gutenberg_download()\nTrueFalse A custom function is only really necessary if you are writting an R package.\nWhen writing a custom function, what is the first step? Write the codeWrite the commentsLoad the packagesCreate the function arguments\nWhat does it mean when we say that a function is ‘vectorized’ in R? The function returns a vectorThe function can take a vector as an argumentThe function can take a vector and operates on each element of the vector\nWhich Tidyverse package allows us to apply non-vectorized functions to vectors? dplyrstringrreadrpurrr"
  },
  {
    "objectID": "recipes/recipe-05/index.html#lab-preparation",
    "href": "recipes/recipe-05/index.html#lab-preparation",
    "title": "05. Collecting and documenting data",
    "section": "Lab preparation",
    "text": "Lab preparation\nBefore beginning Lab 5, make sure you are comfortable with the following:\n\nReading and subsetting data in R\nWriting data in R\nThe project structure of reproducible projects\n\nThe additional skills covered in this lab are:\n\nIdentifying data sources\nAcquiring data through manual and programmatic downloads and APIs\nCreating a data acquisition plan\nDocumenting the data collection process\nWriting a custom function\nDocumenting the data source with a data origin file\n\nYou will have a choice of data source to acquire data from. Before you start the lab, you should consider which data source you would like to use, what strategy you will use to acquire the data, and what data you will acquire. You should also consider the information you need to document the data collection process.\nConsult the Identifying data and data sources guide for some ideas on where to find data sources."
  },
  {
    "objectID": "recipes/recipe-09/index.html",
    "href": "recipes/recipe-09/index.html",
    "title": "09. Recipe",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "recipes/recipe-07/index.html",
    "href": "recipes/recipe-07/index.html",
    "title": "07. Recipe",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "recipes/recipe-01/index.html",
    "href": "recipes/recipe-01/index.html",
    "title": "01. Academic writing with Quarto",
    "section": "",
    "text": "Skills\n\nNumbered sections\nTable of contents\nCross-referencing tables and figures\nIn-line citations and references list"
  },
  {
    "objectID": "recipes/recipe-01/index.html#concepts-and-strategies",
    "href": "recipes/recipe-01/index.html#concepts-and-strategies",
    "title": "01. Academic writing with Quarto",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\nFor many of the style components that we use in Quarto, there is a part that is addressed in the front-matter section and a part that is addressed in the prose section and/ or code block sections.\nTo refresh our memory, the front-matter is fenced by three dashes (---) and is where we set the document attributes. The prose section is where we write the text of the document. The code block section is where we write the code that will be executed and is fenced by three backticks (```) and the name of the code interpreter {r} (R for us).\n---\n1title: \"My document title\"\n2format: pdf\n---\n\n3This is the prose section.\n\n4```{r}\n#| label: example-code-block\n1 + 1\n```\n\n1\n\nThe title of the document\n\n2\n\nThe format of the document\n\n3\n\nThe prose section\n\n4\n\nThe code block section\n\n\nWith this in mind let’s look at each of these elements in turn.\n\nNumbered sections\nTo number sections in Quarto, we use the number_sections key with the value yes. This is set in the front-matter section, nested under the value for the document type to be rendered. For example, to number sections in a PDF document, we would set the number-sections key to true in the front-matter section as follows:\n---\n1title: \"My document title\"\n2format:\n3  pdf:\n4    number-sections: true\n---\n\n1\n\nThe title of the document\n\n2\n\nThe format of the document\n\n3\n\nThe type of document to be rendered, note the identation\n\n4\n\nThe key-value pair to number sections in the PDF document, again note the identation\n\n\nHeaders in the prose section are then numbered automatically. For example, the following markdown:\n# Section\n\n## Subsection\n\n### Subsubsection\n\n#### Subsubsubsection\n\n##### Subsubsubsubsection\nwould render as:\n\n\n\n\n\n\n\n\n\nWe can also control the depth of the numbering by setting the number-depth key in the front-matter section. For example, to number sections and subsections, but not subsubsections, we would set the number-depth key to 2 as follows:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n1    number-depth: 2\n---\n\n1\n\nThe key-value pair to control the depth of the numbering\n\n\nNow the first and second headers are numbered and formated but third and subsequent headers are only formatted.\nIf for some reason you want to turn off numbering for a specific header, you can add {.unnumbered} to the end of the header. For example, the following markdown:\n# Section {.unnumbered}\nThis is particularly useful in academic writing when we want to add a reference, materials, or other section that is not numbered at the end of the document.\n\n\n\n\n\n\n Warning\nNote that if you have a header that is unnumbered, the next header will be numbered as if the unnumbered header did not exist. This can have unexpected results if you have children of an unnumbered header.\n\n\n\n\n\nTable of contents\nFor longer documents including a table of contents can be a useful way to help readers navigate the document. To include a table of contents in Quarto, we use the toc key with the value true. Again, in the front-matter section, nested under the format value, as seen below:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n1    toc: true\n---\n\n1\n\nThe key-value pair to include a table of contents in the PDF document\n\n\n\n\n\n\n\n\n Tip\nFor PDF and Word document outputs, the table of contents will be automatically generated and placed at the beginning of the document. For HTML documents, the table of contents will be placed in the sidebar by default.\n\n\n\nIf if our headers are numbered, they will appeared numbered in the table of contents. If we unnnumbered a header, it will not appear with a section number. As with section numbering, we can also control the depth of the table of contents by setting the toc-depth key in the front-matter section. For example, to include sections and subsections, but not subsubsections, we would set the toc-depth key to 2 as follows:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    toc: true\n1    toc-depth: 2\n---\n\n1\n\nThe key-value pair to control the depth of the table of contents\n\n\nAnd as with section numbering we can avoid listing a header in the table of contents by adding {.unlisted} to the end of the header.\n\n\nCross-referencing tables and figures\nAnother key element in academic writing are using cross-references to tables and figures. This allows us to refer to a table or figure by number without having to manually update the number if we add or remove a table or figure.\nIn this case, we will not need to add anything to the front-matter section. Instead, we will modify keys in the code block section of a code-generated table or figure.\nTo cross-reference a table or figure, we need to add a prefix to the label key’s value. The prefix, either tbl- or fig-, indicates whether the label is for a table or figure. Additionally, table or figure captions can be added with the tbl-cap or fig-cap keys, respectively.\nLet’s look at a basic figure that we can cross-reference. The following code block will generate a very simple scatterplot.\n```{r}\n1#| label: fig-scatterplot\n2#| fig-cap: \"A scatterplot\"\n\nplot(x = 1:10, y = 1:10)\n```\n\n3In @fig-scatterplot we see a scatterplot. ....\n\n1\n\nThe label for the figure. Includes fig- as a prefix.\n\n2\n\nThe caption for the figure.\n\n3\n\nThe in-line reference to the figure. Uses the @ symbol followed by the label value.\n\n\n\n\n\n\n\n\n\nplot(1:10, 1:10)\n\n\n\n\n\n\n\nFigure 1: A scatterplot\n\n\n\n\n\nIn Figure 1 we see a scatterplot. …\n\n\n\nFor tables generated by R, the process is very similar to that of figures. The only difference is that we use the tbl- prefix on the label value and the tbl-cap key instead of the fig-cap key for the caption.\nWe can also create tables using markdown syntax. In this case, the format is a little different. Consider Table Table 1, for example.\n| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| A        | B        | C        |\n| D        | E        | F        |\n\n: A simple table {#tbl-table-1}\n\n\n\n\n\n\n\n\n\nTable 1: A simple table\n\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\n\n\n\n\n\n\n\n\n\nIn-line citations and references list\nThe last element we will cover here is adding citations and a references list to a Quarto document. To add citations we need three things:\n\nA bibliography file\nA reference to the bibliography file in the front-matter section\nA citation in the prose section which is contained in the bibliography file\n\nThe bibliography file is a plain text file that contains the citations that we want to use in our document. The file requires the extension .bib and is formatted using the BibTeX format. BibTeX is a reference syntax that is commonly used in academia.\nLet’s take a look at a sample file, bibliography.bib, that contains a single reference.\n@Manual{R-dplyr,\n  title = {dplyr: A Grammar of Data Manipulation},\n  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},\n  year = {2023},\n  note = {R package version 1.1.4},\n  url = {https://dplyr.tidyverse.org},\n}\nIn this file, we can see that the reference is for a manual entry with @Manual. The type of entry will change what fields are relevant and/ or required. In this entry, we have the cite key R-dplyr, the title, the authors, the year, a note, and a URL. Other entries, and entry types will have different fields.\nYou can find BibTeX formatted references almost everywhere you can find scholarly work. For example, Google Scholar, Web of Science, and Scopus all provide BibTeX formatted references. Additionally, many journals provide BibTeX formatted references for the articles they publish.\n\n\n\n\n\n\n Dive deeper\nManaging your references can be a challenge if you begin to amass a large number of them. There are a number of tools that can help you manage your references. For example, Zotero is a free, open-source reference manager that can help you organize your references and generate BibTeX formatted references.\nZotero also has a browser extension that allows you to easily add references to your Zotero library from your browser.\nFurthermore, Zotero can be connected to RStudio to facilitate the incorporation of BibTeX formatted references in a Quarto document. See the RStudio documentation for more information.\n\n\n\nIn the front-matter of our Quarto document, we need to add a reference to the bibliography file. This is done using the bibliography key. For example, if our bibliography file is called bibliography.bib and is located in the same directory as our Quarto document, we would add the following to the front-matter section:\n---\ntitle: \"My document title\"\nformat: pdf\n1bibliography: bibliography.bib\n---\n\n1\n\nThe key-value pair to include a path to the file which contains the BibTeX formatted references.\n\n\nWith the bibliography file and the reference to the bibliography file in the front-matter section, we can now add citations to our document. To do this, we use the @ symbol followed by the citation key in the prose section. For example, to cite the R-dplyr reference from the bibliography.bib file, we would add @R-dplyr to the prose section as follows:\nThis is a citation to @R-dplyr.\nThe citation will appear as below in the rendered document.\n\n\n\n\n\n\nThis is a citation to Wickham et al. (2023).\n\n\n\nAnd automatically, on rendering the document, a references list will be added to the end of the document. For this reason if you have citations in your document, it is a good idea to include a header section # References at the end of your document.\n\n\n\n\n\n\n Tip\nThere are a number of ways of having your inline citations appear. For example, in parentheses, with multiple citations, only with the year, adding a page number, etc.. For more information on how to format your citations, see the Quarto documentation."
  },
  {
    "objectID": "recipes/recipe-01/index.html#check-your-understanding",
    "href": "recipes/recipe-01/index.html#check-your-understanding",
    "title": "01. Academic writing with Quarto",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nConsider the following front-matter sections, A and B.\n\n\n\n\n\n\nA\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n    number-depth: 3\n    toc: false\n---\n\n\n\n\n\n\n\n\n\nB\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n    toc: true\n    toc-depth: 2\n---\n\n\n\nChoose whether the following statements are true or false.\n\nTRUEFALSE Section numbering will be included in the PDF output for both A and B.\nTRUEFALSE Section numbering will be applied to the first three levels of headers in the PDF output for both A and B.\nTRUEFALSE A table of contents will be included in the PDF output for both A and B.\nTRUEFALSE A table of contents will be included in the PDF output for B, but will only include the first two levels of headers.\n\nNow respond to the following questions.\n\n@tbl-scatterplot@fig-scatterplot@scatterplot will cross-reference a figure with the label fig-scatterplot.\n is the front-matter key to include a path to the file which contains the BibTeX formatted references."
  },
  {
    "objectID": "recipes/recipe-01/index.html#lab-preparation",
    "href": "recipes/recipe-01/index.html#lab-preparation",
    "title": "01. Academic writing with Quarto",
    "section": "Lab preparation",
    "text": "Lab preparation\nThis rounds out our introduction to academic writing in Quarto. In Lab 1 you will have an opportunity to practice these concepts by doing an article summary which includes some of these features using Quarto.\nIn preparation for Lab 1, ensure that you are prepared to do the following:\n\nEdit the front-matter section of a Quarto document to render:\n\na PDF document or a Word document\na document with numbered sections\na document with a table of contents\na document with a path to a bibliography file\n\nAdd an inline citation to the prose section of a Quarto document\n\nAlso, since you will do an article summary, you should be prepared with:\n\nan article of interest related to text analysis that you have read or at least skimmed for the following:\n\nthe research question\nthe data used\nthe methods used\nthe results/ findings of the study\n\na BibTeX formatted reference for the article\n\n\n\n\n\n\n\nIf you do not find an article of interest, you can use Bychkovska and Lee (2017)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "instructors.html#materials",
    "href": "instructors.html#materials",
    "title": "Instructors",
    "section": "Materials",
    "text": "Materials\n\nSlide decks\n\n\nExercises"
  },
  {
    "objectID": "recipes/recipe-08/index.html",
    "href": "recipes/recipe-08/index.html",
    "title": "08. Recipe",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "recipes/recipe-06/index.html",
    "href": "recipes/recipe-06/index.html",
    "title": "06. Recipe",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "recipes/recipe-00/index.html",
    "href": "recipes/recipe-00/index.html",
    "title": "00. Literate Programming",
    "section": "",
    "text": "Skills\n\nIdentify the main components of a Quarto document\nCreate and render a Quarto document\nModify front-matter and prose sections"
  },
  {
    "objectID": "recipes/recipe-00/index.html#concepts-and-strategies",
    "href": "recipes/recipe-00/index.html#concepts-and-strategies",
    "title": "00. Literate Programming",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nLiterate Programming\nFirst introduced by Donald Knuth (1984), the aim of Literate Programming is to be able to combine computer code and text prose in one document. This allows an analyst to run code, view the output of the code, view the code itself, and provide prose description all in one document. In this way, a literate programming document allows for presenting your analysis in a way that performs the computing steps desired and presents it in an easily readable format. Literate programming is now a key component of creating and sharing reproducible research (Gandrud 2015).\n\n\nQuarto\nQuarto is a specific implementation of the literate programming paradigm. In Figure 1 we see an example of Quarto in action. On the left we see the Quarto source code, which is a combination of text and code. On the right we see the output of the Quarto source code as an HTML document.\n\n\n\n\n\n\n\n\nFigure 1: Quarto source (left) and output (right) example.\n\n\n\n\n\nQuarto documents generate various types of output: web documents (HTML), PDFs, Word documents, and many other types of output formats all based on the same source code. While the interleaving of code and prose to create a variety of output documents is one of the most attractive aspects of literate programming and Quarto, it is also possible to create documents with no code at all. It is a very versatile technology as you will come to appreciate.\n\n\n\n\n\n\n Dive deeper\nTo see Quarto in action, please check out the Quarto Gallery for a variety of examples of Quarto documents and their output.\n\n\n\nA Quarto source document is a plain-text file with the extension .qmd that can be opened in any plain text reader. We will be using the RStudio IDE (henceforth RStudio) to create, open, and edit, and generate output from .qmd files but any plain-text reader, such as TextEdit (MacOS) or Notepad (PC) can open these files.\nWith this in mind, let’s now move on to the anatomy of a Quarto document.\n\nAnatomy of a Quarto Document\nAt the most basic level a Quarto document contains two components:\n\na front-matter section and\na prose section.\n\nA third component, a code block, can be interleaved within the prose section to add code to the document. Let’s look at each of these in turn.\n\nFront-matter\nThe front matter of a Quarto document appears, well, at the front of the document (or the top, rather). Referring back to Figure Figure 1, we see the front matter at the top.\n---\ntitle: \"Introduction to Quarto\"\nauthor: \"Jerid Francom\"\nformat: html\n---\nWhen creating a Quarto document with RStudio the default attribute keys are title, author, and format. The front matter is fenced by three dashes ---.\nThe values for the first two keys are pretty straightforward and can be edited as needed. The value for the format attribute can also be edited to tell the .qmd file to generate other output types. Can you guess what value we might use to generate a PDF document? Yep, it’s just pdf. As we work Quarto you will learn more about how to use the RStudio interface to change some of these key-value pairs and add others!\n\n\nProse\nAnywhere below the front matter and not contained within a code block (see below) is open for prose. The prose section(s) have an added functionality in that they are Markdown aware. What does that mean, you say? Well, Markdown refers to a set of plain-text formatting conventions to produce formatted text in the output document. To quote Wikipedia:\n\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber and Aaron Swartz created Markdown in 2004 as a markup language that is appealing to human readers in its source code form. Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n\nWhat this enables us to do is to add simple text conventions to signal how the output should be formatted. Say we want to make some text bold. We just add ** around the text we want to appear bold.\n**bold text**\nWe can also do:\n\nitalics *italics*\nlinks [links](http://wfu.edu)\nstrikethrough ~~strikethrough~~\netc.\n\nFollow this link find more information on basic Markdown syntax.\n\n\nCode blocks\nCode blocks are where the R magic happens. Again, referring to Figure 1, we see that there is the following code block.\n```{r}\n1 + 1\n```\nA code block is bound by three backticks ```. After the first backticks the curly brackets {} allow us to tell Quarto which programming language to use to evaluate (i.e. run) in the code block. In most cases this will be R, hence the the opening curly bracket `{r}`. But there are other languages that can be used in Quarto, such as Python, SQL, and Bash.\nIn the previous example, R is used as a simple calculator adding 1 + 1. Here’s what this code block produces.\n\n1 + 1\n\n&gt; [1] 2\n\n\n```{r}\n#| label: add\n1 + 1\n```\nWe have only mentioned selecting the coding language and labeling the code block, but code blocks have various other options that can be used to determine how the code block should be used. Some common code block options are:\n\nhiding the code: #| echo: false\n\n```{r}\n#| label: add\n#| echo: false\n1 + 1\n```\n\n\n&gt; [1] 2\n\n\n\nhiding the output #| include: false\n\n```{r}\n#| label: add\n#| include: false\n1 + 1\n```\n\netc.\n\n\n\n\nCreate and render a Quarto document\nThe easiest and most efficient way to create a Quarto source file is to use the RStudio point-and-click interface. Just use the toolbar to create a new file and select “Quarto Document…”, as seen in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Creating a new Quarto document in RStudio.\n\n\n\n\n\nThis will provide you a dialogue box asking you to add a title and author to the document and also allows you to select the type of document format to output, as seen in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Dialogue box for creating a new Quarto document in RStudio.\n\n\n\n\n\nEnter a title and author and leave the format set to HTML.\nOn clicking ‘Create’ you will get a Quarto document, as in Figure 4, with some default/ boilerplate prose and code blocks. The prose and code blocks can be deleted, and we can start our own document.\n\n\n\n\n\n\n\n\nFigure 4: Quarto source in RStudio.\n\n\n\n\n\nBut for now, let’s leave things as they are and see how to generate the output report from this document. Click “Render” in the RStudio toolbar. Before it will render, you will be asked to save the file and give it a name.\nOnce you have done that the .qmd file will render in the format you have specified and open in the ‘Viewer’ pane, as seen in Figure 5.\n\n\n\n\n\n\n\n\nFigure 5: Quarto source and HTML output side-by-side in RStudio.\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\nWatch Getting Started with Quarto for a guided tour of Quarto (Çetinkaya-Rundel 2023)."
  },
  {
    "objectID": "recipes/recipe-00/index.html#check-your-understanding",
    "href": "recipes/recipe-00/index.html#check-your-understanding",
    "title": "00. Literate Programming",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE Literate Programming, first introduced by Donald Knuth in 1984, allows the combination of computer code and text prose in one document.\nThe programming paradigm Literate Programming is implemented through QuartoRRStudioGitHub, a platform that facilitates the creation of a variety of output documents based on the same source code.\nWhich of the following components does a basic Quarto document not contain? Front-matter sectionProse sectionBack-matter sectionCode block\nTo generate a PDF document in Quarto, you can edit the format attribute value in the front-matter section to .\nTRUEFALSE The code block options echo and include can be used to hide the code and output, respectively.\nTRUEFALSE In Quarto, a code block, where the programming language code is entered, is bounded by three underscores (_)."
  },
  {
    "objectID": "recipes/recipe-00/index.html#lab-preparation",
    "href": "recipes/recipe-00/index.html#lab-preparation",
    "title": "00. Literate Programming",
    "section": "Lab preparation",
    "text": "Lab preparation\nThis concludes our introduction to literate programming using Quarto. We have covered the basics there but there is much more to explore.\nIn preparation for Lab 0, ensure that you have completed the following:\n\nSetup your computing environment with R and RStudio\nInstalled the necessary packages:\n\nquarto\ntinytex\n\n\nand that you are prepared to do the following:\n\nOpen RStudio and understand the basic interface\nCreate, edit, and render Quarto documents\nUse some basic Markdown syntax to format text\n\nWith this in mind, you are ready to move on to Lab 00."
  },
  {
    "objectID": "recipes/index.html",
    "href": "recipes/index.html",
    "title": "Recipes",
    "section": "",
    "text": "00. Literate Programming\n\n\nAn introduction to Quarto\n\n\nIn this recipe, we will introduce the concept of Literate Programming and describe how to implement this concept through Quarto. I will provide a demonstration of some of the features of Quarto and describe the main structural characteristics of a Quarto document to help you get off and running writing your own documents that combine code and prose. \n\n\n\n\n\n8 min\n\n\n1,449 words\n\n\n\n\n\n\n\n\n\n\n\n\n01. Academic writing with Quarto\n\n\nKey Quarto features for academic writing\n\n\nThe implementation of literate programming we are using in this course is Quarto with R. As we have seen in previously, Quarto provides the ability to combine prose and code in a single document. This is a powerful strategy for creating reproducible documents that can be easily updated and shared. \n\n\n\n\n\n12 min\n\n\n2,302 words\n\n\n\n\n\n\n\n\n\n\n\n\n02. Reading, inspecting, and writing datasets\n\n\nBasics of working with datasets in R\n\n\nThis Recipe guides you through the process of reading, inspecting, and writing datasets using R packages and functions in a Quarto environment. You’ll learn how to effectively combine code and narrative to create a reproducible document that can be shared with others. \n\n\n\n\n\n17 min\n\n\n3,227 words\n\n\n\n\n\n\n\n\n\n\n\n\n03. Descriptive assessment of datasets\n\n\nSummarizing data with statistics, tables, and plots\n\n\nIn this Recipe we will explore appropriate methods for summarizing variables in datasets given the number and informational values of the variable(s). We will build on our understanding of how to summarize data using statistics, tables, and plots. \n\n\n\n\n\n17 min\n\n\n3,290 words\n\n\n\n\n\n\n\n\n\n\n\n\n04. Understanding the computing environment\n\n\nIdentify layers of computing environments and preview Git and GitHub workflows\n\n\nIn this recipe, we will learn how to scaffold a research project and how to use the tools and resources available to us to manage research projects. We will build on our understanding of the computing environment and the structure of reproducible projects and introduce new features of Git and GitHub. \n\n\n\n\n\n11 min\n\n\n2,135 words\n\n\n\n\n\n\n\n\n\n\n\n\n05. Collecting and documenting data\n\n\nSummarizing data with statistics, tables, and plots\n\n\nAt this point, we now have a strong undertanding of the foundations of programming in R and the data science workflow. Previous lessons, recipes, and labs focused on developing these skills while the chapters aimed to provide a conceptual framework for understanding the steps in the data science workflow. We now turn to applying our conceptual knowledge and our technical skills to accomplish the tasks of the data science workflow, starting with data acquisition. \n\n\n\n\n\n25 min\n\n\n4,819 words\n\n\n\n\n\n\n\n\n\n\n\n\n06. Recipe\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n1 words\n\n\n\n\n\n\n\n\n\n\n\n\n07. Recipe\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n1 words\n\n\n\n\n\n\n\n\n\n\n\n\n08. Recipe\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n1 words\n\n\n\n\n\n\n\n\n\n\n\n\n09. Recipe\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n1 words\n\n\n\n\n\n\n\n\n\n\n\n\n10. Recipe\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n1 words\n\n\n\n\n\n\n\n\n\n\n\n\n11. Sharing research\n\n\nCommunicating research findings\n\n\nIn this recipe, I cover the tools and strategies for sharing research findings with the public and peers. We will begin assuming we are using Quarto websites as the primary tool for sharing research findings in both forums. From there, we will enter into some of the details of articles, presentations, and publishing research code and data. \n\n\n\n\n\n13 min\n\n\n2,411 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recipes/recipe-02/index.html",
    "href": "recipes/recipe-02/index.html",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "",
    "text": "Skills\n\nLoading packages into an R session\nReading datasets into R with read_*() functions\nInspecting datasets with dplyr functions\nWriting datasets to a file with write_*() functions"
  },
  {
    "objectID": "recipes/recipe-02/index.html#concepts-and-strategies",
    "href": "recipes/recipe-02/index.html#concepts-and-strategies",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nQuarto documents and code blocks\nAsk you will remember from Recipes 0 and 1, Quarto documents can combine prose and code. The prose is written in Markdown and the code is written in R1. The code is contained in code blocks, which are opened by three backticks (`), the name of the programming language, r, in curly braces {r} and three backticks (`) to close the block. For example, the following minimal Quarto document contains an R code block:\n---\ntitle: My Quarto Document\nformat: pdf\n---\n\n# Goals\n\nThis script ...\n\n```{r}\n#| label: code-block-name\n\n# R code goes here\n```\n\nAs you can see in the code block, the ...\nCode blocks have various options that can be added by using key-value pairs that are prefixed with #|. Some common key-value pairs we will use in this Recipe are:\n\nlabel: A unique name for the code block. This is used to reference the code block.\necho: A boolean value (true or false) that determines whether the code is displayed in the output document.\ninclude: A boolean value (true or false) that determines whether the output of the code is displayed in the output document.\nmessage: A boolean value (true or false) that determines whether the messages from the code are displayed in the output document.\n\n\n\nSetting up the environment\nBefore we can read, inspect, and write data, we need to load the packages that contain the functions we will use. We will use the readr package to read datasets into R and write datasets to disk and the dplyr package to inspect and transform (subset) the data.\nThere are a few ways to load packages into an R session. The most common way is to use the library() function. The library() function loads a package into the R session and stops the script if the package is not available on the current computing environment.\nFor example, the following code block loads the readr and dplyr packages into the R session:\n\n```{r}\n#| label: load-packages\n\n# Load packages\nlibrary(readr) # for reading and writing data\nlibrary(dplyr) # for inspecting and transforming data\n```\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis code block assumes that the readr and dplyr packages are installed on the current computing environment. If the packages are not installed, the code block will stop and display an error message, such as:\nError in library(readr) : there is no package called ‘readr’\nThis error can be addressed by installing the missing package with install.packages(\"readr\") and then re-running the code block. This is not ideal for reproducibility, however, because the code block will stop if the package is not installed. We will consider a more reproducible approach later in the course.\n\n\n\n\n\n\n Dive deeper\nIf you interested in learning about safeguarding package loading in a reproducible way, see the renv package. The renv package is a project-oriented workflow to create a reproducible environment for R projects. For more information, see the renv documentation and/ or Recipe 11\n\n\n\n\n\nUnderstanding the data\nNow that we have our environment set up, we can read the dataset into R. But before we do, we should make sure that we understand the data by looking at the data documentation.\nThe dataset that we will read into our R session based on the Brown Corpus (Francis and Kuçera 1961). I’ve created a data origin file that contains the data documentation for the Brown Corpus, as we can see in Table 1.\n\n# Read and display the data origin file\n\nread_csv(file = \"data/original/brown_passives_do.csv\") |&gt;\n  kable() |&gt;\n  kable_styling() |&gt;\n  column_spec(1, width = \"15em\")\n\n\n\nTable 1: Data origin file for the Brown Corpus.\n\n\n\n\n\n\n\nattribute\ndescription\n\n\n\n\nResource name\nBrown Corpus\n\n\nData source\nhttp://korpus.uib.no/icame/brown/bcm.html\n\n\nData sampling frame\nEdited American English prose from various genres, published in the United States during the calendar year 1961.\n\n\nData collection date(s)\nOriginally published in 1964, revised in 1971 and 1979.\n\n\nData format\nMultiple formats including Form A (original), Form B (stripped version), Form C (tagged version), Bergen Forms I and II, and Brown MARC Form.\n\n\nData schema\n500 samples of approximately 2000 words each, covering a wide range of genres and styles. Includes coding for major and minor headings, special types (italics, bold, etc.), abbreviations, symbols, and other textual features.\n\n\nLicense\nUse restricted for scholarly research in linguistics, stylistics, and other disciplines. Specific copyright restrictions detailed in the manual.\n\n\nAttribution\nW. Nelson Francis and Henry Kucera, Brown University, 1964, revised 1971 and 1979.\n\n\n\n\n\n\n\n\n\n\n\nThis data origin file provides an overview of the original data source. In this case, the dataset we will read into R is a subset of the Brown Corpus which is an aggregate of the use of passive voice. This dataset was developed by the authors of the corpora package (Evert 2023). I’ve exported the dataset to a CSV file, which we will read into R.\nThe data dictionary which describes the dataset we will read appears in Table 2.\n# Read and display the data documentation file\nread_csv(file = \"../data/derived/brown_passives_curated_dd.csv\") |&gt;\n  kable() |&gt;\n  kable_styling()\n\n\n\n\nTable 2: Data dictionary file for the Brown Corpus.\n\n\n\n\n\n\n\nvariable\nname\nvariable_type\ndescription\n\n\n\n\ncat\nCategory\ncategorical\nGenre categories represented by letters\n\n\npassive\nPassive\nnumeric\nNumber of passive verb phrases in the genre\n\n\nn_w\nNumber of words\nnumeric\nNumber of words in the genre\n\n\nn_s\nNumber of sentences\nnumeric\nNumber of sentences in the genre\n\n\nname\nGenre\ncategorical\nGenre name\n\n\n\n\n\n\n\n\n\n\n\nWith this information, we are now in a position to read and inspect the dataset.\n\n\nReading datasets into R with readr\nWe’ve now prepared our Quarto document by loading the packages we will use and and we have reviewed the dataset documentation so that we understand the dataset we will read into R. We are now ready to read the dataset into R.\nR provides a number of functions to read data of many types in R. We will explore many types of data and datasets in this course. For now, we will focus on reading rectangular data into R. Rectangular data is data that is organized in rows and columns, such as a spreadsheet.\nOne of the most common file formats for rectangular data is the comma-separated values (CSV) file. CSV files are text files in which lines represent rows and commas separate columns of data. For example, the sample CSV file snippet below contains three rows and three columns of data:\n\"word\",\"frequency\",\"part_of_speech\"\n\"the\",69971,\"article\"\n\"of\",36412,\"preposition\"\n\"and\",28853,\"conjunction\"\nA CSV file is a type of delimited file, which means that the data is separated by a delimiter. In the case of a CSV file, the delimiter is a comma. Other types of delimited files use different delimiters, such as tab-separated values (TSV) files which use a tab character as the delimiter, or even a pipe (|) or semicolon (;).\nThe readr package provides functions to read rectangular dataset into R. The read_csv() function reads CSV files, the read_tsv() function reads TSV files, and the read_delim() function reads other types of delimited files.\nLet’s use the read_csv() function to read the brown_passives_curated.csv file into R. To do this we will use the file = argument to specify the path to the file. Now, the file “path” is the location of the file on the computer. We can specify this path in two ways:\n\nRelative path: The relative path is the path to the file relative to the current working directory. The current working directory is the directory in which the R session is running.\nAbsolute path: The absolute path is the path to the file from the root directory of the computer.\n\nFor most purpose, the relative path is the better option because it is more portable. For example, if you share your code with someone else, they may have a different absolute path to the file. However, they will likely have the same relative path to the file.\nLet’s say that the directory structure of our project is as follows:\nproject/\n├── data/\n│   ├── original/\n│   │   └── brown_passives_do.csv\n│   └── derived/\n│       └── brown_passives_curated.csv\n└── code/\n    └── reading-inspecting-writing.qmd\nIn this case, the relative path from reading-inspecting-writing.qmd to the brown_passives_curated.csv file is ../data/derived/brown_passives_curated.csv. The .. means “go up one directory” and the rest of the path is the path to the file from the project/ directory.\nWith this in mind, we can read the brown_passives_curated.csv file into R with the following code block:\n#| label: read-dataset-brown-passives-curated\n\n# Read the dataset\nbrown_passives_df &lt;-\n  read_csv(file = \"../data/derived/brown_passives_curated.csv\")\nRunning the above code chunk in our Quarto document will read the dataset into R and assign it to the brown_passives_df variable. It will also show the code used to read the dataset into R. Furthermore, so functions will display messages in the output. For example, the read_csv() function will display a message that various parsing options were used to read the dataset into R.\nThis information can be helpful in an interactive session, as read_csv() tells us the dimensions of the dataset and the data types of each column. But this output is not necessary, and is unnecessarily verbose in a reproducible document.\nWe can hide any messages produced by a function by using the message = false key-value pair in the code block. For example, the following code block will read the dataset into R and assign it to the brown_passives_df variable without displaying any messages:\n#| label: read-dataset-brown-passives-curated\n#| message: false\n\n# Read the dataset\nbrown_passives_df &lt;-\n  read_csv(file = \"../data/derived/brown_passives_curated.csv\")\nNo messages are displayed in the document output.\n\n\nInspecting datasets with dplyr\nThe objective of this section is to demonstrate how to inspect and transform (subset) datasets using the dplyr package. We will use the dplyr package to inspect the dataset we read into R in the previous section.\nReading a CSV file into R will create a data frame object. Thus, I assigned the result to brown_passives_df. The df suffix is a common naming convention for rectangular data frames. It is good practice to use a consistent naming convention for objects in your code. This makes it easier to understand the code and to avoid errors.\nLet’s do get an overview of the dataset by using the glimpse() function from the dplyr package. The glimpse() function displays the dimensions of the data frame and the data types of each column.\n\n# Preview\nglimpse(brown_passives_df)\n\n&gt; Rows: 15\n&gt; Columns: 5\n&gt; $ cat     &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"J\", \"K\", \"L\", \"M\", \"N…\n&gt; $ passive &lt;dbl&gt; 892, 543, 283, 351, 853, 1034, 1460, 837, 2423, 352, 265, 104,…\n&gt; $ n_w     &lt;dbl&gt; 101196, 61535, 40749, 39029, 82010, 110363, 173017, 69446, 181…\n&gt; $ n_s     &lt;dbl&gt; 3684, 2399, 1459, 1372, 3286, 4387, 6537, 2012, 6311, 3983, 36…\n&gt; $ name    &lt;chr&gt; \"press reportage\", \"press editorial\", \"press reviews\", \"religi…\n\n\nIf we want a more, tabular-like view of the data, we can simply print the dataset frame to the console. It’s worth mentioning, that all readr functions return tibbles, so we gain the benefits of tibbles when we read dataset into R with readr functions, one of which is that we do not have to worry that printing a data frame to the console, or our document, will print all of the data.\n\n# Print the data frame\nbrown_passives_df\n\n&gt; # A tibble: 15 × 5\n&gt;    cat   passive    n_w   n_s name            \n&gt;    &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n&gt;  1 A         892 101196  3684 press reportage \n&gt;  2 B         543  61535  2399 press editorial \n&gt;  3 C         283  40749  1459 press reviews   \n&gt;  4 D         351  39029  1372 religion        \n&gt;  5 E         853  82010  3286 skills / hobbies\n&gt;  6 F        1034 110363  4387 popular lore    \n&gt;  7 G        1460 173017  6537 belles lettres  \n&gt;  8 H         837  69446  2012 miscellaneous   \n&gt;  9 J        2423 181426  6311 learned         \n&gt; 10 K         352  68599  3983 general fiction \n&gt; 11 L         265  57624  3673 detective       \n&gt; 12 M         104  14433   873 science fiction \n&gt; 13 N         290  69909  4438 adventure       \n&gt; 14 P         290  70476  4187 romance         \n&gt; 15 R         146  21757   975 humour\n\n\nBy default, printing tibbles will return the first 10 rows and all columns, unless the columns are too numerous to display width-wise.\ndplyr also provides a set of slice_*() functions which allow us to display the data in a tabular fashion, with some additional options. There are three slice_*() functions we will cover here:\n\nslice_head(): Select the first n rows of the data frame.\nslice_tail(): Select the last n rows of the data frame.\nslice_sample(): Select a random sample of n rows from the data frame.\n\nFor example, the following code block will select the first 5 rows of the data frame:\n\n# Select the first 5 rows\nslice_head(brown_passives_df, n = 5)\n\n&gt; # A tibble: 5 × 5\n&gt;   cat   passive    n_w   n_s name            \n&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n&gt; 1 A         892 101196  3684 press reportage \n&gt; 2 B         543  61535  2399 press editorial \n&gt; 3 C         283  40749  1459 press reviews   \n&gt; 4 D         351  39029  1372 religion        \n&gt; 5 E         853  82010  3286 skills / hobbies\n\n\nWe can also select the last 5 rows of the data frame with the slice_tail() function:\n\n# Select the last 5 rows\nslice_tail(brown_passives_df, n = 5)\n\n&gt; # A tibble: 5 × 5\n&gt;   cat   passive   n_w   n_s name           \n&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n&gt; 1 L         265 57624  3673 detective      \n&gt; 2 M         104 14433   873 science fiction\n&gt; 3 N         290 69909  4438 adventure      \n&gt; 4 P         290 70476  4187 romance        \n&gt; 5 R         146 21757   975 humour\n\n\nFinally, we can select a random sample of 5 rows from the data frame with the slice_sample() function:\n\n# Select a random sample of 5 rows\nslice_sample(brown_passives_df, n = 5)\n\n&gt; # A tibble: 5 × 5\n&gt;   cat   passive   n_w   n_s name         \n&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n&gt; 1 D         351 39029  1372 religion     \n&gt; 2 R         146 21757   975 humour       \n&gt; 3 P         290 70476  4187 romance      \n&gt; 4 L         265 57624  3673 detective    \n&gt; 5 H         837 69446  2012 miscellaneous\n\n\nThese functions can be helpful to get a sense of the dataset in different ways. In combination with arrange() function, we can also sort the data frame by a column or columns and then select the first or last rows.\nFor example, the following code block will sort the data frame by the passive column in ascending order and then select the first 5 rows:\n\n# Sort by the `passive` column and select the first 5 rows\nslice_head(arrange(brown_passives_df, passive), n = 5)\n\n&gt; # A tibble: 5 × 5\n&gt;   cat   passive   n_w   n_s name           \n&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n&gt; 1 M         104 14433   873 science fiction\n&gt; 2 R         146 21757   975 humour         \n&gt; 3 L         265 57624  3673 detective      \n&gt; 4 C         283 40749  1459 press reviews  \n&gt; 5 N         290 69909  4438 adventure\n\n\nIf we want to sort be descending order, we can surround the column name with desc(), arrange(desc(passive)).\nNow, the previous code block does what we want, but it is not very readable. Enter the pipe operator. The pipe operator |&gt; is an operator which allows us to chain the output of one function to the input of another function. This allows us to write more readable code.\n\nbrown_passives_df |&gt;\n  arrange(passive) |&gt;\n  slice_head(n = 5)\n\n&gt; # A tibble: 5 × 5\n&gt;   cat   passive   n_w   n_s name           \n&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n&gt; 1 M         104 14433   873 science fiction\n&gt; 2 R         146 21757   975 humour         \n&gt; 3 L         265 57624  3673 detective      \n&gt; 4 C         283 40749  1459 press reviews  \n&gt; 5 N         290 69909  4438 adventure\n\n\nThe result is the same but the code makes more sense. We can read the code from left to right, top to bottom, which is the order in which the functions are executed.\n\n\n\n\n\n\n Dive deeper\nThe native R pipe |&gt; was introduced in R 4.1.0. If you are using an earlier version of R, you can use the magrittr package to load the pipe operator %&gt;%.\nThere are certain advantages to using the magrittr pipe operator, including the ability to use the pipe operator to pass arguments to functions with placeholders. For more information, see the magrittr documentation.\n\n\n\nIn addition to being more legible, using the pipe with each function on its own line allows us to add comments to each line of code. For example, the following code block is the same as the previous code block, but with comments added.\n\n# Sort by the passive column and select the first 5 rows\nbrown_passives_df |&gt;\n  arrange(passive) |&gt;\n  slice_head(n = 5)\n\nIt is a good practice to add comments when writing code, as long as it makes the code more readable and easier to understand for others and for your future self! If the comments are too verbose, and only repeat what the code is ‘saying’, then don’t include them.\n\n\nSubsetting datasets with dplyr\nNow that we have a sense of the data, we can subset the dataset to create a variations of our original data frame. We can subset the data frame by selecting columns and/ or rows.\nIn the R lesson “Packages and Functions”, we saw that base R provides the bracket ([]) operator to subset data frames. The dplyr package provides functions to subset data frames which can be more readable and easier to use.\nLet’s first look a selecting columns. The select() function allows us to select columns by name. For example, the following code block will select the passive and n_w columns from the data frame:\n\n# Select the `passive` and `n_w` columns\nselect(brown_passives_df, passive, n_w)\n\n&gt; # A tibble: 15 × 2\n&gt;    passive    n_w\n&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n&gt;  1     892 101196\n&gt;  2     543  61535\n&gt;  3     283  40749\n&gt;  4     351  39029\n&gt;  5     853  82010\n&gt;  6    1034 110363\n&gt;  7    1460 173017\n&gt;  8     837  69446\n&gt;  9    2423 181426\n&gt; 10     352  68599\n&gt; 11     265  57624\n&gt; 12     104  14433\n&gt; 13     290  69909\n&gt; 14     290  70476\n&gt; 15     146  21757\n\n\nBeyond selecting columns, we can also reorder columns and rename columns. For example, the following code block will select the passive and n_w columns, rename the n_w column to num_words, and reorder the columns so that num_words is the first column:\n\n# Select rename and reorder columns\nbrown_passives_df |&gt;\n  select(num_words = n_w, passive)\n\n&gt; # A tibble: 15 × 2\n&gt;    num_words passive\n&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n&gt;  1    101196     892\n&gt;  2     61535     543\n&gt;  3     40749     283\n&gt;  4     39029     351\n&gt;  5     82010     853\n&gt;  6    110363    1034\n&gt;  7    173017    1460\n&gt;  8     69446     837\n&gt;  9    181426    2423\n&gt; 10     68599     352\n&gt; 11     57624     265\n&gt; 12     14433     104\n&gt; 13     69909     290\n&gt; 14     70476     290\n&gt; 15     21757     146\n\n\n\n\n\n\n\n\n Dive deeper\nselect() also provides a number of helper functions to select columns. For example, we can use the starts_with() function inside the select() call to select columns that start with a certain string. Or we can select columns by their vector type by using where(is.character).\nFor more information, see the select() documentation or use the ?select command in the R console.\n\n\n\nBy selecting some columns and not others, we have effectively dropped the columns we did not select. If it is more effective to drop columns by name, we can use the select() function with the - operator. For example, the following code block will drop the cat column from the data frame:\n\n# Drop the `n_w` column\nbrown_passives_df |&gt;\n  select(-cat)\n\n&gt; # A tibble: 15 × 4\n&gt;    passive    n_w   n_s name            \n&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n&gt;  1     892 101196  3684 press reportage \n&gt;  2     543  61535  2399 press editorial \n&gt;  3     283  40749  1459 press reviews   \n&gt;  4     351  39029  1372 religion        \n&gt;  5     853  82010  3286 skills / hobbies\n&gt;  6    1034 110363  4387 popular lore    \n&gt;  7    1460 173017  6537 belles lettres  \n&gt;  8     837  69446  2012 miscellaneous   \n&gt;  9    2423 181426  6311 learned         \n&gt; 10     352  68599  3983 general fiction \n&gt; 11     265  57624  3673 detective       \n&gt; 12     104  14433   873 science fiction \n&gt; 13     290  69909  4438 adventure       \n&gt; 14     290  70476  4187 romance         \n&gt; 15     146  21757   975 humour\n\n\nLet’s now turn our attention to subsetting rows. The filter() function allows us to select rows by a logical condition. For example, the following code block will select rows where the values of the passive column are less than &lt; 1,000:\n\n# Select rows where `passive` is less than 1,000\nbrown_passives_df |&gt;\n  filter(passive &lt; 1000)\n\n&gt; # A tibble: 12 × 5\n&gt;    cat   passive    n_w   n_s name            \n&gt;    &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n&gt;  1 A         892 101196  3684 press reportage \n&gt;  2 B         543  61535  2399 press editorial \n&gt;  3 C         283  40749  1459 press reviews   \n&gt;  4 D         351  39029  1372 religion        \n&gt;  5 E         853  82010  3286 skills / hobbies\n&gt;  6 H         837  69446  2012 miscellaneous   \n&gt;  7 K         352  68599  3983 general fiction \n&gt;  8 L         265  57624  3673 detective       \n&gt;  9 M         104  14433   873 science fiction \n&gt; 10 N         290  69909  4438 adventure       \n&gt; 11 P         290  70476  4187 romance         \n&gt; 12 R         146  21757   975 humour\n\n\nWe can also use the filter() function to select rows by a character string. For example, the following code block will select rows where the values of the name column are equal to religion:\n\n# Select rows where `name` is equal to `religion`\nbrown_passives_df |&gt;\n  filter(name == \"religion\")\n\n&gt; # A tibble: 1 × 5\n&gt;   cat   passive   n_w   n_s name    \n&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n&gt; 1 D         351 39029  1372 religion\n\n\nThe inequality operator != can be used for character strings as well. To include multiple values, we can use the %in% operator. In this case we can pass a vector of values to the filter() function. For example, the following code block will select rows where the values of the name column are equal to religion or learned:\n\n# Select multiple values\nbrown_passives_df |&gt;\n  filter(name %in% c(\"religion\", \"learned\", \"detective\"))\n\n&gt; # A tibble: 3 × 5\n&gt;   cat   passive    n_w   n_s name     \n&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n&gt; 1 D         351  39029  1372 religion \n&gt; 2 J        2423 181426  6311 learned  \n&gt; 3 L         265  57624  3673 detective\n\n\n\n\n\n\n\n\n Dive deeper\nFor more sophisticated subsetting, we can use the str_detect() function from the stringr package to select rows where the values of the name column contain a certain string. This approach will be enhanced later in the course when we learn about regular expressions.\n\n\n\n\n\nWriting datasets to a file with readr\nFinally, we can write data, including data frames, to a file with the write_*() functions from the readr package. The write_*() functions include:\n\nwrite_csv(): Write a data frame to a CSV file.\nwrite_tsv(): Write a data frame to a TSV file.\nwrite_delim(): Write a data frame to a delimited file with the specified delimiter (|, ;, etc).\n\nTo create a distinct data frame from the one we read into R, let’s subset our brown_passives_df data frame by columns and rows to create a new data frame that contains only the passive, n_w, and name columns and only the rows where the values of the passive column are greater than &gt; 1,000 and assign it to the brown_passives_subset_df.\n\n# Subset the data frame\nbrown_passives_subset_df &lt;-\n  brown_passives_df |&gt;\n  select(passive, n_w, name) |&gt;\n  filter(passive &gt; 1000)\n\nNow the following code block will write the brown_passives_subset_df data frame to a CSV file given the specified file path:\n\n# Write the data frame to a CSV file\nwrite_csv(\n  x = brown_passives_subset_df,\n  file = \"../data/derived/brown_passives_subset.csv\"\n)\n\nGiven the example directory structure we saw earlier, our new file appears in the data/derived/ directory.\nproject/\n├── data/\n│   ├── original/\n│   │   └── brown_passives_do.csv\n│   └── derived/\n│       ├── brown_passives_curated.csv\n│       ├── brown_passives_curated_dd.csv\n│       └── brown_passives_subset.csv\n└── code/\n    └── reading-inspecting-writing.qmd\nThere is much more to learn about reading, inspecting, and writing datasets in R. We will introduce more functions and techniques in the coming lessons. For now, we have learned how to read, inspect, and write datasets using R functions and Quarto code blocks!"
  },
  {
    "objectID": "recipes/recipe-02/index.html#check-your-understanding",
    "href": "recipes/recipe-02/index.html#check-your-understanding",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE The readr package provides functions to read rectangular data into R.\nThe echomessageinclude option in a code block determines whether the code is displayed in the output document.\nTRUEFALSE The dplyr package provides functions to create data dictionaries.\nread_csv()read_tsv()read_delim() is used to read tab-separated values (TSV) files.\nWhich function is in dplyr is used to select columns by name? select()filter()slice_head()\nTRUEFALSE The R pipe operator |&gt; allows us to chain the output of one function to the input of another function."
  },
  {
    "objectID": "recipes/recipe-02/index.html#lab-preparation",
    "href": "recipes/recipe-02/index.html#lab-preparation",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn Lab 2 you will have the opportunity to apply the skills you learned in this Recipe to create a Quarto document that reads, inspects, and writes data.\nIn addition to the knowledge and skills you have developed in Labs 0 and 1, to complete Lab 2, you will need to be able to:\n\nCreate code blocks in a Quarto document\nUnderstand the purpose of the label, echo, message, and include options in a code block\nLoad packages into an R session with library()\nUnderstand how to read and create file relative file paths\nRead datasets into R with the read_csv() function\nInspect data frames with dplyr functions such as glimpse(), slice_head(), slice_tail(), slice_sample(), and arrange().\nUse the |&gt; pipe operator to chain functions together.\nSubset data frames with dplyr functions such as select() and filter().\nWrite data frames to a file with the write_csv() function."
  },
  {
    "objectID": "recipes/recipe-02/index.html#footnotes",
    "href": "recipes/recipe-02/index.html#footnotes",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Code block can be written in other programming languages as well such as Python, Bash, etc.↩︎"
  },
  {
    "objectID": "recipes/recipe-04/index.html",
    "href": "recipes/recipe-04/index.html",
    "title": "04. Understanding the computing environment",
    "section": "",
    "text": "Skills\n\nUnderstanding the components of a reproducible project\nConnecting the computing environment to reproducible project management\nUnderstanding the workflow and project structure\nUsing Git and GitHub to manage a project"
  },
  {
    "objectID": "recipes/recipe-04/index.html#concepts-and-strategies",
    "href": "recipes/recipe-04/index.html#concepts-and-strategies",
    "title": "04. Understanding the computing environment",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nProject components\nReproducible projects are composed of two main components: the computing environment and the project structure. The computing environment is the hardware, operating system, and software that we use to do our work and the project structure is the organization of the files and folders that make up our project. As we can see in Figure 1, each of these components and subcomponents are nested within each other.\n\n\n\n\n\n\nFigure 1: Compontents of a reproducible project\n\n\n\nIn the lesson on the computing environment, we learned about the importance of understanding the computing environment and how to find out information about our computing environment inspecting an R session, as we seen in Snippet 1.\n\n\n\nSnippet 1: Session information output\n\n\n─ Session info ──────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Ventura 13.6.1\n system   x86_64, darwin22.6.0\n ui       unknown\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-11-26\n pandoc   3.1.9 @ /usr/local/bin/pandoc\n\n─ Packages ──────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.2)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.2)\n rlang         1.1.2   2023-11-04 [1] CRAN (R 4.3.2)\n\n [1] /Users/francojc/R/Library\n [2] /usr/local/Cellar/r/4.3.2/lib/R/library\n\n\n\nAs we learned, when the computing environment that is used to create a project is different from the computing environment that is used to reproduce the project, there is a risk that the project will not be reproducible. We will not tackle all the components in Figure 1 at once, however, but instead we will focus here on the project structure. Later on, with more experience, we will address the other components.\n\n\nManaging reproducible projects\nThe project structure is the organization of the files and folders that make up our project. A minimal project structure includes separation for input, process, and output of research, documentation about the project, how to reproduce the project, and the project files themselves. The project structure is important because it helps us organize our work and it helps others understand it.\nThere is no concensus about what the best project structure is. However, the principles covered in Chapter 4 can guide us in developing a project structure that both organizes our work and makes that work understandable to others. With these principles met, we can add additional structure to our project to meet our project-specific needs.\nTo create a reproducible project structure, we need no more than to create a directory and set of files that meet the principles for a minimal reproducible framework. During or after the project, would could back up these directories and files and/ or share them with others in a number of ways.\nAlthough this approach is already a good step in the right direction, it is error prone and will likely lead to inconsistencies across projects. A better approach is to develop, or adopt, a project structure template that can be used for all projects, use version control to track changes to your project, and upload your project to a remote repository where it is backed up (including the version history) and can be shared with others efficiently.\nThis later approach is the one that we will use in subsequent lessons, recipes, and labs in this course.\n\n\nLeveraging Git and GitHub\nAt this point, you are somewhat familiar with Git and Github. You have likely used Git to copy and download a repository from GitHub, say for example the labs for this course. However, what is going on behind the scenes is likely still a bit of a mystery. In this section, we will demystify Git and GitHub, a bit, and learn how to use them together to manage a project in different scenarios.\n\n\n\n\n\n\n Tip\nVerify that you have a working version of Git on your computing environment and make sure that you have a GitHub account. You can refer to Guide 2 for more information on how to do this.\n\n\n\nSo let’s rewind a bit and review what Git and Github are and how they work together. Git is a version control system that allows us to track changes to our project files. It is a command line tool, much like R, that is installed as software. Also like R, we can interact with Git through a graphical user interface (GUI), such as RStudio.\nDirectory and file tracking with Git can be added to a project at any time. A tracked project is called a repository, or repo for short. When used on our computing environment, the repo is called a local repository. There are many benefits to using Git to track changes to local repository, including the ability to revert to previous versions of files, create and edit parallel copies of files and then selectively integrate them, and much more.\nBut the real power of Git is realized in combination with GitHub. Github is a cloud-based remote repository that allows us to store our git-tracked projects. It is a web-based platform which requires an account to use. Once you are signed up, you can connect Git and Github to create remote repositories and upload your local repositories to them. You can also download remote repositories to your computing environment. There are many, many features that Github offers, but for now we will focus a few key features that will help us manage our projects.\nIn Figure 2, I provide a schematic look at the relationship between Github and Git for three common scenarios.\n\n\n\n\n\n\nFigure 2: Key features of GitHub\n\n\n\nScenario A: Clone a remote repository\nIn this scenario, we locate a remote repository on Github that someone has made publically available. Then we clone (copy and download) the repository to our computing environment. Once the repository is cloned locally, we can edit the files as we see fit.\nIn essence, we are just downloading a group of files and folders to our computing environment from Github. This is the scenario that we have been using in this course to download the lab repositories.\nSteps:\n\nLocate a remote repository on Github that someone has made publically available and copy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\n\nScenario B: Fork and clone a remote repository\nThis scenario differs from A in two respects. First, we first fork (copy to) the remote repository to our Github account before cloning it to our computing environment. Second, we commit (log edits) changes to the git tracking system and then push (sync to) the changes to our remote repository.\nIn this case, we are doing more than just downloading a group of files and folders. We are setting up a link between the other person’s remote repository and our own remote repository. We do not have to use this link, but if we do want to, we can pull (sync from) any changes that are made to the other person’s remote repository to our remote repository. This can be useful if we want to keep our remote repository up to date with the other person’s remote repository. Furthermore, this link allows us to propose changes to the other person’s remote repository with a pull request –a request for the other person to pull our changes into their remote repository. This can be useful if we want to collaborate with the other person on the project. Pull and pull request are more advanced features that we will not address at this point.\nThe second difference is that we are using Git to track changes to our local repository and then push those changes to our remote repository. This is a key feature because it allows us to keep track of changes to our project files and folders and revert to previous versions if needed. It also allows us to share our project with others and collaborate with them.\nSteps:\n\nLocate a remote repository on Github that someone has made publically available and click the fork button.\nStill on Github, choose new account as the owner of the forked repository.\nIn the forked repository, copy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\nMake changes to the project files and folders.\nCommit the changes to the git tracking system.\nPush the changes to the remote repository.\n\nScenario C: Create/ Join and clone a remote repository\nThis scenario is similar to B, but instead of forking a remote repository, we create a new remote repository on Github and then clone it to our computing environment. We then commit and push changes to our remote repository. In this case, we are creating a new remote repository and then using Git to track changes to our local repository and push those changes to our remote repository.\nThis scenario is common when we work on our own projects or when we want to collaborate with others on a project. In the latter case, we would create a remote repository and then invite others to collaborate on it. Everyone with permissions to the remote repository can clone it to their computing environment, make changes, and then push those changes to the same remote repository. This allows everyone to work on the same project and keep track of changes to the project files and folders.\nWhen working with multiple people on a project, you can imagine that if I’m working on the project locally and you are working on the project locally, we might make changes to the same files and folders. If we both push our changes to the remote repository, there is a risk that the changes will conflict with each other. Git and Github have features that help us manage these conflicts (pull, fetch, merge, etc.), but we will not address them at this point either.\nSteps:\n\nCreate a new remote repository on Github or accept an invitation to collaborate on a remote repository.\nCopy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\nMake changes to the project files and folders.\nCommit the changes to the git tracking system.\nPush the changes to the remote repository."
  },
  {
    "objectID": "recipes/recipe-04/index.html#summary",
    "href": "recipes/recipe-04/index.html#summary",
    "title": "04. Understanding the computing environment",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we reviewed the components of reproducible research projects: the computing environment and the project structure. The computing environment is the hardware, operating system, and software that we use to do our work and the project structure is the organization of the files and folders that make up our project. Furthermore, we learned more about Git and Github and how they can be used together to manage a project in different scenarios."
  },
  {
    "objectID": "recipes/recipe-04/index.html#check-your-understanding",
    "href": "recipes/recipe-04/index.html#check-your-understanding",
    "title": "04. Understanding the computing environment",
    "section": "Check your understanding",
    "text": "Check your understanding\nSelect the component of the computing environment that of the following are related to:\n\nWindows 10 hardwareoperating systemsoftware\nR version 4.3.1 hardwareoperating systemsoftware\ndplyr_1.1.4 hardwareoperating systemsoftware\n\nSelect the Git name for the following actions:\n\nCopy and download a remote repository to your computing environment cloneforkcommitpush\nLog edits to the git tracking system cloneforkcommitpush\nSync changes to the remote repository cloneforkcommitpush"
  },
  {
    "objectID": "recipes/recipe-04/index.html#lab-preparation",
    "href": "recipes/recipe-04/index.html#lab-preparation",
    "title": "04. Understanding the computing environment",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn lab 4, we will apply what we have learned in this recipe to scaffold our own research project by forking and cloning a research project template repository on Github. We will then edit the project files and folders, commit the changes to the git tracking system, and push the changes to our remote repository on Github.\nBefore beginning Lab 4, make sure that you are comfortable with the following:\n\nCloning a remote repository to your computing environment\nCreating and editing files and folders, in particular Quarto documents.\n\nThe additional knowledge and skills that you will need to complete the lab are covered in this recipe which include:\n\nUnderstanding the components of a reproducible project\nUnderstanding the importance of project structure for reproducible project management\nUsing Git, GitHub, and RStudio to manage a project using:\n\nForking, cloning, editing, commiting, and pushing a repository"
  },
  {
    "objectID": "recipes/recipe-04/index.html#references",
    "href": "recipes/recipe-04/index.html#references",
    "title": "04. Understanding the computing environment",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "recipes/recipe-11/index.html",
    "href": "recipes/recipe-11/index.html",
    "title": "11. Sharing research",
    "section": "",
    "text": "R (or Python) research projects that take advantage of Quarto websites have access to a wide range of tools for sharing research. First, the entire research tool chain can be published as a website, which is a great way to share the research process. Second, the website can be used to share the research findings in particular formats including articles and presentations. Let’s focus in on these later formats and discuss strategies for setting up and formatting research articles and presentations.\nWe will assume the project directory structure in Snippet 1.\n\n\n\nSnippet 1: Quarto website structure for reproducible research\n\n\n1project/\n2  ├── data/\n  │   ├── analysis/\n  │   ├── derived/\n  │   └── original/\n3  ├── process/\n  │   ├── 1_acquire.qmd\n  │   ├── 2_curate.qmd\n  │   ├── 3_transform.qmd\n  │   └── 4_analyze.qmd\n4  ├── reports/\n5  │   ├── figures/\n6  │   ├── slides/\n7  │   │   ├── workshop/\n8  │   │   │    └── index.qmd\n  │   │   └── conference/\n9  │   ├── tables/\n10  │   ├── article.qmd\n11  │   ├── citation-style.csl\n12  │   ├── presentations.qmd\n13  │   └── bibliography.bib\n  ├── renv/\n  │   └── ...\n14  ├── _quarto.yml\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n\n1\n\nProject root: The root directory for the project.\n\n2\n\nData directory: The directory for storing data files.\n\n3\n\nProcess directory: The directory for storing process files.\n\n4\n\nReports directory: The directory for storing reports.\n\n5\n\nFigures directory: The directory for storing figures.\n\n6\n\nSlides directory: The directory for storing presentations.\n\n7\n\nWorkshop directory: The directory for storing a “workshop” presentation.\n\n8\n\nWorkshop presentation file: The file for the “workshop” presentation.\n\n9\n\nTables directory: The directory for storing tables.\n\n10\n\nArticle file: The file for the article.\n\n11\n\nCitation style file: The file for the citation style.\n\n12\n\nPresentations listing file: The file for the presentations listing.\n\n13\n\nReferences file: The file for the references.\n\n14\n\nQuarto configuration file: The file for the Quarto configuration.\n\n\n\n\n\nI will also assume the following Quarto configuration file _quarto.yml in Snippet 2.\n\n\n\nSnippet 2: Quarto configuration file for reproducible research\n\n\nproject:\n  title: \"Web project\"\n  type: website\n1  execute-dir: project\n2  render:\n    - index.qmd\n    - process/1_acquire.qmd\n    - process/2_curate.qmd\n    - process/3_transform.qmd\n    - process/4_analyze.qmd\n    - reports/\n\nwebsite:\n  sidebar:\n3    style: \"docked\"\n4    contents:\n      - index.qmd\n      - section: \"Process\"\n5        contents: process/*\n      - section: \"Reports\"\n6        contents: reports/*\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nexecute:\n7  freeze: auto\n\n1\n\nExecution directory: The root directory for all execution.\n\n2\n\nRender order: The order in which files are rendered.\n\n3\n\nSidebar style: The style of the sidebar.\n\n4\n\nSidebar contents: The contents of the sidebar.\n\n5\n\nProcess contents: The contents of the process section.\n\n6\n\nReports contents: The contents of the reports section.\n\n7\n\nFreeze option: The option for rendering only changed files.\n\n\n\n\n\nLooking more closely at the directory structure in Snippet 1, let’s focus on the aspects that are shared between articles and presentations. You will notice that the reports/ directory contains a figures/ directory for saving figures, a tables/ directory for saving tables, and a references.bib file for saving references. These are shared resources that can be used in both articles and presentations. In the process directory, you can save tables, figures, and other resources that are generated during the research process that you believe will be useful in commmunicating the research findings. Then, when you create your presentations, you can include the same materials in either format with the same files. If changes are made to the figures or tables, they will be updated in both the article and the presentation(s).\nNext, it is worth pointing out some important features that appear in the Snippet 2 configuration file. The execute-dir specifies the root directory for all execution. That is the path to directories and files will be the same no matter from what file the code is executed. The render option specifies the order in which the files are rendered. This is important for ensuring that the research process is executed in the correct order. The files that are executed and rendered for display appear in the website and the style and contents options specify the style and contents of the sidebar, respectively. Another key option is the freeze option under execute. This option specifies that only changed files will be rendered. This helps avoid re-rendering files that have not changed, which can be time-consuming and computationally expensive.\n\n\n\nIn the reports/ directory a file named article.qmd appears. This file, which can be named anything, will be the document in which we will draft the research article. This file is a standard Quarto document. However, we can take advantage of some options that we have not seen so far that adds functionality to the document.\nIn Snippet 3, we see an example of the YAML frontmatter for a Quarto article.\n\n\n\nSnippet 3: Quarto article YAML frontmatter\n\n\ntitle: \"Article\"\ndate: 2024-02-20\n1author:\n  - name: \"Your name\"\n    email: youremail@school.edu\n    affiliation: \"Your affiliation\"\n2abstract: |\n  This is a sample article. It is a work in progress and will be updated as the research progresses.\n3keywords:\n  - article\n  - example\n4csl: citation-style.csl\n5bibliography: ../bibliography.bib\n6citation: true\n7format:\n8  html: default\n9  pdf:\n    number-sections: true\n\n1\n\nAuthor information: The author information for the article.\n\n2\n\nAbstract: The abstract for the article.\n\n3\n\nKeywords: The keywords for the article.\n\n4\n\nCitation style: The citation style for the article.\n\n5\n\nBibliography: The bibliography for the article.\n\n6\n\nCitation: The citation for the article itself.\n\n7\n\nFormat: The format for the article.\n\n8\n\nHTML format: The HTML format for the article (for web presentation)\n\n9\n\nPDF format: The PDF format for the article (for printing)\n\n\n\n\n\nIn addition to typical YAML frontmatter, we see a number of new times. Looking at the first three, we see that we can add author information, an abstract, and keywords. These are standard for articles and are used to provide information about the article to readers.\nWhen rendered, the article header information will now contain this new information, as seen in Figure 1.\n\n\n\n\n\n\nFigure 1: Appearance of the header format for a Quarto article.\n\n\n\nThe next two items are the citation style and bibliography. These are used to create and format citations in the article. The citation style is a CSL file that specifies the citation style. You can find a database of various citation styles at the Zotero Style Repository. You can search for a style or by field. Once you find a style you like, you can download the CSL file and add it to your project. The bibliography is a BibTeX file that contains the references for the article. You can create this file (as mentioned before) in a reference manager like Zotero or Mendeley.\nNow the citation option is not for references that we have gathered. Rather, it is for generating a citation for the current article. This is useful if someone else would like to cite your article. When the article is rendered, the citation will appear at the bottom of the article, as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Appearance of the citation attribution in a Quarto article.\n\n\n\nThere are two other features to mention. One is the format option. Since the article is a Quarto document, it can be rendered in multiple formats. The html option ensures that our article is rendered in HTML format as part of the website. However, in addition, we can add a pdf option that will render the article in PDF format. Note that in Figure 1, the pdf option has created an “Other formats” listing on the right side below the table of contents. Clicking this will open the PDF version of the article.\nAlthough not employed in this example, it is also possible to use more extensive format changes with Quarto extensions. Currently, there are various extensions for different journals and publishing houses. For more information and examples, consult the documentation above.\n\n\n\nIn the reports/ directory we can also include presentations and associated slide decks. A popular web-based presentation framework is reveal.js. This framework is used in Quarto to create presentations. In Snippet 1, the slides/ directory contains a directory for each presentation and an index.qmd file within. The index.qmd file contains the presentation content, which we will see soon. To provide a listings page each presentation, the presentations.qmd file contains special YAML instructions to be a listings page.\nLet’s first dive into the index.qmd file for a presentation and discuss some of the key features. In Snippet 4, we see a basic example of a Quarto presentation.\n\n\n\nSnippet 4: Quarto presentation YAML frontmatter index.qmd\n\n\ntitle: \"Examle presentation\"\ndate: 2024-02-20\nauthor: \"Jerid Francom\"\nformat: revealjs\n\n\n\nThe YAML frontmatter for a Quarto presentation is similar to that of most Quarto documents. The title, date, and author are all included. The format option specifies that the presentation will be rendered in reveal.js format. When rendered, the presentation the slide deck will be interactive and can be navigated by the user. The slide deck will also be responsive and can be viewed on any device.\nIn Figure 3, we see an example of a Quarto presentation rendered in reveal.js format. I will discuss some of the key features of the presentation, in the presentation itself.\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#concepts-and-strategies",
    "href": "recipes/recipe-11/index.html#concepts-and-strategies",
    "title": "11. Sharing research",
    "section": "",
    "text": "R (or Python) research projects that take advantage of Quarto websites have access to a wide range of tools for sharing research. First, the entire research tool chain can be published as a website, which is a great way to share the research process. Second, the website can be used to share the research findings in particular formats including articles and presentations. Let’s focus in on these later formats and discuss strategies for setting up and formatting research articles and presentations.\nWe will assume the project directory structure in Snippet 1.\n\n\n\nSnippet 1: Quarto website structure for reproducible research\n\n\n1project/\n2  ├── data/\n  │   ├── analysis/\n  │   ├── derived/\n  │   └── original/\n3  ├── process/\n  │   ├── 1_acquire.qmd\n  │   ├── 2_curate.qmd\n  │   ├── 3_transform.qmd\n  │   └── 4_analyze.qmd\n4  ├── reports/\n5  │   ├── figures/\n6  │   ├── slides/\n7  │   │   ├── workshop/\n8  │   │   │    └── index.qmd\n  │   │   └── conference/\n9  │   ├── tables/\n10  │   ├── article.qmd\n11  │   ├── citation-style.csl\n12  │   ├── presentations.qmd\n13  │   └── bibliography.bib\n  ├── renv/\n  │   └── ...\n14  ├── _quarto.yml\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n\n1\n\nProject root: The root directory for the project.\n\n2\n\nData directory: The directory for storing data files.\n\n3\n\nProcess directory: The directory for storing process files.\n\n4\n\nReports directory: The directory for storing reports.\n\n5\n\nFigures directory: The directory for storing figures.\n\n6\n\nSlides directory: The directory for storing presentations.\n\n7\n\nWorkshop directory: The directory for storing a “workshop” presentation.\n\n8\n\nWorkshop presentation file: The file for the “workshop” presentation.\n\n9\n\nTables directory: The directory for storing tables.\n\n10\n\nArticle file: The file for the article.\n\n11\n\nCitation style file: The file for the citation style.\n\n12\n\nPresentations listing file: The file for the presentations listing.\n\n13\n\nReferences file: The file for the references.\n\n14\n\nQuarto configuration file: The file for the Quarto configuration.\n\n\n\n\n\nI will also assume the following Quarto configuration file _quarto.yml in Snippet 2.\n\n\n\nSnippet 2: Quarto configuration file for reproducible research\n\n\nproject:\n  title: \"Web project\"\n  type: website\n1  execute-dir: project\n2  render:\n    - index.qmd\n    - process/1_acquire.qmd\n    - process/2_curate.qmd\n    - process/3_transform.qmd\n    - process/4_analyze.qmd\n    - reports/\n\nwebsite:\n  sidebar:\n3    style: \"docked\"\n4    contents:\n      - index.qmd\n      - section: \"Process\"\n5        contents: process/*\n      - section: \"Reports\"\n6        contents: reports/*\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nexecute:\n7  freeze: auto\n\n1\n\nExecution directory: The root directory for all execution.\n\n2\n\nRender order: The order in which files are rendered.\n\n3\n\nSidebar style: The style of the sidebar.\n\n4\n\nSidebar contents: The contents of the sidebar.\n\n5\n\nProcess contents: The contents of the process section.\n\n6\n\nReports contents: The contents of the reports section.\n\n7\n\nFreeze option: The option for rendering only changed files.\n\n\n\n\n\nLooking more closely at the directory structure in Snippet 1, let’s focus on the aspects that are shared between articles and presentations. You will notice that the reports/ directory contains a figures/ directory for saving figures, a tables/ directory for saving tables, and a references.bib file for saving references. These are shared resources that can be used in both articles and presentations. In the process directory, you can save tables, figures, and other resources that are generated during the research process that you believe will be useful in commmunicating the research findings. Then, when you create your presentations, you can include the same materials in either format with the same files. If changes are made to the figures or tables, they will be updated in both the article and the presentation(s).\nNext, it is worth pointing out some important features that appear in the Snippet 2 configuration file. The execute-dir specifies the root directory for all execution. That is the path to directories and files will be the same no matter from what file the code is executed. The render option specifies the order in which the files are rendered. This is important for ensuring that the research process is executed in the correct order. The files that are executed and rendered for display appear in the website and the style and contents options specify the style and contents of the sidebar, respectively. Another key option is the freeze option under execute. This option specifies that only changed files will be rendered. This helps avoid re-rendering files that have not changed, which can be time-consuming and computationally expensive.\n\n\n\nIn the reports/ directory a file named article.qmd appears. This file, which can be named anything, will be the document in which we will draft the research article. This file is a standard Quarto document. However, we can take advantage of some options that we have not seen so far that adds functionality to the document.\nIn Snippet 3, we see an example of the YAML frontmatter for a Quarto article.\n\n\n\nSnippet 3: Quarto article YAML frontmatter\n\n\ntitle: \"Article\"\ndate: 2024-02-20\n1author:\n  - name: \"Your name\"\n    email: youremail@school.edu\n    affiliation: \"Your affiliation\"\n2abstract: |\n  This is a sample article. It is a work in progress and will be updated as the research progresses.\n3keywords:\n  - article\n  - example\n4csl: citation-style.csl\n5bibliography: ../bibliography.bib\n6citation: true\n7format:\n8  html: default\n9  pdf:\n    number-sections: true\n\n1\n\nAuthor information: The author information for the article.\n\n2\n\nAbstract: The abstract for the article.\n\n3\n\nKeywords: The keywords for the article.\n\n4\n\nCitation style: The citation style for the article.\n\n5\n\nBibliography: The bibliography for the article.\n\n6\n\nCitation: The citation for the article itself.\n\n7\n\nFormat: The format for the article.\n\n8\n\nHTML format: The HTML format for the article (for web presentation)\n\n9\n\nPDF format: The PDF format for the article (for printing)\n\n\n\n\n\nIn addition to typical YAML frontmatter, we see a number of new times. Looking at the first three, we see that we can add author information, an abstract, and keywords. These are standard for articles and are used to provide information about the article to readers.\nWhen rendered, the article header information will now contain this new information, as seen in Figure 1.\n\n\n\n\n\n\nFigure 1: Appearance of the header format for a Quarto article.\n\n\n\nThe next two items are the citation style and bibliography. These are used to create and format citations in the article. The citation style is a CSL file that specifies the citation style. You can find a database of various citation styles at the Zotero Style Repository. You can search for a style or by field. Once you find a style you like, you can download the CSL file and add it to your project. The bibliography is a BibTeX file that contains the references for the article. You can create this file (as mentioned before) in a reference manager like Zotero or Mendeley.\nNow the citation option is not for references that we have gathered. Rather, it is for generating a citation for the current article. This is useful if someone else would like to cite your article. When the article is rendered, the citation will appear at the bottom of the article, as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Appearance of the citation attribution in a Quarto article.\n\n\n\nThere are two other features to mention. One is the format option. Since the article is a Quarto document, it can be rendered in multiple formats. The html option ensures that our article is rendered in HTML format as part of the website. However, in addition, we can add a pdf option that will render the article in PDF format. Note that in Figure 1, the pdf option has created an “Other formats” listing on the right side below the table of contents. Clicking this will open the PDF version of the article.\nAlthough not employed in this example, it is also possible to use more extensive format changes with Quarto extensions. Currently, there are various extensions for different journals and publishing houses. For more information and examples, consult the documentation above.\n\n\n\nIn the reports/ directory we can also include presentations and associated slide decks. A popular web-based presentation framework is reveal.js. This framework is used in Quarto to create presentations. In Snippet 1, the slides/ directory contains a directory for each presentation and an index.qmd file within. The index.qmd file contains the presentation content, which we will see soon. To provide a listings page each presentation, the presentations.qmd file contains special YAML instructions to be a listings page.\nLet’s first dive into the index.qmd file for a presentation and discuss some of the key features. In Snippet 4, we see a basic example of a Quarto presentation.\n\n\n\nSnippet 4: Quarto presentation YAML frontmatter index.qmd\n\n\ntitle: \"Examle presentation\"\ndate: 2024-02-20\nauthor: \"Jerid Francom\"\nformat: revealjs\n\n\n\nThe YAML frontmatter for a Quarto presentation is similar to that of most Quarto documents. The title, date, and author are all included. The format option specifies that the presentation will be rendered in reveal.js format. When rendered, the presentation the slide deck will be interactive and can be navigated by the user. The slide deck will also be responsive and can be viewed on any device.\nIn Figure 3, we see an example of a Quarto presentation rendered in reveal.js format. I will discuss some of the key features of the presentation, in the presentation itself.\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#check-your-understanding",
    "href": "recipes/recipe-11/index.html#check-your-understanding",
    "title": "11. Sharing research",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\nTRUEFALSE Using Quarto websites for sharing research findings is the only way to meet the requirements of a reproducible research project.\nUsing Snippet 2, what option specifies the order in which files are rendered? execute-dirrendersidebarcontentsfreeze\nWhat is the name of the framework that Quarto uses to create presentations? \nTBD\nTBD\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#lab-preparation",
    "href": "recipes/recipe-11/index.html#lab-preparation",
    "title": "11. Sharing research",
    "section": "Lab preparation",
    "text": "Lab preparation\nTBD"
  },
  {
    "objectID": "guides/guide-05/index.html",
    "href": "guides/guide-05/index.html",
    "title": "Guide 05",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "guides/guide-03/index.html",
    "href": "guides/guide-03/index.html",
    "title": "Guide 03",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "1. Setting up an R environment\n\n\n\n\n\nIn this guide, we will explore options for setting up an R environment. We will discuss local, remote, and virtual environments. Each have their own advantages and shortcomings. The best option for you will depend on your needs and preferences. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide 02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide 03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide 04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide 05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]