[
  {
    "objectID": "guides/guide-06/index.html",
    "href": "guides/guide-06/index.html",
    "title": "06. Identifying data and data sources",
    "section": "",
    "text": "Hello."
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "01. Setting up an R environment\n\n\n\n\n\nIn this guide, we will explore options for setting up an R environment. We will discuss local, remote, and virtual environments. Each have their own advantages and shortcomings. The best option for you will depend on your needs and preferences. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n02. Installing and managing R packages\n\n\n\n\n\nIn this guide, we will cover how to install and manage R packages. We will discuss two primary methods for installing packages: using the RStudio IDE interface and using the R console. We will also cover how to attach and detach packages in an R session, and how to manage packages by listing, updating, and removing them. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03. Working with the interactive R programming lessons\n\n\n\n\n\nIn this guide, we provide an overview of the interactive R programming lessons, explain how to access the lessons, use the lessons, and remove the lessons. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n04. Setting up Git and GitHub\n\n\n\n\n\nIn this guide, we will cover the basics of setting up Git and GitHub. We will also cover the basics of using Git and GitHub to manage a project. This guide is intended for beginners who are new to Git and GitHub. It is also intended for those who are new to using Git and GitHub with R. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n05. Creating reproducible examples\n\n\n\n\n\nIn this guide, we will explore how to create reproducible examples using {reprex}. Reproducible examples are essential for effective communication and collaboration among data scientists and statisticians. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n06. Identifying data and data sources\n\n\n\n\n\nIn this guide, we will explore how to create reproducible examples using the {reprex}. Reproducible examples are essential for effective communication and collaboration among data scientists and statisticians. \n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/guide-03/index.html",
    "href": "guides/guide-03/index.html",
    "title": "03. Working with the interactive R programming lessons",
    "section": "",
    "text": "Outcomes\n\n\nRecall the purpose of the interactive R programming lessons.\nSet up the interactive R programming lessons.\nUnderstand how to use the interactive R programming lessons."
  },
  {
    "objectID": "guides/guide-03/index.html#purpose",
    "href": "guides/guide-03/index.html#purpose",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Purpose",
    "text": "Purpose\nThe interactive R programming lessons are designed to help you learn R programming by doing. The lessons are interactive, meaning you can complete them directly in R and feedback is provided to ensure that you understand the concepts covered and can complete the lessons. The lessons cover a range of topics. In the early chapters, the lessons cover R fundamentals, such as data types, data structures, and functions. In the later chapters, the lessons prepare you for working with the programming concepts connected to the textbook, recipes, and labs.\nYou can preview the topics covered in the lessons by visiting the Lessons repository on GitHub. To access and to complete the lessons read below."
  },
  {
    "objectID": "guides/guide-03/index.html#access-the-lessons",
    "href": "guides/guide-03/index.html#access-the-lessons",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Access the lessons",
    "text": "Access the lessons\nTo access the interactive R programming lessons, you need to install the {swirl} package and download the lessons.\ninstall.packages(\"swirl\")\nlibrary(swirl)\ninstall_course_github(\"qtalr/Lessons\")\nThis code only needs to be run once. After you have installed the {swirl} package and downloaded the lessons, you can use the lessons whenever you want."
  },
  {
    "objectID": "guides/guide-03/index.html#use-the-lessons",
    "href": "guides/guide-03/index.html#use-the-lessons",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Use the lessons",
    "text": "Use the lessons\nTo use the interactive R programming lessons, you need to load the {swirl} package and run the swirl() function.\nlibrary(swirl)\nswirl()\n\n\n\n\n\n\nFigure 1: Run swirl(), select a course and a lesson to start\n\n\n\nWhen you run the swirl() function, you will be prompted to select a course. Select the course you want to complete and follow the prompts to complete the lessons.\nEach lesson will include a series of questions and exercises that you need to complete. You will be typing code directly into the R console to complete the exercises which will help you get accustomed to working with R in the R console.\n\n\n\n\n\n\n Tip\nQuick tip for working in the R console, you can use the up and down arrow keys on your keyboard to navigate through your command history. This can be helpful if you want to reuse a command that you previously ran."
  },
  {
    "objectID": "guides/guide-03/index.html#uninstall-the-lessons-optional",
    "href": "guides/guide-03/index.html#uninstall-the-lessons-optional",
    "title": "03. Working with the interactive R programming lessons",
    "section": "Uninstall the lessons (optional)",
    "text": "Uninstall the lessons (optional)\nIf you want to uninstall the interactive R programming lessons, you can run the following code.\nlibrary(swirl)\nuninstall_course(\"Lessons\")\nThis code will remove the current lessons from your computer. If you want to reinstall the lessons, you can run the code above again."
  },
  {
    "objectID": "guides/guide-05/index.html",
    "href": "guides/guide-05/index.html",
    "title": "05. Creating reproducible examples",
    "section": "",
    "text": "Outcomes"
  },
  {
    "objectID": "guides/guide-05/index.html#introduction",
    "href": "guides/guide-05/index.html#introduction",
    "title": "05. Creating reproducible examples",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is a reproducible example?\nReproducible examples are crucial for effectively communicating problems, solutions, and ideas in the world of data science. In this post, we will discuss the importance of reproducible examples and demonstrate how to create them using {reprex} (Bryan et al. 2024) in R\nA reproducible example, often referred to as a “reprex,” is a minimal, self-contained piece of code that demonstrates a specific issue or concept. It should include:\n\nA brief description of the problem or question\nThe necessary data to reproduce the issue\nThe R code used to generate the output\nThe actual output, including any error messages or warnings\n\n\n\nWhy use {reprex}?\n{reprex} in R streamlines the process of creating reproducible examples by:\n\nAutomatically capturing code, input data, and output\nFormatting the example for easy sharing on various platforms (e.g., GitHub, Stack Overflow)\nEncouraging best practices for creating clear and concise examples\n\n\n\nInstalling and loading {reprex}\nTo get started with {reprex}, first install it from CRAN and load it into your R session:\ninstall.packages(\"reprex\")\nlibrary(reprex)"
  },
  {
    "objectID": "guides/guide-05/index.html#creating-a-reproducible-example-with-reprex",
    "href": "guides/guide-05/index.html#creating-a-reproducible-example-with-reprex",
    "title": "05. Creating reproducible examples",
    "section": "Creating a reproducible example with {reprex}",
    "text": "Creating a reproducible example with {reprex}\nIn this section, we will demonstrate how to create a reproducible example using {reprex}.\n\nBasic usage\nTo create a simple reprex, write your R code and then call the reprex() function:\nlibrary(reprex)\n\ncode &lt;- '\nx &lt;- 1:10\nmean(x)\n'\n\nreprex(input = code)\nThis will generate a formatted output that includes the code, input data, and results.\n\n\nCustomizing output format\nYou can customize the output format of your reprex by specifying the venue argument. For example, to create a reprex suitable for GitHub, use:\nreprex(input = code, venue = \"gh\")\n\n\nIncluding data/ datasets\nWhen your example requires specific data, you can include it using the dput() function:\ndata &lt;- data.frame(x = 1:10, y = 11:20)\ndata_dput &lt;- dput(data)\n\ncode_with_data &lt;- '\ndata &lt;- {{ data_dput }}\nplot(data$x, data$y)\n'\n\nreprex(input = code_with_data)\nThis will incorporate the data into your reprex, allowing others to reproduce your example easily.\n\n\nSharing your reproducible example\nOnce you have created your reprex, you can share it on various platforms such as GitHub, Stack Overflow, or via email. The formatted output generated by {reprex} ensures that your example is easy to read and understand."
  },
  {
    "objectID": "guides/guide-05/index.html#conclusion",
    "href": "guides/guide-05/index.html#conclusion",
    "title": "05. Creating reproducible examples",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have discussed the importance of reproducible examples and demonstrated how to create them using {reprex} in R. By creating clear and concise reprexes, you can effectively communicate problems, solutions, and ideas with your peers and collaborators. Give {reprex} a try and see how it can improve your workflow!"
  },
  {
    "objectID": "guides/guide-05/index.html#references",
    "href": "guides/guide-05/index.html#references",
    "title": "05. Creating reproducible examples",
    "section": "References",
    "text": "References\n\n\nStackOverflow: R, Git, RStudio, GitHub\nReddit: R, Git, RStudio, Github\nRStudio Community\nhttps://reprex.tidyverse.org/\nhttps://github.com/MilesMcBain/datapasta\n\n\n\ninstall.packages(\"reprex\") # install reprex package\n\n\n# Create a vector with 100 random values from the normal distribution\nset.seed(123) # set seed for reproducibility\nmy_vec &lt;- rnorm(100) # random normal vector\n\nsummary(my_vec) # 5 number summary\n\n\nreprex::reprex() # run reprex\n\nDatapasta is a package that allows you to copy and paste data frames from RStudio into a reprex. This is a very useful tool for creating reproducible examples. Here is an example of how to use datapasta to create a reprex.\n\n# install the datapasta package\ninstall.packages(\"datapasta\")\n\n\n# load the datapasta package\nlibrary(datapasta)\n\n\n# Create a data frame with 100 random values from the normal distribution\nset.seed(123) # set seed for reproducibility\nmy_df &lt;- data.frame(x = rnorm(100)) # random normal data frame\n\n\n# copy the data frame to the clipboard\ndpasta(my_df)"
  },
  {
    "objectID": "recipes/recipe-11/index.html",
    "href": "recipes/recipe-11/index.html",
    "title": "11. Sharing research",
    "section": "",
    "text": "R (or Python) research projects that take advantage of Quarto websites have access to a wide range of tools for sharing research. First, the entire research tool chain can be published as a website, which is a great way to share the research process. Second, the website can be used to share the research findings in particular formats including articles and presentations. Let’s focus in on these later formats and discuss strategies for setting up and formatting research articles and presentations.\nWe will assume the project directory structure in Snippet 1.\n\n\n\nSnippet 1: Quarto website structure for reproducible research\n\n\n1project/\n2  ├── data/\n  │   ├── analysis/\n  │   ├── derived/\n  │   └── original/\n3  ├── process/\n  │   ├── 1_acquire.qmd\n  │   ├── 2_curate.qmd\n  │   ├── 3_transform.qmd\n  │   └── 4_analyze.qmd\n4  ├── reports/\n5  │   ├── figures/\n6  │   ├── slides/\n7  │   │   ├── workshop/\n8  │   │   │    └── index.qmd\n  │   │   └── conference/\n9  │   ├── tables/\n10  │   ├── article.qmd\n11  │   ├── citation-style.csl\n12  │   ├── presentations.qmd\n13  │   └── bibliography.bib\n  ├── renv/\n  │   └── ...\n14  ├── _quarto.yml\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n\n1\n\nProject root: The root directory for the project.\n\n2\n\nData directory: The directory for storing data files.\n\n3\n\nProcess directory: The directory for storing process files.\n\n4\n\nReports directory: The directory for storing reports.\n\n5\n\nFigures directory: The directory for storing figures.\n\n6\n\nSlides directory: The directory for storing presentations.\n\n7\n\nWorkshop directory: The directory for storing a “workshop” presentation.\n\n8\n\nWorkshop presentation file: The file for the “workshop” presentation.\n\n9\n\nTables directory: The directory for storing tables.\n\n10\n\nArticle file: The file for the article.\n\n11\n\nCitation style file: The file for the citation style.\n\n12\n\nPresentations listing file: The file for the presentations listing.\n\n13\n\nReferences file: The file for the references.\n\n14\n\nQuarto configuration file: The file for the Quarto configuration.\n\n\n\n\n\nI will also assume the following Quarto configuration file _quarto.yml in Snippet 2.\n\n\n\nSnippet 2: Quarto configuration file for reproducible research\n\n\nproject:\n  title: \"Web project\"\n  type: website\n1  execute-dir: project\n2  render:\n    - index.qmd\n    - process/1_acquire.qmd\n    - process/2_curate.qmd\n    - process/3_transform.qmd\n    - process/4_analyze.qmd\n    - reports/\n\nwebsite:\n  sidebar:\n3    style: \"docked\"\n4    contents:\n      - index.qmd\n      - section: \"Process\"\n5        contents: process/*\n      - section: \"Reports\"\n6        contents: reports/*\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nexecute:\n7  freeze: auto\n\n1\n\nExecution directory: The root directory for all execution.\n\n2\n\nRender order: The order in which files are rendered.\n\n3\n\nSidebar style: The style of the sidebar.\n\n4\n\nSidebar contents: The contents of the sidebar.\n\n5\n\nProcess contents: The contents of the process section.\n\n6\n\nReports contents: The contents of the reports section.\n\n7\n\nFreeze option: The option for rendering only changed files.\n\n\n\n\n\nLooking more closely at the directory structure in Snippet 1, let’s focus on the aspects that are shared between articles and presentations. You will notice that the reports/ directory contains a figures/ directory for saving figures, a tables/ directory for saving tables, and a references.bib file for saving references. These are shared resources that can be used in both articles and presentations. In the process directory, you can save tables, figures, and other resources that are generated during the research process that you believe will be useful in commmunicating the research findings. Then, when you create your presentations, you can include the same materials in either format with the same files. If changes are made to the figures or tables, they will be updated in both the article and the presentation(s).\nNext, it is worth pointing out some important features that appear in the Snippet 2 configuration file. The execute-dir specifies the root directory for all execution. That is the path to directories and files will be the same no matter from what file the code is executed. The render option specifies the order in which the files are rendered. This is important for ensuring that the research process is executed in the correct order. The files that are executed and rendered for display appear in the website and the style and contents options specify the style and contents of the sidebar, respectively. Another key option is the freeze option under execute. This option specifies that only changed files will be rendered. This helps avoid re-rendering files that have not changed, which can be time-consuming and computationally expensive.\n\n\n\nIn the reports/ directory a file named article.qmd appears. This file, which can be named anything, will be the document in which we will draft the research article. This file is a standard Quarto document. However, we can take advantage of some options that we have not seen so far that adds functionality to the document.\nIn Snippet 3, we see an example of the YAML frontmatter for a Quarto article.\n\n\n\nSnippet 3: Quarto article YAML frontmatter\n\n\ntitle: \"Article\"\ndate: 2024-02-20\n1author:\n  - name: \"Your name\"\n    email: youremail@school.edu\n    affiliation: \"Your affiliation\"\n2abstract: |\n  This is a sample article. It is a work in progress and will be updated as the research progresses.\n3keywords:\n  - article\n  - example\n4csl: citation-style.csl\n5bibliography: ../bibliography.bib\n6citation: true\n7format:\n8  html: default\n9  pdf:\n    number-sections: true\n\n1\n\nAuthor information: The author information for the article.\n\n2\n\nAbstract: The abstract for the article.\n\n3\n\nKeywords: The keywords for the article.\n\n4\n\nCitation style: The citation style for the article.\n\n5\n\nBibliography: The bibliography for the article.\n\n6\n\nCitation: The citation for the article itself.\n\n7\n\nFormat: The format for the article.\n\n8\n\nHTML format: The HTML format for the article (for web presentation)\n\n9\n\nPDF format: The PDF format for the article (for printing)\n\n\n\n\n\nIn addition to typical YAML frontmatter, we see a number of new times. Looking at the first three, we see that we can add author information, an abstract, and keywords. These are standard for articles and are used to provide information about the article to readers.\nWhen rendered, the article header information will now contain this new information, as seen in Figure 1.\n\n\n\n\n\n\nFigure 1: Appearance of the header format for a Quarto article.\n\n\n\nThe next two items are the citation style and bibliography. These are used to create and format citations in the article. The citation style is a CSL file that specifies the citation style. You can find a database of various citation styles at the Zotero Style Repository. You can search for a style or by field. Once you find a style you like, you can download the CSL file and add it to your project. The bibliography is a BibTeX file that contains the references for the article. You can create this file (as mentioned before) in a reference manager like Zotero or Mendeley.\nNow the citation option is not for references that we have gathered. Rather, it is for generating a citation for the current article. This is useful if someone else would like to cite your article. When the article is rendered, the citation will appear at the bottom of the article, as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Appearance of the citation attribution in a Quarto article.\n\n\n\nThere are two other features to mention. One is the format option. Since the article is a Quarto document, it can be rendered in multiple formats. The html option ensures that our article is rendered in HTML format as part of the website. However, in addition, we can add a pdf option that will render the article in PDF format. Note that in Figure 1, the pdf option has created an “Other formats” listing on the right side below the table of contents. Clicking this will open the PDF version of the article.\nAlthough not employed in this example, it is also possible to use more extensive format changes with Quarto extensions. Currently, there are various extensions for different journals and publishing houses. For more information and examples, consult the documentation above.\n\n\n\nIn the reports/ directory we can also include presentations and associated slide decks. A popular web-based presentation framework is reveal.js. This framework is used in Quarto to create presentations. In Snippet 1, the slides/ directory contains a directory for each presentation and an index.qmd file within. The index.qmd file contains the presentation content, which we will see soon. To provide a listings page each presentation, the presentations.qmd file contains special YAML instructions to be a listings page.\nLet’s first dive into the index.qmd file for a presentation and discuss some of the key features. In Snippet 4, we see a basic example of a Quarto presentation.\n\n\n\nSnippet 4: Quarto presentation YAML frontmatter index.qmd\n\n\ntitle: \"Examle presentation\"\ndate: 2024-02-20\nauthor: \"Jerid Francom\"\nformat: revealjs\n\n\n\nThe YAML frontmatter for a Quarto presentation is similar to that of most Quarto documents. The title, date, and author are all included. The format option specifies that the presentation will be rendered in reveal.js format. When rendered, the presentation the slide deck will be interactive and can be navigated by the user. The slide deck will also be responsive and can be viewed on any device.\nIn Figure 3, we see an example of a Quarto presentation rendered in reveal.js format. I will discuss some of the key features of the presentation, in the presentation itself.\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#concepts-and-strategies",
    "href": "recipes/recipe-11/index.html#concepts-and-strategies",
    "title": "11. Sharing research",
    "section": "",
    "text": "R (or Python) research projects that take advantage of Quarto websites have access to a wide range of tools for sharing research. First, the entire research tool chain can be published as a website, which is a great way to share the research process. Second, the website can be used to share the research findings in particular formats including articles and presentations. Let’s focus in on these later formats and discuss strategies for setting up and formatting research articles and presentations.\nWe will assume the project directory structure in Snippet 1.\n\n\n\nSnippet 1: Quarto website structure for reproducible research\n\n\n1project/\n2  ├── data/\n  │   ├── analysis/\n  │   ├── derived/\n  │   └── original/\n3  ├── process/\n  │   ├── 1_acquire.qmd\n  │   ├── 2_curate.qmd\n  │   ├── 3_transform.qmd\n  │   └── 4_analyze.qmd\n4  ├── reports/\n5  │   ├── figures/\n6  │   ├── slides/\n7  │   │   ├── workshop/\n8  │   │   │    └── index.qmd\n  │   │   └── conference/\n9  │   ├── tables/\n10  │   ├── article.qmd\n11  │   ├── citation-style.csl\n12  │   ├── presentations.qmd\n13  │   └── bibliography.bib\n  ├── renv/\n  │   └── ...\n14  ├── _quarto.yml\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n\n1\n\nProject root: The root directory for the project.\n\n2\n\nData directory: The directory for storing data files.\n\n3\n\nProcess directory: The directory for storing process files.\n\n4\n\nReports directory: The directory for storing reports.\n\n5\n\nFigures directory: The directory for storing figures.\n\n6\n\nSlides directory: The directory for storing presentations.\n\n7\n\nWorkshop directory: The directory for storing a “workshop” presentation.\n\n8\n\nWorkshop presentation file: The file for the “workshop” presentation.\n\n9\n\nTables directory: The directory for storing tables.\n\n10\n\nArticle file: The file for the article.\n\n11\n\nCitation style file: The file for the citation style.\n\n12\n\nPresentations listing file: The file for the presentations listing.\n\n13\n\nReferences file: The file for the references.\n\n14\n\nQuarto configuration file: The file for the Quarto configuration.\n\n\n\n\n\nI will also assume the following Quarto configuration file _quarto.yml in Snippet 2.\n\n\n\nSnippet 2: Quarto configuration file for reproducible research\n\n\nproject:\n  title: \"Web project\"\n  type: website\n1  execute-dir: project\n2  render:\n    - index.qmd\n    - process/1_acquire.qmd\n    - process/2_curate.qmd\n    - process/3_transform.qmd\n    - process/4_analyze.qmd\n    - reports/\n\nwebsite:\n  sidebar:\n3    style: \"docked\"\n4    contents:\n      - index.qmd\n      - section: \"Process\"\n5        contents: process/*\n      - section: \"Reports\"\n6        contents: reports/*\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nexecute:\n7  freeze: auto\n\n1\n\nExecution directory: The root directory for all execution.\n\n2\n\nRender order: The order in which files are rendered.\n\n3\n\nSidebar style: The style of the sidebar.\n\n4\n\nSidebar contents: The contents of the sidebar.\n\n5\n\nProcess contents: The contents of the process section.\n\n6\n\nReports contents: The contents of the reports section.\n\n7\n\nFreeze option: The option for rendering only changed files.\n\n\n\n\n\nLooking more closely at the directory structure in Snippet 1, let’s focus on the aspects that are shared between articles and presentations. You will notice that the reports/ directory contains a figures/ directory for saving figures, a tables/ directory for saving tables, and a references.bib file for saving references. These are shared resources that can be used in both articles and presentations. In the process directory, you can save tables, figures, and other resources that are generated during the research process that you believe will be useful in commmunicating the research findings. Then, when you create your presentations, you can include the same materials in either format with the same files. If changes are made to the figures or tables, they will be updated in both the article and the presentation(s).\nNext, it is worth pointing out some important features that appear in the Snippet 2 configuration file. The execute-dir specifies the root directory for all execution. That is the path to directories and files will be the same no matter from what file the code is executed. The render option specifies the order in which the files are rendered. This is important for ensuring that the research process is executed in the correct order. The files that are executed and rendered for display appear in the website and the style and contents options specify the style and contents of the sidebar, respectively. Another key option is the freeze option under execute. This option specifies that only changed files will be rendered. This helps avoid re-rendering files that have not changed, which can be time-consuming and computationally expensive.\n\n\n\nIn the reports/ directory a file named article.qmd appears. This file, which can be named anything, will be the document in which we will draft the research article. This file is a standard Quarto document. However, we can take advantage of some options that we have not seen so far that adds functionality to the document.\nIn Snippet 3, we see an example of the YAML frontmatter for a Quarto article.\n\n\n\nSnippet 3: Quarto article YAML frontmatter\n\n\ntitle: \"Article\"\ndate: 2024-02-20\n1author:\n  - name: \"Your name\"\n    email: youremail@school.edu\n    affiliation: \"Your affiliation\"\n2abstract: |\n  This is a sample article. It is a work in progress and will be updated as the research progresses.\n3keywords:\n  - article\n  - example\n4csl: citation-style.csl\n5bibliography: ../bibliography.bib\n6citation: true\n7format:\n8  html: default\n9  pdf:\n    number-sections: true\n\n1\n\nAuthor information: The author information for the article.\n\n2\n\nAbstract: The abstract for the article.\n\n3\n\nKeywords: The keywords for the article.\n\n4\n\nCitation style: The citation style for the article.\n\n5\n\nBibliography: The bibliography for the article.\n\n6\n\nCitation: The citation for the article itself.\n\n7\n\nFormat: The format for the article.\n\n8\n\nHTML format: The HTML format for the article (for web presentation)\n\n9\n\nPDF format: The PDF format for the article (for printing)\n\n\n\n\n\nIn addition to typical YAML frontmatter, we see a number of new times. Looking at the first three, we see that we can add author information, an abstract, and keywords. These are standard for articles and are used to provide information about the article to readers.\nWhen rendered, the article header information will now contain this new information, as seen in Figure 1.\n\n\n\n\n\n\nFigure 1: Appearance of the header format for a Quarto article.\n\n\n\nThe next two items are the citation style and bibliography. These are used to create and format citations in the article. The citation style is a CSL file that specifies the citation style. You can find a database of various citation styles at the Zotero Style Repository. You can search for a style or by field. Once you find a style you like, you can download the CSL file and add it to your project. The bibliography is a BibTeX file that contains the references for the article. You can create this file (as mentioned before) in a reference manager like Zotero or Mendeley.\nNow the citation option is not for references that we have gathered. Rather, it is for generating a citation for the current article. This is useful if someone else would like to cite your article. When the article is rendered, the citation will appear at the bottom of the article, as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Appearance of the citation attribution in a Quarto article.\n\n\n\nThere are two other features to mention. One is the format option. Since the article is a Quarto document, it can be rendered in multiple formats. The html option ensures that our article is rendered in HTML format as part of the website. However, in addition, we can add a pdf option that will render the article in PDF format. Note that in Figure 1, the pdf option has created an “Other formats” listing on the right side below the table of contents. Clicking this will open the PDF version of the article.\nAlthough not employed in this example, it is also possible to use more extensive format changes with Quarto extensions. Currently, there are various extensions for different journals and publishing houses. For more information and examples, consult the documentation above.\n\n\n\nIn the reports/ directory we can also include presentations and associated slide decks. A popular web-based presentation framework is reveal.js. This framework is used in Quarto to create presentations. In Snippet 1, the slides/ directory contains a directory for each presentation and an index.qmd file within. The index.qmd file contains the presentation content, which we will see soon. To provide a listings page each presentation, the presentations.qmd file contains special YAML instructions to be a listings page.\nLet’s first dive into the index.qmd file for a presentation and discuss some of the key features. In Snippet 4, we see a basic example of a Quarto presentation.\n\n\n\nSnippet 4: Quarto presentation YAML frontmatter index.qmd\n\n\ntitle: \"Examle presentation\"\ndate: 2024-02-20\nauthor: \"Jerid Francom\"\nformat: revealjs\n\n\n\nThe YAML frontmatter for a Quarto presentation is similar to that of most Quarto documents. The title, date, and author are all included. The format option specifies that the presentation will be rendered in reveal.js format. When rendered, the presentation the slide deck will be interactive and can be navigated by the user. The slide deck will also be responsive and can be viewed on any device.\nIn Figure 3, we see an example of a Quarto presentation rendered in reveal.js format. I will discuss some of the key features of the presentation, in the presentation itself.\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#check-your-understanding",
    "href": "recipes/recipe-11/index.html#check-your-understanding",
    "title": "11. Sharing research",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\nTRUEFALSE Using Quarto websites for sharing research findings is the only way to meet the requirements of a reproducible research project.\nUsing Snippet 2, what option specifies the order in which files are rendered? execute-dirrendersidebarcontentsfreeze\nWhat is the name of the framework that Quarto uses to create presentations? \nTBD\nTBD\nTBD"
  },
  {
    "objectID": "recipes/recipe-11/index.html#lab-preparation",
    "href": "recipes/recipe-11/index.html#lab-preparation",
    "title": "11. Sharing research",
    "section": "Lab preparation",
    "text": "Lab preparation\nTBD"
  },
  {
    "objectID": "recipes/recipe-04/index.html",
    "href": "recipes/recipe-04/index.html",
    "title": "04. Understanding the computing environment",
    "section": "",
    "text": "Skills\n\nUnderstanding the components of a reproducible project\nConnecting the computing environment to reproducible project management\nUnderstanding the workflow and project structure\nUsing Git and GitHub to manage a project"
  },
  {
    "objectID": "recipes/recipe-04/index.html#concepts-and-strategies",
    "href": "recipes/recipe-04/index.html#concepts-and-strategies",
    "title": "04. Understanding the computing environment",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nProject components\nReproducible projects are composed of two main components: the computing environment and the project structure. The computing environment is the hardware, operating system, and software that we use to do our work and the project structure is the organization of the files and folders that make up our project. As we can see in Figure 1, each of these components and subcomponents are nested within each other.\n\n\n\n\n\n\nFigure 1: Compontents of a reproducible project\n\n\n\nIn the lesson on the computing environment, we learned about the importance of understanding the computing environment and how to find out information about our computing environment inspecting an R session, as we seen in Snippet 1.\n\n\n\nSnippet 1: Session information output\n\n\n─ Session info ──────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Ventura 13.6.1\n system   x86_64, darwin22.6.0\n ui       unknown\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-11-26\n pandoc   3.1.9 @ /usr/local/bin/pandoc\n\n─ Packages ──────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.2)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.2)\n rlang         1.1.2   2023-11-04 [1] CRAN (R 4.3.2)\n\n [1] /Users/francojc/R/Library\n [2] /usr/local/Cellar/r/4.3.2/lib/R/library\n\n\n\nAs we learned, when the computing environment that is used to create a project is different from the computing environment that is used to reproduce the project, there is a risk that the project will not be reproducible. We will not tackle all the components in Figure 1 at once, however, but instead we will focus here on the project structure. Later on, with more experience, we will address the other components.\n\n\nManaging reproducible projects\nThe project structure is the organization of the files and folders that make up our project. A minimal project structure includes separation for input, process, and output of research, documentation about the project, how to reproduce the project, and the project files themselves. The project structure is important because it helps us organize our work and it helps others understand it.\nThere is no concensus about what the best project structure is. However, the principles covered in Chapter 4 can guide us in developing a project structure that both organizes our work and makes that work understandable to others. With these principles met, we can add additional structure to our project to meet our project-specific needs.\nTo create a reproducible project structure, we need no more than to create a directory and set of files that meet the principles for a minimal reproducible framework. During or after the project, would could back up these directories and files and/ or share them with others in a number of ways.\nAlthough this approach is already a good step in the right direction, it is error prone and will likely lead to inconsistencies across projects. A better approach is to develop, or adopt, a project structure template that can be used for all projects, use version control to track changes to your project, and upload your project to a remote repository where it is backed up (including the version history) and can be shared with others efficiently.\nThis later approach is the one that we will use in subsequent lessons, recipes, and labs in this course.\n\n\nLeveraging Git and GitHub\nAt this point, you are somewhat familiar with Git and Github. You have likely used Git to copy and download a repository from GitHub, say for example the labs for this course. However, what is going on behind the scenes is likely still a bit of a mystery. In this section, we will demystify Git and GitHub, a bit, and learn how to use them together to manage a project in different scenarios.\n\n\n\n\n\n\n Tip\nVerify that you have a working version of Git on your computing environment and make sure that you have a GitHub account. You can refer to Guide 2 for more information on how to do this.\n\n\n\nSo let’s rewind a bit and review what Git and Github are and how they work together. Git is a version control system that allows us to track changes to our project files. It is a command line tool, much like R, that is installed as software. Also like R, we can interact with Git through a graphical user interface (GUI), such as RStudio.\nDirectory and file tracking with Git can be added to a project at any time. A tracked project is called a repository, or repo for short. When used on our computing environment, the repo is called a local repository. There are many benefits to using Git to track changes to local repository, including the ability to revert to previous versions of files, create and edit parallel copies of files and then selectively integrate them, and much more.\nBut the real power of Git is realized in combination with GitHub. Github is a cloud-based remote repository that allows us to store our git-tracked projects. It is a web-based platform which requires an account to use. Once you are signed up, you can connect Git and Github to create remote repositories and upload your local repositories to them. You can also download remote repositories to your computing environment. There are many, many features that Github offers, but for now we will focus a few key features that will help us manage our projects.\nIn Figure 2, I provide a schematic look at the relationship between Github and Git for three common scenarios.\n\n\n\n\n\n\nFigure 2: Key features of GitHub\n\n\n\nScenario A: Clone a remote repository\nIn this scenario, we locate a remote repository on Github that someone has made publically available. Then we clone (copy and download) the repository to our computing environment. Once the repository is cloned locally, we can edit the files as we see fit.\nIn essence, we are just downloading a group of files and folders to our computing environment from Github. This is the scenario that we have been using in this course to download the lab repositories.\nSteps:\n\nLocate a remote repository on Github that someone has made publically available and copy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\n\nScenario B: Fork and clone a remote repository\nThis scenario differs from A in two respects. First, we first fork (copy to) the remote repository to our Github account before cloning it to our computing environment. Second, we commit (log edits) changes to the git tracking system and then push (sync to) the changes to our remote repository.\nIn this case, we are doing more than just downloading a group of files and folders. We are setting up a link between the other person’s remote repository and our own remote repository. We do not have to use this link, but if we do want to, we can pull (sync from) any changes that are made to the other person’s remote repository to our remote repository. This can be useful if we want to keep our remote repository up to date with the other person’s remote repository. Furthermore, this link allows us to propose changes to the other person’s remote repository with a pull request –a request for the other person to pull our changes into their remote repository. This can be useful if we want to collaborate with the other person on the project. Pull and pull request are more advanced features that we will not address at this point.\nThe second difference is that we are using Git to track changes to our local repository and then push those changes to our remote repository. This is a key feature because it allows us to keep track of changes to our project files and folders and revert to previous versions if needed. It also allows us to share our project with others and collaborate with them.\nSteps:\n\nLocate a remote repository on Github that someone has made publically available and click the fork button.\nStill on Github, choose new account as the owner of the forked repository.\nIn the forked repository, copy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\nMake changes to the project files and folders.\nCommit the changes to the git tracking system.\nPush the changes to the remote repository.\n\nScenario C: Create/ Join and clone a remote repository\nThis scenario is similar to B, but instead of forking a remote repository, we create a new remote repository on Github and then clone it to our computing environment. We then commit and push changes to our remote repository. In this case, we are creating a new remote repository and then using Git to track changes to our local repository and push those changes to our remote repository.\nThis scenario is common when we work on our own projects or when we want to collaborate with others on a project. In the latter case, we would create a remote repository and then invite others to collaborate on it. Everyone with permissions to the remote repository can clone it to their computing environment, make changes, and then push those changes to the same remote repository. This allows everyone to work on the same project and keep track of changes to the project files and folders.\nWhen working with multiple people on a project, you can imagine that if I’m working on the project locally and you are working on the project locally, we might make changes to the same files and folders. If we both push our changes to the remote repository, there is a risk that the changes will conflict with each other. Git and Github have features that help us manage these conflicts (pull, fetch, merge, etc.), but we will not address them at this point either.\nSteps:\n\nCreate a new remote repository on Github or accept an invitation to collaborate on a remote repository.\nCopy the clone URL.\nOpen RStudio and create a new project from version control.\nPaste the clone URL into the repository URL field.\nChoose a project parent directory (where the project will be saved).\n(optional) Rename the project directory.\nMake changes to the project files and folders.\nCommit the changes to the git tracking system.\nPush the changes to the remote repository."
  },
  {
    "objectID": "recipes/recipe-04/index.html#summary",
    "href": "recipes/recipe-04/index.html#summary",
    "title": "04. Understanding the computing environment",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we reviewed the components of reproducible research projects: the computing environment and the project structure. The computing environment is the hardware, operating system, and software that we use to do our work and the project structure is the organization of the files and folders that make up our project. Furthermore, we learned more about Git and Github and how they can be used together to manage a project in different scenarios."
  },
  {
    "objectID": "recipes/recipe-04/index.html#check-your-understanding",
    "href": "recipes/recipe-04/index.html#check-your-understanding",
    "title": "04. Understanding the computing environment",
    "section": "Check your understanding",
    "text": "Check your understanding\nSelect the component of the computing environment that of the following are related to:\n\nWindows 10 hardwareoperating systemsoftware\nR version 4.3.1 hardwareoperating systemsoftware\ndplyr_1.1.4 hardwareoperating systemsoftware\n\nSelect the Git name for the following actions:\n\nCopy and download a remote repository to your computing environment cloneforkcommitpush\nLog edits to the git tracking system cloneforkcommitpush\nSync changes to the remote repository cloneforkcommitpush"
  },
  {
    "objectID": "recipes/recipe-04/index.html#lab-preparation",
    "href": "recipes/recipe-04/index.html#lab-preparation",
    "title": "04. Understanding the computing environment",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn lab 4, we will apply what we have learned in this recipe to scaffold our own research project by forking and cloning a research project template repository on Github. We will then edit the project files and folders, commit the changes to the git tracking system, and push the changes to our remote repository on Github.\nBefore beginning Lab 4, make sure that you are comfortable with the following:\n\nCloning a remote repository to your computing environment\nCreating and editing files and folders, in particular Quarto documents.\n\nThe additional knowledge and skills that you will need to complete the lab are covered in this recipe which include:\n\nUnderstanding the components of a reproducible project\nUnderstanding the importance of project structure for reproducible project management\nUsing Git, GitHub, and RStudio to manage a project using:\n\nForking, cloning, editing, commiting, and pushing a repository"
  },
  {
    "objectID": "recipes/recipe-04/index.html#references",
    "href": "recipes/recipe-04/index.html#references",
    "title": "04. Understanding the computing environment",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "recipes/recipe-02/index.html",
    "href": "recipes/recipe-02/index.html",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "",
    "text": "Skills\n\nLoading packages into an R session\nReading datasets into R with read_*() functions\nInspecting datasets with {dplyr} functions\nWriting datasets to a file with write_*() functions"
  },
  {
    "objectID": "recipes/recipe-02/index.html#concepts-and-strategies",
    "href": "recipes/recipe-02/index.html#concepts-and-strategies",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nQuarto documents and code blocks\nAsk you will remember from Recipes 0 and 1, Quarto documents can combine prose and code. The prose is written in Markdown and the code is written in R1. The code is contained in code blocks, which are opened by three backticks (`), the name of the programming language, r, in curly braces {r} and three backticks (`) to close the block. For example, the following minimal Quarto document contains an R code block:\n---\ntitle: My Quarto Document\nformat: pdf\n---\n\n# Goals\n\nThis script ...\n\n```{r}\n#| label: code-block-name\n\n# R code goes here\n```\n\nAs you can see in the code block, the ...\nCode blocks have various options that can be added by using key-value pairs that are prefixed with #|. Some common key-value pairs we will use in this Recipe are:\n\nlabel: A unique name for the code block. This is used to reference the code block.\necho: A boolean value (true or false) that determines whether the code is displayed in the output document.\ninclude: A boolean value (true or false) that determines whether the output of the code is displayed in the output document.\nmessage: A boolean value (true or false) that determines whether the messages from the code are displayed in the output document.\n\n\n\nSetting up the environment\nBefore we can read, inspect, and write data, we need to load the packages that contain the functions we will use. We will use {readr} to read datasets into R and write datasets to disk and {dplyr} to inspect and transform (subset) the data.\nThere are a few ways to load packages into an R session. The most common way is to use the library() function. The library() function loads a package into the R session and stops the script if the package is not available on the current computing environment.\nFor example, the following code block loads {readr} and {dplyr} into the R session:\n\n```{r}\n#| label: load-packages\n\n# Load packages\nlibrary(readr) # for reading and writing data\nlibrary(dplyr) # for inspecting and transforming data\n```\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis code block assumes that {readr} and {dplyr} are installed on the current computing environment. If the packages are not installed, the code block will stop and display an error message, such as:\nError in library(readr) : there is no package called ‘readr’\nThis error can be addressed by installing the missing package with install.packages(\"readr\") and then re-running the code block. This is not ideal for reproducibility, however, because the code block will stop if the package is not installed. We will consider a more reproducible approach later in the course.\n\n\n\n\n\n\n Dive deeper\nIf you interested in learning about safeguarding package loading in a reproducible way, see {renv}. {renv} is a project-oriented workflow to create a reproducible environment for R projects. For more information, see the renv documentation and/ or Recipe 11\n\n\n\n\n\nUnderstanding the data\nNow that we have our environment set up, we can read the dataset into R. But before we do, we should make sure that we understand the data by looking at the data documentation.\nThe dataset that we will read into our R session based on the Brown Corpus (Francis and Kuçera 1961). I’ve created a data origin file that contains the data documentation for the Brown Corpus, as we can see in Table 1.\n\n# Read and display the data origin file\n\nread_csv(file = \"data/original/brown_passives_do.csv\") |&gt;\n  kable() |&gt;\n  kable_styling() |&gt;\n  column_spec(1, width = \"15em\")\n\n\n\nTable 1: Data origin file for the Brown Corpus.\n\n\n\n\n\n\n\nattribute\ndescription\n\n\n\n\nResource name\nBrown Corpus\n\n\nData source\nhttp://korpus.uib.no/icame/brown/bcm.html\n\n\nData sampling frame\nEdited American English prose from various genres, published in the United States during the calendar year 1961.\n\n\nData collection date(s)\nOriginally published in 1964, revised in 1971 and 1979.\n\n\nData format\nMultiple formats including Form A (original), Form B (stripped version), Form C (tagged version), Bergen Forms I and II, and Brown MARC Form.\n\n\nData schema\n500 samples of approximately 2000 words each, covering a wide range of genres and styles. Includes coding for major and minor headings, special types (italics, bold, etc.), abbreviations, symbols, and other textual features.\n\n\nLicense\nUse restricted for scholarly research in linguistics, stylistics, and other disciplines. Specific copyright restrictions detailed in the manual.\n\n\nAttribution\nW. Nelson Francis and Henry Kucera, Brown University, 1964, revised 1971 and 1979.\n\n\n\n\n\n\n\n\n\n\n\nThis data origin file provides an overview of the original data source. In this case, the dataset we will read into R is a subset of the Brown Corpus which is an aggregate of the use of passive voice. This dataset was developed by the authors of {corpora} (Evert 2023). I’ve exported the dataset to a CSV file, which we will read into R.\nThe data dictionary which describes the dataset we will read appears in Table 2.\n# Read and display the data documentation file\nread_csv(file = \"../data/derived/brown_passives_curated_dd.csv\") |&gt;\n  kable() |&gt;\n  kable_styling()\n\n\n\n\nTable 2: Data dictionary file for the Brown Corpus.\n\n\n\n\n\n\n\nvariable\nname\nvariable_type\ndescription\n\n\n\n\ncat\nCategory\ncategorical\nGenre categories represented by letters\n\n\npassive\nPassive\nnumeric\nNumber of passive verb phrases in the genre\n\n\nn_w\nNumber of words\nnumeric\nNumber of words in the genre\n\n\nn_s\nNumber of sentences\nnumeric\nNumber of sentences in the genre\n\n\nname\nGenre\ncategorical\nGenre name\n\n\n\n\n\n\n\n\n\n\n\nWith this information, we are now in a position to read and inspect the dataset.\n\n\nReading datasets into R with {readr}\nWe’ve now prepared our Quarto document by loading the packages we will use and and we have reviewed the dataset documentation so that we understand the dataset we will read into R. We are now ready to read the dataset into R.\nR provides a number of functions to read data of many types in R. We will explore many types of data and datasets in this course. For now, we will focus on reading rectangular data into R. Rectangular data is data that is organized in rows and columns, such as a spreadsheet.\nOne of the most common file formats for rectangular data is the comma-separated values (CSV) file. CSV files are text files in which lines represent rows and commas separate columns of data. For example, the sample CSV file snippet below contains three rows and three columns of data:\n\"word\",\"frequency\",\"part_of_speech\"\n\"the\",69971,\"article\"\n\"of\",36412,\"preposition\"\n\"and\",28853,\"conjunction\"\nA CSV file is a type of delimited file, which means that the data is separated by a delimiter. In the case of a CSV file, the delimiter is a comma. Other types of delimited files use different delimiters, such as tab-separated values (TSV) files which use a tab character as the delimiter, or even a pipe (|) or semicolon (;).\n{readr} provides functions to read rectangular dataset into R. The read_csv() function reads CSV files, the read_tsv() function reads TSV files, and the read_delim() function reads other types of delimited files.\nLet’s use the read_csv() function to read the brown_passives_curated.csv file into R. To do this we will use the file = argument to specify the path to the file. Now, the file “path” is the location of the file on the computer. We can specify this path in two ways:\n\nRelative path: The relative path is the path to the file relative to the current working directory. The current working directory is the directory in which the R session is running.\nAbsolute path: The absolute path is the path to the file from the root directory of the computer.\n\nFor most purpose, the relative path is the better option because it is more portable. For example, if you share your code with someone else, they may have a different absolute path to the file. However, they will likely have the same relative path to the file.\nLet’s say that the directory structure of our project is as follows:\nproject/\n├── data/\n│   ├── original/\n│   │   └── brown_passives_do.csv\n│   └── derived/\n│       └── brown_passives_curated.csv\n└── code/\n    └── reading-inspecting-writing.qmd\nIn this case, the relative path from reading-inspecting-writing.qmd to the brown_passives_curated.csv file is ../data/derived/brown_passives_curated.csv. The .. means “go up one directory” and the rest of the path is the path to the file from the project/ directory.\nWith this in mind, we can read the brown_passives_curated.csv file into R with the following code block:\n#| label: read-dataset-brown-passives-curated\n\n# Read the dataset\nbrown_passives_df &lt;-\n  read_csv(file = \"../data/derived/brown_passives_curated.csv\")\nRunning the above code chunk in our Quarto document will read the dataset into R and assign it to the brown_passives_df variable. It will also show the code used to read the dataset into R. Furthermore, so functions will display messages in the output. For example, the read_csv() function will display a message that various parsing options were used to read the dataset into R.\nThis information can be helpful in an interactive session, as read_csv() tells us the dimensions of the dataset and the data types of each column. But this output is not necessary, and is unnecessarily verbose in a reproducible document.\nWe can hide any messages produced by a function by using the message = false key-value pair in the code block. For example, the following code block will read the dataset into R and assign it to the brown_passives_df variable without displaying any messages:\n#| label: read-dataset-brown-passives-curated\n#| message: false\n\n# Read the dataset\nbrown_passives_df &lt;-\n  read_csv(file = \"../data/derived/brown_passives_curated.csv\")\nNo messages are displayed in the document output.\n\n\nInspecting datasets with {dplyr}\nThe objective of this section is to demonstrate how to inspect and transform (subset) datasets using {dplyr}. We will use {dplyr} to inspect the dataset we read into R in the previous section.\nReading a CSV file into R will create a data frame object. Thus, I assigned the result to brown_passives_df. The df suffix is a common naming convention for rectangular data frames. It is good practice to use a consistent naming convention for objects in your code. This makes it easier to understand the code and to avoid errors.\nLet’s do get an overview of the dataset by using the glimpse() function from {dplyr}. The glimpse() function displays the dimensions of the data frame and the data types of each column.\n\n# Preview\nglimpse(brown_passives_df)\n\nRows: 15\nColumns: 5\n$ cat     &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"J\", \"K\", \"L\", \"M\", \"N…\n$ passive &lt;dbl&gt; 892, 543, 283, 351, 853, 1034, 1460, 837, 2423, 352, 265, 104,…\n$ n_w     &lt;dbl&gt; 101196, 61535, 40749, 39029, 82010, 110363, 173017, 69446, 181…\n$ n_s     &lt;dbl&gt; 3684, 2399, 1459, 1372, 3286, 4387, 6537, 2012, 6311, 3983, 36…\n$ name    &lt;chr&gt; \"press reportage\", \"press editorial\", \"press reviews\", \"religi…\n\n\nIf we want a more, tabular-like view of the data, we can simply print the dataset frame to the console. It’s worth mentioning, that all {readr} functions return tibbles, so we gain the benefits of tibbles when we read dataset into R with {readr} functions, one of which is that we do not have to worry that printing a data frame to the console, or our document, will print all of the data.\n\n# Print the data frame\nbrown_passives_df\n\n# A tibble: 15 × 5\n   cat   passive    n_w   n_s name            \n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n 1 A         892 101196  3684 press reportage \n 2 B         543  61535  2399 press editorial \n 3 C         283  40749  1459 press reviews   \n 4 D         351  39029  1372 religion        \n 5 E         853  82010  3286 skills / hobbies\n 6 F        1034 110363  4387 popular lore    \n 7 G        1460 173017  6537 belles lettres  \n 8 H         837  69446  2012 miscellaneous   \n 9 J        2423 181426  6311 learned         \n10 K         352  68599  3983 general fiction \n11 L         265  57624  3673 detective       \n12 M         104  14433   873 science fiction \n13 N         290  69909  4438 adventure       \n14 P         290  70476  4187 romance         \n15 R         146  21757   975 humour          \n\n\nBy default, printing tibbles will return the first 10 rows and all columns, unless the columns are too numerous to display width-wise.\n{dplyr} also provides a set of slice_*() functions which allow us to display the data in a tabular fashion, with some additional options. There are three slice_*() functions we will cover here:\n\nslice_head(): Select the first n rows of the data frame.\nslice_tail(): Select the last n rows of the data frame.\nslice_sample(): Select a random sample of n rows from the data frame.\n\nFor example, the following code block will select the first 5 rows of the data frame:\n\n# Select the first 5 rows\nslice_head(brown_passives_df, n = 5)\n\n# A tibble: 5 × 5\n  cat   passive    n_w   n_s name            \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n1 A         892 101196  3684 press reportage \n2 B         543  61535  2399 press editorial \n3 C         283  40749  1459 press reviews   \n4 D         351  39029  1372 religion        \n5 E         853  82010  3286 skills / hobbies\n\n\nWe can also select the last 5 rows of the data frame with the slice_tail() function:\n\n# Select the last 5 rows\nslice_tail(brown_passives_df, n = 5)\n\n# A tibble: 5 × 5\n  cat   passive   n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 L         265 57624  3673 detective      \n2 M         104 14433   873 science fiction\n3 N         290 69909  4438 adventure      \n4 P         290 70476  4187 romance        \n5 R         146 21757   975 humour         \n\n\nFinally, we can select a random sample of 5 rows from the data frame with the slice_sample() function:\n\n# Select a random sample of 5 rows\nslice_sample(brown_passives_df, n = 5)\n\n# A tibble: 5 × 5\n  cat   passive    n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 D         351  39029  1372 religion       \n2 G        1460 173017  6537 belles lettres \n3 C         283  40749  1459 press reviews  \n4 R         146  21757   975 humour         \n5 M         104  14433   873 science fiction\n\n\nThese functions can be helpful to get a sense of the dataset in different ways. In combination with arrange() function, we can also sort the data frame by a column or columns and then select the first or last rows.\nFor example, the following code block will sort the data frame by the passive column in ascending order and then select the first 5 rows:\n\n# Sort by the `passive` column and select the first 5 rows\nslice_head(arrange(brown_passives_df, passive), n = 5)\n\n# A tibble: 5 × 5\n  cat   passive   n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 M         104 14433   873 science fiction\n2 R         146 21757   975 humour         \n3 L         265 57624  3673 detective      \n4 C         283 40749  1459 press reviews  \n5 N         290 69909  4438 adventure      \n\n\nIf we want to sort be descending order, we can surround the column name with desc(), arrange(desc(passive)).\nNow, the previous code block does what we want, but it is not very readable. Enter the pipe operator. The pipe operator |&gt; is an operator which allows us to chain the output of one function to the input of another function. This allows us to write more readable code.\n\nbrown_passives_df |&gt;\n  arrange(passive) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 5\n  cat   passive   n_w   n_s name           \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1 M         104 14433   873 science fiction\n2 R         146 21757   975 humour         \n3 L         265 57624  3673 detective      \n4 C         283 40749  1459 press reviews  \n5 N         290 69909  4438 adventure      \n\n\nThe result is the same but the code makes more sense. We can read the code from left to right, top to bottom, which is the order in which the functions are executed.\n\n\n\n\n\n\n Dive deeper\nThe native R pipe |&gt; was introduced in R 4.1.0. If you are using an earlier version of R, you can use {magrittr} to load the pipe operator %&gt;%.\nThere are certain advantages to using the {magrittr} pipe operator, including the ability to use the pipe operator to pass arguments to functions with placeholders. For more information, see the magrittr documentation.\n\n\n\nIn addition to being more legible, using the pipe with each function on its own line allows us to add comments to each line of code. For example, the following code block is the same as the previous code block, but with comments added.\n\n# Sort by the passive column and select the first 5 rows\nbrown_passives_df |&gt;\n  arrange(passive) |&gt;\n  slice_head(n = 5)\n\nIt is a good practice to add comments when writing code, as long as it makes the code more readable and easier to understand for others and for your future self! If the comments are too verbose, and only repeat what the code is ‘saying’, then don’t include them.\n\n\nSubsetting datasets with {dplyr}\nNow that we have a sense of the data, we can subset the dataset to create a variations of our original data frame. We can subset the data frame by selecting columns and/ or rows.\nIn the R lesson “Packages and Functions”, we saw that base R provides the bracket ([]) operator to subset data frames. {dplyr} provides functions to subset data frames which can be more readable and easier to use.\nLet’s first look a selecting columns. The select() function allows us to select columns by name. For example, the following code block will select the passive and n_w columns from the data frame:\n\n# Select the `passive` and `n_w` columns\nselect(brown_passives_df, passive, n_w)\n\n# A tibble: 15 × 2\n   passive    n_w\n     &lt;dbl&gt;  &lt;dbl&gt;\n 1     892 101196\n 2     543  61535\n 3     283  40749\n 4     351  39029\n 5     853  82010\n 6    1034 110363\n 7    1460 173017\n 8     837  69446\n 9    2423 181426\n10     352  68599\n11     265  57624\n12     104  14433\n13     290  69909\n14     290  70476\n15     146  21757\n\n\nBeyond selecting columns, we can also reorder columns and rename columns. For example, the following code block will select the passive and n_w columns, rename the n_w column to num_words, and reorder the columns so that num_words is the first column:\n\n# Select rename and reorder columns\nbrown_passives_df |&gt;\n  select(num_words = n_w, passive)\n\n# A tibble: 15 × 2\n   num_words passive\n       &lt;dbl&gt;   &lt;dbl&gt;\n 1    101196     892\n 2     61535     543\n 3     40749     283\n 4     39029     351\n 5     82010     853\n 6    110363    1034\n 7    173017    1460\n 8     69446     837\n 9    181426    2423\n10     68599     352\n11     57624     265\n12     14433     104\n13     69909     290\n14     70476     290\n15     21757     146\n\n\n\n\n\n\n\n\n Dive deeper\nselect() also provides a number of helper functions to select columns. For example, we can use the starts_with() function inside the select() call to select columns that start with a certain string. Or we can select columns by their vector type by using where(is.character).\nFor more information, see the select() documentation or use the ?select command in the R console.\n\n\n\nBy selecting some columns and not others, we have effectively dropped the columns we did not select. If it is more effective to drop columns by name, we can use the select() function with the - operator. For example, the following code block will drop the cat column from the data frame:\n\n# Drop the `n_w` column\nbrown_passives_df |&gt;\n  select(-cat)\n\n# A tibble: 15 × 4\n   passive    n_w   n_s name            \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n 1     892 101196  3684 press reportage \n 2     543  61535  2399 press editorial \n 3     283  40749  1459 press reviews   \n 4     351  39029  1372 religion        \n 5     853  82010  3286 skills / hobbies\n 6    1034 110363  4387 popular lore    \n 7    1460 173017  6537 belles lettres  \n 8     837  69446  2012 miscellaneous   \n 9    2423 181426  6311 learned         \n10     352  68599  3983 general fiction \n11     265  57624  3673 detective       \n12     104  14433   873 science fiction \n13     290  69909  4438 adventure       \n14     290  70476  4187 romance         \n15     146  21757   975 humour          \n\n\nLet’s now turn our attention to subsetting rows. The filter() function allows us to select rows by a logical condition. For example, the following code block will select rows where the values of the passive column are less than &lt; 1,000:\n\n# Select rows where `passive` is less than 1,000\nbrown_passives_df |&gt;\n  filter(passive &lt; 1000)\n\n# A tibble: 12 × 5\n   cat   passive    n_w   n_s name            \n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           \n 1 A         892 101196  3684 press reportage \n 2 B         543  61535  2399 press editorial \n 3 C         283  40749  1459 press reviews   \n 4 D         351  39029  1372 religion        \n 5 E         853  82010  3286 skills / hobbies\n 6 H         837  69446  2012 miscellaneous   \n 7 K         352  68599  3983 general fiction \n 8 L         265  57624  3673 detective       \n 9 M         104  14433   873 science fiction \n10 N         290  69909  4438 adventure       \n11 P         290  70476  4187 romance         \n12 R         146  21757   975 humour          \n\n\nWe can also use the filter() function to select rows by a character string. For example, the following code block will select rows where the values of the name column are equal to religion:\n\n# Select rows where `name` is equal to `religion`\nbrown_passives_df |&gt;\n  filter(name == \"religion\")\n\n# A tibble: 1 × 5\n  cat   passive   n_w   n_s name    \n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 D         351 39029  1372 religion\n\n\nThe inequality operator != can be used for character strings as well. To include multiple values, we can use the %in% operator. In this case we can pass a vector of values to the filter() function. For example, the following code block will select rows where the values of the name column are equal to religion or learned:\n\n# Select multiple values\nbrown_passives_df |&gt;\n  filter(name %in% c(\"religion\", \"learned\", \"detective\"))\n\n# A tibble: 3 × 5\n  cat   passive    n_w   n_s name     \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1 D         351  39029  1372 religion \n2 J        2423 181426  6311 learned  \n3 L         265  57624  3673 detective\n\n\n\n\n\n\n\n\n Dive deeper\nFor more sophisticated subsetting, we can use the str_detect() function from {stringr} to select rows where the values of the name column contain a certain string. This approach will be enhanced later in the course when we learn about regular expressions.\n\n\n\n\n\nWriting datasets to a file with {readr}\nFinally, we can write data, including data frames, to a file with the write_*() functions from {readr}. The write_*() functions include:\n\nwrite_csv(): Write a data frame to a CSV file.\nwrite_tsv(): Write a data frame to a TSV file.\nwrite_delim(): Write a data frame to a delimited file with the specified delimiter (|, ;, etc).\n\nTo create a distinct data frame from the one we read into R, let’s subset our brown_passives_df data frame by columns and rows to create a new data frame that contains only the passive, n_w, and name columns and only the rows where the values of the passive column are greater than &gt; 1,000 and assign it to the brown_passives_subset_df.\n\n# Subset the data frame\nbrown_passives_subset_df &lt;-\n  brown_passives_df |&gt;\n  select(passive, n_w, name) |&gt;\n  filter(passive &gt; 1000)\n\nNow the following code block will write the brown_passives_subset_df data frame to a CSV file given the specified file path:\n\n# Write the data frame to a CSV file\nwrite_csv(\n  x = brown_passives_subset_df,\n  file = \"../data/derived/brown_passives_subset.csv\"\n)\n\nGiven the example directory structure we saw earlier, our new file appears in the data/derived/ directory.\nproject/\n├── data/\n│   ├── original/\n│   │   └── brown_passives_do.csv\n│   └── derived/\n│       ├── brown_passives_curated.csv\n│       ├── brown_passives_curated_dd.csv\n│       └── brown_passives_subset.csv\n└── code/\n    └── reading-inspecting-writing.qmd\nThere is much more to learn about reading, inspecting, and writing datasets in R. We will introduce more functions and techniques in the coming lessons. For now, we have learned how to read, inspect, and write datasets using R functions and Quarto code blocks!"
  },
  {
    "objectID": "recipes/recipe-02/index.html#check-your-understanding",
    "href": "recipes/recipe-02/index.html#check-your-understanding",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE {readr} provides functions to read rectangular data into R.\nThe echomessageinclude option in a code block determines whether the code is displayed in the output document.\nTRUEFALSE {dplyr} provides functions to create data dictionaries.\nread_csv()read_tsv()read_delim() is used to read tab-separated values (TSV) files.\nWhich function is in {dplyr} is used to select columns by name? select()filter()slice_head()\nTRUEFALSE The R pipe operator |&gt; allows us to chain the output of one function to the input of another function."
  },
  {
    "objectID": "recipes/recipe-02/index.html#lab-preparation",
    "href": "recipes/recipe-02/index.html#lab-preparation",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn Lab 2 you will have the opportunity to apply the skills you learned in this Recipe to create a Quarto document that reads, inspects, and writes data.\nIn addition to the knowledge and skills you have developed in Labs 0 and 1, to complete Lab 2, you will need to be able to:\n\nCreate code blocks in a Quarto document\nUnderstand the purpose of the label, echo, message, and include options in a code block\nLoad packages into an R session with library()\nUnderstand how to read and create file relative file paths\nRead datasets into R with the read_csv() function\nInspect data frames with {dplyr} functions such as glimpse(), slice_head(), slice_tail(), slice_sample(), and arrange().\nUse the |&gt; pipe operator to chain functions together.\nSubset data frames with {dplyr} functions such as select() and filter().\nWrite data frames to a file with the write_csv() function."
  },
  {
    "objectID": "recipes/recipe-02/index.html#footnotes",
    "href": "recipes/recipe-02/index.html#footnotes",
    "title": "02. Reading, inspecting, and writing datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Code block can be written in other programming languages as well such as Python, Bash, etc.↩︎"
  },
  {
    "objectID": "recipes/index.html",
    "href": "recipes/index.html",
    "title": "Recipes",
    "section": "",
    "text": "00. Literate Programming\n\n\nAn introduction to Quarto\n\n\nIn this recipe, we will introduce the concept of Literate Programming and describe how to implement this concept through Quarto. I will provide a demonstration of some of the features of Quarto and describe the main structural characteristics of a Quarto document to help you get off and running writing your own documents that combine code and prose. \n\n\n\n\n\n8 min\n\n\n1,451 words\n\n\n\n\n\n\n\n\n\n\n\n\n01. Academic writing with Quarto\n\n\nKey Quarto features for academic writing\n\n\nThe implementation of literate programming we are using in this course is Quarto with R. As we have seen in previously, Quarto provides the ability to combine prose and code in a single document. This is a powerful strategy for creating reproducible documents that can be easily updated and shared. \n\n\n\n\n\n12 min\n\n\n2,312 words\n\n\n\n\n\n\n\n\n\n\n\n\n02. Reading, inspecting, and writing datasets\n\n\nBasics of working with datasets in R\n\n\nThis Recipe guides you through the process of reading, inspecting, and writing datasets using R packages and functions in a Quarto environment. You’ll learn how to effectively combine code and narrative to create a reproducible document that can be shared with others. \n\n\n\n\n\n16 min\n\n\n3,197 words\n\n\n\n\n\n\n\n\n\n\n\n\n03. Descriptive assessment of datasets\n\n\nSummarizing data with statistics, tables, and plots\n\n\nIn this Recipe we will explore appropriate methods for summarizing variables in datasets given the number and informational values of the variable(s). We will build on our understanding of how to summarize data using statistics, tables, and plots. \n\n\n\n\n\n17 min\n\n\n3,271 words\n\n\n\n\n\n\n\n\n\n\n\n\n04. Understanding the computing environment\n\n\nIdentify layers of computing environments and preview Git and GitHub workflows\n\n\nIn this recipe, we will learn how to scaffold a research project and how to use the tools and resources available to us to manage research projects. We will build on our understanding of the computing environment and the structure of reproducible projects and introduce new features of Git and GitHub. \n\n\n\n\n\n11 min\n\n\n2,136 words\n\n\n\n\n\n\n\n\n\n\n\n\n05. Collecting and documenting data\n\n\nAcquiring datasets with the Project Gutenberg API\n\n\nAt this point, we now have a strong undertanding of the foundations of programming in R and the data science workflow. Previous lessons, recipes, and labs focused on developing these skills while the chapters aimed to provide a conceptual framework for understanding the steps in the data science workflow. We now turn to applying our conceptual knowledge and our technical skills to accomplish the tasks of the data science workflow, starting with data acquisition. \n\n\n\n\n\n25 min\n\n\n4,821 words\n\n\n\n\n\n\n\n\n\n\n\n\n06. Organizing and documenting data\n\n\nCurating semi-structured data\n\n\nAfter acquiring data, the next step in process is to organize data that is not tabular into a curated dataset. A curated dataset is a tidy dataset that reflects the data without major modifications. This dataset serves as a more general starting point for further data transformation. In this recipe, we will focus on curating data from a semi-structured format. \n\n\n\n\n\n25 min\n\n\n4,847 words\n\n\n\n\n\n\n\n\n\n\n\n\n07. Transforming and documenting data\n\n\nPrepare and enrich datasets for analysis\n\n\nThe curated dataset reflects a tidy version of the original data. This data is relatively project-neutral. A such, project-specific changes are often made to bring the data more in line with the research goals. This may include modifying the unit of observation and/ or adding additional attributes to the data. This process may generate one or more new datasets that are used for analysis. In this recipe, we will explore a practical example of transforming data. \n\n\n\n\n\n19 min\n\n\n3,663 words\n\n\n\n\n\n\n\n\n\n\n\n\n08. Employing exploratory methods\n\n\nDescriptive analysis and unsupervised machine learning\n\n\nExploratory analysis is a wide-ranging term that encompasses many different methods. In this recipe, we will focus on the methods that are most commonly used in the analysis of textual data. These include frequency and distributional analysis, clustering, and word embedding models. \n\n\n\n\n\n29 min\n\n\n5,781 words\n\n\n\n\n\n\n\n\n\n\n\n\n09. Building predictive models\n\n\nSupervised machine learning\n\n\nThis recipe will cover the process of building a predictive model to classify text into one of three Spanish dialects: Argentinian, Mexican, or Spanish. We will take a step-by-step approach that includes data preparation, model training and evaluation, and result interpretation. We will see practical examples of how to apply the {tidymodels} framework to build and evaluate a predictive model. \n\n\n\n\n\n29 min\n\n\n5,752 words\n\n\n\n\n\n\n\n\n\n\n\n\n10. Building inference models\n\n\nSimulation-based Null Hypothesis Testing\n\n\nIn this recipe, we will explore statistical modeling and data analysis with using a practical research hypothesis in the area of Second Language Acquisition and Teaching. We’ll use {infer} to understand inference-based models. You’ll learn how to work with key variables, examine data distributions, and employ statistical methods to test hypotheses about their relationships. Our discussion will also involve improving our computing skills through practical exercises in data manipulation, visualization, and statistical analysis. This will provide you with the necessary tools to prepare, conduct, and interpret complex datasets and analyses. \n\n\n\n\n\n17 min\n\n\n3,271 words\n\n\n\n\n\n\n\n\n\n\n\n\n11. Sharing research\n\n\nCommunicating research findings\n\n\nIn this recipe, I cover the tools and strategies for sharing research findings with the public and peers. We will begin assuming we are using Quarto websites as the primary tool for sharing research findings in both forums. From there, we will enter into some of the details of articles, presentations, and publishing research code and data. \n\n\n\n\n\n13 min\n\n\n2,411 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recipes/recipe-00/index.html",
    "href": "recipes/recipe-00/index.html",
    "title": "00. Literate Programming",
    "section": "",
    "text": "Skills\n\nIdentify the main components of a Quarto document\nCreate and render a Quarto document\nModify front-matter and prose sections"
  },
  {
    "objectID": "recipes/recipe-00/index.html#concepts-and-strategies",
    "href": "recipes/recipe-00/index.html#concepts-and-strategies",
    "title": "00. Literate Programming",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nLiterate Programming\nFirst introduced by Donald Knuth (1984), the aim of Literate Programming is to be able to combine computer code and text prose in one document. This allows an analyst to run code, view the output of the code, view the code itself, and provide prose description all in one document. In this way, a literate programming document allows for presenting your analysis in a way that performs the computing steps desired and presents it in an easily readable format. Literate programming is now a key component of creating and sharing reproducible research (Gandrud 2015).\n\n\nQuarto\nQuarto is a specific implementation of the literate programming paradigm. In Figure 1 we see an example of Quarto in action. On the left we see the Quarto source code, which is a combination of text and code. On the right we see the output of the Quarto source code as an HTML document.\n\n\n\n\n\n\n\n\nFigure 1: Quarto source (left) and output (right) example.\n\n\n\n\n\nQuarto documents generate various types of output: web documents (HTML), PDFs, Word documents, and many other types of output formats all based on the same source code. While the interleaving of code and prose to create a variety of output documents is one of the most attractive aspects of literate programming and Quarto, it is also possible to create documents with no code at all. It is a very versatile technology as you will come to appreciate.\n\n\n\n\n\n\n Dive deeper\nTo see Quarto in action, please check out the Quarto Gallery for a variety of examples of Quarto documents and their output.\n\n\n\nA Quarto source document is a plain-text file with the extension .qmd that can be opened in any plain text reader. We will be using the RStudio IDE (henceforth RStudio) to create, open, and edit, and generate output from .qmd files but any plain-text reader, such as TextEdit (MacOS) or Notepad (PC) can open these files.\nWith this in mind, let’s now move on to the anatomy of a Quarto document.\n\nAnatomy of a Quarto Document\nAt the most basic level a Quarto document contains two components:\n\na front-matter section and\na prose section.\n\nA third component, a code block, can be interleaved within the prose section to add code to the document. Let’s look at each of these in turn.\n\nFront-matter\nThe front matter of a Quarto document appears, well, at the front of the document (or the top, rather). Referring back to Figure Figure 1, we see the front matter at the top.\n---\ntitle: \"Introduction to Quarto\"\nauthor: \"Jerid Francom\"\nformat: html\n---\nWhen creating a Quarto document with RStudio the default attribute keys are title, author, and format. The front matter is fenced by three dashes ---.\nThe values for the first two keys are pretty straightforward and can be edited as needed. The value for the format attribute can also be edited to tell the .qmd file to generate other output types. Can you guess what value we might use to generate a PDF document? Yep, it’s just pdf. As we work Quarto you will learn more about how to use the RStudio interface to change some of these key-value pairs and add others!\n\n\nProse\nAnywhere below the front matter and not contained within a code block (see below) is open for prose. The prose section(s) have an added functionality in that they are Markdown aware. What does that mean, you say? Well, Markdown refers to a set of plain-text formatting conventions to produce formatted text in the output document. To quote Wikipedia:\n\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber and Aaron Swartz created Markdown in 2004 as a markup language that is appealing to human readers in its source code form. Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n\nWhat this enables us to do is to add simple text conventions to signal how the output should be formatted. Say we want to make some text bold. We just add ** around the text we want to appear bold.\n**bold text**\nWe can also do:\n\nitalics *italics*\nlinks [links](http://wfu.edu)\nstrikethrough ~~strikethrough~~\netc.\n\nFollow this link find more information on basic Markdown syntax.\n\n\nCode blocks\nCode blocks are where the R magic happens. Again, referring to Figure 1, we see that there is the following code block.\n```{r}\n1 + 1\n```\nA code block is bound by three backticks ```. After the first backticks the curly brackets {} allow us to tell Quarto which programming language to use to evaluate (i.e. run) in the code block. In most cases this will be R, hence the the opening curly bracket `{r}`. But there are other languages that can be used in Quarto, such as Python, SQL, and Bash.\nIn the previous example, R is used as a simple calculator adding 1 + 1. Here’s what this code block produces.\n\n1 + 1\n\n[1] 2\n\n\n```{r}\n#| label: add\n1 + 1\n```\nWe have only mentioned selecting the coding language and labeling the code block, but code blocks have various other options that can be used to determine how the code block should be used. Some common code block options are:\n\nhiding the code: #| echo: false\n\n```{r}\n#| label: add\n#| echo: false\n1 + 1\n```\n\n\n[1] 2\n\n\n\nhiding the output #| include: false\n\n```{r}\n#| label: add\n#| include: false\n1 + 1\n```\n\netc.\n\n\n\n\nCreate and render a Quarto document\nThe easiest and most efficient way to create a Quarto source file is to use the RStudio point-and-click interface. Just use the toolbar to create a new file and select “Quarto Document…”, as seen in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Creating a new Quarto document in RStudio.\n\n\n\n\n\nThis will provide you a dialogue box asking you to add a title and author to the document and also allows you to select the type of document format to output, as seen in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Dialogue box for creating a new Quarto document in RStudio.\n\n\n\n\n\nEnter a title and author and leave the format set to HTML.\nOn clicking ‘Create’ you will get a Quarto document, as in Figure 4, with some default/ boilerplate prose and code blocks. The prose and code blocks can be deleted, and we can start our own document.\n\n\n\n\n\n\n\n\nFigure 4: Quarto source in RStudio.\n\n\n\n\n\nBut for now, let’s leave things as they are and see how to generate the output report from this document. Click “Render” in the RStudio toolbar. Before it will render, you will be asked to save the file and give it a name.\nOnce you have done that the .qmd file will render in the format you have specified and open in the ‘Viewer’ pane, as seen in Figure 5.\n\n\n\n\n\n\n\n\nFigure 5: Quarto source and HTML output side-by-side in RStudio.\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\nWatch Getting Started with Quarto for a guided tour of Quarto (Çetinkaya-Rundel 2023)."
  },
  {
    "objectID": "recipes/recipe-00/index.html#check-your-understanding",
    "href": "recipes/recipe-00/index.html#check-your-understanding",
    "title": "00. Literate Programming",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE Literate Programming, first introduced by Donald Knuth in 1984, allows the combination of computer code and text prose in one document.\nThe programming paradigm Literate Programming is implemented through QuartoRRStudioGitHub, a platform that facilitates the creation of a variety of output documents based on the same source code.\nWhich of the following components does a basic Quarto document not contain? Front-matter sectionProse sectionBack-matter sectionCode block\nTo generate a PDF document in Quarto, you can edit the format attribute value in the front-matter section to .\nTRUEFALSE The code block options echo and include can be used to hide the code and output, respectively.\nTRUEFALSE In Quarto, a code block, where the programming language code is entered, is bounded by three underscores (_)."
  },
  {
    "objectID": "recipes/recipe-00/index.html#lab-preparation",
    "href": "recipes/recipe-00/index.html#lab-preparation",
    "title": "00. Literate Programming",
    "section": "Lab preparation",
    "text": "Lab preparation\nThis concludes our introduction to literate programming using Quarto. We have covered the basics there but there is much more to explore.\nIn preparation for Lab 0, ensure that you have completed the following:\n\nSetup your computing environment with R and RStudio\nInstalled the necessary packages:\n\nquarto\ntinytex\n\n\nand that you are prepared to do the following:\n\nOpen RStudio and understand the basic interface\nCreate, edit, and render Quarto documents\nUse some basic Markdown syntax to format text\n\nWith this in mind, you are ready to move on to Lab 00."
  },
  {
    "objectID": "recipes/recipe-06/index.html",
    "href": "recipes/recipe-06/index.html",
    "title": "06. Organizing and documenting data",
    "section": "",
    "text": "Skills\n\nReading and parsing semi-structured data\nCreating a custom function and iterating over a collection of files\nCombining the results into a single dataset\nDocumenting the data curation process and resulting dataset\nIn this recipe, we will make use of {readr}, {dplyr}, {stringr}, and {purrr}, employ regular expressions to parse the semi-structured data, and use {qtalrkit} to document the dataset. Let’s load those packages now.\n# Load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(fs)\nlibrary(qtalrkit)\nIn Lab 6, we will apply what we learn in this recipe to curate and document acquired data."
  },
  {
    "objectID": "recipes/recipe-06/index.html#concepts-and-strategies",
    "href": "recipes/recipe-06/index.html#concepts-and-strategies",
    "title": "06. Organizing and documenting data",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nAssessing the data\nAcquired data can be in a variety of formats. This will range from unstructured data such as running text to structured data such as tabular data. Semi-structured data is somewhere in between. It has some structure, but it is not as well defined as structured data and requires some work to organize it into a tidy dataset.\nAs a semi-structured example we will work with the The Switchboard Dialog Act Corpus (SWDA) (University of Colorado Boulder 2008) which extends the Switchboard Corpus with speech act annotation.\n\n\n\n\n\n\n Tip\nIf you would like to download and decompress the data yourself, you can do so by running the following code:\n\nqtalrkit::get_compressed_data(\n  url = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz\",\n  target_dir = \"data/original/swda/\"\n)\n\n\n\nAs a starting point, let’s assume you have acquired the SWDA corpus and decompressed it into your project’s data/original/swda/ directory, as seen below.\ndata/\n├── analysis/\n├── derived/\n└── original/\n    └── swda/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\nThe first step is to inspect the data directory and file structure (and of course any documentation files).\nThe README file contains basic information about the resource, the doc/ directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with sw… contain individual conversation files.\nTaking a closer look at the first conversation file directory, sw00utt/ we can see that it contains files with the .utt extension.\n├── sw00utt\n│   ├── sw_0001_4325.utt\n│   ├── sw_0002_4330.utt\n│   ├── sw_0003_4103.utt\n│   ├── sw_0004_4327.utt\n│   ├── sw_0005_4646.utt\nLet’s take a look inside a conversation file (sw_0001_4325.utt) to see how it is structured internally. You can do this by opening the file in a text editor or by using the read_lines() function from the {readr} package.\n\n\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:   4325_1632_1519\nTOPIC#:     323\nDATE:       920323\nTRANSCRIBER:    glp\nUTT_CODER:  tc\nDIFFICULTY: 1\nTOPICALITY: 3\nNATURALNESS:    2\nECHO_FROM_B:    1\nECHO_FROM_A:    4\nSTATIC_ON_A:    1\nSTATIC_ON_B:    1\nBACKGROUND_A:   1\nBACKGROUND_B:   2\nREMARKS:        None.\n\n=========================================================================\n  \n\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }   \n\nqy^d          B.2 utt1: [ [ I guess, +   \n\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /  \n\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /  \n\nqy          A.5 utt1: Does it say something? /  \n\nsd          B.6 utt1: I think it usually does.  /\n\n\nThere are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of = characters. Second, the header contains meta-information of various types. Third, the conversation text is interleaved with an annotation scheme.\nSome of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let’s take a look at the README file.\nIn this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 ‘DAMSL’ dialog act labels. The README file refers us to the doc/manual.august1.html file for more information on this scheme.\nAt this point we open the the doc/manual.august1.html file in a browser and do some investigation. We find out that ‘DAMSL’ stands for ‘Discourse Annotation and Markup System of Labeling’ and that the first characters of each line of the conversation text correspond to one or a combination of labels for each utterance. So for our first utterances we have:\no = \"Other\"\nqw = \"Wh-Question\"\nqy^d = \"Declarative Yes-No-Question\"\n+ = \"Segment (multi-utterance)\"\nEach utterance is also labeled for speaker (‘A’ or ‘B’), speaker turn (‘1’, ‘2’, ‘3’, etc.), and each utterance within that turn (‘utt1’, ‘utt2’, etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.\nNow let’s turn to the meta-data in the header. We see here that there is information about the creation of the file: ‘FILENAME’, ‘TOPIC’, ‘DATE’, etc. The doc/manual.august1.html file doesn’t have much to say about this information so I returned to the LDC Documentation and found more information in the Online Documentation section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the caller_tab.csv file. This tabular file does not contain column names, but the caller_doc.txt does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the ‘FILENAME’ information contained three pieces of useful information delimited by underscores _.\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:   4325_1632_1519\nTOPIC#:     323\nDATE:       920323\nTRANSCRIBER:    glp\nThe first information is the document id (4325), the second and third correspond to the speaker number: the first being speaker A (1632) and the second speaker B (1519).\nIn sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of = characters. The header section contains a ‘FILENAME’ line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let’s set out to create a tidy dataset with the column structure as in Table 1.\n\n\n\n\nTable 1: Idealized curated dataset\n\n\n\n\n\n\n\nvariable\nname\ntype\ndescription\n\n\n\n\ndoc_id\nDocument ID\ncharacter\nThe unique identifier for the conversation\n\n\ndamsl_tag\nDAMSL Tag\ncharacter\nThe DAMSL tag for the utterance\n\n\nspeaker\nSpeaker\ncharacter\nThe speaker of the utterance\n\n\nturn_num\nTurn Number\ncharacter\nThe turn number of the utterance\n\n\nutterance_num\nUtterance Number\nnumeric\nThe utterance number of the utterance\n\n\nutterance_text\nUtterance Text\ncharacter\nThe text of the utterance\n\n\nspeaker_id\nSpeaker ID\ncharacter\nThe unique identifier for the speaker\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy the data\nThere are many ways to approach the task of tidying the data in general, and this semi-structured data in particular. In this recipe, we will take a step-by-step approach to parsing the semi-structured data in one file and then apply this process to all of the files in the corpus using a custom function.\nLet’s begin by reading one of the conversation files into R as a character vector using the read_lines() function from {readr}.\n\n# Read a single file as character vector\ndoc_chr &lt;-\n  read_lines(file = \"data/original/swda/sw00utt/sw_0001_4325.utt\")\n\nTo isolate the vector element that contains the document and speaker ids, we use str_subset() from {stringr}. This function takes two arguments, a string and a pattern, and returns any vector element that matches the pattern.\nIn this case we are looking for a pattern that matches three groups of digits separated by underscores. To test out a pattern, we can use the str_view() function. We will use the regular expression character class \\\\d for digits and the + operator to match 1 or more contiguous digits. We then separate three groups of \\\\d+ with underscores _. The result is \\\\d+_\\\\d+_\\\\d+.\n\n# Test out a pattern\ndoc_chr |&gt;\n  str_view(pattern = \"\\\\d+_\\\\d+_\\\\d+\")\n\n[15] │ FILENAME:{\\t}&lt;4325_1632_1519&gt;\n\n\nWe can see that this pattern matches the line we are looking for. Now we can use this pattern with str_subset() to return the vector element that contains this pattern.\n\n# Isolate the vector element that contains the document and speaker ids\nstr_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\")\n\n[1] \"FILENAME:\\t4325_1632_1519\"\n\n\n\n\n\n\n\n\n Tip\nRegular Expressions are a powerful pattern matching syntax. They are used extensively in text manipulation and we will see them again and again.\nTo develop regular expressions, it is helpful to have a tool that allows you to interactively test your pattern matching. {stringr} has a handy function str_view() which allows for interactive pattern matching. A good website to practice Regular Expressions is RegEx101. You can also install {regexplain} (Aden-Buie 2021) in R to get access to a useful RStudio Addin.\n\n\n\nThe next step is to extract the three digit sequences that correspond to the doc_id, speaker_a_id, and speaker_b_id. First we extract the pattern that we have identified with str_extract() and then we can break up the single character vector into multiple parts based on the underscore _. The str_split() function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors.\n\nstr_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |&gt; # isolate vector element\n  str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |&gt; # extract the pattern\n  str_split(\"_\") # split the character vector by underscore\n\n[[1]]\n[1] \"4325\" \"1632\" \"1519\"\n\n\nA list is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same –hence the tabular format).\nIn this case we have a list of length 1, whose sole element is a character vector of length 3 –one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our str_split() function we don’t want the results to be conflated as a single character vector blurring the distinction between the individual character vectors.\nIn this case, however, we want to extract the three elements of the character vector and assign them to meaningful variable names. To do this we will use the unlist() function which will convert the list into a single character vector. We will assign this result to speaker_info_chr.\n\nspeaker_info_chr &lt;-\n  str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n  str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n  str_split(\"_\") |&gt;\n  unlist() # convert the list to a character vector\n\n# Preview\nspeaker_info_chr\n\n[1] \"4325\" \"1632\" \"1519\"\n\n\nspeaker_info_chr is now a character vector of length three. Let’s subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.\n\ndoc_id &lt;- speaker_info_chr[1] # extract by index\nspeaker_a_id &lt;- speaker_info_chr[2] # extract by index\nspeaker_b_id &lt;- speaker_info_chr[3] # extract by index\n\nThe next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of = separates the header section from the text section. What we need to do is to index the point in our character vector doc_chr where that line occurs and then subset the doc_chr from that point until the end of the character vector.\nLet’s first find the point where the = sequence occurs. We will again use the str_view() to test out a pattern that matches a contiguous sequence of =.\n\nstr_view(doc_chr, \"=+\")\n\n[31] │ &lt;=========================================================================&gt;\n\n\nSo for this file we see there is one element that matches and that element’s index is 31.\nNow it is important to keep in mind that we are working with a single file from the swda/ data. Since our plan is to use this code to apply to other files, we need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the =+ pattern will match =, or ==, or ===, etc. it is not implausible to believe that there might be a = character on some other line in one of the other files.\nLet’s update our regular expression to avoid this potential scenario by only matching sequences of three or more =. In this case we will make use of the curly bracket operators {}.\n\nstr_view(doc_chr, \"={3,}\")\n\n[31] │ &lt;=========================================================================&gt;\n\n\nWe will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for ===, ====, etc.\nTo extract just the index of the match, we can use the str_which() function with the same pattern. This will return the index of the vector element that matches the pattern. However, consider what we are doing. We actually are using this index to subset the vector, so we need to increment the index by 1 to get the next vector element. Let’s do this and then assign the result to text_start_index.\n\n# Find where text starts\ntext_start_index &lt;- str_which(doc_chr, \"={3,}\") + 1\n\nThe index for the end of the text is simply the length of the doc_chr vector. We can use the length() function to get this index.\n\n# Find where text ends\ntext_end_index &lt;- length(doc_chr)\n\nWe now have the bookends, so to speak, for our text section. To extract the text we subset the doc_chr vector by these indices.\n\n# Extract text between indices\ntext &lt;- doc_chr[text_start_index:text_end_index]\n\n# Preview\nhead(text)\n\n[1] \"  \"                                       \n[2] \"\"                                         \n[3] \"o          A.1 utt1: Okay.  /\"            \n[4] \"qw          A.1 utt2: {D So, }   \"        \n[5] \"\"                                         \n[6] \"qy^d          B.2 utt1: [ [ I guess, +   \"\n\n\nThe text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the str_trim() function which by default will remove leading and trailing whitespace from each line.\n\n# Remove leading and trailing whitespace\ntext &lt;- str_trim(text)\n\n# Preview\nhead(text)\n\n[1] \"\"                                      \n[2] \"\"                                      \n[3] \"o          A.1 utt1: Okay.  /\"         \n[4] \"qw          A.1 utt2: {D So, }\"        \n[5] \"\"                                      \n[6] \"qy^d          B.2 utt1: [ [ I guess, +\"\n\n\nTo remove blank lines we will use str_subset() to subset the text vector. The .+ pattern will match elements that are not blank. We will assign the result to text overwriting the original text vector.\n\n# Remove blank lines\ntext &lt;- str_subset(text, \".+\")\n\n# Preview\nhead(text)\n\n[1] \"o          A.1 utt1: Okay.  /\"                                                                  \n[2] \"qw          A.1 utt2: {D So, }\"                                                                 \n[3] \"qy^d          B.2 utt1: [ [ I guess, +\"                                                         \n[4] \"+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\"\n[5] \"+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\"                        \n[6] \"qy          A.5 utt1: Does it say something? /\"                                                 \n\n\nOur first step towards a tidy dataset is to now combine the doc_id and each element of text in a data frame, leaving aside our speaker ids. We will use the tibble() function and pass the variables as named arguments.\n\n# Combine info and text into a data frame\nswda_df &lt;- tibble(doc_id, text)\n\n# Preview\nslice_head(swda_df, n = 5)\n\n# A tibble: 5 × 2\n  doc_id text                                                                   \n  &lt;chr&gt;  &lt;chr&gt;                                                                  \n1 4325   o          A.1 utt1: Okay.  /                                          \n2 4325   qw          A.1 utt2: {D So, }                                         \n3 4325   qy^d          B.2 utt1: [ [ I guess, +                                 \n4 4325   +          A.3 utt1: What kind of experience [ do you, + do you ] have…\n5 4325   +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n\n\nWith our data now in a data frame, it’s time to parse the text column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns.\nTo do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row.\nThe best way to learn regular expressions is to use them. To this end I’ve included a link to the interactive regular expression practice website regex101.\nOpen this site and copy the text below into the ‘TEST STRING’ field.\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }\nqy^d          B.2 utt1: [ [ I guess, +\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\nqy          A.5 utt1: Does it say something? /\nsd          B.6 utt1: I think it usually does.  /\nad          B.6 utt2: You might try, {F uh, }  /\nh          B.6 utt3: I don't know,  /\nad          B.6 utt4: hold it down a little longer,  /\n\n\n\n\n\nRegEx101\n\n\n\n\nNow manually type the following regular expressions into the ‘REGULAR EXPRESSION’ field one-by-one (each is on a separate line). Notice what is matched as you type and when you’ve finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.\n^.+?\\s\n[AB]\\.\\d+\nutt\\d+\n:.+$\nAs you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text.\nTo apply these expressions to our data and extract this information into separate columns we will make use of the mutate() and str_extract() functions. mutate() will take our data frame and create new columns with values we match and extract from each row in the data frame with str_extract().\n\n\n\n\n\n\n Tip\nNotice that str_extract() is different than str_extract_all(). When we work with mutate() each row will be evaluated in turn, therefore we only need to make one match per row.\n\n\n\nI’ve chained each of these steps in the code below, dropping the original text column with select(-text), and overwriting swda_df with the results.\n\n# Extract column information from `text`\nswda_df &lt;-\n  swda_df |&gt; # current dataset\n  mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |&gt; # damsl tags\n  mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |&gt; # speaker_turn pairs\n  mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |&gt; # utterance number\n  mutate(utterance_text = str_extract(text, \":.+$\")) |&gt; # utterance text\n  select(-text) # drop the `text` column\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 5\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n$ speaker_turn   &lt;chr&gt; \"A.1\", \"A.1\", \"B.2\", \"A.3\", \"B.4\", \"A.5\", \"B.6\", \"B.6\",…\n$ utterance_num  &lt;chr&gt; \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n$ utterance_text &lt;chr&gt; \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n\n\n\n\n\n\n\n\n Warning\nOne twist you will notice is that regular expressions in R require double backslashes (\\\\) where other programming environments use a single backslash (\\).\n\n\n\nThere are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the speaker_turn column into speaker and turn_num columns and second we need to remove unwanted characters from the damsl_tag, utterance_num, and utterance_text columns.\nTo separate the values of a column into two columns we use the separate_wider_delim() function. It takes a column to separate, a delimiter to use to separate the values, and a character vector of the names of the new columns to create.\n\n# Separate speaker_turn into distinct columns\nswda_df &lt;-\n  swda_df |&gt;\n  separate_wider_delim(\n    cols = speaker_turn,\n    delim = \".\",\n    names = c(\"speaker\", \"turn_num\")\n  )\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 6\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n$ utterance_text &lt;chr&gt; \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n\n\nTo remove unwanted leading or trailing whitespace we apply the str_trim() function. For removing other characters we matching the character(s) and replace them with an empty string (\"\") with the str_replace() function. Again, I’ve chained these functions together and overwritten data with the results.\n\n# Clean up column information\nswda_df &lt;-\n  swda_df |&gt; # current dataset\n  mutate(damsl_tag = str_trim(damsl_tag)) |&gt; # remove leading/ trailing whitespace\n  mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |&gt; # remove 'utt'\n  mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |&gt; # remove ': '\n  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 6\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n\n\nTo round out our tidy dataset for this single conversation file we will connect the speaker_a_id and speaker_b_id with speaker A and B in our current dataset adding a new column speaker_id. The case_when() function does exactly this: allows us to map rows of speaker with the value “A” to speaker_a_id and rows with value “B” to speaker_b_id.\n\n# Link speaker with speaker_id\nswda_df &lt;-\n  swda_df |&gt; # current dataset\n  mutate(speaker_id = case_when( # create speaker_id\n    speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n    speaker == \"B\" ~ speaker_b_id, # speaker_b_id value when B\n    TRUE ~ NA_character_ # NA otherwise\n  ))\n\n# Preview\nglimpse(swda_df)\n\nRows: 159\nColumns: 7\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nWe now have the tidy dataset we set out to create. But this dataset only includes one conversation file! We want to apply this code to all 1,155 conversation files in the swda/ corpus.\nThe approach will be to create a custom function which groups the code we’ve done for this single file and then iteratively send each file from the corpus through this function and combine the results into one data frame.\nHere’s the custom function with some extra code to print a progress message for each file when it runs.\n\n# [ ] add to {qtalrkit}, note the convention of `extract_` prefix for curation functions. In combination with `get_compressed_data()` this corpus can be curated with few steps.\n\nextract_swda_data &lt;- function(file) {\n  # Progress message\n  file_basename &lt;- basename(file) # file name\n  message(\"Processing \", file_basename, \"\\n\")\n\n  # Read `file` by lines\n  doc_chr &lt;- read_lines(file)\n\n  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`\n  speaker_info_chr &lt;-\n    str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n    str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |&gt;\n    str_split(\"_\") |&gt;\n    unlist()\n\n  doc_id &lt;- speaker_info_chr[1]\n  speaker_a_id &lt;- speaker_info_chr[2]\n  speaker_b_id &lt;- speaker_info_chr[3]\n\n  # Extract `text`\n  text_start_index &lt;- str_which(doc_chr, \"={3,}\") + 1\n  text_end_index &lt;- length(doc_chr)\n\n  text &lt;-\n    doc_chr[text_start_index:text_end_index] |&gt;\n    str_trim() |&gt;\n    str_subset(\".+\")\n\n  swda_df &lt;- tibble(doc_id, text) # tidy format `doc_id` and `text`\n\n  # Extract column information from `text`\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |&gt; # damsl tags\n    mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |&gt; # speaker_turn pairs\n    mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |&gt; # utterance number\n    mutate(utterance_text = str_extract(text, \":.+$\")) |&gt; # utterance text\n    select(-text) # drop the `text` column\n\n  # Separate speaker_turn into distinct columns\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    separate_wider_delim(\n      cols = speaker_turn,\n      delim = \".\",\n      names = c(\"speaker\", \"turn_num\")\n    )\n\n  # Clean up column information\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    mutate(damsl_tag = str_trim(damsl_tag)) |&gt; # remove leading/ trailing whitespace\n    mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |&gt; # remove 'utt'\n    mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |&gt; # remove ': '\n    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\n  # Link speaker with speaker_id\n  swda_df &lt;-\n    swda_df |&gt; # current dataset\n    mutate(speaker_id = case_when( # create speaker_id\n      speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n      speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B\n    ))\n\n  message(\"Processed \", file_basename, \"\\n\")\n  return(swda_df)\n}\n\nAs a sanity check we will run the extract_swda_data() function on a the conversation file we were just working on to make sure it works as expected.\n\n# Process a single file (test)\nextract_swda_data(\n  file = \"../data/original/swda/sw00utt/sw_0001_4325.utt\"\n) |&gt;\n  glimpse()\n\n\n\nRows: 159\nColumns: 7\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nLooks good!\nSo now it’s time to create a vector with the paths to all of the conversation files. The ls_dif() function from {fs} interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (regexp = \\\\.utt$) so we don’t accidentally include other files in the corpus. recurse set to TRUE means we will get the full path to each file.\n\n# List all conversation files\nswda_files_chr &lt;-\n  dir_ls(\n    path = \"../data/original/swda/\", # source directory\n    recurse = TRUE, # traverse all sub-directories\n    type = \"file\", # only return files\n    regexp = \"\\\\.utt$\"\n  ) # only return files ending in .utt\n\nhead(swda_files_chr) # preview file paths\n\n\n\ndata/original/swda/sw00utt/sw_0001_4325.utt\ndata/original/swda/sw00utt/sw_0002_4330.utt\ndata/original/swda/sw00utt/sw_0003_4103.utt\ndata/original/swda/sw00utt/sw_0004_4327.utt\ndata/original/swda/sw00utt/sw_0005_4646.utt\ndata/original/swda/sw00utt/sw_0006_4108.utt\n\n\nTo pass each conversation file in the vector of paths to our conversation files iteratively to the extract_swda_data() function we use map_dfr(). This will apply the function to each conversation file and return a data frame for each and then combine the results into a single data frame.\n\n# Process all conversation files\nswda_df &lt;-\n  swda_files_chr |&gt; # pass file names\n  map_dfr(extract_swda_data) # read and tidy iteratively\n\n# Preview\nglimpse(swda_df)\n\nRows: 223,606\nColumns: 7\n$ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nWe now see that we have 223, 606 observations (individual utterances in this dataset). The structure of the data frame matches our idealized dataset in Table @ref(tab:swda-idealized-dataset).\nIt also is a good idea to inspect the data frame to ensure that the data is as expected. One is to check for missing values. We can use the skim() function from {skimr} to get a quick summary of the data frame. Another is to spot check the data frame to see if the values are as expected. As we are working with a fairly large dataset, we can use the slice_sample() function from {dplyr} to randomly sample a subset of rows from the data frame.\n\n\nDocumentation\nWe now have a tidy dataset, but we need to document the data curation process and the resulting dataset. The script used to curate the data should be cleaned up and well documented in prose and code comments.\nWe then need to write the dataset to disk and create a data dictionary. We will make sure to add the curated dataset to the derived/ directory and the data dictionary close to the dataset.\n\n# Write to disk\ndir_create(path = \"data/derived/swda/\") # create swda subdirectory\n\nwrite_csv(swda_df,\n  file = \"data/derived/swda/swda_curated.csv\"\n)\n\nThe directory structure now looks like this:\ndata/\n├── analysis/\n├── derived/\n│   └── swda/\n│       └── swda_curated.csv\n└── original/\n    └── swda/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\nThe data dictionary file will contain information about the dataset variables and their values. This file can be created manually and edited with a text editor or spreadsheet software. Or alternatively, the scaffolding for a CSV file can be generated with the create_data_dictionary() function from {qtalrkit}.\n\n# Create data dictionary\ncreate_data_dictionary(\n  data = swda,\n  file_path = \"data/derived/swda/swda_dd.csv\"\n)"
  },
  {
    "objectID": "recipes/recipe-06/index.html#summary",
    "href": "recipes/recipe-06/index.html#summary",
    "title": "06. Organizing and documenting data",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we learned how to read and parse semi-structured data, create a custom function and iterate over a collection of files, combine the results into a single dataset, and document the data curation process and resulting dataset.\nThe skills we used in this recipe include regular expressions, the {readr}, {dplyr}, {stringr}, and {purrr}, and {qtalrkit} for documenting the dataset."
  },
  {
    "objectID": "recipes/recipe-06/index.html#check-your-understanding",
    "href": "recipes/recipe-06/index.html#check-your-understanding",
    "title": "06. Organizing and documenting data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nThe first thing that should be done in the data curation process is to know what packages you are going to useexplore the data documentation and understand the resourceread the data into Rparse the data into a tidy dataset.\nThe read_lines() function from {readr} will read a file into R as a character vectordata framelistmatrix.\nTRUEFALSE The separate_wider_delim() function from {tidyr} will separate a column into two or more columns based on a delimiter (e.g. -, ., etc.).\nWhich of the following functions from {stringr} will return vector elements which contain a match for a pattern? str_subset()str_extract()str_replace()str_trim()\nThe map_dfr() function from {purrr} will apply a function to each element of a vector and return a listnested data framedata frame with rows combineddata frame with columns combined.\nA data dictionary is a document that describes the data curation processdata analysis processdataset variables and their valuesdata visualization process."
  },
  {
    "objectID": "recipes/recipe-06/index.html#lab-preparation",
    "href": "recipes/recipe-06/index.html#lab-preparation",
    "title": "06. Organizing and documenting data",
    "section": "Lab preparation",
    "text": "Lab preparation\nBefore beginning Lab 6, review and ensure that you are familiar with the following:\n\nVector, data frame, and list data structures\nSubsetting and indexing vectors, data frames, and lists\nBasic regular expressions such as character classes, quantifiers, and anchors\nReading, writing, and manipulating files\nCreating and employing custom functions\n\nIn this lab, we will practice these skills and expand our use of the {readr}, {dplyr}, {stringr}, and {purrr} to curate and document a dataset.\nYou will have a choice of data to curate. Before you start the lab, you should consider which data source you would like to use, what the idealized structure the curated dataset will take, and what strategies you will likely employ to curate the dataset. You should also consider the information you need to document the data curation process."
  },
  {
    "objectID": "recipes/recipe-08/index.html",
    "href": "recipes/recipe-08/index.html",
    "title": "08. Employing exploratory methods",
    "section": "",
    "text": "Skills\n\nTokenize and prepare features for analysis\nConduct frequency and dispersion analysis including measures and visualizations\nTrain word embeddings and create a Term-Document Matrix\nConduct a word embedding analysis\nThe approach to exploratory analysis is rarely linear, but rather an interative cycle of the steps in Table 1. This cycle is repeated until the research question(s) have been addressed.\nWe will model how to explore iteratively using the output of one method to inform the next and ultimately to address the research question. For this reason, the subsequent sections of this recipe are grouped by research question rather than by approach step or method.\nLet’s get started by loading some of the packages we will likely use.\nlibrary(dplyr)      # for data manipulation\nlibrary(stringr)    # for string manipulation\nlibrary(tidyr)      # for data tidying\nlibrary(tidytext)   # for text analysis\nlibrary(ggplot2)    # for data visualization"
  },
  {
    "objectID": "recipes/recipe-08/index.html#concepts-and-strategies",
    "href": "recipes/recipe-08/index.html#concepts-and-strategies",
    "title": "08. Employing exploratory methods",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\nWe will use the SOTU corpus to demonstrate the different methods. We will select a subset of the corpus (post-1945) and explore the question:\n\nHow has the language of the SOTU changed over time?\n\nThis will include methods such as frequency and distributional analysis, dimensionality reduction, and word embedding models.\nLet’s look at the first few rows of the data to get a sense of what we have.\n\nsotu_df\n\n# # A tibble: 73 × 4\n#    president   year party      address                                          \n#    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                                            \n#  1 Truman      1947 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  2 Truman      1948 Democratic \"Mr. President, Mr. Speaker, and Members of the …\n#  3 Truman      1949 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  4 Truman      1950 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  5 Truman      1951 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  6 Truman      1952 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  7 Eisenhower  1953 Republican \"Mr. President, Mr. Speaker, Members of the Eigh…\n#  8 Eisenhower  1954 Republican \"Mr. President, Mr. Speaker, Members of the Eigh…\n#  9 Eisenhower  1955 Republican \"Mr. President, Mr. Speaker, Members of the Cong…\n# 10 Eisenhower  1956 Republican \"My Fellow Citizens: This morning I sent to the …\n# # ℹ 63 more rows\n\n\nWe can see that the dataset contains the president, year, party, and address for each SOTU address.\nNow let’s view a statistical overview summary of the data by using the skim() function.\nskimr::skim(sotu_df)\n\n\n# ── Data Summary ────────────────────────\n#                            Values \n# Name                       sotu_df\n# Number of rows             73     \n# Number of columns          4      \n# _______________________           \n# Column type frequency:            \n#   character                3      \n#   numeric                  1      \n# ________________________          \n# Group variables            None   \n# \n# ── Variable type: character ────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate  min   max empty n_unique whitespace\n# 1 president             0             1    4    10     0       12          0\n# 2 party                 0             1   10    10     0        2          0\n# 3 address               0             1 6160 51947     0       73          0\n# \n# ── Variable type: numeric ──────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate  mean   sd   p0  p25  p50  p75 p100\n# 1 year                  0             1 1984. 21.6 1947 1965 1984 2002 2020\n#   hist \n# 1 ▇▇▇▇▇\n\n\nWe can see that the dataset contains 73 rows –corresponding to the number of SOTU addresses. Looking at missing values, we can see that there are no missing values for any of the variables. Taking a closer look at each variable, the president variable is a character vector with 12 unique presidents. There are two unique parties and 73 unique addresses. The year variable is numeric with a minimum value of 1947 and a maximum value of 2020, therefore the addresses include 73 years.\n\n\nIdentify\nWith a general sense of the data, we can now move on to the first step in the exploratory analysis workflow: identifying variables of interest.\n\nWhat linguistic variables might be of interest to address this question?\n\nWords might be the most obvious variable, but we might also be interested in other linguistic variables such as parts of speech, syntactic structures, or some combination of these.\nLet’s start with words. If we look at words, we might be interested in the frequency of words, the distribution of words, or the meaning of words. We might also be interested in the relationship between words. For example, we might be interested in the co-occurrence of words or the similarity of words. These, and more, are all possible approaches that we might consider.\nAnother consideration is if we want to do comparisons across time, across presidents, across parties, etc.. In our research question, we have already identified that we want to compare across time so that will be our focus. However, what we mean by “time” is not clear. Do we mean across years, across decades, across presidencies, etc.? We will need to make a decision about how we want to define time, but we can fold this into our exploratory analysis, as we will see below.\nLet’s posit the following sub-questions:\n\nWhat are the most frequent words across time periods?\nHow does the distribution of words change across time periods?\nHow does the meaning of words change across time periods?\n\nWe will use these sub-questions to guide our exploratory analysis.\n\n\nInspect\nThe next step is to inspect the data. We will transform the data as necessary to prepare it for analysis and then do some diagnostic checks to make sure that the data is ready for analysis.\nSince we will be working with words, let’s tokenize the addresses variable to extract the words and maintain a tidy dataset. We will use the unnest_tokens() function from {tidytext} to do this. Let’s apply the function and assign the result to a new variable called sotu_words_df.\n\n# Tokenize the words by year (with numbers removed)\nsotu_words_df &lt;-\n  sotu_df |&gt;\n  unnest_tokens(word, address, strip_numeric = TRUE)\n\nLet’s continue by looking at whether there is a relationship between the number of words and years. We can do this by using the count() function on the year variable. This will group and count the number of observations (words) per year as n.\nWe can then visualize this with a line plot where the x-axis is the year and the y-axis is the number of words n. I’ll add the plot to a variable called p so that we can add layers to it.\n\n# Get the number of words per year --------------------------------\np &lt;-\n  sotu_words_df |&gt;\n  count(year) |&gt;\n  ggplot(aes(x = year, y = n)) +\n  geom_line()\n\n# View\np\n\n\n\n\n\n\n\nFigure 1: Number of words per year\n\n\n\n\n\n\n\n\n\n\n\n Tip\nThe count() function is a wrapper for the summarize() function. It is a convenient way to count the number of observations in a dataset.\n\n# the output of\ndf |&gt;\n  count(var1)\n\n# is equivalent to\ndf |&gt;\n  group_by(var1) |&gt;\n  summarize(n = n()) |&gt;\n  ungroup()\nThe difference is that count() grouping is added and removed automatically. In other cases, we can mimic this behavior for other operations inside a summarize() or mutate() function by using the .by argument. For example:\ndf |&gt;\n  summarize(n = n(), .by = var1)\n\n\n\nWe can see from Figure 2 that the number of words per year varies, sometimes quite a bit. To get a sense of the relationship between the number of words and the year, we can add a linear trend line to the plot. We can do this by adding the geom_smooth() function to the plot. We will set the method argument to \"lm\" to use a linear model.\n\np + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nThis plot shows that there is a positive relationship between the number of words and the year –that is the number of words increases over time. But the line is not a great fit and furthermore the angle of the line is more horizontal than vertical. This suggests that the relationship is not a strong one. We can confirm this by calculating the correlation between the number of words and the year. We can do this by using the cor.test() function on the year and n variables inside a summarize() function.\n\n# Get the correlation between the number of words and the year ----\nsotu_words_df |&gt;\n  count(year) |&gt;\n  summarize(cor = cor.test(year, n)$estimate)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.342\n\n\nSo, even though we are working to inspect our data, we already have a finding. The number of words increases over time, despite the fact that the relationship is not a strong one.\nNow, let’s turn our attention to the frequency of individual words. Let’s start by looking at the most frequent words for the entire corpus. We can do this by grouping by the word variable and then summarizing the number of times each word occurs. We will then arrange the data in descending order by the number of times each word occurs.\n\n# Get the most frequent words ------------------------------------\n\nsotu_words_df |&gt;\n  count(word, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\n# A tibble: 10 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the   21565\n 2 and   14433\n 3 to    13679\n 4 of    13087\n 5 in     8116\n 6 we     7327\n 7 a      7137\n 8 our    6822\n 9 that   5391\n10 for    4513\n\n\nThe usual suspects are at the top of the list. This is not surprising, and will likely be the case for most corpora, and sizeable subcorpora –in our case the time periods. Let’s address this.\nWe could use a stopwords list to just eliminate many of these common words, but that might be a bit too agressive and we will likely lose some words that we want to keep. Considering a more nuanced approach, we will use the \\(tf\\)-\\(idf\\) transformation to attenuate the effect of words that are common across all time periods, and on the flip side, to promote the effect of words that are more distinctive to each time period.\nIn order to do this, we will need to calculate the \\(tf\\)-\\(idf\\) for each word in each time period. To keep things simple, we will calculate the \\(tf\\)-\\(idf\\) for each word in each decade. We will do this by creating a new variable called decade that is the year rounded down to the nearest decade. Then we can group by this decade variable and then count the number of times each word occurs. We will then calculate the \\(tf\\)-\\(idf\\) for each word in each decade using the bind_tf_idf() function from {tidytext}. We will then arrange the data in descending order by the \\(tf\\)-\\(idf\\) value.\n\n# Get the tf-idf of words by decade ------------------------------\n\nsotu_words_tfidf_df &lt;-\n  sotu_words_df |&gt;\n  mutate(decade = floor(year / 10) * 10) |&gt;\n  count(decade, word) |&gt;\n  bind_tf_idf(word, decade, n) |&gt;\n  arrange(decade, desc(tf_idf))\n\n# Preview\nsotu_words_tfidf_df\n\n# A tibble: 40,073 × 6\n   decade word               n       tf   idf   tf_idf\n    &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   1940 boycotts           5 0.000344 2.20  0.000756\n 2   1940 jurisdictional     7 0.000482 1.50  0.000725\n 3   1940 interpretation     4 0.000275 2.20  0.000605\n 4   1940 unjustified        4 0.000275 2.20  0.000605\n 5   1940 insecurity         5 0.000344 1.50  0.000518\n 6   1940 arbitration        3 0.000207 2.20  0.000454\n 7   1940 output             6 0.000413 1.10  0.000454\n 8   1940 unjustifiable      3 0.000207 2.20  0.000454\n 9   1940 management        25 0.00172  0.251 0.000433\n10   1940 rental             4 0.000275 1.50  0.000414\n# ℹ 40,063 more rows\n\n\nOK. Even the preview shows that we are getting a more interesting list of words.\nLet’s look at the top 10 words for each decade. We group by decade and then slice the top 10 words by \\(tf\\)-\\(idf\\) value with slice_max(). Then we will use the reorder_within() function from {tidytext} to reorder the words within each facet by the \\(tf\\)-\\(idf\\) value.\nWe will visualize this with a bar chart where word is on the x-axis and the height of the bar is the \\(tf\\)-\\(idf\\) value. We will also facet the plot by decade. I’ve flipped the coordinates so that the words are on the y-axis and the bars are horizontal. This is a personal preference, but I find it easier to read the words this way.\n\nsotu_words_tfidf_df |&gt;\n  group_by(decade) |&gt;\n  slice_max(n = 10, tf_idf) |&gt;\n  ungroup() |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  mutate(word = reorder_within(word, desc(tf_idf), decade)) |&gt;\n  ggplot(aes(word, tf_idf, fill = decade)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~decade, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 3: Visualize the top 10 words by decade\n\n\n\n\n\nScanning the top words for the decades we can see some words that signal contemporary issues. This is a hint that we are picking up on some of the changes in the language of the SOTU over time.\n\n\n\n\n\n\n Dive deeper\nThe plot above makes use of the reorder_within() and scale_x_reordered() functions from {tidytext}. These functions allow us to reorder the words within each facet by the \\(tf\\)-\\(idf\\) value. This is a nice way to visualize the most distinctive words for each decade. These are more advanced functions. If you are interested in learning more about them, you can read more about them in the help documentation ?tidytext::reorder_within\n\n\n\nTo my eye, however, the 1940s and the 2020s don’t seem to jump out at me in the same way. Let’s take a closer look at the 1940s and the 2020s in our original dataset.\n\nsotu_df |&gt;\n  filter(year &lt; 1950 | year &gt;= 2020)\n\n# A tibble: 4 × 4\n  president  year party      address                                            \n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                                              \n1 Truman     1947 Democratic \"Mr. President, Mr. Speaker, Members of the Congre…\n2 Truman     1948 Democratic \"Mr. President, Mr. Speaker, and Members of the 80…\n3 Truman     1949 Democratic \"Mr. President, Mr. Speaker, Members of the Congre…\n4 Trump      2020 Republican \"Madam Speaker, Mr. Vice President, Members of Con…\n\n\nWell, that explains it. There are only 3 addresses in the 1940s and 1 address in the 2020s. This is not enough data to get a good sense of the language of the SOTU for these decades. Let’s remove these decades from our original dataset as not being representative of the language of the SOTU.\nAnother consideration that catches my eye in looking at the top words by decade is that our words like “communist” and “communists” are being counted separately. That is fine, but what if we want to count these as the same word? We can do this by lemmatizing the words –that is reducing the words to their root form. We can do this using the lemmatize_words() function from {textstem}.\nSo consider this example:\n\n# Lemmatize the words --------------------------------------------\nwords_chr &lt;- c(\"freedom\", \"free\", \"frees\", \"freeing\", \"freed\")\ntextstem::lemmatize_words(words_chr)\n\n[1] \"freedom\" \"free\"    \"free\"    \"free\"    \"free\"   \n\n\n\n\n\n\n\n\n Dive deeper\nBy default, the lemmatize_words() function uses a lookup table for English to lemmatize words. This is a simple approach that works well for many cases. However, it is not perfect. For example, it will not lemmatize words that are not in the lookup table.\nIf you want to lemmatize words that are not in the lookup table, or you want to lemmatize words in another language, you can create or add to a lookup table. You can read more about this in the help documentation ?textstem::lemmatize_words().\nA resource for lemma lookup tables can be found here https://github.com/michmech/lemmatization-lists.\n\n\n\nWith these considerations in mind, let’s update our sotu_df dataset to remove the 1940s and 2020s, tokenize and lemmatize the words, and add a decade variable.\n\n# Update the dataset ----------------------------------------------\nsotu_terms_df &lt;-\n  sotu_df |&gt;\n  filter(year &gt;= 1950 & year &lt; 2020) |&gt; # Remove the 1940s and 2020s\n  unnest_tokens(word, address, strip_numeric = TRUE) |&gt; # Tokenize the words\n  mutate(lemma = textstem::lemmatize_words(word)) |&gt; # Lemmatize the words\n  mutate(decade = floor(year / 10) * 10) |&gt; # Add a decade variable\n  select(president, decade, year, party, word, lemma) #\n\n# Preview\nsotu_terms_df\n\n# A tibble: 368,586 × 6\n   president decade  year party      word      lemma    \n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n 1 Truman      1950  1950 Democratic mr        mr       \n 2 Truman      1950  1950 Democratic president president\n 3 Truman      1950  1950 Democratic mr        mr       \n 4 Truman      1950  1950 Democratic speaker   speaker  \n 5 Truman      1950  1950 Democratic members   member   \n 6 Truman      1950  1950 Democratic of        of       \n 7 Truman      1950  1950 Democratic the       the      \n 8 Truman      1950  1950 Democratic congress  congress \n 9 Truman      1950  1950 Democratic a         a        \n10 Truman      1950  1950 Democratic year      year     \n# ℹ 368,576 more rows\n\n\nThis inspection process could go on for a while. We could continue to inspect the data and make changes to the dataset but it is often the case that in the process of analysis we will often run into issues that require us to go back and make changes to the dataset. So we will move on to the next step in the exploratory analysis workflow.\n\n\nInterrogate\nNow that we have our data in a tidy format, we can move on to the next step in the exploratory analysis workflow: interrogating the data. We will submit the selected variables to descriptive or unsupervised learning methods to provide quantitative measures to evaluate.\n\nFrequency\n\nWhat are the most frequent words across time periods?\n\nWe have already made some progress on this question in the inspection phase, but now we can do it again with the updated dataset.\n\n# Get the most frequent lemmas by decade --------------------------\n\nsotu_lemmas_tfidf_df &lt;-\n  sotu_terms_df |&gt;\n  count(decade, lemma) |&gt; # Count the lemmas by decade\n  bind_tf_idf(lemma, decade, n) |&gt; # Calculate the tf-idf\n  arrange(decade, desc(tf_idf))\n\n# Preview\nsotu_lemmas_tfidf_df\n\n# A tibble: 26,435 × 6\n   decade lemma            n       tf   idf   tf_idf\n    &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1   1950 armament        16 0.000321 1.95  0.000625\n 2   1950 imperialism      9 0.000181 1.95  0.000351\n 3   1950 shall          107 0.00215  0.154 0.000331\n 4   1950 disarmament     13 0.000261 1.25  0.000327\n 5   1950 mineral          8 0.000160 1.95  0.000312\n 6   1950 mobilization    12 0.000241 1.25  0.000302\n 7   1950 survivor        12 0.000241 1.25  0.000302\n 8   1950 expenditure     42 0.000842 0.336 0.000283\n 9   1950 adequate        25 0.000501 0.560 0.000281\n10   1950 constantly      11 0.000221 1.25  0.000276\n# ℹ 26,425 more rows\n\n\nNow we can visualize the top 10 lemmas for each decade, as we did above, but for the lemmas instead of the words and for seven full decades.\n\nsotu_lemmas_tfidf_df |&gt;\n  group_by(decade) |&gt;\n  slice_max(n = 10, tf_idf) |&gt;\n  ungroup() |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  mutate(lemma = reorder_within(lemma, desc(tf_idf), decade)) |&gt;\n  ggplot(aes(lemma, tf_idf, fill = decade)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~decade, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 4: Visualize the top 10 lemmas by decade\n\n\n\n\n\n\n\nDistribution\n\nHow does the distribution of words change across time periods?\n\nOK. Now let’s focus on word frequency distributions over time. We will return to the sotu_terms_df.\n\nsotu_terms_df\n\n# A tibble: 368,586 × 6\n   president decade  year party      word      lemma    \n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n 1 Truman      1950  1950 Democratic mr        mr       \n 2 Truman      1950  1950 Democratic president president\n 3 Truman      1950  1950 Democratic mr        mr       \n 4 Truman      1950  1950 Democratic speaker   speaker  \n 5 Truman      1950  1950 Democratic members   member   \n 6 Truman      1950  1950 Democratic of        of       \n 7 Truman      1950  1950 Democratic the       the      \n 8 Truman      1950  1950 Democratic congress  congress \n 9 Truman      1950  1950 Democratic a         a        \n10 Truman      1950  1950 Democratic year      year     \n# ℹ 368,576 more rows\n\n\nWe want to get a sense of how the word distributions change over time. We need to calculate the frequency of words for each year, first off. So we need to go back to the sotu_terms_df dataset and group by year and word and then count the number of times each word occurs.\nSince we will lose the lemma variable in this process, we will add it back after the \\(tf\\)-\\(idf\\) transformation by using the mutate() function and the textstem::lemmatize_words() function.\n\nsotu_terms_tfidf_df &lt;-\n  sotu_terms_df |&gt;\n  count(year, word) |&gt; # Count the words by year\n  bind_tf_idf(word, year, n) |&gt; # Calculate the tf-idf\n  mutate(lemma = textstem::lemmatize_words(word)) |&gt; # Lemmatize the words\n  arrange(year, desc(tf_idf))\n\n# Preview\nsotu_terms_tfidf_df\n\n# A tibble: 97,213 × 7\n    year word               n       tf   idf  tf_idf lemma         \n   &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         \n 1  1950 enjoyment          3 0.000585 3.54  0.00207 enjoyment     \n 2  1950 rent               3 0.000585 2.85  0.00167 rend          \n 3  1950 widespread         3 0.000585 2.85  0.00167 widespread    \n 4  1950 underdeveloped     2 0.000390 4.23  0.00165 underdeveloped\n 5  1950 ideals             7 0.00136  1.14  0.00156 ideal         \n 6  1950 transmit           3 0.000585 2.62  0.00153 transmit      \n 7  1950 peoples            8 0.00156  0.976 0.00152 people        \n 8  1950 democratic        12 0.00234  0.623 0.00146 democratic    \n 9  1950 expenditures       7 0.00136  1.06  0.00144 expenditure   \n10  1950 businessmen        3 0.000585 2.44  0.00143 businessman   \n# ℹ 97,203 more rows\n\n\nWith this format, we can visualize the distinctiveness of words over time. All we need to do is to filter the data to the lemmas we are interested first.\nLet’s just start with some random poltical-oriented words.\n\nplot_terms &lt;- c(\"crime\", \"law\", \"free\", \"terror\", \"family\", \"government\")\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.25) +\n  facet_wrap(~lemma, scales = \"free_y\")\n\n\n\n\n\n\n\nFigure 5: Distictiveness of political words over time\n\n\n\n\n\nWe can see in Figure 5 that the distinctiveness of these lemmas varies over time. Now, in this plot I’ve used a small span value for the geom_smooth() function to get a sense of the more fine-grained changes over time. However, this is may be too fine-grained. We can adjust this by increasing the span value. Let’s try a span value of 0.5.\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  facet_wrap(~lemma, scales = \"free_y\")\n\n\n\n\n\n\n\nFigure 6: Distinctiveness of political words over time\n\n\n\n\n\nFigure @ref(fig:sotu-terms-tfidf-df-political-smooth-detail) seems to be picking up on some of the more general word usage trends over time.\nAnother thing to note about the way we plotted the data is that we used the facet_wrap() function to create a separate plot for each word but we used the scales = \"free_y\" allowing the y-axis to vary for each plot. This means we are not comparing the y-axis values across plots and thus can not say anything about the differing magnitudes from a visual inspection.\nTo address this, we can remove the scales = \"free_y\" argument and use the default which will fix the x- and y-axis scales across plots.\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  facet_wrap(~lemma)\n\n\n\n\n\n\n\nFigure 7: Distinctiveness of political words over time\n\n\n\n\n\nIn Figure 7, we can clearly see that the magnitude of the words “crime” and “terror” are much higher than the other words we happend to select. Furthermore, these words have interesting patterns. In particular, “terror” has two peaks on around 1980 and another around the turn of the century. “Crime” also has two distinctive peaks on in the 1970s and one in the 1990s.\nThe terms that I selected before were somewhat arbitrary. How can I identify the words that have changed the most drastically over these years from the data itself? We can do this by creating a term-document matrix with and then calculating the standard deviation of the \\(tf\\)-\\(idf\\) values for each word. This will give us a sense of the words that have changed the most over time.\n\n# Create TDM with words-year and tf-idf values\nsotu_word_tfidf_mat &lt;-\n  sotu_terms_tfidf_df |&gt;\n  cast_tdm(word, year, tf_idf) |&gt;\n  as.matrix()\n\n# Preview\ndim(sotu_word_tfidf_mat)\n\n[1] 13109    69\n\nsotu_word_tfidf_mat[1:5, 1:5]\n\n                Docs\nTerms               1950    1951     1952 1953 1954\n  enjoyment      0.00207 0.00000 0.000661    0    0\n  rent           0.00167 0.00000 0.000000    0    0\n  widespread     0.00167 0.00000 0.000000    0    0\n  underdeveloped 0.00165 0.00000 0.000000    0    0\n  ideals         0.00156 0.00171 0.000853    0    0\n\n\nTo calculate the standard deviation of the \\(tf\\)-\\(idf\\) values for each word, we can use the apply() function to iterate over each row of the matrix and calculate the standard deviation of the values in each row. You can think of the apply() function as a cousin of the map() function. The apply() function iterates over the rows or columns of a matrix or data frame and applies a function to each row or column. We choose whether the function is applied to the rows or columns with the MARGIN argument. We can set MARGIN = 1 to apply the function to the rows and MARGIN = 2 to apply the function to the columns.\n\n# Calculate the standard deviation of the tf-idf values for each word\nsotu_words_sd &lt;-\n  apply(sotu_word_tfidf_mat, MARGIN = 1, FUN = sd, na.rm = TRUE)\n\n# Preview seed words\nsotu_words_sd |&gt;\n  sort(decreasing = TRUE) |&gt;\n  head(100)\n\n     vietnam      hussein       saddam         salt         iraq        shall \n    0.001188     0.001138     0.001121     0.001067     0.000976     0.000887 \n         oil        iraqi       that's   inspectors        qaida       terror \n    0.000798     0.000786     0.000733     0.000715     0.000702     0.000662 \n  terrorists         it's        crude       soviet    terrorist     activity \n    0.000651     0.000647     0.000635     0.000633     0.000632     0.000630 \n    covenant arrangements      session           al  disarmament       steven \n    0.000611     0.000609     0.000596     0.000596     0.000589     0.000588 \n     wartime      barrels         isil    mentioned        ought        elvin \n    0.000573     0.000569     0.000567     0.000559     0.000558     0.000556 \n      iraqis         ryan        we're         kids        let's          gun \n    0.000551     0.000539     0.000539     0.000535     0.000533     0.000532 \n     rebekah           cj        camps      empower         cory        we've \n    0.000531     0.000527     0.000524     0.000523     0.000522     0.000518 \n         92d  afghanistan    childcare        100th        music         gulf \n    0.000516     0.000514     0.000513     0.000507     0.000500     0.000494 \n        21st   extremists   foundation      picture     beguiled  contemplate \n    0.000491     0.000489     0.000487     0.000486     0.000473     0.000473 \n   enumerate    hurriedly   hysterical  ingredients        smile       smiles \n    0.000473     0.000473     0.000473     0.000473     0.000473     0.000473 \n    wagehour     josefina        shi'a        alice     alliance    seventies \n    0.000473     0.000471     0.000468     0.000468     0.000467     0.000464 \n      herman       joshua      matthew        julie         11th       border \n    0.000463     0.000463     0.000463     0.000462     0.000461     0.000459 \n     persian    recommend    communist        mayor    nicaragua       planes \n    0.000456     0.000453     0.000447     0.000443     0.000442     0.000442 \n      trevor        corey       kenton      preston        seong     property \n    0.000440     0.000439     0.000439     0.000439     0.000439     0.000439 \n     regimes       disarm      mention    peacetime   localities    objective \n    0.000437     0.000436     0.000436     0.000433     0.000429     0.000428 \n         i'm       surtax        banks       pounds       foster          she \n    0.000424     0.000423     0.000423     0.000422     0.000421     0.000418 \n        isis  sandinistas  discharging         gold \n    0.000417     0.000416     0.000416     0.000415 \n\n\nNow we can choose from the top words that have changed the most over time. Here’s another selection of words based on the standard deviation of the \\(tf\\)-\\(idf\\) values.\n\nplot_terms &lt;- c(\"equality\", \"right\", \"community\", \"child\", \"woman\", \"man\")\n\nsotu_terms_tfidf_df |&gt;\n  filter(lemma %in% plot_terms) |&gt;\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~lemma)\n\n\n\n\n\n\n\nFigure 8: Distinctiveness of words over time\n\n\n\n\n\nWe can see that the words I selected based on the standard deviation, in Figure 8, can either increase, decrease, or fluctuate over time.\n\n\nMeaning\n\nHow does the meaning of words change across time periods?\n\nFor this approach we need to turn to word embeddings. Word embeddings have been show to capture distributional semantics –that is the meaning of words based on their distribution in a corpus (Hamilton, Leskovec, and Jurafsky 2016).\nSince our research question is aimed a change over time we are performing a diachronic analysis. This means that we will need to create word embeddings for each time period, identify the common vocabulary across time periods, and then align the word embeddings to a common space before we can compare them.\nLet’s load some packages that we will need.\n\nlibrary(fs) # for file system functions\nlibrary(PsychWordVec) # for working with word embeddings\nlibrary(purrr) # for iterating over lists\n\nLet’s first start by creating sub-corpora for each decade. We will write these to disk so that we can use them again if necessary. Note that data or datasets that are generated in the process of analysis are often stored in an analysis/ folder inside of the main data folder to keep them separate from the other data acquired or derived in the project.\nWe will use the str_c() function to summarize the words for each address into a single string by decade. We will then use the write_lines() function to write the string to a text file. The pwalk() function from {purrr} is a convenient way to iterate over multiple arguments (decade and address in this case) in a function without returning a value to the console. The p in pwalk() stands for “parallel” and indicates that the function will iterate over the arguments in parallel.\n# Create sub-corpora for each decade and write to disk ------------\nsotu_terms_df |&gt;\n  summarize(address = str_c(lemma, collapse = \" \"), .by = decade) |&gt;\n  select(decade, address) |&gt;\n  pwalk(\\(decade, address) {\n    file_name &lt;- str_c(\"../data/analysis/sotu/\", decade, \"s.txt\")\n    write_lines(address, file_name)\n  })\nNow I have written a function to read the text files, train the word embeddings for each, write the word embeddings to disk, and then load them back as VectorSpaceModel objects in a list.\n\n\n\n\n\n\n Tip\nA note on the training of the word embeddings. There are two main approaches to training word embeddings: Continuous Bag of Words (CBOW) and Skip-gram. CBOW is better for more common words and larger datasets. Skip-gram is better for less common words and smaller datasets. Given the varying sizes of the sub-corpora, Skip-gram might be more suitable as it may capture more nuances in less frequent words. Furthermore, the number of dimensions is a hyperparameter that needs to be tuned. The default is 50, but I have chosen 100 as we are attempting to capture more nuanced changes in the language of the SOTU.\n\n\n\n\ncreate_embeddings &lt;- function(dir_path, dims = 100) {\n  # Get the text file paths\n  txt_files &lt;- dir_ls(dir_path, regexp = \"\\\\.txt$\")\n  # Train the word embeddings\n  models &lt;-\n    txt_files |&gt;\n    map(\\(file) {\n      train_wordvec(\n        text = file,\n        dims = 100,\n        normalize = TRUE\n      )\n    })\n  # Modify the list names\n  names(models) &lt;-\n    names(models) |&gt;\n    basename() |&gt;\n    str_remove(\"\\\\.txt\")\n\n  # Convert to embed matrices\n  models &lt;- map(models, as_embed)\n\n  return(models)\n}\n\nWe can now apply the create_embeddings() custom function to the decade sub-corpora text files in ../analysis/sotu/.\nsotu_vec_mods &lt;- create_embeddings(\"../analysis/sotu/\")\nEach word embedding model is a matrix with the words as the rows and the dimensions as the columns as an element of the sotu_vec_mods list. Each of these elements have the name of the decade. We can extract an element from a list using the pluck() function from {purrr}.\n\n# Extract an element from a list\nsotu_vec_mods |&gt; pluck(\"1950s\")\n\n                        dim1 ...     dim100\n   1: the             0.2910 ... &lt;100 dims&gt;\n   2: of              0.2229 ... &lt;100 dims&gt;\n   3: and             0.2818 ... &lt;100 dims&gt;\n   4: be              0.1370 ... &lt;100 dims&gt;\n   5: to              0.2336 ... &lt;100 dims&gt;\n-----                                      \n1164: appear          0.2471 ... &lt;100 dims&gt;\n1165: allegiance      0.2689 ... &lt;100 dims&gt;\n1166: accumulate      0.2158 ... &lt;100 dims&gt;\n1167: accomplishment  0.2241 ... &lt;100 dims&gt;\n1168: accept          0.2440 ... &lt;100 dims&gt;\n\n\n\n\n\n\n\n\n Tip\nSince we have our models in a list, we will be using {purrr} functions quite a bit. Here’s a quick summary of some of the {purrr} functions we will be using.\n\nmap() iterates over a list and applies a function to each element of the list. It returns a list.\nwalk() iterates over a list and applies a function to each element of the list. It does not return a value.\n\nEach of these has a parallel version, pmap() and pwalk(), which iterate over multiple lists in parallel, and a version that iterates over named lists, imap() and iwalk(). The p in pmap() and pwalk() stands for “parallel” and indicates that the function will iterate over the arguments in parallel. The i in imap() and iwalk() stands for “indexed” and indicates that the function will iterate over the arguments in parallel and return the index of the list element.\n\n\n\nThese embeddings can be explored in a number of ways. For example, we can get the words closest to a given word in the vector space for each decade. {PsychWordVec} has a function, most_similar(), that will do this for us. We can use the map() function to iterate over each model in the list and get the words with the most similar vectors. We will then use the str_c() function to summarize the words into a single string for each decade.\n\n# Get the closest words\nsotu_vec_mods |&gt;\n  map(\\(mod) {\n    most_similar(mod, \"freedom\", verbose = FALSE) # get words similar to \"freedom\"\n  }) |&gt;\n  map(\\(res) {\n    str_c(res$word, collapse = \", \") # summarize the words into a single string\n  })\n\n$`1950s`\n[1] \"justice, peace, man, not, spirit, ideal, peaceful, fight, us, preserve\"\n\n$`1960s`\n[1] \"free, unity, berlin, asia, independence, course, communist, nation, europe, both\"\n\n$`1970s`\n[1] \"defend, close, destroy, found, shape, side, deter, honor, root, international\"\n\n$`1980s`\n[1] \"democracy, peace, struggle, defend, everywhere, democratic, secure, country, fighter, free\"\n\n$`1990s`\n[1] \"liberty, define, all, lead, perfect, share, force, communism, shape, latin\"\n\n$`2000s`\n[1] \"east, democracy, friend, liberty, woman, determine, military, middle, region, remain\"\n\n$`2010s`\n[1] \"heart, vision, remind, hero, hopeful, dignity, capitol, celebrate, journey, honor\"\n\n\nThe previous example shows that the closest words to “freedom” in each decade in a synchronic manner. We can inspect these synchronic changes and draw conclusions from them. However, we are interested in diachronic changes. To do this, we will need to align the word embeddings to a common space.\nNow we will identify the common words across the decades and subset the word embeddings to the common vocabulary. The map() function iterates over each model in the list and returns the rownames with rownames() (words) for each model. The reduce() function then iterates over the list of words and returns the intersection of the words across the models with intersect().\n\n# Extract the common vocabulary -----------------------------------\ncommon_vocab &lt;-\n  sotu_vec_mods |&gt;\n  map(rownames) |&gt;\n  reduce(intersect)\n\nlength(common_vocab)\n\n[1] 534\n\nhead(common_vocab)\n\n[1] \"the\" \"of\"  \"and\" \"be\"  \"to\"  \"in\" \n\n\nThere are 534 words in the common vocabulary. Now we can subset the word embeddings to the common vocabulary. We will use the map() function to iterate over each model in the list and subset the model to the common vocabulary using the common_vocab variable as part of the bracket notation subset.\n\n# Subset the models to the common vocabulary ----------------------\nsotu_vec_common_mods &lt;-\n  sotu_vec_mods |&gt;\n  map(\\(mod) {\n    mod[common_vocab, ] # subset each model to the common vocabulary\n  })\n\nsotu_vec_common_mods |&gt; pluck(\"1950s\")\n\n                   dim1 ...     dim100\n  1: the         0.2910 ... &lt;100 dims&gt;\n  2: of          0.2229 ... &lt;100 dims&gt;\n  3: and         0.2818 ... &lt;100 dims&gt;\n  4: be          0.1370 ... &lt;100 dims&gt;\n  5: to          0.2336 ... &lt;100 dims&gt;\n----                                  \n530: happen      0.2186 ... &lt;100 dims&gt;\n531: everything  0.2229 ... &lt;100 dims&gt;\n532: different   0.2373 ... &lt;100 dims&gt;\n533: debate      0.2244 ... &lt;100 dims&gt;\n534: city        0.2320 ... &lt;100 dims&gt;\n\nsotu_vec_common_mods |&gt; pluck(\"2010s\")\n\n                   dim1 ...     dim100\n  1: the        -0.1464 ... &lt;100 dims&gt;\n  2: of         -0.1187 ... &lt;100 dims&gt;\n  3: and        -0.1090 ... &lt;100 dims&gt;\n  4: be         -0.1162 ... &lt;100 dims&gt;\n  5: to         -0.0842 ... &lt;100 dims&gt;\n----                                  \n530: happen     -0.1037 ... &lt;100 dims&gt;\n531: everything -0.0982 ... &lt;100 dims&gt;\n532: different  -0.1032 ... &lt;100 dims&gt;\n533: debate     -0.1111 ... &lt;100 dims&gt;\n534: city       -0.1179 ... &lt;100 dims&gt;\n\n\nSo now each of the models in the list has the same vocabulary and the same number of dimensions, 100.\nNow we can align the models to a common space. The reason that the models need to be aligned is that the word embeddings are trained on different corpora. This means that the words will be represented in different spaces. We will use the Orthogonal Procrustes solution to align the models to a common coordinate space. {PsychWordVec} (Bao 2023) has a function, orth_procrustes(), that will do this for us. In the process of aligning the models, the models are converted to plain matrices so we will need to convert them back to embed matrix objects.\n\n# Align the models to a common space\nsotu_aligned_mods &lt;-\n  sotu_vec_common_mods |&gt;\n  map(\\(mod) {\n    orth_procrustes(sotu_vec_common_mods[[1]], mod) # align to the first model\n  }) |&gt;\n  map(\\(mod) {\n    emb &lt;- as_embed(mod) # convert to a embed matrix object\n    emb\n  })\n\nHaving a model for each decade which is aligned in vocabulary and space, we can now use these models to compare words across time. There are a number of ways we can compare words across time.\nIn our first approach, let’s consider the semantic displacement of words over time in the vector space. We will do this by calculating the cosine difference between the word embeddings for a word in each decade. Once we have the cosine difference, we can visualize the change over time.\n\n# Calculate the cosine difference between the models --------------\nword &lt;- \"freedom\"\n\nword_vectors &lt;-\n  sotu_aligned_mods |&gt;\n  map(\\(mod) {\n    mod[word, ]\n  })\n\ndifferences &lt;-\n  word_vectors |&gt;\n  map(\\(vec) {\n    cos_dist(vec, word_vectors[[1]])\n  })\n\ndifferences\n\n$`1950s`\n[1] 0\n\n$`1960s`\n[1] 0.0236\n\n$`1970s`\n[1] 0.0437\n\n$`1980s`\n[1] 0.0354\n\n$`1990s`\n[1] 0.0203\n\n$`2000s`\n[1] 0.0207\n\n$`2010s`\n[1] 0.0333\n\n\nWe now have a list with the cosine difference for each decade compared to the first decade (“1950s”). We can visualize this with a line plot where the x-axis is the decade and the y-axis is the cosine difference. We will add a linear trend line to the plot to get a sense of the overall trend.\nI’ll write a function to get the differences for a word and then return a data frame with the decade and the difference. This will make it easier to visualize the differences for multiple words.\n\n# Function to get the cosine difference over time\n\nget_cosine_diff &lt;- function(word, models) {\n  word_vectors &lt;- map(models, \\(mod) {\n    mod[word, ]\n  })\n\n  differences &lt;- map(word_vectors, \\(vec) {\n    cos_dist(vec, word_vectors[[1]])\n  })\n\n  tibble(word, decade = basename(names(differences)), difference = unlist(differences))\n}\n\nget_cosine_diff(word = \"freedom\", models = sotu_aligned_mods)\n\n# A tibble: 7 × 3\n  word    decade difference\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n1 freedom 1950s      0     \n2 freedom 1960s      0.0236\n3 freedom 1970s      0.0437\n4 freedom 1980s      0.0354\n5 freedom 1990s      0.0203\n6 freedom 2000s      0.0207\n7 freedom 2010s      0.0333\n\n\nLet’s create a function which performs this for us and can plot multiple words at the same time.\n\nplot_words &lt;-\n  c(\"freedom\", \"nation\", \"country\", \"america\")\n\nplot_words |&gt;\n  map(get_cosine_diff, models = sotu_aligned_mods) |&gt;\n  bind_rows() |&gt;\n  arrange(decade) |&gt;\n  ggplot(aes(decade, difference, group = word, color = word)) +\n  geom_smooth(se = FALSE, span = 1) +\n  labs(title = word)\n\n\n\n\n\n\n\nFigure 9\n\n\n\n\n\nWe can then focus in on a particular word and the nearest words to that word in each decade. We will use the map() function to iterate over each model in the list and get the closest words to a word. We will then use the str_c() function to summarize the words into a single string for each decade.\n\n# Function to get the closest words to a word ---------------------\nword &lt;- \"america\"\n\nsotu_vec_common_mods |&gt;\n  map(\\(mod) {\n    most_similar(mod, word, verbose = FALSE)\n  }) |&gt;\n  map(\\(mod) {\n    str_c(mod$word, collapse = \", \")\n  }) |&gt;\n  enframe(name = \"decade\", value = \"words\") |&gt;\n  unnest(words)\n\n# A tibble: 7 × 2\n  decade words                                                                  \n  &lt;chr&gt;  &lt;chr&gt;                                                                  \n1 1950s  people, upon, life, democracy, standard, american, fail, within, deep,…\n2 1960s  leadership, agency, whole, serious, basic, historic, respect, responsi…\n3 1970s  where, liberty, determine, future, faith, happen, always, achieve, tru…\n4 1980s  never, great, let, ready, nation, faith, future, we, hope, struggle    \n5 1990s  liberty, freedom, important, duty, moment, arm, ahead, great, always, …\n6 2000s  remember, common, moment, first, rest, sense, beyond, share, reach, ch…\n7 2010s  strong, state, once, may, believe, union, your, among, citizen, forward\n\n\nAnother approach is to visualize the vector space that words occupy over time. To do this we will collapse the word embeddings for each decade into a single matrix. We will append the decade to each word as not to lose the decade information. The we will extract the first two principal components of the matrix, so that we can visualize the data in two dimensions. We will then plot the data with a scatter plot where the x-axis is the first principal component and the y-axis is the second principal component. We will label the points with the words.\n\n# Visualize the vector space of words over time -------------------\nsotu_joined_mods &lt;-\n  sotu_aligned_mods |&gt;\n  imap(\\(mod, index) {\n    rownames(mod) &lt;- str_c(rownames(mod), \"_\", index)\n    mod\n  }) |&gt;\n  reduce(rbind) |&gt;\n  as_embed()\n\nPCA on the word embeddings, yes! With the aligned models the results are much more sensible and interesting. The individual words are grouped closer together across time, in general, but there are exceptions.\n\nsotu_joined_pca &lt;-\n  sotu_joined_mods |&gt;\n  scale() |&gt;\n  prcomp()\n\nsotu_pca_df &lt;-\n  as_tibble(sotu_joined_pca$x[, 1:2]) |&gt;\n  mutate(word = names(sotu_joined_pca$x[, 1]))\n\nsotu_pca_df |&gt;\n  filter(str_detect(word, \"^(nation|country|america)_\")) |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = word)) +\n  geom_point() +\n  ggrepel::geom_text_repel()\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\nThese kinds of visualizations can be very useful for exploring the data and drawing conclusions."
  },
  {
    "objectID": "recipes/recipe-08/index.html#summary",
    "href": "recipes/recipe-08/index.html#summary",
    "title": "08. Employing exploratory methods",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we have explored the State of the Union addresses from 1950 to 2019. We have used a number of tools and techniques to explore the data and draw conclusions. We have used {tidytext} to tokenize and lemmatize the words in the addresses. We have used {word2vec} to train word embeddings for each decade. We have used {PsychWordVec} to align the word embeddings to a common space. We have used {wordVectors} to explore the word embeddings. We have used {ggplot2} to visualize the data.\nThese strategies, and others, can be used to explore these questions or other questions in more depth. Exploratory analysis is where your creativity and curiosity can shine."
  },
  {
    "objectID": "recipes/recipe-08/index.html#check-your-understanding",
    "href": "recipes/recipe-08/index.html#check-your-understanding",
    "title": "08. Employing exploratory methods",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\nIn text analysis,  is used to transform the effect of words that are common across all time periods and promote the effect of words that are more distinctive to each time period.\nWhat is the correct method to use when adding a linear trend line to a plot in ggplot2? geom_smooth(method = ‘lm’)geom_line()geom_bar()geom_point()\nTRUEFALSE When working with lists, walk() is like map() but does not return a value.\nTRUEFALSE When creating word embeddings, the CBOW model is better suited for less common words and smaller datasets compared to Skip-gram.\nThe process of reducing the number of features in a dataset while retaining as much information as possible is known as  reduction."
  },
  {
    "objectID": "recipes/recipe-08/index.html#lab-preparation",
    "href": "recipes/recipe-08/index.html#lab-preparation",
    "title": "08. Employing exploratory methods",
    "section": "Lab preparation",
    "text": "Lab preparation\n\nIn preparation for Lab 8, review and and ensure that you are familiar with the following concepts:\n\nTokenizing text\nGenerating frequency and dispersion measures\nCreating Term-Document Matrices\nUsing {purrr} to iterate over lists\nVisualizations with {ggplot2}\n\nIn this lab, you will have the opportunity to apply the concepts from the materials in this chapter to a new dataset. You should consider the dataset and the questions that you want to ask of the data. You should also consider the tools and techniques that you will use to explore the data and draw conclusions. You will be asked to submit your code and a brief report of your findings."
  },
  {
    "objectID": "instructors.html#materials",
    "href": "instructors.html#materials",
    "title": "Instructors",
    "section": "Materials",
    "text": "Materials\n\nSlide decks\n\n\nExercises"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting started",
    "section": "",
    "text": "The primary resources available for the textbook include the following:\n\n\n\nTable 1: Resources Kit for the textbook\n\n\n\n\n\n\n\n\n\n\nResource\nDescription\nLocation\n\n\n\n\nLessons\nA series of interactive R programming lessons using {swirl} that accompany the textbook.\nGitHub\n\n\nRecipes\nA collection of coding demonstrations and examples that illustrate how to perform specific tasks in the text analysis workflow.\nResources Kit\n\n\nLabs\nA series of hands-on exercises that guide you through the process of conducting text analysis research.\nGitHub\n\n\n\n\n\n\nIn addition to these main resources, the Resources Kit also includes the following:\n\n\n\nTable 2: Additional resources in the Resources Kit\n\n\n\n\n\n\n\n\n\n\nResource\nDescription\nLocation\n\n\n\n\nGuides\nA collection of guides that provide additional information and instructions on how to use the resources available in the kit.\nResources Kit\n\n\nInstructors\nInformation for instructors who are using the textbook in their courses.\nResources Kit"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Getting started",
    "section": "",
    "text": "The primary resources available for the textbook include the following:\n\n\n\nTable 1: Resources Kit for the textbook\n\n\n\n\n\n\n\n\n\n\nResource\nDescription\nLocation\n\n\n\n\nLessons\nA series of interactive R programming lessons using {swirl} that accompany the textbook.\nGitHub\n\n\nRecipes\nA collection of coding demonstrations and examples that illustrate how to perform specific tasks in the text analysis workflow.\nResources Kit\n\n\nLabs\nA series of hands-on exercises that guide you through the process of conducting text analysis research.\nGitHub\n\n\n\n\n\n\nIn addition to these main resources, the Resources Kit also includes the following:\n\n\n\nTable 2: Additional resources in the Resources Kit\n\n\n\n\n\n\n\n\n\n\nResource\nDescription\nLocation\n\n\n\n\nGuides\nA collection of guides that provide additional information and instructions on how to use the resources available in the kit.\nResources Kit\n\n\nInstructors\nInformation for instructors who are using the textbook in their courses.\nResources Kit"
  },
  {
    "objectID": "index.html#steps-to-get-started",
    "href": "index.html#steps-to-get-started",
    "title": "Getting started",
    "section": "Steps to get started",
    "text": "Steps to get started\nTo get started with the textbook and the resources available in the kit, follow these steps:\n\nChoose and set up your preferred R environment. See the Setting up an R environment guide for a description of the possible setups and instructions on how to set up your R environment.\nInstall the key packages used in the textbook. These include {tidyverse}, {tinytex}, {swirl}, and {qtkit}. See the Installing and managing R packages guide for instructions on how to install these (and other) packages.\nLoad the interactive R programming lessons in your R environment. See the Working with the interactive R programming lessons guide for instructions on how to do this.\n\nWith these steps completed, you will be ready to start working with the textbook and the resources available in the kit. Refer back to this site for additional information and resources as needed."
  },
  {
    "objectID": "recipes/recipe-01/index.html",
    "href": "recipes/recipe-01/index.html",
    "title": "01. Academic writing with Quarto",
    "section": "",
    "text": "Skills\n\nNumbered sections\nTable of contents\nCross-referencing tables and figures\nIn-line citations and references list"
  },
  {
    "objectID": "recipes/recipe-01/index.html#concepts-and-strategies",
    "href": "recipes/recipe-01/index.html#concepts-and-strategies",
    "title": "01. Academic writing with Quarto",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\nFor many of the style components that we use in Quarto, there is a part that is addressed in the front-matter section and a part that is addressed in the prose section and/ or code block sections.\nTo refresh our memory, the front-matter is fenced by three dashes (---) and is where we set the document attributes. The prose section is where we write the text of the document. The code block section is where we write the code that will be executed and is fenced by three backticks (```) and the name of the code interpreter {r} (R for us).\n---\n1title: \"My document title\"\n2format: pdf\n---\n\n3This is the prose section.\n\n4```{r}\n#| label: example-code-block\n1 + 1\n```\n\n1\n\nThe title of the document\n\n2\n\nThe format of the document\n\n3\n\nThe prose section\n\n4\n\nThe code block section\n\n\nWith this in mind let’s look at each of these elements in turn.\n\nNumbered sections\nTo number sections in Quarto, we use the number_sections key with the value yes. This is set in the front-matter section, nested under the value for the document type to be rendered. For example, to number sections in a PDF document, we would set the number-sections key to true in the front-matter section as follows:\n---\n1title: \"My document title\"\n2format:\n3  pdf:\n4    number-sections: true\n---\n\n1\n\nThe title of the document\n\n2\n\nThe format of the document\n\n3\n\nThe type of document to be rendered, note the identation\n\n4\n\nThe key-value pair to number sections in the PDF document, again note the identation\n\n\nHeaders in the prose section are then numbered automatically. For example, the following markdown:\n# Section\n\n## Subsection\n\n### Subsubsection\n\n#### Subsubsubsection\n\n##### Subsubsubsubsection\nwould render as:\n\n\n\n\n\n\n\n\n\nWe can also control the depth of the numbering by setting the number-depth key in the front-matter section. For example, to number sections and subsections, but not subsubsections, we would set the number-depth key to 2 as follows:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n1    number-depth: 2\n---\n\n1\n\nThe key-value pair to control the depth of the numbering\n\n\nNow the first and second headers are numbered and formated but third and subsequent headers are only formatted.\nIf for some reason you want to turn off numbering for a specific header, you can add {.unnumbered} to the end of the header. For example, the following markdown:\n# Section {.unnumbered}\nThis is particularly useful in academic writing when we want to add a reference, materials, or other section that is not numbered at the end of the document.\n\n\n\n\n\n\n Warning\nNote that if you have a header that is unnumbered, the next header will be numbered as if the unnumbered header did not exist. This can have unexpected results if you have children of an unnumbered header.\n\n\n\n\n\nTable of contents\nFor longer documents including a table of contents can be a useful way to help readers navigate the document. To include a table of contents in Quarto, we use the toc key with the value true. Again, in the front-matter section, nested under the format value, as seen below:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n1    toc: true\n---\n\n1\n\nThe key-value pair to include a table of contents in the PDF document\n\n\n\n\n\n\n\n\n Tip\nFor PDF and Word document outputs, the table of contents will be automatically generated and placed at the beginning of the document. For HTML documents, the table of contents will be placed in the sidebar by default.\n\n\n\nIf if our headers are numbered, they will appeared numbered in the table of contents. If we unnnumbered a header, it will not appear with a section number. As with section numbering, we can also control the depth of the table of contents by setting the toc-depth key in the front-matter section. For example, to include sections and subsections, but not subsubsections, we would set the toc-depth key to 2 as follows:\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    toc: true\n1    toc-depth: 2\n---\n\n1\n\nThe key-value pair to control the depth of the table of contents\n\n\nAnd as with section numbering we can avoid listing a header in the table of contents by adding {.unlisted} to the end of the header.\n\n\nCross-referencing tables and figures\nAnother key element in academic writing are using cross-references to tables and figures. This allows us to refer to a table or figure by number without having to manually update the number if we add or remove a table or figure.\nIn this case, we will not need to add anything to the front-matter section. Instead, we will modify keys in the code block section of a code-generated table or figure.\nTo cross-reference a table or figure, we need to add a prefix to the label key’s value. The prefix, either tbl- or fig-, indicates whether the label is for a table or figure. Additionally, table or figure captions can be added with the tbl-cap or fig-cap keys, respectively.\nLet’s look at a basic figure that we can cross-reference. The following code block will generate a very simple scatterplot.\n```{r}\n1#| label: fig-scatterplot\n2#| fig-cap: \"A scatterplot\"\n\nplot(x = 1:10, y = 1:10)\n```\n\n3In @fig-scatterplot we see a scatterplot. ....\n\n1\n\nThe label for the figure. Includes fig- as a prefix.\n\n2\n\nThe caption for the figure.\n\n3\n\nThe in-line reference to the figure. Uses the @ symbol followed by the label value.\n\n\n\n\n\n\n\n\n\nplot(1:10, 1:10)\n\n\n\n\n\n\n\nFigure 1: A scatterplot\n\n\n\n\n\nIn Figure 1 we see a scatterplot. …\n\n\n\nFor tables generated by R, the process is very similar to that of figures. The only difference is that we use the tbl- prefix on the label value and the tbl-cap key instead of the fig-cap key for the caption.\nWe can also create tables using markdown syntax. In this case, the format is a little different. Consider Table Table 1, for example.\n| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| A        | B        | C        |\n| D        | E        | F        |\n\n: A simple table {#tbl-table-1}\n\n\n\n\n\n\n\n\n\nTable 1: A simple table\n\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\n\n\n\n\n\n\n\n\n\nIn-line citations and references list\nThe last element we will cover here is adding citations and a references list to a Quarto document. To add citations we need three things:\n\nA bibliography file\nA reference to the bibliography file in the front-matter section\nA citation in the prose section which is contained in the bibliography file\n\nThe bibliography file is a plain text file that contains the citations that we want to use in our document. The file requires the extension .bib and is formatted using the BibTeX format. BibTeX is a reference syntax that is commonly used in academia.\nLet’s take a look at a sample file, bibliography.bib, that contains a single reference.\n@Manual{R-dplyr,\n  title = {dplyr: A Grammar of Data Manipulation},\n  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},\n  year = {2023},\n  note = {R package version 1.1.4},\n  url = {https://dplyr.tidyverse.org},\n}\nIn this file, we can see that the reference is for a manual entry with @Manual. The type of entry will change what fields are relevant and/ or required. In this entry, we have the cite key R-dplyr, the title, the authors, the year, a note, and a URL. Other entries, and entry types will have different fields.\nYou can find BibTeX formatted references almost everywhere you can find scholarly work. For example, Google Scholar, Web of Science, and Scopus all provide BibTeX formatted references. Additionally, many journals provide BibTeX formatted references for the articles they publish.\n\n\n\n\n\n\n Dive deeper\nManaging your references can be a challenge if you begin to amass a large number of them. There are a number of tools that can help you manage your references. For example, Zotero is a free, open-source reference manager that can help you organize your references and generate BibTeX formatted references.\nZotero also has a browser extension that allows you to easily add references to your Zotero library from your browser.\nFurthermore, Zotero can be connected to RStudio to facilitate the incorporation of BibTeX formatted references in a Quarto document. See the RStudio documentation for more information.\n\n\n\nIn the front-matter of our Quarto document, we need to add a reference to the bibliography file. This is done using the bibliography key. For example, if our bibliography file is called bibliography.bib and is located in the same directory as our Quarto document, we would add the following to the front-matter section:\n---\ntitle: \"My document title\"\nformat: pdf\n1bibliography: bibliography.bib\n---\n\n1\n\nThe key-value pair to include a path to the file which contains the BibTeX formatted references.\n\n\nWith the bibliography file and the reference to the bibliography file in the front-matter section, we can now add citations to our document. To do this, we use the @ symbol followed by the citation key in the prose section. For example, to cite the R-dplyr reference from the bibliography.bib file, we would add @R-dplyr to the prose section as follows:\nThis is a citation to @R-dplyr.\nThe citation will appear as below in the rendered document.\n\n\n\n\n\n\nThis is a citation to Wickham et al. (2023).\n\n\n\nAnd automatically, on rendering the document, a references list will be added to the end of the document. For this reason if you have citations in your document, it is a good idea to include a header section # References at the end of your document.\n\n\n\n\n\n\n Tip\nThere are a number of ways of having your inline citations appear. For example, in parentheses, with multiple citations, only with the year, adding a page number, etc.. For more information on how to format your citations, see the Quarto documentation."
  },
  {
    "objectID": "recipes/recipe-01/index.html#check-your-understanding",
    "href": "recipes/recipe-01/index.html#check-your-understanding",
    "title": "01. Academic writing with Quarto",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nConsider the following front-matter sections, A and B.\n\n\n\n\n\n\nA\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n    number-depth: 3\n    toc: false\n---\n\n\n\n\n\n\n\n\n\nB\n---\ntitle: \"My document title\"\nformat:\n  pdf:\n    number-sections: true\n    toc: true\n    toc-depth: 2\n---\n\n\n\nChoose whether the following statements are true or false.\n\nTRUEFALSE Section numbering will be included in the PDF output for both A and B.\nTRUEFALSE Section numbering will be applied to the first three levels of headers in the PDF output for both A and B.\nTRUEFALSE A table of contents will be included in the PDF output for both A and B.\nTRUEFALSE A table of contents will be included in the PDF output for B, but will only include the first two levels of headers.\n\nNow respond to the following questions.\n\n@tbl-scatterplot@fig-scatterplot@scatterplot will cross-reference a figure with the label fig-scatterplot.\n is the front-matter key to include a path to the file which contains the BibTeX formatted references."
  },
  {
    "objectID": "recipes/recipe-01/index.html#lab-preparation",
    "href": "recipes/recipe-01/index.html#lab-preparation",
    "title": "01. Academic writing with Quarto",
    "section": "Lab preparation",
    "text": "Lab preparation\nThis rounds out our introduction to academic writing in Quarto. In Lab 1 you will have an opportunity to practice these concepts by doing an article summary which includes some of these features using Quarto.\nIn preparation for Lab 1, ensure that you are prepared to do the following:\n\nEdit the front-matter section of a Quarto document to render:\n\na PDF document or a Word document\na document with numbered sections\na document with a table of contents\na document with a path to a bibliography file\n\nAdd an inline citation to the prose section of a Quarto document\n\nAlso, since you will do an article summary, you should be prepared with:\n\nan article of interest related to text analysis that you have read or at least skimmed for the following:\n\nthe research question\nthe data used\nthe methods used\nthe results/ findings of the study\n\na BibTeX formatted reference for the article\n\n\n\n\n\n\n\nIf you do not find an article of interest, you can use Bychkovska and Lee (2017)."
  },
  {
    "objectID": "recipes/recipe-07/index.html",
    "href": "recipes/recipe-07/index.html",
    "title": "07. Transforming and documenting data",
    "section": "",
    "text": "Skills\n\nText normalization and tokenization\nCreating new variables by splitting, merging, and recoding existing variables\nAugmenting data with additional variables from other sources or resources\nIn this recipe, we will employ a variety of tools and techniques to accomplish these tasks. Let’s load the packages we will need for this recipe. Let’s load the packages we will need for this recipe.\n# Load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(qtalrkit)\nIn Lab 7, we will apply what we have learned in this recipe to a new dataset."
  },
  {
    "objectID": "recipes/recipe-07/index.html#concepts-and-strategies",
    "href": "recipes/recipe-07/index.html#concepts-and-strategies",
    "title": "07. Transforming and documenting data",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\n\nCurated datasets are often project-neutral. That is, they are not necessarily designed to answer a specific research question. Rather, they are designed to be flexible enough to be used in a variety of projects. This is a good thing, but it also means that we will likely need to transform the data to bring it more in line with our research goals. This can include normalizing text, modifying the unit of observation and/ or adding additional attributes to the data.\n\nIn this recipe, we will explore a practical example of transforming data. We will start with a curated dataset and transform it to reflect a specific research goal. The dataset we will use is the MASC dataset (Ide et al. 2008). This dataset contains a collection of words from a variety of genres and modalities of American English.\n\n\n\n\n\n\n Tip\nThe MASC dataset is a curated version of the original data. This data is relatively project-neutral.\nIf you would like to acquire the original data and curate it for use in this recipe, you can do so by running the following code:\n# Acquire the original data\nqtalrkit::get_compressed_data(\n  url = \"..\",\n  target_dir = \"data/original/masc/\"\n)\n\n# Curate the data\n\n# ... write a function and add it to the package\n\n\n\nAs a starting point, I will assume that the curated dataset is available in the data/derived/masc/ directory, as seen below.\ndata/\n├── analysis/\n├── derived/\n│   ├── masc_curated_dd.csv\n│   ├── masc/\n│   │   ├── masc_curated.csv\n├── original/\n│   ├── masc_do.csv\n│   ├── masc/\n│   │   ├── ...\nThe first step is to inspect the data dictionary file. This file contains information about the variables in the dataset. It is also a good idea to review the data origin file, which contains information about the original data source.\nLooking at the data dictionary, in Table 1.\n\n\n\n\nTable 1: Data dictionary for the MASC dataset\n\n\n\n\n\n\n\nvariable\nname\ndescription\nvariable_type\n\n\n\n\nfile\nFile\nID number of the source file\ncharacter\n\n\nref\nReference\nReference number within the source file\ninteger\n\n\nbase\nBase\nBase form of the word (lemma)\ncharacter\n\n\nmsd\nMSD\nPart-of-speech tag (PENN tagset)\ncharacter\n\n\nstring\nString\nText content of the word\ncharacter\n\n\ntitle\nTitle\nTitle of the source file\ncharacter\n\n\nsource\nSource\nName of the source\ncharacter\n\n\ndate\nDate\nDate of the source file (if available)\ncharacter\n\n\nclass\nClass\nClassification of the source. Modality and genre\ncharacter\n\n\ndomain\nDomain\nDomain or topic of the source\ncharacter\n\n\n\n\n\n\n\n\n\n\n\nLet’s read in the data and take a glimpse at it.\n\n# Read the data\nmasc_curated &lt;- read_csv(\"data/derived/masc/masc_curated.csv\")\n\n# Preview\nglimpse(masc_curated)\n\nRows: 591,097\nColumns: 10\n$ file   &lt;chr&gt; \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"…\n$ ref    &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ base   &lt;chr&gt; \"december\", \"1998\", \"your\", \"contribution\", \"to\", \"goodwill\", \"…\n$ msd    &lt;chr&gt; \"NNP\", \"CD\", \"PRP$\", \"NN\", \"TO\", \"NNP\", \"MD\", \"VB\", \"JJR\", \"IN\"…\n$ string &lt;chr&gt; \"December\", \"1998\", \"Your\", \"contribution\", \"to\", \"Goodwill\", \"…\n$ title  &lt;chr&gt; \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"…\n$ source &lt;chr&gt; \"ICIC Corpus of Philanthropic Fundraising Discourse\", \"ICIC Cor…\n$ date   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ class  &lt;chr&gt; \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", …\n$ domain &lt;chr&gt; \"philanthropic fundraising discourse\", \"philanthropic fundraisi…\n\n\nWe may also want to do a summary overview of the dataset with {skimr}. This will give us a sense of the data types and the number of missing values.\n── Data Summary ───────────────────────\n                           Values\nName                       masc_curated\nNumber of rows             591097\nNumber of columns          10\n_______________________\nColumn type frequency:\n  character                9\n  numeric                  1\n________________________\nGroup variables            None\n\n── Variable type: character ───────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 file                  0         1       3  40     0      392          0\n2 base                  4         1.00    1  99     0    28010          0\n3 msd                   0         1       1   8     0       60          0\n4 string               25         1.00    1  99     0    39474          0\n5 title                 0         1       3 203     0      373          0\n6 source             5732         0.990   3 139     0      348          0\n7 date              94002         0.841   4  17     0       62          0\n8 class                 0         1       5   5     0       18          0\n9 domain            18165         0.969   4  35     0       21          0\n\n── Variable type: numeric ─────────────\n  skim_variable n_missing complete_rate  mean    sd p0 p25  p50  p75  p100 hist\n1 ref                   0             1 3854. 4633.  0 549 2033 5455 24519 ▇▂▁▁▁\nIn summary, the dataset contains 591,097 observations and 10 variables. The unit of observation is the word. The variable names are somewhat opaque, but the data dictionary provides some context that will help us understand the data.\nNow we want to consider how we plan to use this data in our analysis. Let’s assume that we want to use this data to explore lexical variation in the MASC dataset across modalities and genres. We will want to transform the data to reflect this goal.\nIn Table 2, we see an idealized version of the dataset we would like to have.\n\n\n\n\nTable 2: Idealized version of the MASC dataset\n\n\n\n\n\n\n\nvariable\nname\ntype\ndescription\n\n\n\n\ndoc_id\nDocument ID\nnumeric\nA unique identifier for each document\n\n\nmodality\nModality\ncharacter\nThe modality of the document (e.g., spoken, written)\n\n\ngenre\nGenre\ncharacter\nThe genre of the document (e.g., blog, newspaper)\n\n\nterm_num\nTerm number\nnumeric\nThe position of the term in the document\n\n\nterm\nTerm\ncharacter\nThe word\n\n\nlemma\nLemma\ncharacter\nThe lemma of the word\n\n\npos\nPart-of-speech\ncharacter\nThe part-of-speech tag of the word\n\n\n\n\n\n\n\n\n\n\n\nOf note, in this recipe we will derive a single transformed dataset. In other projects, you may want to generate various datasets with different units of observations. It all depends on your research question and the research aim that you are adopting.\n\n\nTransforming data\nTo get from the curated dataset to the idealized dataset, we will need to perform a number of transformations. Some of these transformations will be relatively straightforward, while others will require more work. Let’s start with the easy ones.\n\nLet’s drop the variables that we will not use and at the same time rename the variables to make them more intuitive.\n\nWe will use the select() function to drop or rename variables.\n\n# Drop and rename variables\nmasc_df &lt;-\n  masc_curated |&gt;\n  select(\n    doc_id = file,\n    term_num = ref,\n    term = string,\n    lemma = base,\n    pos = msd,\n    mod_gen = class\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 6\n   doc_id    term_num term         lemma        pos   mod_gen\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  \n 1 110CYL067        0 December     december     NNP   WR LT  \n 2 110CYL067        1 1998         1998         CD    WR LT  \n 3 110CYL067        2 Your         your         PRP$  WR LT  \n 4 110CYL067        3 contribution contribution NN    WR LT  \n 5 110CYL067        4 to           to           TO    WR LT  \n 6 110CYL067        5 Goodwill     goodwill     NNP   WR LT  \n 7 110CYL067        6 will         will         MD    WR LT  \n 8 110CYL067        7 mean         mean         VB    WR LT  \n 9 110CYL067        8 more         more         JJR   WR LT  \n10 110CYL067        9 than         than         IN    WR LT  \n# ℹ 591,087 more rows\n\n\nThat’s a good start on the structure.\n\nNext, we will split the mod_gen variable into two variables: modality and genre.\n\nWe have a variable mod_gen that contains two pieces of information: modality and genre (e.g., WR LT). The information appears to separated by a space. We can make sure this is the case by tabulating the values. The count() function will count the number of occurrences of each value in a variable, and as a side effect it will summarize the values of the variable so we can see if there are any unexpected values.\n\n# Tabulate mod_gen\nmasc_df |&gt;\n  count(mod_gen) |&gt;\n  arrange(-n)\n\n# A tibble: 18 × 2\n   mod_gen     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 SP TR   71630\n 2 WR EM   62036\n 3 WR FC   38608\n 4 WR ES   34938\n 5 WR FT   34373\n 6 WR BL   33278\n 7 WR JO   33042\n 8 WR JK   32420\n 9 WR NP   31225\n10 SP MS   29879\n11 WR NF   29531\n12 WR TW   28128\n13 WR GV   27848\n14 WR TG   27624\n15 WR LT   26468\n16 SP FF   23871\n17 WR TC   19419\n18 SP TP    6779\n\n\nLooks good, our values are separated by a space. We can use the separate_wider_delim() function from {tidyr} to split the variable into two variables. We will use the delim argument to specify the delimiter and the names argument to specify the names of the new variables.\n\n# Split mod_gen into modality and genre\nmasc_df &lt;-\n  masc_df |&gt;\n  separate_wider_delim(\n    cols = mod_gen,\n    delim = \" \",\n    names = c(\"modality\", \"genre\")\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id    term_num term         lemma        pos   modality genre\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1 110CYL067        0 December     december     NNP   WR       LT   \n 2 110CYL067        1 1998         1998         CD    WR       LT   \n 3 110CYL067        2 Your         your         PRP$  WR       LT   \n 4 110CYL067        3 contribution contribution NN    WR       LT   \n 5 110CYL067        4 to           to           TO    WR       LT   \n 6 110CYL067        5 Goodwill     goodwill     NNP   WR       LT   \n 7 110CYL067        6 will         will         MD    WR       LT   \n 8 110CYL067        7 mean         mean         VB    WR       LT   \n 9 110CYL067        8 more         more         JJR   WR       LT   \n10 110CYL067        9 than         than         IN    WR       LT   \n# ℹ 591,087 more rows\n\n\n\nCreate a document id variable.\n\nNow that we have the variables we want, we can turn our attention to the values of the variables. Let’s start with the doc_id variable. This may a good variable to use as the document id. If we take a look at the values, however, we can see that the values are not very informative.\nLet’s use the distinct() function to only show the unique values of the variable. We will also chain a slice_sample() function to randomly select a sample of the values. This will give us a sense of the values in the variable.\n\n# Preview doc_id\nmasc_df |&gt;\n  distinct(doc_id) |&gt;\n  slice_sample(n = 10)\n\n\n\n# A tibble: 10 × 1\n   doc_id             \n   &lt;chr&gt;              \n 1 JurassicParkIV-INT \n 2 111367             \n 3 NYTnewswire6       \n 4 sw2014-ms98-a-trans\n 5 52713              \n 6 new_clients        \n 7 cable_spool_fort   \n 8 jokes10            \n 9 wsj_2465           \n10 wsj_0158           \n\n\nYou can run this code various times to get a different sample of values.\nSince the doc_id variable is not informative, let’s replace the variable’s values with numeric values. In the end, we want a digit for each unique document and we want the words in each document to be grouped together.\nTo do this we will need to group the data by doc_id and then generate a new number for each group. We can achieve this by passing the data grouped by doc_id (group_by()) to the mutate() function and then using the cur_group_id() function to generate a number for each group.\n\n# Recode doc_id\nmasc_df &lt;-\n  masc_df |&gt;\n  group_by(doc_id) |&gt;\n  mutate(doc_id = cur_group_id()) |&gt;\n  ungroup()\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre\n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1      1        0 December     december     NNP   WR       LT   \n 2      1        1 1998         1998         CD    WR       LT   \n 3      1        2 Your         your         PRP$  WR       LT   \n 4      1        3 contribution contribution NN    WR       LT   \n 5      1        4 to           to           TO    WR       LT   \n 6      1        5 Goodwill     goodwill     NNP   WR       LT   \n 7      1        6 will         will         MD    WR       LT   \n 8      1        7 mean         mean         VB    WR       LT   \n 9      1        8 more         more         JJR   WR       LT   \n10      1        9 than         than         IN    WR       LT   \n# ℹ 591,087 more rows\n\n\nTo check, we can again apply the count() function.\n\n# Check\nmasc_df |&gt;\n  count(doc_id) |&gt;\n  arrange(-n)\n\n# A tibble: 392 × 2\n   doc_id     n\n    &lt;int&gt; &lt;int&gt;\n 1    158 24520\n 2    300 22261\n 3    112 18459\n 4    113 17986\n 5    215 17302\n 6    312 14752\n 7    311 13376\n 8    200 13138\n 9    217 11753\n10    186 10665\n# ℹ 382 more rows\n\n\nWe have 392 unique documents in the dataset. We also can see that the word lengths vary quite a bit. That’s something we will need to keep in mind as we move forward into the analysis.\n\nCheck the values of the pos variable.\n\nThe pos variable contains the part-of-speech tags for each word. The PENN Treebank tagset is used. Let’s take a look at the values to get familiar with them, and also to see if there are any unexpected values.\nLet’s use the slice_sample() function to randomly select a sample of the values. This will give us a sense of the values in the variable.\n# Preview pos\nmasc_df |&gt;\n  slice_sample(n = 10)\n\n\n# A tibble: 10 × 7\n   doc_id term_num term          lemma         pos   modality genre\n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1    303     2511 proliferation proliferation NN    WR       TC   \n 2     76     5245 And           and           CC    WR       FT   \n 3    300    17170 DAVY          davy          NNP   SP       MS   \n 4     80     5341 ”             ”             NN    WR       FT   \n 5    171      900 .             .             .     WR       TG   \n 6    166     2588 out           out           RP    WR       BL   \n 7     67       58 organization  organization  NN    WR       LT   \n 8    216     2944 include       include       VB    WR       TG   \n 9    234     1304 donation      donation      NN    WR       LT   \n10    231     3539 say           say           VB    WR       NF   \n\n\nAfter running this code a few times, we can see that the many of the values are as expected. There are, however, some unexpected values. In particular, some punctuation and symbols are tagged as nouns.\nWe can get a better appreciation for the unexpected values by filtering the data to only show non alpha-numeric values (^\\\\W+$) in the term column and then tabulating the values by term and pos.\n\n# Filter and tabulate\nmasc_df |&gt;\n  filter(str_detect(term, \"^\\\\W+$\")) |&gt;\n  count(term, pos) |&gt;\n  arrange(-n) |&gt;\n  print(n = 20)\n\n# A tibble: 152 × 3\n   term  pos       n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 \",\"   ,     27112\n 2 \".\"   .     26256\n 3 \"\\\"\"  ''     5495\n 4 \":\"   :      4938\n 5 \"?\"   .      3002\n 6 \")\"   )      2447\n 7 \"(\"   (      2363\n 8 \"-\"   :      1778\n 9 \"!\"   .      1747\n10 \"/\"   NN     1494\n11 \"’\"   NN     1319\n12 \"-\"   -      1213\n13 \"”\"   NN     1076\n14 \"“\"   NN     1061\n15 \"]\"   NN     1003\n16 \"[\"   NN     1001\n17 \";\"   :       991\n18 \"--\"  :       772\n19 \"&gt;\"   NN      752\n20 \"...\" ...     716\n# ℹ 132 more rows\n\n\nAs we can see from the sample above and from the PENN tagset documentation, most punctuation is tagged as the punctuation itself. For example, the period is tagged as . and the comma is tagged as ,. Let’s edit the data to reflect this.\nLet’s look at the code, and then we will discuss it.\n\n# Recode\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(pos = case_when(\n    str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),\n    TRUE ~ pos\n  ))\n\n# Check\nmasc_df |&gt;\n  filter(str_detect(term, \"^\\\\W+$\")) |&gt; # preview\n  count(term, pos) |&gt;\n  arrange(-n) |&gt;\n  print(n = 20)\n\n# A tibble: 127 × 3\n   term  pos       n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 \",\"   \",\"   27113\n 2 \".\"   \".\"   26257\n 3 \"\\\"\"  \"\\\"\"   5502\n 4 \":\"   \":\"    4939\n 5 \"?\"   \"?\"    3002\n 6 \"-\"   \"-\"    2994\n 7 \")\"   \")\"    2447\n 8 \"(\"   \"(\"    2363\n 9 \"!\"   \"!\"    1747\n10 \"/\"   \"/\"    1495\n11 \"’\"   \"’\"    1325\n12 \"”\"   \"”\"    1092\n13 \"“\"   \"“\"    1078\n14 \"]\"   \"]\"    1003\n15 \"[\"   \"[\"    1001\n16 \";\"   \";\"     993\n17 \"--\"  \"-\"     772\n18 \"&gt;\"   \"&gt;\"     753\n19 \"...\" \".\"     747\n20 \"'\"   \"'\"     741\n# ℹ 107 more rows\n\n\nThe case_when() function allows us to specify a series of conditions and values. The first condition is that the term variable contains only non alpha-numeric characters. If it does, then we want to replace the value of the pos variable with the first character of the term variable, str_sub(term, start = 1, end = 1). If the condition is not met, then we want to keep the original value of the pos variable, TRUE ~ pos.\nWe can see that our code worked by filtering the data to only show non alpha-numeric values (^\\\\W+$) in the term column and then tabulating the values by term and pos.\nFor completeness, I will also recode the lemma values for these values as well as the lemma can some times be multiple punctuation marks (e.g. !!!!!, ---, etc.) for these terms.\n\n# Recode\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(lemma = case_when(\n    str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),\n    TRUE ~ lemma\n  ))\n\n# Check\nmasc_df |&gt;\n  filter(str_detect(term, \"^\\\\W+$\")) |&gt; # preview\n  count(term, lemma) |&gt;\n  arrange(-n) |&gt;\n  print(n = 20)\n\n# A tibble: 127 × 3\n   term  lemma     n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 \",\"   \",\"   27113\n 2 \".\"   \".\"   26257\n 3 \"\\\"\"  \"\\\"\"   5502\n 4 \":\"   \":\"    4939\n 5 \"?\"   \"?\"    3002\n 6 \"-\"   \"-\"    2994\n 7 \")\"   \")\"    2447\n 8 \"(\"   \"(\"    2363\n 9 \"!\"   \"!\"    1747\n10 \"/\"   \"/\"    1495\n11 \"’\"   \"’\"    1325\n12 \"”\"   \"”\"    1092\n13 \"“\"   \"“\"    1078\n14 \"]\"   \"]\"    1003\n15 \"[\"   \"[\"    1001\n16 \";\"   \";\"     993\n17 \"--\"  \"-\"     772\n18 \"&gt;\"   \"&gt;\"     753\n19 \"...\" \".\"     747\n20 \"'\"   \"'\"     741\n# ℹ 107 more rows\n\n\n\nCheck the values of the modality variable.\n\nThe modality variable contains the modality tags for each document. Let’s take a look at the values.\nLet’s tabulate the values with count().\n\n# Tabulate modality\nmasc_df |&gt;\n  count(modality)\n\n# A tibble: 2 × 2\n  modality      n\n  &lt;chr&gt;     &lt;int&gt;\n1 SP       132159\n2 WR       458938\n\n\nWe see that the values are SP and WR, which stand for spoken and written, respectively. To make this a bit more transparent, we can recode these values to Spoken and Written. We will use the case_when() function to do this.\n\n# Recode modality\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(\n    modality = case_when(\n      modality == \"SP\" ~ \"Spoken\",\n      modality == \"WR\" ~ \"Written\"\n    )\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre\n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n 1      1        0 December     december     NNP   Written  LT   \n 2      1        1 1998         1998         CD    Written  LT   \n 3      1        2 Your         your         PRP$  Written  LT   \n 4      1        3 contribution contribution NN    Written  LT   \n 5      1        4 to           to           TO    Written  LT   \n 6      1        5 Goodwill     goodwill     NNP   Written  LT   \n 7      1        6 will         will         MD    Written  LT   \n 8      1        7 mean         mean         VB    Written  LT   \n 9      1        8 more         more         JJR   Written  LT   \n10      1        9 than         than         IN    Written  LT   \n# ℹ 591,087 more rows\n\n\n\nCheck the values of the genre variable.\n\nLet’s look at the values of the genre variable.\n\n# Tabulate genre\nmasc_df |&gt;\n  count(genre) |&gt;\n  print(n = Inf)\n\n# A tibble: 18 × 2\n   genre     n\n   &lt;chr&gt; &lt;int&gt;\n 1 BL    33278\n 2 EM    62036\n 3 ES    34938\n 4 FC    38608\n 5 FF    23871\n 6 FT    34373\n 7 GV    27848\n 8 JK    32420\n 9 JO    33042\n10 LT    26468\n11 MS    29879\n12 NF    29531\n13 NP    31225\n14 TC    19419\n15 TG    27624\n16 TP     6779\n17 TR    71630\n18 TW    28128\n\n\nThese genre labels are definitely cryptic. The data dictionary does not list these labels and their more verbose descriptions. However, looking at the original data’s README, we can find the file (resource-headers.xml) that lists these genre labels.\n1. 'BL' for blog\n2. 'NP' is newspaper\n3. 'EM' is email\n4. 'ES' is essay\n5. 'FT' is fictlets\n6. 'FC' is fiction\n7. 'GV' is government\n8. 'JK' is jokes\n9. 'JO' is journal\n10. 'LT' is letters\n11. 'MS' is movie script\n12. 'NF' is non-fiction\n13. 'FF' is face-to-face\n14. 'TC' is technical\n15. 'TG' is travel guide\n16. 'TP' is telephone\n17. 'TR' is transcript\n18. 'TW' is twitter\nNow we can again use the case_when() function. This time we will see if genre is equal to one of the genre labels and if it is, then we will replace the value with the more verbose description.\n\n# Recode genre\nmasc_df &lt;-\n  masc_df |&gt;\n  mutate(\n    genre = case_when(\n      genre == \"BL\" ~ \"Blog\",\n      genre == \"NP\" ~ \"Newspaper\",\n      genre == \"EM\" ~ \"Email\",\n      genre == \"ES\" ~ \"Essay\",\n      genre == \"FT\" ~ \"Fictlets\",\n      genre == \"FC\" ~ \"Fiction\",\n      genre == \"GV\" ~ \"Government\",\n      genre == \"JK\" ~ \"Jokes\",\n      genre == \"JO\" ~ \"Journal\",\n      genre == \"LT\" ~ \"Letters\",\n      genre == \"MS\" ~ \"Movie script\",\n      genre == \"NF\" ~ \"Non-fiction\",\n      genre == \"FF\" ~ \"Face-to-face\",\n      genre == \"TC\" ~ \"Technical\",\n      genre == \"TG\" ~ \"Travel guide\",\n      genre == \"TP\" ~ \"Telephone\",\n      genre == \"TR\" ~ \"Transcript\",\n      genre == \"TW\" ~ \"Twitter\"\n    )\n  )\n\nmasc_df\n\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre  \n    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  \n 1      1        0 December     december     NNP   Written  Letters\n 2      1        1 1998         1998         CD    Written  Letters\n 3      1        2 Your         your         PRP$  Written  Letters\n 4      1        3 contribution contribution NN    Written  Letters\n 5      1        4 to           to           TO    Written  Letters\n 6      1        5 Goodwill     goodwill     NNP   Written  Letters\n 7      1        6 will         will         MD    Written  Letters\n 8      1        7 mean         mean         VB    Written  Letters\n 9      1        8 more         more         JJR   Written  Letters\n10      1        9 than         than         IN    Written  Letters\n# ℹ 591,087 more rows\n\n\nDuring the process of transformation and afterwards, it is a good idea to tabulate and/ or visualize the dataset. This provides us an opportunity to get to know the dataset better and also may help us identify inconsistencies that we would like to address in the transformation, or at least be aware of as we move towards analysis.\n\n# How many documents are in each modality?\nmasc_df |&gt;\n  distinct(doc_id, modality) |&gt;\n  count(modality) |&gt;\n  arrange(-n)\n\n# A tibble: 2 × 2\n  modality     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Written    371\n2 Spoken      21\n\n# How many documents are in each genre?\nmasc_df |&gt;\n  distinct(doc_id, genre) |&gt;\n  count(genre) |&gt;\n  arrange(-n)\n\n# A tibble: 18 × 2\n   genre            n\n   &lt;chr&gt;        &lt;int&gt;\n 1 Email          174\n 2 Newspaper       54\n 3 Letters         49\n 4 Blog            21\n 5 Jokes           16\n 6 Journal         12\n 7 Essay            8\n 8 Fiction          7\n 9 Travel guide     7\n10 Face-to-face     6\n11 Movie script     6\n12 Technical        6\n13 Fictlets         5\n14 Government       5\n15 Non-fiction      5\n16 Telephone        5\n17 Transcript       4\n18 Twitter          2\n\n# What is the averge length of documents (in words)?\nmasc_df |&gt;\n  group_by(doc_id) |&gt;\n  summarize(n = n()) |&gt;\n  summarize(\n    mean = mean(n),\n    median = median(n),\n    min = min(n),\n    max = max(n)\n  )\n\n# A tibble: 1 × 4\n   mean median   min   max\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 1508.   418.    45 24520\n\nmasc_df |&gt;\n  group_by(doc_id) |&gt;\n  summarize(n = n()) |&gt;\n  ggplot(aes(x = n)) +\n  geom_density()\n\n\n\n\n\n\n\n# What is the distribution of the length of documents by modality?\nmasc_df |&gt;\n  group_by(doc_id, modality) |&gt;\n  summarize(n = n()) |&gt;\n  ggplot(aes(x = n, fill = modality)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n# What is the distribution of the length of documents by genre?\nmasc_df |&gt;\n  group_by(doc_id, modality, genre) |&gt;\n  summarize(n = n()) |&gt;\n  ggplot(aes(x = genre, y = n)) +\n  geom_boxplot() +\n  facet_wrap(~ modality, scales = \"free_x\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nOnce we are satisfied with the structure and values of the dataset, we can save it to a file. We will use the write_csv() function from {readr} to do this.\n\n# Save the data\nwrite_csv(masc_df, \"data/derived/masc/masc_transformed.csv\")\n\nThe structure of the data/ directory in our project should now look like this:\ndata/\n├── analysis/\n├── derived/\n│   ├── masc_curated_dd.csv\n│   ├── masc/\n│   │   ├── masc_curated.csv\n│   │   ├── masc_transformed.csv\n├── original/\n\n\nDocumenting data\nThe last step is to document the process and the resulting dataset(s). In this particular case we only derived one transformed dataset. The documentation steps are the same as in the curation step. We will organize and document the process file (often a .qmd file) and then create a data dictionary for each of the transformed datasets. The create_data_dictionary() function can come in handy for scaffolding the data dictionary file.\n\n# Create a data dictionary\ncreate_data_dictionary(\n  data = masc_df,\n  file_path = \"data/derived/masc/masc_transformed_dd.csv\"\n)"
  },
  {
    "objectID": "recipes/recipe-07/index.html#summary",
    "href": "recipes/recipe-07/index.html#summary",
    "title": "07. Transforming and documenting data",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we have looked at an example of transforming a curated dataset. This recipe included operations such as:\n\nText normalization\nVariable recoding\nSplitting variables\n\nIn other projects, the transformation steps will inevitably differ, but these strategies are commonly necessary in almost any project.\nJust as with other steps in the data preparation process, it is important to document the transformation steps. This will help you and others understand the process and the resulting dataset(s)."
  },
  {
    "objectID": "recipes/recipe-07/index.html#check-your-understanding",
    "href": "recipes/recipe-07/index.html#check-your-understanding",
    "title": "07. Transforming and documenting data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nWhich function would you use to remove duplicate rows in a dataset? group_by()mutate()distinct()filter()\nTRUEFALSE The str_c() function from {stringr} is used to separate strings rather than combine them.\nTRUEFALSE The count() function from {dplyr} is used to tabulate the values of a variable.\nIf you want to recode the age of learners into categories such as “child”, “teen”, and “adult” based on their age, which function should you use? mutate()case_when()unite()separate_wider_delim()\nTo normalize text by removing leading and trailing whitespace, you use the () function from {stringr}.\nTo normalize text by converting all characters to lowercase, you use the () function from {stringr}."
  },
  {
    "objectID": "recipes/recipe-07/index.html#lab-preparation",
    "href": "recipes/recipe-07/index.html#lab-preparation",
    "title": "07. Transforming and documenting data",
    "section": "Lab preparation",
    "text": "Lab preparation\n\nIn preparation for Lab 7, review and ensure you are comfortable with the following:\n\nVector, data frame, and list data structures\nSubsetting and filtering data structures with and without regular expressions\nReshaping datasets by rows and columns\n\nIn this lab, we will practice these skills and expand our knowledge of data preparation by transforming and documenting data with Tidyverse packages such as {dplyr}, {tidyr}, and {stringr}.\nYou will have a choice of a dataset to transform. Before you start the lab, you should consider which dataset you would like to use, what the idealized structure the transformed dataset will take, and what strategies you will likely employ to transform the dataset. You should also consider the information you need to document the data transformation process."
  },
  {
    "objectID": "recipes/recipe-09/index.html",
    "href": "recipes/recipe-09/index.html",
    "title": "09. Building predictive models",
    "section": "",
    "text": "Skills\n\nHow to identify variables of interest\nHow to inspect datasets\nHow to interrogate datasets and iteratively develop a model to improve performance\nHow to interpret results of predictive models\nThe workflow for building a predictive model is shown in Table 1. Note that Step 6 includes an optional step to iterate on the model to improve performance. This is optional because it is not always necessary to iterate on the model. However, it is often the case that the first model you build is not the best model. So it is good to be prepared to iterate on the model.\nLet’s get started by loading some of the key packages we will use in this recipe.\nlibrary(tidymodels) # for modeling\nlibrary(textrecipes) # for text preprocessing\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation\nlibrary(stringr) # for string manipulation\nlibrary(tidytext) # for text manipulation\nlibrary(ggplot2) # for visualization\nlibrary(janitor) # for tabyl()\n\ntidymodels_prefer() # avoid function name conflicts"
  },
  {
    "objectID": "recipes/recipe-09/index.html#concepts-and-strategies",
    "href": "recipes/recipe-09/index.html#concepts-and-strategies",
    "title": "09. Building predictive models",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\n\nWe will use the ACTIV-ES corpus to build a predictive model that can classify text as one of three dialects of Spanish: Argentinian, Mexican, or Spanish. We will frame this as a supervised learning problem, where we have a set of texts that have been labeled with the dialect of Spanish that they are written in. In contrast to the classification task in the chapter, which was binary, this is a multiclass classification task, where we are trying to classify each document as one of three classes.\n\nLet’s preview the structure of the ACTIVES dataset.\n\naes_df\n\n# # A tibble: 430 × 3\n#    doc_id variety   text                                                        \n#     &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;                                                       \n#  1 199500 Argentina No está , señora . Aquí tampoco . No aparece , señora . ¿ D…\n#  2 184782 Argentina ALGUIEN AL TELÉFONO . LA ANGUSTIA . Ah , no , no , no mi hi…\n#  3  47823 Argentina Habrá que cumplir su última voluntad , ¿ el medallón ? Lo v…\n#  4 282622 Argentina Sucedió en Hualfin Esta es la historia de tres generaciones…\n#  5  62433 Argentina 10 secuestros en 10 días ! Y no hay el menor índice . Bueno…\n#  6  70250 Argentina Y preguntada que fue sí reconocen el cadáver exhumado ... y…\n#  7  71897 Argentina ¡ Jeremías ! ¡ Jeremías ! ¡ No dejés parir a tu mujer ! Sei…\n#  8 333883 Argentina Usted . Usted que frecuenta el éxito como una costumbre más…\n#  9 333954 Argentina Miles de campanas nos traen , a través de los siglos , el t…\n# 10 175243 Argentina Y ? Enseguida viene , fue al baño . Bueno , pero la mesa la…\n# # ℹ 420 more rows\n\n\nThis dataset contains 430 documents, each of which is labeled with the variety of Spanish that it is written in and the text of the document. A document ID is also included, which we will be able to use to index the documents. The variety vector is a factor. As this will be the outcome variable in our predictive model, this is good as most predictive models require classification variables to be factors.\nLet’s get a sense of the distribution of the variety variable.\n\naes_df |&gt;\n  tabyl(variety) |&gt;\n  adorn_pct_formatting(digits = 1)\n\n   variety   n percent\n Argentina 128   29.8%\n    Mexico 119   27.7%\n     Spain 183   42.6%\n\n\nWe can see that the dataset is somewhat balanced, with Peninsular Spanish comprising the larger portion of the texts.\n\n\nAnalysis\nAt this point we can start to approach building a predictive model that can distinguish between the Spanish varieties using the text. We will first start by applying steps 1 and 2 of the workflow. We will then apply steps 3-5 iteratively to build, evaluate, and improve the model, as necessary, before applying it to the test data to assess and interpret the results.\nLet’s go ahead and perform steps 1 and 2. To split the data into training and testing sets, we will use {rsample}. The initial_split() function, sets up the splits and we use variety as the stratification variable to ensure that the training and testing sets are representative of the distribution of the outcome variable. As this process is random, we will set the seed for reproducibility. Finally, we call the training() and testing() functions to extract the training and testing sets.\n\n# Set the seed for reproducibility\nset.seed(1234)\n\n# Split the data into training and testing sets\naes_split &lt;-\n  initial_split(\n    data = aes_df,\n    prop = 0.8,\n    strata = variety\n  )\n\naes_train &lt;- training(aes_split) # extract the training set\naes_test &lt;- testing(aes_split) # extract the testing set\n\nWe will then set the base recipe which formally identifies the relationship between the predictor and outcome variables. The recipe() function from {recipes} is used to create a recipe.\n\n# Set the base recipe\naes_base_rec &lt;-\n  recipe(\n    formula = variety ~ text,\n    data = aes_train\n  )\n\nWe now have steps 1 and 2 of the workflow completed. We have identified the variables of interest and split the data into training and testing sets.\nOne more thing we will do here is to set up the cross-validation folds. Every time we fit a model to the training data, we will want to evaluate the model’s performance on the training data. However, we don’t want to do this in a way that is biased –testing the model on the same data that it was trained on! For this reason, we will use cross-validation to split the training data into multiple training and validation sets which represent different splits of the training data.\nWe will use the vfold_cv() function from {rsample} to set up the cross-validation folds. We will use 10 folds, which is a common number of folds to use. We will also use the strata argument to ensure that the folds are representative of the distribution of the outcome variable.\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Set up the cross-validation folds\ncv_folds &lt;-\n  vfold_cv(\n    data = aes_train,\n    v = 10,\n    strata = variety\n  )\n\nWith these common steps completed, we can now apply and reapply steps 3-5 of the workflow to build, evaluate, and improve the model.\n\nApproach 1\nIn our first approach, let’s start simple by using words as features and apply a logistic regression model. We won’t be completely naive, however, as we are familiar with the undo influence of the most frequent words. To address this, we will apply a term frequency-inverse document frequency (TF-IDF) transformation to the text in order to downweight the influence of the most frequent words and promote words that are more indicative of each class. Furthermore, we know that we will want to use a regularized regression model to avoid overfitting to particular words.\nTo get started, we will use {textrecipes} to add steps to our aes_base_rec recipe to preprocess the text. We will use the step_tokenize() function to tokenize the text. This tokenization process will likely result in a very large number of terms, most of which will not be informative and will add computational overhead. We will want to restrict the number of terms with the step_tokenfilter() function. However, it is not clear how many terms we should restrict the tokens to. For now, we will start with 1,000 tokens, but we will likely want to revisit this later. We will also use the step_tfidf() function to apply a TF-IDF transformation to the text setting smooth_idf to FALSE.\n\n# Add preprocessing steps to the recipe\naes_rec &lt;-\n  aes_base_rec |&gt;\n  step_tokenize(text) |&gt;\n  step_tokenfilter(text, max_tokens = 1000) |&gt;\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Preview the recipe\naes_rec\n\nTo implement the recipe and to preview the text preprocessing steps we apply the prep() and bake() functions.\n\naes_bake &lt;-\n  aes_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# Preview\ndim(aes_bake)\n\n[1]  343 1001\n\naes_bake[1:5, 1:5]\n\n# A tibble: 5 × 5\n  variety   tfidf_text_1 tfidf_text_10 tfidf_text_15 tfidf_text_2\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Argentina     0.000206     0.000599       0.000402      0      \n2 Argentina     0            0              0             0      \n3 Argentina     0            0.0000887      0             0      \n4 Argentina     0.00437      0              0.00142       0.00381\n5 Argentina     0            0.00155        0             0.00124\n\n\nWe now have a recipe that will tokenize the text, restrict the tokens to the most common 1,000 tokens, and create a TF-IDF matrix.\nIt is not a bad idea to inspect the features at this point to make sure that the preprocessing steps have been applied correctly and to gauge what this feature selection looks like so that when it comes time to interpret the model, we have a sense of what the model is doing.\nAs TF-IDF is going to be the main feature in our model, let’s visualize the top 20 terms by class. To do this, we will use {dplyr} to get the median TF-IDF score for each word by class, convert the data from wide to long format using the pivot_longer() function, and then use {ggplot2} to visualize the data.\n\n# Sum the term frequencies by class\nclass_freq_wide &lt;-\n  aes_bake |&gt;\n  group_by(variety) |&gt;\n  summarize(\n    across(\n      starts_with(\"tfidf_\"),\n      median\n    )\n  ) |&gt;\n  ungroup()\n\n# Convert the data from wide to long format\nclass_freq_long &lt;-\n  class_freq_wide |&gt;\n  pivot_longer(\n    cols = starts_with(\"tfidf_\"),\n    names_to = \"term\",\n    values_to = \"tfidf\"\n  ) |&gt;\n  mutate(term = str_remove(term, \"tfidf_text_\"))\n\n# Visualize the top 20 terms by class\nclass_freq_long |&gt;\n  slice_max(n = 20, order_by = tfidf, by = variety) |&gt;\n  mutate(term = reorder_within(term, tfidf, variety)) |&gt;\n  ggplot(aes(x = term, y = tfidf)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~variety, scales = \"free_y\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 1: Top 20 terms by class\n\n\n\n\n\nIn Figure 1, we see the words that are most indicative of each language variety. If you are familiar with Spanish, you can probably detect some variety-specific terms. For example, “vos” is a pronoun used in Argentinian Spanish and “os” is a pronoun used in Peninsular Spanish. There is also some overlap between the varieties, such as “tienes” and “te”.\nAnother point to note is the difference in magnitude of the TF-IDF scores between the Argentinian and other varieties. This suggests that the Argentinian variety is more distinct from the other varieties than the other varieties are from each other. Among the most distinctive terms are verbal forms that are specific to Argentinian Spanish, such as “tenés” and “sos”.\nNow let’s create a model specification. We will use the multinom_reg() function from {parsnip} to create a multinomial logistic regression model, as we have multiple classes in our prediction task. We will use the “glmnet” engine, which will allow us to apply regularization to the model, arbitrarily set to 0.01. We will use the set_engine() function to set the engine and the set_mode() function to set the mode to “classification”.\n\n# Create a model specification\naes_spec &lt;-\n  multinom_reg(\n    penalty = 0.01,\n    mixture = 1\n  ) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nTo combine the recipe and the model specification, we will use {workflows}. We will use the workflow() function and pass add_recipe(aes_rec) and add_model(aes_spec) as arguments to add the recipe and the model specification to the workflow.\n\n# Create a workflow\naes_wf &lt;-\n  workflow() |&gt;\n  add_recipe(aes_rec) |&gt;\n  add_model(aes_spec)\n\nWe can now use the cross-validation folds that we set up earlier. We will use the fit_resamples() function to fit the model to the training data using the cross-validation folds.\n\n# Fit the model to the training data\naes_train_fit &lt;-\n  aes_wf |&gt;\n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nWe can now evaluate the model’s performance on the training data. We will use the collect_metrics() function to collect the metrics from the cross-validation folds.\n\n# Evaluate the model's performance on the training data\naes_train_fit |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.804    10  0.0210 Preprocessor1_Model1\n2 brier_class multiclass 0.147    10  0.0130 Preprocessor1_Model1\n3 roc_auc     hand_till  0.923    10  0.0109 Preprocessor1_Model1\n\n\nWe can see that the model has a mean accuracy of 80.4% and ROC-AUC of 14.7%. That pretty good for a first pass. To get a sense of how good (or bad) it is, let’s compare it to a baseline model.\nA baseline model is the simplest model that we can use to compare the performance of our model to. A common baseline model is a model that always predicts the most frequent class. In our case, this is Peninsular Spanish, which accounts for 42.6% of the data. So it is clear that our model is doing much better than a baseline model which will have an accuracy score of 42.6%.\nWe can visualize the correct and incorrect predictions using a confusion matrix. We will use the conf_mat_resampled() function from {yardstick} to create the confusion matrix and the autoplot() function from {ggfortify} to visualize it.\n\naes_train_fit |&gt;\n  conf_mat_resampled(tidy = FALSE) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nFigure 2: Confusion matrix for the model in Approach 1\n\n\n\n\n\nThe left-downward diagonal of the confusion matrix represents the average number of documents correctly predicted for the aggregated model across the cross-validation folds. Other cells represent the average number of documents incorrectly predicted for each class-class combination. You can read these by using the row label to identify the predicted class and the column label to identify the actual class. So, for example, the model predicted Mexico \\(n\\) times when the actual class was Argentina. And so on.\n\n\nApproach 2\nIn our first approach we applied a TF-IDF transformation to the text and used a regularized multinomial logistic regression model. We also restricted the tokens to the 1,000 most frequent tokens and arbitrarily set the regularization parameter to 0.01. This resulted in an aggregate accuracy score of 80.4% on the training data. This is a good start, but see if we can do better.\nIn this second approach, let’s try to improve the model by applying a more principled approach to feature and hyperparameter selection.\nTo do this we will ‘tune’ the max_tokens and penalty hyperparameters in our recipe and model specifications, respectively. We need to update our recipe and model specification to include placeholders for these parameters replacing the previous values with tune(). We will also need to update our workflow to include the updated recipe and model specification.\n\n# Update the recipe\naes_rec &lt;-\n  aes_base_rec |&gt;\n  step_tokenize(text) |&gt;\n  step_tokenfilter(text, max_tokens = tune()) |&gt; # adds placeholder\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Update the model specification\naes_spec &lt;-\n  multinom_reg(\n    penalty = tune(), # adds placeholder\n    mixture = 1\n  ) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nWe can now create a workflow that includes the recipe and the model specification.\n\n# Create a workflow\naes_wf &lt;-\n  workflow() |&gt;\n  add_recipe(aes_rec) |&gt;\n  add_model(aes_spec)\n\nNow we set up the range of values for both the max_tokens and penalty hyperparameters. The grid_regular() function from {dials} will allow us to specify a grid of values for each hyperparameter.\n\n# Set the hyperparameter grid\naes_grid &lt;-\n  grid_regular(\n    max_tokens(range = c(250, 2000)),\n    penalty(range = c(-3, -1)),\n    levels = c(max_tokens = 5, penalty = 10)\n  )\n\naes_grid\n\n# A tibble: 50 × 2\n   max_tokens penalty\n        &lt;int&gt;   &lt;dbl&gt;\n 1        250 0.001  \n 2        687 0.001  \n 3       1125 0.001  \n 4       1562 0.001  \n 5       2000 0.001  \n 6        250 0.00167\n 7        687 0.00167\n 8       1125 0.00167\n 9       1562 0.00167\n10       2000 0.00167\n# ℹ 40 more rows\n\n\nThe range = argument specifies the range of values to include in the grid. For max_tokens, this is straightforward. For penalty, we are specifying the range of values on the log scale. So the range of values is 0.001 to 0.1. The levels argument specifies the number of values to include in the grid. In this case, we will include 5 values for max_tokens and 10 values for penalty. This will result in 50 combinations of hyperparameter values.\nWe will then pass our aes_wf workflow to the tune_grid() function with the grid values we specified to tune the hyperparameters.\n\n# Tune the hyperparameters\naes_tune &lt;-\n  aes_wf |&gt;\n  tune_grid(\n    resamples = cv_folds,\n    grid = aes_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nThe aes_grid object is a tibble which contains the grid all the combinations of hyperparameter values. In this case, there are 50 combinations. That means we are going to fit 50 models to the training data! This is a lot of models, but it is worth it to get a more robust estimate of the model’s performance.\nWe can use the collect_metrics() function to collect the metrics from the cross-validation folds for each of our tuning parameters, but this will result in a lot of output. Instead, we can use the autoplot() function to visualize the metrics.\n\n# Plot the collected metrics\n\naes_tune |&gt; autoplot()\n\n\n\n\n\n\n\nFigure 3: Metrics for model tuning in Approach 2\n\n\n\n\n\nWe see some variation across the folds in the accuracy and ROC-AUC scores. This will help us make a more informed decision about which hyperparameters to use.\nThe metric to use to select the best model is something to consider.Accuracy is an important measure, but does not tell the whole story. In particular, accuracy does not tell us how well the model is doing for each class –only the overall correct and incorrect predictions. To get a better sense of how the model is doing across the classes, we can pay attention to the ROC-AUC score. The ROC-AUC score is a measure of the area under the receiver operating characteristic (ROC) curve. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) for each class at different probability thresholds. This measure is useful because it is not affected by class imbalance.\nLet’s select the best model based on the ROC-AUC score.\n\n# Get the best model\naes_tune_best &lt;-\n  aes_tune |&gt;\n  select_best(metric = \"roc_auc\")\n\naes_tune_best\n\n# A tibble: 1 × 3\n  penalty max_tokens .config              \n    &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1  0.0215       1562 Preprocessor4_Model07\n\n\nWe can now update our workflow with the best hyperparameters.\n\n# Update the workflow\naes_wf &lt;-\n  aes_wf |&gt;\n  finalize_workflow(aes_tune_best)\n\naes_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.0215443469003188\n  mixture = 1\n\nComputational engine: glmnet \n\n\nWe can now see that the updated workflow will replace the tune() placeholders with the best hyperparameters we selected.\nLet’s again perform a resampled fit on the training data using our new tuned model and then compare our results with the previous, abritrarily tuned model.\n\n# Fit the model to the training data\naes_train_fit &lt;-\n  aes_wf |&gt;\n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Evaluate the model's performance on the training data\naes_train_fit |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.819    10  0.0135 Preprocessor1_Model1\n2 brier_class multiclass 0.130    10  0.0101 Preprocessor1_Model1\n3 roc_auc     hand_till  0.938    10  0.0104 Preprocessor1_Model1\n\n\nThe accuracy score has improved just a bit, from an aggregate score of 80.4% to 81.9%.\nIn all likelihood, we would want to continue to iterate on this model, applying different feature selection and engineering procedures, different models, and different hyperparameters –I will consider some suggestions in the next section. However, for the sake of time, we will stop here and train our final model on the training data and then apply the model to the test data to assess and interpret the results.\n\nInterpreting the model\nAt this stage we are ready to interpret the model. We first fit the model to the training data, then apply the model to the test data, and evaluate the model’s performance on the test data. Finally, we will dig into the model to interpret the importance of the features to help us understand what the model is doing and what it can tell us about words that are indicative, or not, of each variety.\nLet’s fit our final model to the training data and evaluate it on the testing data using the last_fit() function which takes our updated workflow and the original split we created earlier which is stored in aes_split.\n\n# Fit the final model\naes_final_fit &lt;- last_fit(aes_wf, aes_split)\n\nWe can now collect the performance metrics from the testing data.\n\n# Get the performance metrics\naes_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass     0.736 Preprocessor1_Model1\n2 roc_auc     hand_till      0.872 Preprocessor1_Model1\n3 brier_class multiclass     0.180 Preprocessor1_Model1\n\n\nThe accuracy of this model on the test data is 73.6%. This is lower than the accuracy on the training data. Should we be surprised? Not really. The model was trained on the training data, so it is not surprising that it would perform better on the training data than the test data, despite the fact that we used cross-validation to evaluate the model on the training data. This is a good reminder that the model is not perfect and that we should not expect it to be.\nWhat does the ‘kap’ metric mean? The Kappa statistic is a measure of agreement between the predicted and actual classes. It is a measure of agreement that is corrected for the possibility that some correct prediction may have occurred by chance. The kappa statistic ranges from 0 to 1, with 0 indicating no agreement above chance and 1 indicating perfect agreement. In this case, the kappa statistic is 87.2%, which indicates that there is a moderate amount of agreement between the predicted and actual classes.\nLet’s explore if there is a difference in performance across the classes. To do this, we will use the conf_mat() function from {yardstick} to create the confusion matrix and the autoplot() function from {ggfortify} to visualize it.\n\naes_final_fit |&gt;\n  collect_predictions() |&gt;\n  conf_mat(truth = variety, estimate = .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nFigure 4: Confusion matrix for the model in Approach 2\n\n\n\n\n\nWe can see that the model is doing a good job of predicting Peninsular Spanish, but is not doing as well with the other varieties. This is not surprising given that Peninsular Spanish is the most frequent class in the data. This is a good reminder that accuracy is not the only metric to consider when evaluating a model. We can get a better sense of how the model is doing across the classes by looking at the ROC-AUC score.\n\n# Get the ROC-AUC score\naes_final_fit |&gt;\n  collect_predictions() |&gt;\n  roc_curve(truth = variety, .pred_Argentina:.pred_Spain) |&gt;\n  autoplot()\n\n\n\n\n\n\n\nFigure 5: ROC plot for the final fitted model\n\n\n\n\n\nTaken together, we have a decent model that can predict the variety of Spanish that a text is written in. We can also see that although prediction accuracy appears higher for Peninsular Spanish, the ROC-AUC curves suggest that the model is doing a better job of predicting the other varieties based on the features.\nThere is still room for improvement –as we recognized earlier. However, it is important that we do not start to use the testing data to improve the model. The testing data should only be used to evaluate the model. If we start to use the testing data to improve the model, we will no longer have an unbiased estimate of the model’s performance.\nLet’s now dig into our model’s features to explore what words are driving the model’s predictions. The approach to do this will depend on the model. In this case, we used a multinomial logistic regression model, which is a linear model. This means that we can interpret the model’s coefficients to understand the importance of the features. Coefficients that are positive indicate that the feature is associated with the reference class and coefficients that are negative indicate that the feature is associated with the non-reference class. For classification tasks with two classes, this is straightforward to interpret.\nThe issue here, however, is that we have more than two classes (i.e., Argentina, Mexico, and Spain). In these cases, the coefficients estimates for each class need to be extracted and standardized to be compared across classes.\nWe can do this using the extract_fit_parsnip() function from {parsnip}. This will extract the model object from the workflow object. The tidy() function from {broom} will then organize the coefficients (log-odds) for each predictor terms for each outcome class. We can then use the filter() function from {dplyr} to remove the intercept term and the mutate() function from {dplyr} to remove the “tfidf_text_” prefix from the term names so that they are more legible.\n\n# Get the coefficients\naes_coefs &lt;-\n  aes_final_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy() |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(term = str_remove(term, \"tfidf_text_\"))\n\nslice_sample(aes_coefs, n = 10)\n\n# A tibble: 10 × 4\n   class     term      estimate penalty\n   &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1 Mexico    vengó            0  0.0215\n 2 Mexico    tú               0  0.0215\n 3 Mexico    tin              0  0.0215\n 4 Mexico    papel            0  0.0215\n 5 Spain     están            0  0.0215\n 6 Argentina perdido          0  0.0215\n 7 Argentina caballero        0  0.0215\n 8 Spain     señora           0  0.0215\n 9 Argentina casar            0  0.0215\n10 Spain     serás            0  0.0215\n\n\nNow to standardize the log-odds coefficients so that they are comparable across the classes, we will use the scale() function from base R to transform the coeffients such that each class has a mean of 0 and a standard deviation of 1. scale() returns a matrix, so we will use the as.vector() function to convert the matrix to a vector.\n\naes_coefs_z &lt;-\n  aes_coefs |&gt;\n  group_by(class) |&gt;\n  mutate(z_score = as.vector(scale(estimate))) |&gt;\n  ungroup()\n\nslice_sample(aes_coefs_z, n = 10)\n\n# A tibble: 10 × 5\n   class     term     estimate penalty z_score\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Argentina iba             0  0.0215 -0.0857\n 2 Spain     pudo            0  0.0215 -0.0320\n 3 Spain     pará            0  0.0215 -0.0320\n 4 Argentina dicho           0  0.0215 -0.0857\n 5 Argentina sirve           0  0.0215 -0.0857\n 6 Mexico    cuesta          0  0.0215 -0.0787\n 7 Spain     demonios        0  0.0215 -0.0320\n 8 Argentina punto           0  0.0215 -0.0857\n 9 Mexico    también         0  0.0215 -0.0787\n10 Spain     cabeza          0  0.0215 -0.0320\n\n\nFinally, let’s visualize the top 25 terms by class. Note that we are using the reorder_within() and scale_x_reordered() functions from {tidytext} to reorder the terms in such a way that our facets allow for distinct terms on the x-axis for each class. Then the coord_flip() function from {ggplot2} is used to flip the axes for easier reading.\n\naes_coefs_z |&gt;\n  mutate(term = reorder_within(term, z_score, class)) |&gt;\n  slice_max(n = 25, order_by = z_score, by = class) |&gt;\n  ggplot(aes(x = term, y = z_score)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~class, scales = \"free_y\") +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 6: Top 25 terms by class\n\n\n\n\n\nWe can assess the distinct features for each class and also gauge the magnitude of the estimates. We should be cautious, however, as these terms are derived from our model that only performs moderately well.\n\n\n\nOther approaches\nAs we have seen, there are many decisions to make when building a predictive model. We have only scratched the surface of the options available. In this section, I will briefly consider some other approaches that may be of interest.\nFeatures:\nWe used words in this recipe and in the chapter classification task. This is merely in order to keep the focus on the process of building a predictive model. There are many other features that could be used. For example, we could use n-grams, character n-grams, or word embeddings. {textrecipes} provides many options for text preprocessing and feature engineering.\nLet’s look at how we can derive other linguistic units using {textrecipes}. First, let’s set up a simple dataset and base recipe.\n\ndf &lt;- tibble(\n  outcome = factor(c(\"a\", \"a\", \"b\", \"b\")),\n  date = as.Date(c(\"2020-01-01\", \"2021-06-14\", \"2020-11-05\", \"2023-12-25\")),\n  text = c(\n    \"This is a fantastic sentence.\",\n    \"This is another great sentence.\",\n    \"This is a third, boring sentence.\",\n    \"This is a fourth and final sentence.\"\n  )\n)\n\nbase_rec &lt;- recipe(outcome ~ text, data = df)\n\nNow, say instead of words, we were interested in deriving word \\(n\\)-grams as our terms. We again use the step_tokenize() function in our recipe. This time, however, we add a value to the token = argument. In this case, we will use “ngrams”. {textrecipes} uses the tokenization engine from {tokenizers}, so the types of tokenization available are the same as those available (see help(tokenizers) for more information).\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\", # word n-grams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"this is a\"            \"is a fantastic\"       \"a fantastic sentence\"\n\n[[2]]\n[1] \"this is another\"        \"is another great\"       \"another great sentence\"\n\n[[3]]\n[1] \"this is a\"             \"is a third\"            \"a third boring\"       \n[4] \"third boring sentence\"\n\n[[4]]\n[1] \"this is a\"          \"is a fourth\"        \"a fourth and\"      \n[4] \"fourth and final\"   \"and final sentence\"\n\n\nBy default tokens = \"ngrams\" produces trigrams.\nAnother option is to use character n-grams. This is useful when we want to capture information about the morphology of the words. For character n-grams, we can use “character_shingle”.\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"character_shingle\" # character n-grams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afa\" \"fan\" \"ant\" \"nta\" \"tas\" \"ast\"\n[13] \"sti\" \"tic\" \"ics\" \"cse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[2]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"san\" \"ano\" \"not\" \"oth\" \"the\" \"her\" \"erg\"\n[13] \"rgr\" \"gre\" \"rea\" \"eat\" \"ats\" \"tse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[3]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"sat\" \"ath\" \"thi\" \"hir\" \"ird\" \"rdb\" \"dbo\"\n[13] \"bor\" \"ori\" \"rin\" \"ing\" \"ngs\" \"gse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[4]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afo\" \"fou\" \"our\" \"urt\" \"rth\" \"tha\"\n[13] \"han\" \"and\" \"ndf\" \"dfi\" \"fin\" \"ina\" \"nal\" \"als\" \"lse\" \"sen\" \"ent\" \"nte\"\n[25] \"ten\" \"enc\" \"nce\"\n\n\nBy default tokens = \"character_shingle\" also produces trigrams.\nNow, say we want to change the number of words in each n-gram or character n-gram. We can do this using the options = argument. This is where we pass tokenizer-specific options. For example, to change the number of words in each n-gram, we can use the n = argument.\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2) # word bigrams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"this is\"            \"is a\"               \"a fantastic\"       \n[4] \"fantastic sentence\"\n\n[[2]]\n[1] \"this is\"        \"is another\"     \"another great\"  \"great sentence\"\n\n[[3]]\n[1] \"this is\"         \"is a\"            \"a third\"         \"third boring\"   \n[5] \"boring sentence\"\n\n[[4]]\n[1] \"this is\"        \"is a\"           \"a fourth\"       \"fourth and\"    \n[5] \"and final\"      \"final sentence\"\n\n\nIf you would like to calculate multiple \\(n\\)-gram windows, you can pass the n_min = argument.\n\nbase_rec |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2, n_min = 1) # word unigrams and bigrams\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"this\"               \"this is\"            \"is\"                \n[4] \"is a\"               \"a\"                  \"a fantastic\"       \n[7] \"fantastic\"          \"fantastic sentence\" \"sentence\"          \n\n[[2]]\n[1] \"this\"           \"this is\"        \"is\"             \"is another\"    \n[5] \"another\"        \"another great\"  \"great\"          \"great sentence\"\n[9] \"sentence\"      \n\n[[3]]\n [1] \"this\"            \"this is\"         \"is\"              \"is a\"           \n [5] \"a\"               \"a third\"         \"third\"           \"third boring\"   \n [9] \"boring\"          \"boring sentence\" \"sentence\"       \n\n[[4]]\n [1] \"this\"           \"this is\"        \"is\"             \"is a\"          \n [5] \"a\"              \"a fourth\"       \"fourth\"         \"fourth and\"    \n [9] \"and\"            \"and final\"      \"final\"          \"final sentence\"\n[13] \"sentence\"      \n\n\nNames and values of the arguments that options = will take will depend on the type of tokenization specified.\nWe could also use metadata, such as the year the text was written, the author, the genre, etc. In these cases, will will update our base recipe to include the metadata as predictors and then we can use the necessary preprocessing steps to prepare the metadata for modeling using functions from the recipes() package (i.e., `step_normalize(), step_dummy(), etc.).\n\nbase_rec &lt;- recipe(outcome ~ date + text, data = df) # add date\n\nbase_rec |&gt;\n  step_tokenize(text) |&gt;\n  step_date(date, features = c(\"year\")) |&gt; # extract the year\n  prep() |&gt;\n  juice()\n\n# A tibble: 4 × 4\n  date             text outcome date_year\n  &lt;date&gt;      &lt;tknlist&gt; &lt;fct&gt;       &lt;int&gt;\n1 2020-01-01 [5 tokens] a            2020\n2 2021-06-14 [5 tokens] a            2021\n3 2020-11-05 [6 tokens] b            2020\n4 2023-12-25 [7 tokens] b            2023\n\n\nWe could also use other features derived from the text, such as word length, syntactic complexity, sentiment, readability, etc. A number of stylistic features are available using the step_textfeature() function, some 26 (see ?count_functions). However, it is also possible to derive your own features working with the original dataset and then adding the features\n\ndf &lt;-\n  df |&gt;\n  left_join(\n    # Calculate word count and average word length\n    df |&gt;\n      unnest_tokens(word, text, drop = FALSE) |&gt;\n      group_by(text) |&gt;\n      summarize(\n        word_count = n(),\n        avg_word_length = mean(nchar(word))\n      )\n  )\n\nrecipe(\n  outcome ~ ., # use all variables\n  data = df\n) |&gt;\n  step_tokenize(text) |&gt;\n  step_tf(text) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 4 × 16\n  date       word_count avg_word_length outcome tf_text_a tf_text_and\n  &lt;date&gt;          &lt;int&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;int&gt;       &lt;int&gt;\n1 2020-01-01          5            4.8  a               1           0\n2 2021-06-14          5            5.2  a               0           0\n3 2020-11-05          6            4.33 b               1           0\n4 2023-12-25          7            4.14 b               1           1\n# ℹ 10 more variables: tf_text_another &lt;int&gt;, tf_text_boring &lt;int&gt;,\n#   tf_text_fantastic &lt;int&gt;, tf_text_final &lt;int&gt;, tf_text_fourth &lt;int&gt;,\n#   tf_text_great &lt;int&gt;, tf_text_is &lt;int&gt;, tf_text_sentence &lt;int&gt;,\n#   tf_text_third &lt;int&gt;, tf_text_this &lt;int&gt;\n\n\nModels:\nA big advantage to using the {tidymodels} approach to modeling is that it allows us to easily try different models. We have used a multinomial logistic regression model in this recipe, but we could also try other models, such as a random forest model, a support vector machine, or a neural network. We can do this by simply changing the model specification in our workflow.\nFor example, we could use a random forest model. We would first need to update our model specification to use the rand_forest() function from {parsnip} to create a random forest model. We would also need to update the engine to use {ranger}, which is a fast implementation of random forest models. Finally, we would need to update the mode to “classification”.\n\n# Create a model specification\naes_spec &lt;-\n  # Random Forest\n  rand_forest(\n    mtry = 10,\n    trees = 1000\n  ) |&gt;\n  set_engine(\"ranger\") |&gt; # use the ranger engine\n  set_mode(\"classification\")\n\nIt is important to understand that different models have different hyperparameters. As we say with the logistic_reg() and multinom_reg() models, we can tune the penalty hyperparameter. However, this is not the case for all models. For example, the rand_forest() model does not have a penalty hyperparameter. Instead, it has a mtry hyperparameter, which is the number of variables to consider at each split. We can tune this hyperparameter in the same way that we tuned the penalty hyperparameter using tune(), grid_regular(), and tune_grid().\nOther models to consider for text classification include Naive Bayes, Support Vector Machines, and Neural Networks. The {tidymodels} framework supports all of these models.\nA last point to consider is whether we will want to be able to interpret the features that drive the model’s performance. If so, we will want to use a model that allows us to interpret the features. For example, we could use a linear model, such as a logistic regression model, or a tree-based model, such as a random forest model. However, we would not be able to interpret the features of a neural network model.\nFurthermore, the methods we use to interpret the features will depend on the model. For example, we can interpret the features of a linear model by looking at the coefficients. However, we cannot interpret the features of a random forest model in the same way. Instead, we can use the vip() function from {vip} to visualize the importance of the features."
  },
  {
    "objectID": "recipes/recipe-09/index.html#summary",
    "href": "recipes/recipe-09/index.html#summary",
    "title": "09. Building predictive models",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we’ve covered the foundational skills needed to construct a predictive (classification) model using the tidymodels framework. We examined the key steps in predictive modeling: identifying data, dividing it into training and test sets, preprocessing, iterative model training, and result interpretation.\nWe used a dataset of Spanish texts from three different varieties to demonstrate the process iterating over two approaches. In the first approach, we used a multinomial logistic regression model with TF-IDF features. In the second approach, we tuned the hyperparameters of the model and the preprocessing steps to improve the model’s performance. We also touched upon alternative methods, like incorporating other features such as n-grams and experimenting with other models such as random forests, which may prove useful in text classification tasks.\nWith the matierals in this chapter you should now have an understanding of how to build and understand a text classification model in R, equipped with insights to further develop your predictive analysis projects."
  },
  {
    "objectID": "recipes/recipe-09/index.html#check-your-understanding",
    "href": "recipes/recipe-09/index.html#check-your-understanding",
    "title": "09. Building predictive models",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nTRUEFALSE There are two basic types of prediction models: regression and classification.\nWhat is the purpose of splitting data into training and testing sets? To make computation fasterTo avoid overfitting the modelTo decrease the size of the datasetTo make the model simpler\nWhat is the purpose of cross-validation? To make computation fasterTo avoid overfitting the modelTo evaluate the model’s performanceTo make the model simpler\nWhich of the following models would not be appropriate for a classification task? Logistic regressionRandom forestSupport vector machineLinear regression\nIterative improvement in modeling involves: Changing the modelChanging the hyperparametersChanging the preprocessing stepsAll of the above\nTRUEFALSE Feature importance measures are uniform across models."
  },
  {
    "objectID": "recipes/recipe-09/index.html#lab-preparation",
    "href": "recipes/recipe-09/index.html#lab-preparation",
    "title": "09. Building predictive models",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn preparation for Lab 9, review and ensure that you are familiar with the following concepts:\n\nBuilding feature engineering pipelines with {recipes}\nBuilding model specifications with {parsnip}\nIterative model training, evaluation, and improvement with {workflows}, {tune}, and {yardstick}\n\nIn this lab, you will have an opportunity to apply these concepts to a new dataset and classification task. You should consider the dataset and the task in be performed in the lab and think about how you might approach the task from a feature engineering and model selection perspective. You will be asked to submit you code and a brief reflection on your approach and the results."
  },
  {
    "objectID": "recipes/recipe-09/index.html#references",
    "href": "recipes/recipe-09/index.html#references",
    "title": "09. Building predictive models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "recipes/recipe-05/index.html",
    "href": "recipes/recipe-05/index.html",
    "title": "05. Collecting and documenting data",
    "section": "",
    "text": "Skills\n\nFinding data sources\nData collection strategies\nData documentation"
  },
  {
    "objectID": "recipes/recipe-05/index.html#concepts-and-strategies",
    "href": "recipes/recipe-05/index.html#concepts-and-strategies",
    "title": "05. Collecting and documenting data",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nFinding data sources\nTo find data sources, it is best to have a research question in mind. This will help to narrow the search for data sources. However, finding data sources can also be a good way to generate research questions. In either case, it takes some sleuthing to find data sources that will work for your research question. In addition, to the data source itself, you will also need to consider the permissions and licensing of the data source. It is best to consider these early in the process to avoid surprises later. Finally, you will also need to consider the data format and how it will be used in the analysis. It can be the case that a data source seems ideal, but the data format is not conducive to the analysis you would like to do.\n\n\n\n\n\n\n Tip\nConsult the Identifying data and data sources guide for some ideas on where to find data sources.\n\n\n\nIn this recipe, we will consider some hypothetical reseach aimed at exploring potential similarities and differences in the lexical, syntactic, and/ or stylistic features between American and English literature during the mid 19th century.\n\n\n\n\n\n\n Dive deeper\nIf you are interested in understanding a literary analysis perspective to text analysis, I highly recommend Matthew Jockers’ book Text Analysis with R for Students of Literature (Jockers 2014). This book is a great resource for understanding how to apply text analysis to literary analysis.\n\n\n\nProject Gutenberg is a great source of data for this research question. Project Gutenberg is a volunteer effort to digitize and archive cultural works. The great majority of the works in the Project Gutenberg database are in the public domain in the United States. This means that the works can be freely used and shared.\nFurthermore, {gutenbergr} provides an API for accessing the Project Gutenberg database. This means that we can use R to access the Project Gutenberg database and download the text and metadata for the works we are interested in. {gutenbergr} also provides a number of data frames that can help us to identify the works we are interested in.\n\n\nData collection strategy\nLet’s now turn to the data collection strategy. There are a number of data collection strategies that can be used to acquire data for a text analysis project. In the chapter, we covered manual and programmatic downloads and APIs. Here we will use an R package which will provide an API for accessing the data source.\n\n\n\n\n\n\n Dive deeper\nIf you are interested in learning about another data collection strategy, web scraping, I suggest you look at the Web scraping with R guide.\n\n\n\nWe will load {dplyr}, {readr}, and {gutenbergr} to prepare for the data collection process.\n\n# Load packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(gutenbergr)\n\nThe main workhorse of {gutenbergr} is the gutenberg_download(). It’s only required argument is the id(s) used by Project Gutenberg to index all of the works in their database. This function will then download the text of the work(s) and return a data frame with the gutenberg id and the text of the work(s).\nSo how do we find the gutenberg ids? The manual method is to go to the Project Gutenberg website and search for the work you are interested in. For example, let’s say we are interested in the work “A Tale of Two Cities” by Charles Dickens. We can search for this work on the Project Gutenberg website and then click on the link to the work. The url for this work is: https://www.gutenberg.org/ebooks/98. The gutenberg id is the number at the end of the url, in this case 98.\nThis will work for individual works, but why wouldn’t we just download the text from the Project Gutenberg website? For the works on Project Gutenberg this would be perfectly fine. We can share the text with others as the license for the works on Project Gutenberg are in the public domain.\nHowever, what if are interested in downloading multiple works? As the number of works increases, the time it takes to manually download each work increases. Furthermore, {gutenbergr} provides a number of additional attributes that can be downloaded and organized along side the text. Finally, the results of the gutenberg_download() function are returned as a data frame which can be easily manipulated and analyzed in R.\nIn our data acquisition plan, we want to collect works from a number of authors. So it will be best to leverge {gutenbergr} to download the works we are interested in. To do this we need to know the gutenberg ids for the works we are interested in.\nConvienently, {gutenbergr} also includes a number of data frames that contain meta data for the works in the Project Gutenberg database. These data frames include meta data for works in the Project Gutenberg database (gutenberg_metadata), authors (gutenberg_authors), and subjects (gutenberg_subjects).\nLet’s take a look at the structure of these data frames.\n\nglimpse(gutenberg_metadata)\n\nRows: 72,569\nColumns: 8\n$ gutenberg_id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;int&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_authors)\n\nRows: 23,980\nColumns: 7\n$ gutenberg_author_id &lt;int&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;int&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;int&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_subjects)\n\nRows: 231,741\nColumns: 3\n$ gutenberg_id &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\nFrom this overvew, we can see that there are 72,569 works in the Project Gutenberg database. We can also see that there are 23,980 authors and 231,741 subjects.\nAs we dicussed, each work in the Project Gutenberg database has a gutenberg id. The gutenberg_id appears in the gutenberg_metadata and also in the gutenberg_subjects data frame. This common attribute means that a work with a particular gutenberg id can be linked to the subject(s) associated with that work. Another important attribute is is the gutenberg_author_id which links the work to the author(s) of that work. Yes, the author name is in the gutenberg_metadata data frame, but the gutenberg_author_id can be used to link the work to the gutenberg_authors data frame which contains additional information about authors.\n\n\n\n\n\n\n Tip\n{gutenbergr} is periodically updated. To check to see when each data frame was last updated run:\nattr(gutenberg_metadata, \"date_updated\")\n\n\n\nLet’s now describe a few more attributes that will be useful for our data acquisition plan. In the gutenberg_subjects data frame, we have subject_type and subject. The subject_type is the type of subject classification system used to classify the work. If you tabulate this column, you will see that there are two types of subject classification systems used: Library of Congress Classification (lcc) and Library of Congress Subject Headings (lcsh). The subject column contains the subject code for the work. For lsch the subject code is a descriptive character string and for lcc the subject code is an id as a character string that is a combination of letters (and numbers) that the Library of Congress uses to classify works.\nFor our data acquistion plan, we will use the lcc subject classification system to select works from the Library of Congress Classification for English Literature (PR) and American Literature (PS).\nIn the gutenberg_authors data frame, we have the birthdate and deathdate attributes. These attributes will be useful for filtering the authors that lived during the mid 19th century.\nWith this overview of {gutenbergr} and the data frames that it contains, we can now begin to develop our data acquisition plan.\n\nSelect the authors that lived during the mid 19th century from the gutenberg_authors data frame.\nSelect the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the gutenberg_subjects data frame.\nSelect works from gutenberg_metadata that are associated with the authors and subjects selected in steps 1 and 2.\nDownload the text and metadata for the works selected in step 3 using the gutenberg_download() function.\nWrite the data to disk in an appropriate format.\n\n\n\nData collection\nLet’s take each of these steps in turn. First, we need to select the authors that lived during the mid 19th century from the gutenberg_authors data frame. To do this we will use the filter() function. We will pass the gutenberg_authors data frame to the filter() function and then use the birthdate column to select the authors that were born after 1800 and died before 1880 –this year is chosen as the mid 19th century is generally considered to be the period from 1830 to 1870. We will then assign the result to the variable name authors.\n\nauthors &lt;-\n  gutenberg_authors |&gt;\n  filter(\n    birthdate &gt; 1800,\n    deathdate &lt; 1880\n  )\n\nThat’s it! We now have a data frame with the authors that lived during the mid 19th century, some 787 authors in total. This will span all subjects and languages, so this isn’t the final number of authors we will be working with.\nThe next step is to select the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the gutenberg_subjects data frame. To do this we will use the filter() function again. We will pass the gutenberg_subjects data frame to the filter() function and then use the subject_type and subject columns to select the works that are associated with the Library of Congress Classification for English Literature (PR) and American Literature (PS). We will then assign the result to the variable name subjects.\n\nsubjects &lt;-\n  gutenberg_subjects |&gt;\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n\nNow, we have a data frame with the subjects that we are interested in. Let’s inspect this data frame to see how many works we have for each subject.\n\nsubjects |&gt;\n  count(subject)\n\n# A tibble: 2 × 2\n  subject     n\n  &lt;chr&gt;   &lt;int&gt;\n1 PR       9926\n2 PS      10953\n\n\nThe next step is to subset the gutenberg_metadata data frame to select works from the authors and subjects selected in the previous steps. Again, we will use filter() to do this. We will pass the gutenberg_metadata data frame to the filter() function and then use the gutenberg_author_id and gutenberg_id columns to select the works that are associated with the authors and subjects selected in the previous steps. We will then assign the result to the variable name works.\n\nworks &lt;-\n  gutenberg_metadata |&gt;\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n\nworks\n\n# A tibble: 1,014 × 8\n   gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n          &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n 1           33 The Sca… Hawth…                  28 en       \"Harvard Classics/…\n 2           46 A Chris… Dicke…                  37 en       \"Children's Litera…\n 3           71 On the … Thore…                  54 en       \"\"                 \n 4           77 The Hou… Hawth…                  28 en       \"Best Books Ever L…\n 5           98 A Tale … Dicke…                  37 en       \"Historical Fictio…\n 6          205 Walden,… Thore…                  54 en       \"\"                 \n 7          258 Poems b… Gordo…                 145 en       \"\"                 \n 8          271 Black B… Sewel…                 154 en       \"Best Books Ever L…\n 9          292 Beauty … Taylo…                 167 en       \"\"                 \n10          394 Cranford Gaske…                 220 en       \"\"                 \n# ℹ 1,004 more rows\n# ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\n\nFiltering the gutenberg_metadata data frame by the authors and subjects selected in the previous steps, we now have a data frame with 1,014 works. This is the final number of works we will be working with so we can now download the text and metadata for these works using the gutenberg_download() function.\nA few things to note about the gutenberg_download() function. First, it is vectorized, that is, it can take a single value or multiple values for the argument gutenberg_id. This is good as we will be passing a vector of gutenberg ids to the function. A small fraction of the works on Project Gutenberg are not in the public domain and therefore cannot be downloaded, this is documented in the rights column. Furthermore, not all of the works have text available, as seen in the has_text column. Finally, the gutenberg_download() function returns a data frame with the gutenberg id and the text of the work(s) –but we can also select additional attributes to be returned by passing a character vector of the attribute names to the argument meta_fields. The column names of the gutenberg_metadata data frame contains the available attributes.\nWith this in mind, let’s do a quick test before we download all of the works. Let’s select the first 5 works from the works data frame that fit our criteria and then download the text and metadata for these works using the gutenberg_download() function. We will then assign the result to the variable name works_sample.\n\nworks_sample &lt;-\n  works |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  slice_head(n = 5) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  )\n\nworks_sample\n\n# A tibble: 34,385 × 6\n   gutenberg_id text        title author gutenberg_author_id gutenberg_bookshelf\n          &lt;int&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;              \n 1           33 \"The Scarl… The … Hawth…                  28 Harvard Classics/M…\n 2           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 3           33 \"by Nathan… The … Hawth…                  28 Harvard Classics/M…\n 4           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 5           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 6           33 \"Contents\"  The … Hawth…                  28 Harvard Classics/M…\n 7           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 8           33 \" THE CUST… The … Hawth…                  28 Harvard Classics/M…\n 9           33 \" THE SCAR… The … Hawth…                  28 Harvard Classics/M…\n10           33 \" I. THE P… The … Hawth…                  28 Harvard Classics/M…\n# ℹ 34,375 more rows\n\n\nLet’s inspect the works_sample data frame. First, from the output we can see that all of our meta data attributes were returned. Second, we can see that the text column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded, even blank lines. To make sure that we have the correct number of works, we can use the count() function to count the number of works by gutenberg_id.\n\nworks_sample |&gt;\n  count(gutenberg_id)\n\n# A tibble: 4 × 2\n  gutenberg_id     n\n         &lt;int&gt; &lt;int&gt;\n1           33  8212\n2          258 11050\n3          271  5997\n4          292  9126\n\n\nYes, we have 5 works and we can see how many lines are in each of these works.\nWe could now run this code on the entire works data frame and then write the data to disk like so:\nworks |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |&gt;\n  write_csv(file = \"data/original/gutenberg/works.csv\")\nThis would accomplish the primary goal of our data acquisition plan.\nHowever, there is some key functionality that we are missing if we would like to make this code more reproducible-friendly. First, we are not checking to see if the data already exists on disk. If we already have run this code in our script, we likely do not want to run it again. Second, we may want to use this code again with different parameters, for example, we may want to retrieve different subject codes, or different time periods, or other languages.\nAll three of these additional features can be accomplished with writing a custom function. Let’s take a look at the code we have written so far and see how we can turn this into a custom function.\n# Get authors within years\nauthors &lt;-\n  gutenberg_authors |&gt;\n  filter(\n    birthdate &gt; 1800,\n    deathdate &lt; 1880\n  )\n# Get LCC subjects\nsubjects &lt;-\n  gutenberg_subjects |&gt;\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n# Get works based on authors and subjects\nworks &lt;-\n  gutenberg_metadata |&gt;\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n# Download works\nworks |&gt;\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |&gt;\n  write_csv(file = \"data/original/gutenberg/works.csv\")\n\nBuild the custom function\n\nNameArgumentsCode: commentsCode: packagesCode: data checkCode: authorsCode: subjectCode: worksCode: downloadCode: write\n\n\nLet’s start to create our function by creating a name and calling the function() function. We will name our function get_gutenberg_works().\nget_gutenberg_works &lt;- function() {\n\n}\n\n\nNow, we need to think of the arguments that we would like to pass to our function so they can be used to customize the data acquisition process. First, we want to check to see if the data already exists on disk. To do this we will need to pass the path to the data file to our function. We will name this argument target_file.\nget_gutenberg_works &lt;- function(target_file) {\n\n}\nNext, we want to pass the subject code that the works should be associated with. We will name this argument lcc_subject.\nget_gutenberg_works &lt;- function(target_file, lcc_subject) {\n\n}\nFinally, we want to pass the birth year and death year that the authors should be associated with. We will name these arguments birth_year and death_year.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n\n}\n\n\nWe now turn to the code. I like to start by creating comments to describe the steps inside the function before adding code.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe have some packages we want to make sure are installed and loaded. We will use the {pacman} package to do this. We will use the p_load() function to install and load the packages. We will pass the character vector of package names to the p_load() function.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe need to create the code to check if the data exists. We will use an if statement to do this. If the data does exist, we will print a message to the console that the data already exists and stop the function. If the data does not exist, we will create the directory structure and continue with the data acquisition process. I will use {fs} (Hester, Wickham, and Csárdi 2024) in this code so I will load the library at the top of the function.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nLet’s now add the code to get the authors within the years. We will now use the birth_year and death_year arguments to filter the gutenberg_authors data frame.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nUsing the lcc_subject argument, we will now filter the gutenberg_subjects data frame.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe will use the authors and subjects data frames to filter the gutenberg_metadata data frame as before.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n\n  # Write works to disk\n}\n\n\nWe will now use the works data frame to download the text and metadata for the works using the gutenberg_download() function and assign it to results.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results &lt;-\n    works |&gt;\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |&gt;\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n}\n\n\nFinally, we will write the results data frame to disk using the write_csv() function and the target_file argument.\nget_gutenberg_works &lt;- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir &lt;- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors &lt;-\n    gutenberg_authors |&gt;\n    filter(\n      birthdate &gt; birth_year,\n      deathdate &lt; death_year\n    )\n\n  # Get LCC subjects\n  subjects &lt;-\n    gutenberg_subjects |&gt;\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works &lt;-\n    gutenberg_metadata |&gt;\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results &lt;-\n    works |&gt;\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |&gt;\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n  write_csv(results, file = target_file)\n}\n\n\n\n\n\nUsing the custom function\nWe now have a function, get_gutenberg_works(), that we can use to acquire works from Project Gutenberg for a given LCC code for authors that lived during a given time period. We now have a flexible function that we can use to acquire data.\nWe can add this function to the script in which we use it, or we can add it to a separate script and source it into any script in which we want to use it.\n# Source function\nsource(\"get_gutenberg_works.R\")\n\n# Get works for PR and PS for authors born between 1800 and 1880\nget_gutenberg_works(\n  target_file = \"data/original/gutenberg/works.csv\",\n  lcc_subject = c(\"PR\", \"PS\"),\n  birth_year = 1800,\n  death_year = 1880\n)\nAnother option is to add this function to your own package. This is a great option if you plan to use this function in multiple projects or share it with others. Since I have already created a package for this book, {qtkit}, I’ve added this function, with some additional functionality, to the package.\n\n# Load package\nlibrary(qtalrkit)\n\n# Get works for fiction for authors born between 1870 and 1920\nget_gutenberg_works(\n  target_dir = \"data/original/gutenberg/\",\n  lcc_subject = \"PZ\",\n  birth_year = 1870,\n  death_year = 1920\n)\nThis modified function will create a directory structure for the data file if it does not already exist. It will also create a file name for the data file based on the arguments passed to the function.\n\n\n\nData documentation\nFinding data sources and collecting data are important steps in the acquisition process. However, it is also important to document the data collection process. This is important so that you, and others, can reproduce the data collection process.\nIn data acquisition, the documentation is includes the code, code comments, and prose in the process file used to acquire the data and also a data origin file. The data origin file is a text file that describes the data source and the data collection process.\nThe {qtkit} package includes a function, create_data_origin(), that can be used to scaffold a data origin file. This simply takes a file path and creates a data origin file in CSV format.\nattribute,description\nResource name,The name of the resource.\nData source,\"URL, DOI, etc.\"\nData sampling frame,\"Language, language variety, modality, genre, etc.\"\nData collection date(s),The dates the data was collected.\nData format,\".txt, .csv, .xml, .html, etc.\"\nData schema,\"Relationships between data elements: files, folders, etc.\"\nLicense,\"CC BY, CC BY-SA, etc.\"\nAttribution,Citation information.\nThe you edit this file and ensure that it contains all of the information needed to document the data. Make sure that this file is near the data file so that it is easy to find.\ndata\n  ├── analysis/\n  ├── derived/\n  └── original/\n      ├── works_do.csv\n      └── gutenberg/\n          ├── works_pr.csv\n          └── works_ps.csv"
  },
  {
    "objectID": "recipes/recipe-05/index.html#summary",
    "href": "recipes/recipe-05/index.html#summary",
    "title": "05. Collecting and documenting data",
    "section": "Summary",
    "text": "Summary\nIn this recipe, we have covered acquiring data for a text analysis project. We used the {gutenbergr} (Johnston and Robinson 2023) to acquire works from Project Gutenberg. After exploring the resources available, we established an acquisition plan. We then used R to implement our plan. To make our code more reproducible-friendly, we wrote a custom function to acquire the data. Finally, we discussed the importance of documenting the data collection process and introduced the data origin file."
  },
  {
    "objectID": "recipes/recipe-05/index.html#check-your-understanding",
    "href": "recipes/recipe-05/index.html#check-your-understanding",
    "title": "05. Collecting and documenting data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nIn the chapter and in this recipe, strategies for acquiring data were discussed. Which of the following was not discussed as a strategy for acquiring data? Direct downloadProgrammatic downloadAPIsWeb scraping\nIn this recipe, we used {gutenbergr} to acquire works from Project Gutenberg. What is the name of the function that we used to acquire the actual text? gutenberg_metatagutenberg_get()gutenberg_search()gutenberg_download()\nTrueFalse A custom function is only really necessary if you are writting an R package.\nWhen writing a custom function, what is the first step? Write the codeWrite the commentsLoad the packagesCreate the function arguments\nWhat does it mean when we say that a function is ‘vectorized’ in R? The function returns a vectorThe function can take a vector as an argumentThe function can take a vector and operates on each element of the vector\nWhich Tidyverse package allows us to apply non-vectorized functions to vectors? dplyrstringrreadrpurrr"
  },
  {
    "objectID": "recipes/recipe-05/index.html#lab-preparation",
    "href": "recipes/recipe-05/index.html#lab-preparation",
    "title": "05. Collecting and documenting data",
    "section": "Lab preparation",
    "text": "Lab preparation\nBefore beginning Lab 5, make sure you are comfortable with the following:\n\nReading and subsetting data in R\nWriting data in R\nThe project structure of reproducible projects\n\nThe additional skills covered in this lab are:\n\nIdentifying data sources\nAcquiring data through manual and programmatic downloads and APIs\nCreating a data acquisition plan\nDocumenting the data collection process\nWriting a custom function\nDocumenting the data source with a data origin file\n\nYou will have a choice of data source to acquire data from. Before you start the lab, you should consider which data source you would like to use, what strategy you will use to acquire the data, and what data you will acquire. You should also consider the information you need to document the data collection process.\nConsult the Identifying data and data sources guide for some ideas on where to find data sources."
  },
  {
    "objectID": "recipes/recipe-03/index.html",
    "href": "recipes/recipe-03/index.html",
    "title": "03. Descriptive assessment of datasets",
    "section": "",
    "text": "Skills\n\nSummary overviews of datasets with {skimr}\nSummary statistics with {dplyr}\nCreating Quarto tables with {knitr}\nCreating Quarto plots with {ggplot2}"
  },
  {
    "objectID": "recipes/recipe-03/index.html#concepts-and-strategies",
    "href": "recipes/recipe-03/index.html#concepts-and-strategies",
    "title": "03. Descriptive assessment of datasets",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\nIn this Recipe, we will use the PassiveBrownFam dataset from {corpora} (Evert 2023). This dataset contains information on the passive voice usage in the Brown family of corpora. The dataset contains 11 variables and 2,449 observations.\nI have assigned this dataset to the object brown_fam_df and have made minor modifications to the variable names to improve the readability of the dataset.\n\n\n# Load packages\nlibrary(dplyr)\n\n# Read the dataset from {corpora}\nbrown_fam_df &lt;-\n  corpora::PassiveBrownFam |&gt; # reference the dataset\n  as_tibble() # convert to a tibble\n\n# Rename variables\nbrown_fam_df &lt;-\n  brown_fam_df |&gt; # pass the original dataset\n  rename( # rename variables: new_name = old_name\n    lang_variety = lang,\n    num_words = n.words,\n    active_verbs = act,\n    passive_verbs = pass,\n    total_verbs = verbs,\n    percent_passive = p.pass\n  )\n\n# Preview\nglimpse(brown_fam_df)\n\nRows: 2,499\nColumns: 11\n$ id              &lt;chr&gt; \"brown_A01\", \"brown_A02\", \"brown_A03\", \"brown_A04\", \"b…\n$ corpus          &lt;fct&gt; Brown, Brown, Brown, Brown, Brown, Brown, Brown, Brown…\n$ section         &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, …\n$ genre           &lt;fct&gt; press reportage, press reportage, press reportage, pre…\n$ period          &lt;fct&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, …\n$ lang_variety    &lt;fct&gt; AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE,…\n$ num_words       &lt;int&gt; 2080, 2116, 2051, 2095, 2111, 2102, 2099, 2069, 2058, …\n$ active_verbs    &lt;int&gt; 164, 154, 135, 128, 170, 166, 165, 163, 153, 169, 132,…\n$ passive_verbs   &lt;int&gt; 40, 25, 34, 25, 32, 21, 31, 19, 39, 23, 17, 10, 15, 26…\n$ total_verbs     &lt;int&gt; 204, 179, 169, 153, 202, 187, 196, 182, 192, 192, 149,…\n$ percent_passive &lt;dbl&gt; 19.61, 13.97, 20.12, 16.34, 15.84, 11.23, 15.82, 10.44…\n\n\nYou can learn more about these variables by reading the dataset documentation with ?corpora::PassiveBrownFam.\n\nStatistical overviews\nUnderstanding our data is of utmost importance before, during, and after analysis. After we get to know our data by inspecting the data origin, dictionary, and structure, we then move to summarizing the data.\nA statistical overview of the data is a good place to start as it gives us a sense of all of the variables and variable types in the dataset. We can use {skimr} to create a statistical overview of the data, using the very convienent skim() function.\nLet’s create a statistical overview of the brown_fam_df dataset.\n\n# Load packages\nlibrary(skimr)\n\n# Create a statistical overview of the `brown_fam_df` dataset\nskim(brown_fam_df)\n\n\n# ── Data Summary ────────────────────────\n#                            Values      \n# Name                       brown_fam_df\n# Number of rows             2499        \n# Number of columns          11          \n# _______________________                \n# Column type frequency:                 \n#   character                1           \n#   factor                   5           \n#   numeric                  5           \n# ________________________               \n# Group variables            None        \n# \n# ── Variable type: character ────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate min max empty n_unique whitespace\n# 1 id                    0             1   7   9     0     2499          0\n# \n# ── Variable type: factor ───────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate ordered n_unique\n# 1 corpus                0             1 FALSE          5\n# 2 section               0             1 FALSE         15\n# 3 genre                 0             1 FALSE         15\n# 4 period                0             1 FALSE          3\n# 5 lang_variety          0             1 FALSE          2\n#   top_counts                            \n# 1 BLO: 500, Bro: 500, LOB: 500, FLO: 500\n# 2 J: 400, G: 381, F: 228, A: 220        \n# 3 lea: 400, bel: 381, pop: 228, pre: 220\n# 4 196: 1000, 199: 999, 193: 500         \n# 5 BrE: 1500, AmE: 999                   \n# \n# ── Variable type: numeric ──────────────────────────────────────────────────────\n#   skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50\n# 1 num_words               0             1 2165.  97.8  1406     2127    2163  \n# 2 active_verbs            0             1  179.  56.6    39      139     170  \n# 3 passive_verbs           0             1   25.7 12.9     2       16      23  \n# 4 total_verbs             0             1  204.  49.1    66      170     196  \n# 5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1\n#      p75   p100 hist \n# 1 2200   4397   ▁▇▁▁▁\n# 2  214    551   ▃▇▂▁▁\n# 3   32     86   ▆▇▂▁▁\n# 4  234    571   ▃▇▂▁▁\n# 5   18.2   67.7 ▇▅▁▁▁\n\n\nThe output of the skim() function contains a lot of information but it essentially has two parts: a summary of the dataset and a summary of each variable in the dataset. The summary of each of the variables, however, is grouped by variable type. Remember, each of our variables in a data frame is a vector and each vector has a type.\nWe have already learned about different types of vectors in R, including character, numeric, and logical. In this dataset, we are presented with a new type of vector: a factor. A factor is essentially a character vector that contains a set of discrete values, or levels. Factors can be ordered or unordered and can contain levels that are not present in the data.\nNow, looking at each of the variable types, we can see that we have 1 character variable, 5 factor variables, and 5 numeric variables. Each of these variable types assume a different set of summary statistics. For example, we can calculate the mean of a numeric variable but not of a character variable. Or, we can count the number of unique values in a character variable but not in a numeric variable.\nFor all variables, skim() will also provide the number of missing values and the percent of non-missing values.\nInspecting the entire dataset is a good place to start but at some point we often want focus in on a set of variables. We can add the yank() function to extract the statistical overview of a set of variables by their variable types.\nLet’s extract the statistical overview of the numeric variables in the brown_fam_df dataset.\n\n# Extract the statistical overview of the numeric variables\nbrown_fam_df |&gt;\n  skim() |&gt;\n  yank(\"numeric\")\n\n── Variable type: numeric ─────────────────────────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50    p75   p100 hist\n1 num_words               0             1 2165.  97.8  1406     2127    2163   2200   4397   ▁▇▁▁▁\n2 active_verbs            0             1  179.  56.6    39      139     170    214    551   ▃▇▂▁▁\n3 passive_verbs           0             1   25.7 12.9     2       16      23     32     86   ▆▇▂▁▁\n4 total_verbs             0             1  204.  49.1    66      170     196    234    571   ▃▇▂▁▁\n5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1   18.2   67.7 ▇▅▁▁▁\n\n\nSummary statistics of particular variables\nThese summary statistics are useful but for a preliminary and interactive use, but it is oftent the case that we will want to focus in on a particular variable or set of variables and their potential relationships to other variables.\nWe can use {dplyr} to calculate summary statistics for a particular variable or set of variables. We can use the group_by() function to group the data by a particular variable or variables. Then we can use the summarize() function to calculate summary statistics for the grouped data.\nFor example, let’s calculate the mean and median of the percent_passive variable in the brown_fam_df dataset grouped by the lang_variety variable.\n\n# Mean and median of `percent_passive` grouped by `lang_variety`\nbrown_fam_df |&gt;\n  group_by(lang_variety) |&gt;\n  summarize(\n    mean_percent_passive = mean(percent_passive),\n    median_percent_passive = median(percent_passive)\n  )\n\n# A tibble: 2 × 3\n  lang_variety mean_percent_passive median_percent_passive\n  &lt;fct&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;\n1 AmE                          12.9                   11.0\n2 BrE                          14.8                   13.3\n\n\nThe result is a 2x3 data frame which includes both the mean and median of the percent_passive variable for each of the two levels of the lang_variety variable.\nThe group_by() function can also be used to group by multiple variables. For example, let’s calculate the mean and median of the percent_passive variable in the brown_fam_df dataset grouped by the lang_variety and genre variables.\n\n# Mean and median of `percent_passive` grouped by\n# `lang_variety` and `genre`\nbrown_fam_df |&gt;\n  group_by(lang_variety, genre) |&gt;\n  summarize(\n    mean_percent_passive = mean(percent_passive),\n    median_percent_passive = median(percent_passive)\n  )\n\n# A tibble: 30 × 4\n# Groups:   lang_variety [2]\n   lang_variety genre            mean_percent_passive median_percent_passive\n   &lt;fct&gt;        &lt;fct&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;\n 1 AmE          press reportage                 11.5                   11.0 \n 2 AmE          press editorial                 10.6                   10.1 \n 3 AmE          press reviews                    9.54                   9.77\n 4 AmE          religion                        14.3                   14.3 \n 5 AmE          skills / hobbies                14.9                   13.9 \n 6 AmE          popular lore                    14.0                   12.7 \n 7 AmE          belles lettres                  12.0                   11.7 \n 8 AmE          miscellaneous                   23.5                   23.3 \n 9 AmE          learned                         21.3                   18.3 \n10 AmE          general fiction                  6.22                   5.89\n# ℹ 20 more rows\n\n\nFor numeric variables, such as percent_passive, there are a number of summary statistics that we can calculate. We’ve seen the R functions for mean and median but we can also calculate the standard deviation (sd()), variance (var()), minimum (min()), maximum (max()), interquartile range (IQR()), median absolute deviation (mad()), and quantiles (quantile()). All these calculations make sense for numeric variables but not for character variables.\nFor character variables, and factors, the summary statistics are more limited. We can calculate the number of observations (n()) and/ or the number of unique values (n_distinct()). Let’s now summarize the number of observations n() grouped by the genre variable in the brown_fam_df dataset.\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  group_by(genre) |&gt;\n  summarize(\n    n = n(),\n  )\n\n# A tibble: 15 × 2\n   genre                n\n   &lt;fct&gt;            &lt;int&gt;\n 1 press reportage    220\n 2 press editorial    135\n 3 press reviews       85\n 4 religion            85\n 5 skills / hobbies   186\n 6 popular lore       228\n 7 belles lettres     381\n 8 miscellaneous      150\n 9 learned            400\n10 general fiction    145\n11 detective          120\n12 science fiction     30\n13 adventure          144\n14 romance            145\n15 humour              45\n\n\nJust as before, we can add multiple grouping variables to group_by(). Let’s add lang_variety to the grouping and calculate the number of observations n() grouped by the genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  )\n\n# A tibble: 30 × 3\n# Groups:   genre [15]\n   genre            lang_variety     n\n   &lt;fct&gt;            &lt;fct&gt;        &lt;int&gt;\n 1 press reportage  AmE             88\n 2 press reportage  BrE            132\n 3 press editorial  AmE             54\n 4 press editorial  BrE             81\n 5 press reviews    AmE             34\n 6 press reviews    BrE             51\n 7 religion         AmE             34\n 8 religion         BrE             51\n 9 skills / hobbies AmE             72\n10 skills / hobbies BrE            114\n# ℹ 20 more rows\n\n\n\n\n\n\n\n\n Tip\nThe result of calculating the number of observations for a character or factor variable is known as a frequency table. Grouping two or more categorical variables is known as a cross-tabulation or a contingency table.\n\n\n\nNow, we can also pipe the results of a group_by() and summarize() to another function. This can be to say sort, select, or filter the results. It can also be to perform another summary function. It is important, however, to remember that the result of a group_by() produces a grouped data frame. Subsequent functions will be applied to the grouped data frame. This can lead to unexpected results if the original grouping is not relevant for the subsequent function. To avoid this, we can use the ungroup() function to remove the grouping after the relevant grouped summary statistics have been calculated.\nLet’s return to calculating the number of observations n() grouped by the genre and lang_variety variables in the brown_fam_df dataset. But let’s add another summary which uses the n variable to calculate the mean and median number of observations.\nIf we do not use the ungroup() function, the mean and median will be calculated for each genre collapsed across lang_variety.\n\n# Mean and median of `n` grouped by `genre`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  ) |&gt;\n  summarize(\n    mean_n = mean(n),\n    median_n = median(n)\n  )\n\n# A tibble: 15 × 3\n   genre            mean_n median_n\n   &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1 press reportage   110      110  \n 2 press editorial    67.5     67.5\n 3 press reviews      42.5     42.5\n 4 religion           42.5     42.5\n 5 skills / hobbies   93       93  \n 6 popular lore      114      114  \n 7 belles lettres    190.     190. \n 8 miscellaneous      75       75  \n 9 learned           200      200  \n10 general fiction    72.5     72.5\n11 detective          60       60  \n12 science fiction    15       15  \n13 adventure          72       72  \n14 romance            72.5     72.5\n15 humour             22.5     22.5\n\n\nTherefore we see that we have a mean and median calculated for the number of documents in the corpus for each of the 15 genres.\nIf we use the ungroup() function, the mean and median will be calculated for all genres. Note we will use the ungroup() function between these summaries to clear the grouping before calculating the mean and median.\n\n# Number of observations for each `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  group_by(genre, lang_variety) |&gt;\n  summarize(\n    n = n(),\n  ) |&gt;\n  ungroup() |&gt;\n  summarize(\n    mean_n = mean(n),\n    median_n = median(n)\n  )\n\n# A tibble: 1 × 2\n  mean_n median_n\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   83.3       72\n\n\nNow we see that we have a mean and median calculated across all genres.\n\nBefore we leave this section, let’s look some other ways to create frequency and contingency tables for character and factor variables. A shortcut to calculate a frequency table for a character or factor variable is to use the count() function from {dplyr}.\nLet’s calculate the number of observations grouped by the genre variable in the brown_fam_df dataset.\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  count(genre)\n\n# A tibble: 15 × 2\n   genre                n\n   &lt;fct&gt;            &lt;int&gt;\n 1 press reportage    220\n 2 press editorial    135\n 3 press reviews       85\n 4 religion            85\n 5 skills / hobbies   186\n 6 popular lore       228\n 7 belles lettres     381\n 8 miscellaneous      150\n 9 learned            400\n10 general fiction    145\n11 detective          120\n12 science fiction     30\n13 adventure          144\n14 romance            145\n15 humour              45\n\n\nWe can also add multiple grouping variables to count() and create contingency tables.\nLet’s add lang_variety to the grouping and create a cross-tabulation for genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  count(genre, lang_variety)\n\n# A tibble: 30 × 3\n   genre            lang_variety     n\n   &lt;fct&gt;            &lt;fct&gt;        &lt;int&gt;\n 1 press reportage  AmE             88\n 2 press reportage  BrE            132\n 3 press editorial  AmE             54\n 4 press editorial  BrE             81\n 5 press reviews    AmE             34\n 6 press reviews    BrE             51\n 7 religion         AmE             34\n 8 religion         BrE             51\n 9 skills / hobbies AmE             72\n10 skills / hobbies BrE            114\n# ℹ 20 more rows\n\n\nNote that the results of count() are not grouped so we do not need to use the ungroup() function before calculating subsequent summary statistics.\nAnother way to create frequency and contingency tables is to use the tabyl() function from {janitor} (Firke 2023). Let’s create a frequency table for the genre variable in the brown_fam_df dataset.\n\n# Load packages\nlibrary(janitor)\n\n# Frequency table for `genre`\nbrown_fam_df |&gt;\n  tabyl(genre)\n\n            genre   n percent\n  press reportage 220  0.0880\n  press editorial 135  0.0540\n    press reviews  85  0.0340\n         religion  85  0.0340\n skills / hobbies 186  0.0744\n     popular lore 228  0.0912\n   belles lettres 381  0.1525\n    miscellaneous 150  0.0600\n          learned 400  0.1601\n  general fiction 145  0.0580\n        detective 120  0.0480\n  science fiction  30  0.0120\n        adventure 144  0.0576\n          romance 145  0.0580\n           humour  45  0.0180\n\n\nIn addition to providing frequency counts, the tabyl() function also provides the percent of observations for each level of the variable. And, we can add up to three grouping variables to tabyl() as well.\nLet’s add lang_variety to the grouping and create a contingency table for the genre and lang_variety variables in the brown_fam_df dataset.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbrown_fam_df |&gt;\n  tabyl(genre, lang_variety)\n\n            genre AmE BrE\n  press reportage  88 132\n  press editorial  54  81\n    press reviews  34  51\n         religion  34  51\n skills / hobbies  72 114\n     popular lore  96 132\n   belles lettres 150 231\n    miscellaneous  60  90\n          learned 160 240\n  general fiction  58  87\n        detective  48  72\n  science fiction  12  18\n        adventure  57  87\n          romance  58  87\n           humour  18  27\n\n\nThe results do not include the percent of observations for each level of the variable as it is not clear how to calculate the percent of observations for each level of the variable when there are multiple grouping variables. We must specify if we want to calculate the percent of observations by row or by column.\n\n\n\n\n\n\n Dive deeper\n{janitor} includes a variety of adorn_*() functions to add additional information to the results of tabyl(), including percentages, frequencies, and totals. Feel free to explore these functions on your own. We will return to this topic again later in the course.\n\n\n\n\n\nCreating Quarto tables\nSummarizing the data is not only useful for our understanding of the data as part of our analysis but also for communicating the data in reports, manuscripts, and presentations.\nOne way to communicate summary statistics is with tables. In Quarto, we can use {knitr} (Xie 2024) in combination with code block options to produce formatted tables which we can cross-reference in our prose sections.\nLet’s create an object from the cross-tabulation for the genre and lang_variety variables in the brown_fam_df dataset to work with.\n\n# Cross-tabulation for `genre` and `lang_variety`\nbf_genre_lang_ct &lt;-\n  brown_fam_df |&gt;\n  tabyl(genre, lang_variety)\n\nTo create a table in Quarto, we use the kable() function. The kable() function takes a data frame (or matrix) as an argument. The format argument will be derived from the Quarto document format (‘html’, ‘pdf’, etc.).\n\n# Load packages\nlibrary(knitr)\n\n# Create a table in Quarto\nkable(bf_genre_lang_ct)\n\n\n\n\ngenre\nAmE\nBrE\n\n\n\n\npress reportage\n88\n132\n\n\npress editorial\n54\n81\n\n\npress reviews\n34\n51\n\n\nreligion\n34\n51\n\n\nskills / hobbies\n72\n114\n\n\npopular lore\n96\n132\n\n\nbelles lettres\n150\n231\n\n\nmiscellaneous\n60\n90\n\n\nlearned\n160\n240\n\n\ngeneral fiction\n58\n87\n\n\ndetective\n48\n72\n\n\nscience fiction\n12\n18\n\n\nadventure\n57\n87\n\n\nromance\n58\n87\n\n\nhumour\n18\n27\n\n\n\n\n\nTo add a caption to the table and to enable cross-referencing, we use the code block options label and tbl-cap. The label option takes a label prefixed with tbl- to create a cross-reference to the table. The tbl-cap option takes a caption for the table, in quotation marks.\n#| label: tbl-brown-genre-lang-ct\n#| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"\n\n# Create a table in Quarto\nkable(bf_genre_lang_ct)\nNow we can cross-reference the table with the @tbl-brown-genre-lang-ct syntax. So the following Quarto document will produce the following prose with a cross-reference to the formatted table output.\n\nAs we see in @tbl-brown-genre-lang-ct, the distribution of `genre` is similar across `lang_variety`.\n\n```{r}\n#| label: tbl-brown-genre-lang-ct\n#| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"\n\n# Print cross-tabulation\nkable(bf_genre_lang_ct)\n```\n\nAs we see in Table 1, the distribution of genre is similar across lang_variety.\n\n\n\n\nTable 1: Cross-tabulation of genre and lang_variety\n\n\n\n\n\n\ngenre\nAmE\nBrE\n\n\n\n\npress reportage\n88\n132\n\n\npress editorial\n54\n81\n\n\npress reviews\n34\n51\n\n\nreligion\n34\n51\n\n\nskills / hobbies\n72\n114\n\n\npopular lore\n96\n132\n\n\nbelles lettres\n150\n231\n\n\nmiscellaneous\n60\n90\n\n\nlearned\n160\n240\n\n\ngeneral fiction\n58\n87\n\n\ndetective\n48\n72\n\n\nscience fiction\n12\n18\n\n\nadventure\n57\n87\n\n\nromance\n58\n87\n\n\nhumour\n18\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\n{kableExtra} (Zhu 2024) provides additional functionality for formatting tables in Quarto.\n\n\n\n\n\nCreating Quarto plots\nWhere tables are useful for communicating summary statistics for numeric and character variables, plots are useful for communicating relationships between variables especially when one or more of the variables is numeric. Furthermore, for complex relationships, plots can be more effective than tables.\nIn Quarto, we can use {ggplot2} (Wickham et al. 2024) in combination with code block options to produce formatted plots which we can cross-reference in our prose sections.\nLet’s see this in action with a simple histogram of the percent_passive variable in the brown_fam_df dataset. The Quarto document will produce the following prose with a cross-reference to the formatted plot output.\nAs we see in @fig-brown-fam-percent-passive-hist, the distribution of `percent_passive` is skewed to the right.\n\n```{r}\n#| label: fig-brown-fam-percent-passive-hist\n#| fig-cap: \"Histogram of `percent_passive`\"\n\n# Create a histogram in Quarto\nggplot(brown_fam_df) +\n  geom_histogram(aes(x = percent_passive))\n```\n\nAs we see in Figure 1, the distribution of percent_passive is skewed to the right.\n\n\n\n\n\n\n\n\nFigure 1: Histogram of percent_passive\n\n\n\n\n\n\n{ggplot2} implements the ‘Grammar of Graphics’ approach to creating plots. This approach is based on the idea that plots can be broken down into components, or layers, and that each layer can be manipulated independently.\nThe main components are data, aesthetics, and geometries. Data is the data frame that contains the variables to be plotted. Aesthetics are the variables that will be mapped to the x-axis, y-axis (as well as color, shape, size, etc.). Geometries are the visual elements that will be used to represent the data, such as points, lines, bars, etc..\nAs discussed in the R lesson “Visual Summaries”, the aes() function is used to map variables to aesthetics and can be added to the ggplot() function or to the geom_*() function depending on whether the aesthetic is mapped to all geometries or to a specific geometry, respectively.\nTake a look at the following stages of the earlier plot in each of the tabs below.\n\nStages\n\nDataAestheticsGeometries\n\n\nThe data layer does not produce a plot but it is the foundation of the plot.\n\n# Data layer\nggplot(brown_fam_df)\n\n\n\n\n\n\n\n\n\n\nThe aesthetics layer does not produce a plot but it maps the variables to the aesthetics to be used in the plot.\n\n# Aesthetics layer\nggplot(brown_fam_df, aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\nThe geometries layer produces the plot connecting the data and aesthetics layers in the particular way specified by the geometries, in this case a histogram.\n\n# Geometries layer\nggplot(brown_fam_df, aes(x = percent_passive)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right plot\nJust as with tables, the type of summary we choose to communicate with a plot depends on the type of variables we are working with and the relationships between those variables.\nBelow I’ve included a few examples of plots that can be used to communicate different types of variables and relationships.\n\n\nSingle numeric variable\n\nHistogramDensity plot\n\n\n\n# Histogram\nggplot(brown_fam_df) +\n  geom_histogram(aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\n\n# Density plot\nggplot(brown_fam_df) +\n  geom_density(aes(x = percent_passive))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumeric and categorical variables\n\nDensity plotBoxplotViolin plot\n\n\n\n# Density plot\nggplot(brown_fam_df) +\n  geom_density(\n    aes(\n      x = percent_passive,\n      fill = lang_variety\n    ),\n    alpha = 0.5 # adds transparency\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Boxplot\nggplot(brown_fam_df) +\n  geom_boxplot(\n    aes(\n      x = lang_variety,\n      y = percent_passive\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Violin plot\nggplot(brown_fam_df) +\n  geom_violin(\n    aes(\n      x = lang_variety,\n      y = percent_passive\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numeric variables\n\nScatterplotScatterplot with regression line\n\n\n\n# Scatterplot\nggplot(brown_fam_df) +\n  geom_point(\n    aes(\n      x = active_verbs,\n      y = passive_verbs\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n# Scatterplot with regression line\nggplot(\n  brown_fam_df,\n  aes(\n    x = active_verbs,\n    y = passive_verbs\n  )\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther variable combinations\nIn these examples, we have only looked at the most common variable combinations for one and two variable plots. There are more sophisticated plots that can be used for other variable combinations using {ggplot2}. For now, we will leave these for another time."
  },
  {
    "objectID": "recipes/recipe-03/index.html#check-your-understanding",
    "href": "recipes/recipe-03/index.html#check-your-understanding",
    "title": "03. Descriptive assessment of datasets",
    "section": "Check your understanding",
    "text": "Check your understanding\n\nA factor is a character vector augmented to include information about the discrete values, or levels, of the vector. TRUEFALSE\nWhat is the difference between a frequency table and a contingency table? A frequency table is a cross-tabulation of two or more categorical variables.A contingency table is a cross-tabulation of two or more categorical variables.\nThe skimrdplyrggplot2knitr package is used to create formatted tables in R.\nTo add a geometry layer, such as geom_histogram(), to a ggplot object the |&gt; operator is used. TRUEFALSE\nTo visualize the relationship between two numeric variables, a histogramdensity plotboxplotviolin plotscatterplot is often used.\nWhen the aes() function is added to the ggplot() function, the aesthetic is mapped to all geometries. TRUEFALSE"
  },
  {
    "objectID": "recipes/recipe-03/index.html#lab-preparation",
    "href": "recipes/recipe-03/index.html#lab-preparation",
    "title": "03. Descriptive assessment of datasets",
    "section": "Lab preparation",
    "text": "Lab preparation\n\nBefore beginning Lab 3, learners should be comfortable with the skills and knowledge developed in the previous recipes and labs. In this lab, you will have a chance to use these skills and those introduced in this Recipe to provide a descriptive assessment of a dataset that includes statistics, tables, and plots using Quarto and R.\nThe additional skills and knowledge you will need to complete Lab 3 include:\n\nSummarizing data with {skimr}\nSummarizing data with {dplyr}\nCreating Quarto tables with {knitr}\nCreating Quarto plots with {ggplot2}"
  },
  {
    "objectID": "recipes/recipe-10/index.html",
    "href": "recipes/recipe-10/index.html",
    "title": "10. Building inference models",
    "section": "",
    "text": "Skills\n\nidentify and map the hypothesis statement to the appropriate response and explanatory variables\nemploy simulation-based methods for statistical inference\ninterpret and evaluate the results of inferential models\nStatistical inference is the most structured approach to data analysis. It is the process of using data to draw conclusions about a population, therefore the underlying data and the process to conduct the analysis must be rigorous and exploration is limited and iteration is avoided. The workflow for building inference-based models can be seen in Table 1.\nBefore we begin, let’s load the packages we will use in this recipe.\nlibrary(readr) # for reading data\nlibrary(kableExtra) # for table formatting\nlibrary(dplyr) # for data wrangling\nlibrary(skimr) # for data summaries\nlibrary(janitor) # for data cleaning/ tablulations\nlibrary(ggplot2) # for data visualizations\nlibrary(infer) # for statistical inference"
  },
  {
    "objectID": "recipes/recipe-10/index.html#concepts-and-strategies",
    "href": "recipes/recipe-10/index.html#concepts-and-strategies",
    "title": "10. Building inference models",
    "section": "Concepts and strategies",
    "text": "Concepts and strategies\n\nOrientation\nIn the area of Second Language Acquisition and Teaching, the relationship between learner proficiency and particular linguistic variables has been a topic of interest for decades. In this section, we will explore the relationship between placement scores and lexical features of learner sample writing. The goal is to determine the extent to which a simplified set of lexical features can be employed as a diagnostic tool for assessing overall learner proficiency.\nThe background here comes from a study by Crossley et al. (2010), who investigated the relationship between lexical features and learner proficiency. The authors used a corpus of learner writing samples which were assessed and scored by human raters. Then a set of lexical features were extracted from the writing samples and subjected to a series of statistical analyses. The results suggested three key variables which were highly correlated with the human ratings of learner proficiency: lexical diversity, word hypernymy values, and content word frequency.\nIn this sample study, I will suggest that the variables lexical diversity and content word frequency actually represent a single underlying construct, which I will call “lexical sophistication”. Lexical diversity aims to gauge the range of vocabulary used by the learner. In the context of L2 writing, it is less likely that demonstrated range of unique vocabulary is what constitutes proficiency –as a lexically diverse text in which the writer uses primarily spoken register vocabulary is unlikely to be considered in academic writing contexts. Instead, rather it is more likely that the ability to use more sophisticated vocabulary is what is being measured. On the other hand, content word frequency purports to gauge the degree to which more infrequent words are used in L2 writing. However, I would argue that this is also a measure of lexical sophistication. In other words, the ability to use more sophisticated vocabulary inherently taps into the use of more infrequent words.\nThe goal of this study is to determine the extent to which this construct can be used as a proxy for lexical diversity and content word frequency, and thus as a diagnostic tool for assessing learner proficiency. The research statement is as follows:\n\nThe lexical sophistication of learner writing is positively correlated with learner proficiency.\n\nOperationalizing this statement requires a few steps. First, we need to identify the variables which will be used to represent the construct of lexical sophistication.\nIn addition, ideally the variables used to represent lexical sophistication should be easy to extract from learner writing samples, if this is to be used as a diagnostic tool. Plausible linguistic variables to consider in this study are the following:\n\nNumber of syllables per word\nNumber of morphemes per word\n\nIn addtion, to these variables, we will also consider word frequency estimates to maintain consistency with the original study. I will also consider the number of characters per word, although not strictly linguistic in nature, this is a variable which is easy to extract from learner writing samples and is likely to be correlated with the number of syllables and/ or morphemes per word.\nSecond, we need to identify the variables which will be used to represent learner proficiency. In this study, we will use the placement scores of the learners as a proxy for proficiency. The placement scores are based on the results of a placement test which was administered to the learners prior to the writing samples being collected.\nThe hypothesis statement is as follows:\n\nLearner proficiency as measured by placement scores is positively correlated with lexical sophistication as measured by the number of syllables per word, number of morphemes per word, and word frequency estimates.\n\n\n\nAnalysis\nThe dataset used in this study is is transformed version of the Pittsburgh English Language Institute Corpus (PELIC). Writing samples and placement scores were extracted from the corpus for learners in the English for Academic Purposes (EAP) program. The tokenized writing samples were filtered for content words (i.e. nouns, verbs, adjectives). Subsequently, lexical features were joined from the English Lexicon Project (ELP) database by word form.\nThe data dictionary for the dataset is as follows:\n\n\n\n\nTable 2: Data dictionary for the transformed PELIC/ ELP dataset\n\n\n\n\n\n\n\nVariable\nName\nType\nDescription\n\n\n\n\nid\nID\ncategorical\nUnique identifier for each learner\n\n\nplacement\nPlacement\nnumeric\nNumerical value indicating score on the placement test for each learner (0-100)\n\n\nchars\nCharacters\nnumeric\nMean number of characters per word in the text sample\n\n\nsylls\nSyllables\nnumeric\nMean number of syllables per word in the text sample\n\n\nmorphs\nMorphemes\nnumeric\nMean number of morphemes per word in the text sample\n\n\nfreq\nFrequency\nnumeric\nMean frequency of occurrence per word in the text sample\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify\nIn Table 2, we can see the variables which will be used to represent lexical sophistication and learner proficiency. The explanatory variables which will be used to represent lexical sophistication are chars, sylls, morphs, and freq. The response variable which will be used to represent learner proficiency is placement.\nLet’s read in the dataset.\n\npelic &lt;- read_csv(\"data/derived/pelic/pelic_transformed.csv\")\n\npelic\n\n# A tibble: 276 × 6\n   id    placement chars sylls morphs     freq\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 aa0          80  6.01  1.90   1.66  498076.\n 2 aa1          66  5.96  1.85   1.53  657678.\n 3 ab8          40  5.81  1.81   1.52  375383.\n 4 ac3          65  5.98  1.81   1.52  419318.\n 5 ad4          72  6.26  1.93   1.69  345658.\n 6 ad9          56  5.82  1.72   1.57  375747.\n 7 ae0          61  4.99  1.53   1.35 1087512.\n 8 ae2          81  6.32  1.98   1.69  403618.\n 9 ae4          69  5.23  1.68   1.45  788536.\n10 ae9          71  5.64  1.75   1.47  586251.\n# ℹ 266 more rows\n\n\nThe dataset contains 276 observations for our variables of interest. Let’s now map the hypothesis statement to the appropriate response and explanatory variables.\nplacement ~ chars + sylls + morphs + freq\nWe will specify the relationship between the response and explanatory variables using the formula notation in the interrogation phase. The explanatory variables will be used in an additive model using multiple linear regression.\n\n\nInspect\nFirst step is to get a statistical overview of the dataset. We can use the skim() function from {skimr} to get a statistical summary of the dataset.\npelic |&gt;\n  skim()\n\n\n── Data Summary ────────────────────────\n                           Values\nName                       pelic \nNumber of rows             276   \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  character                1     \n  numeric                  5     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 id                    0             1   3   3     0      276          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate      mean         sd        p0       p25\n1 placement             0             1     59.2      12.7       32        48   \n2 chars                 0             1      5.79      0.473      3.75      5.50\n3 sylls                 0             1      1.79      0.169      1.25      1.68\n4 morphs                0             1      1.53      0.127      1.12      1.44\n5 freq                  0             1 556751.   160046.    165138.   453424.  \n        p50       p75       p100 hist \n1     59        69         88    ▃▆▇▆▃\n2      5.79      6.08       8.07 ▁▂▇▁▁\n3      1.78      1.88       2.66 ▁▇▆▁▁\n4      1.53      1.62       1.97 ▁▅▇▃▁\n5 544026.   631784.   1379696.   ▂▇▂▁▁\n\n\nWe are looking for any missing values or other anomalies in the dataset. We can see that there are no missing values in the dataset.\nLet’s now inspect the variables of interest to get a sense of the distributions of the variables. There are really three things we are looking to find out:\n\nWhat is the distribution of the variables individually?\nWhat is the relationship between the explanatory variables and the response variable?\nWhat is the relationship (if any) between the variables?\n\nSince we are working with numeric variables, we would create a histogram or density plot for each individual variable. Then a scatterplot for the relationship between the explanatory variables and the response variable. Finally, a correlation matrix for the relationship between the variables.\nI would like to introduce a shortcut to create all three types of visualizations in one plot. The ggpairs() function from {GGally} creates a matrix of plots for all combinations of variables in the dataset. We can specify the type of plot to create for each combination of variables. Let’s create a matrix of plots for the variables of interest.\n\n# Load the GGally package\nlibrary(GGally)\n\n# Create the matrix of plots\npelic |&gt;\n  select(placement, chars, sylls, morphs, freq) |&gt;\n  ggpairs()\n\n\n\n\n\n\n\nFigure 1: Matrix of plots for the variables of interest\n\n\n\n\n\nThe plot in Figure 1 shows a lot of information. Let’s break it down.\nThe diagonal plots show the distribution of each variable. In these plots we are looking for any outliers or skewness in the distributions. We can see that on the whole the distributions are fairly normal. Our simulation-based inference methods do not require the data to be normally distributed, but highly skewed distributions can compress the range of the data and thus affect the results.\nThe lower triangle plots show the scatterplots for the relationship between the explanatory variables and the response variable and in the upper triangle we see the correlation coefficients for the relationship between the variables.\nLet’s focus on the upper triangle, specifically the first row of statistics reported. The first row shows the correlation between the response variable and the explanatory variables. All variables show a positive correlation with the response variable, except for frequency, which is negative. This is what we predicted in the hypothesis statement. The strengths of these correlations are fairly weak, especially for frequency. We will let our model determine the strength of the relationship and whether it is statistically significant, but it’s worth noting.\nNow let’s focus on the upper triangle for the second, third, and fourth row of correlation measures. These show the correlation between the explanatory variables themselves. The variables chars, sylls, and morphs are highly intercorrelated. This is not surprising since the number of characters in a word is related to the number of syllables and morphemes. Yet if we consider all three in our model we run the risk of multicollinearity. So we need to decide which of these variables to include in our model.\nOne way to do this is assess the theoretical importance of each variable. In this case, we might consider the number of syllables and morphemes to be more important than the number of characters. Another perspective is to see which of the remaining two variables is least correlated with freq with the hopes of capturing non-overlapping variance in the response variable. In this case, sylls is less correlated with freq than morphs. So we will include sylls and freq in our model.\nIf we were to use the response variable placement as our reason for selecting the explanatory variables, we would be committing the logical fallacy of circular reasoning –in essence, tailoring the model to the data.\nSo lets select sylls and freq as our explanatory variables for our final model.\n\npelic &lt;-\n  pelic |&gt;\n  select(placement, sylls, freq)\n\nA last thing to consider before we enter into the model building phase, is to address the fact that the sylls and freq variables are on very distinct scales. In regression modeling, this can cause problems both for fitting the model and for interpreting the model.\nWe can address this by normalizing the variables. This will transform the variables to have a mean of zero and a standard deviation of one. This is known as a z-score. Z-score normalization does not change the distribution nor the relationship between the variables, but it does make the variables more comparable.\n\n# Function to get z-score\nget_z_score &lt;- function(x) {\n  (x - mean(x)) / sd(x)\n}\n\n# Normalize the variables\npelic &lt;-\n  pelic |&gt;\n  mutate(\n    sylls_z = get_z_score(sylls),\n    freq_z = get_z_score(freq)\n  )\n\n\n\nInterrogate\nNow we will analyze the data using {infer}. {infer} is a framework for conducting statistical inference using simulation-based methods. The steps for using {infer} are as follows:\n\nSpecify the model relationships\nCalculate the model statistics (fit)\nSimulate the null distribution\nCalculate the \\(p\\)-value\nSimulate model statistics (fit)\nCalculate the confidence interval\nCalculate the effect size\n\nStep 1. We will use the specify() function to add the formula notation to specify the relationship between the response and explanatory variables.\n\n# Specify the model\npelic_spec &lt;-\n  pelic |&gt;\n  specify(placement ~ sylls_z + freq_z)\n\npelic_spec\n\nResponse: placement (numeric)\nExplanatory: sylls_z (numeric), freq_z (numeric)\n# A tibble: 276 × 3\n   placement sylls_z freq_z\n       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1        80   0.691 -0.367\n 2        66   0.381  0.631\n 3        40   0.157 -1.13 \n 4        65   0.123 -0.859\n 5        72   0.822 -1.32 \n 6        56  -0.378 -1.13 \n 7        61  -1.49   3.32 \n 8        81   1.15  -0.957\n 9        69  -0.624  1.45 \n10        71  -0.189  0.184\n# ℹ 266 more rows\n\n\nStep 2. We will use the fit() function to calculate the model statistics.\n\n# Calculate the model statistics\npelic_obs_fit &lt;-\n  pelic_spec |&gt;\n  fit()\n\npelic_obs_fit\n\n# A tibble: 3 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept   59.2  \n2 sylls_z      2.91 \n3 freq_z       0.617\n\n\nWe now have the calculated model statistics. The model statistics in a linear regression model are the intercept and the slopes for the explanatory variables. The intercept is the predicted value of the response variable when all explanatory variables are zero. The slopes are the predicted change in the response variable for a one unit change in the explanatory variable.\nStep 3. At this point we need to use hypothesize() and to use ‘independence’ as our null hypothesis. The hypothesize() function takes the model object pelic_spec and the type of null hypothesis we are assuming, in this case ‘independence’. Then we will pass this to generate() to create a null distribution. The generate() function takes the model object, the number of simulations, and the type of simulation as arguments. The type of simulation for models with multiple independent variables is permute.\n\n# Create the null distribution\npelic_null_fit &lt;-\n  pelic_spec |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 10000, type = \"permute\") |&gt;\n  fit()\n\n\n\n\n\n\n\n Tip\nThe larger the number of reps the more reliable the results will be. Note, that the generate() function will take a while to run for large reps and depending on your computer’s memory, it may crash. If this happens, try reducing the number of reps.\n\n\n\nStep 4. The get_p_value() function takes the model object and the null distribution object as arguments. We choose a “two-sided” test because we are interested in whether the explanatory variables are positively or negatively correlated with the response variable.\n\n# Calculate the p-value\npelic_p_value &lt;-\n  get_pvalue(\n    x = pelic_null_fit,\n    obs_stat = pelic_obs_fit,\n    direction = \"two-sided\"\n  )\n\npelic_p_value\n\n# A tibble: 3 × 2\n  term      p_value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 freq_z     0.445 \n2 intercept  1     \n3 sylls_z    0.0012\n\n\nOK. So results suggest that syllables is a significant predictor of placement scores, but frequency is not. Let’s now move to the interpretation phase.\n\n\nInterpret\nOur signficant \\(p\\)-value suggests that the explanatory variable sylls_z is a significant predictor of the response variable placement. However, a \\(p\\)-value is an arbitrary threshold. We need to consider the likelihood of the observed test statistic is different from zero. We can do this by calculating the confidence interval.\nStep 5. We will need to simulate the model statistics again, but this time we will use the generate() function with the type “bootstrap” to simulate the model statistics with replacement giving us a distribution of model statistics.\n\n# Create the bootstrap distribution\npelic_bootstrap_fit &lt;-\n  pelic_spec |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  fit()\n\nStep 6. Calculate the confidence interval.\nWe get the confidence interval by using the get_ci() function. The get_ci() function takes the bootstrapped model object and the observed statistics as the estimates to calculate the confidence interval. We will use the default confidence level of 95%.\n\n# Calculate the confidence interval\npelic_obs_ci &lt;-\n  pelic_bootstrap_fit |&gt;\n  get_ci(point_estimate = pelic_obs_fit)\n\npelic_obs_ci\n\n# A tibble: 3 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 freq_z      -0.906     2.21\n2 intercept   57.7      60.7 \n3 sylls_z      1.26      4.81\n\n\nThe confidence interval underscores the results of the \\(p\\)-value. The confidence interval does not include zero for syllables, but does for frequency. Comparing the confidence interval to the actual observed statistics, we can see how close the observed statistics are to the confidence interval margins.\n\npelic_obs_fit\n\n# A tibble: 3 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept   59.2  \n2 sylls_z      2.91 \n3 freq_z       0.617\n\n\nThe observed statistic for sylls_z is nicely within the confidence interval.\nStep 7. Calculate effect size\nWhether a explanatory variable is significant or not is one thing, but gauging the magnitude of the relationship is another. Looking at the observed fit of the model, we can see that the coefficient for syllables is 2.91. But what does this mean? It means the the model predicts that for an increase of our syllable measure by one unit, the placement score will increase by 2.91 units.\nNow the placement score is still in the original scale, so this means that we are dealing with 2.91 score points. For the syllables, however, we have standardized the variable, so we are dealing with standard deviations –less straightforward to interpret. The good news is we can back-transform the standardized variable to the original scale by multiplying by the standard deviation and adding the mean from the original variable.\n\n# Back-transform the standardized variable\n(2.91 * sd(pelic$sylls)) + mean(pelic$sylls)\n\n[1] 2.28\n\n\nTherefore a mean increase of 1 syllable is associated with a 2.28 point increase in placement score.\nAnother helpful way to interpret the results is to consider how much of the variance in the dependent variable is explained by the independent variable. \\(R^2\\) is a typical measure of effect size. To calculate \\(R^2\\) we need the correlation coefficient (\\(r\\)), which can be calculated by dividing the coefficient by the standard deviation of the response variable. Then we square the correlation coefficient to get \\(R^2\\).\n\n# Correlation coefficient and R^2\npelic_obs_fit |&gt;\n  mutate(\n    r = estimate / sd(pelic_spec$placement),\n    r2 = r^2\n  )\n\n# A tibble: 3 × 4\n  term      estimate      r       r2\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept   59.2   4.64   21.6    \n2 sylls_z      2.91  0.229   0.0524 \n3 freq_z       0.617 0.0485  0.00235\n\n\nWe can see her that the \\(R^2\\) is 0.052, which means that the number of syllables explains 5.2% of the variance in placement scores. This means that syllables don’t explain all that much in the variation in placement scores. But it is a significant relationship."
  },
  {
    "objectID": "recipes/recipe-10/index.html#summary",
    "href": "recipes/recipe-10/index.html#summary",
    "title": "10. Building inference models",
    "section": "Summary",
    "text": "Summary\nIn summary, this programming tutorial has provided a comprehensive guide to building inference-based models using the infer package in R. We have explored the concept of lexical sophistication as a diagnostic tool for assessing learner proficiency in Second Language Acquisition and Teaching. Through a detailed workflow involving identifying variables, inspecting data distributions, interrogating the dataset with statistical procedures, and interpreting results, we have demonstrated the importance of careful variable selection and normalization in regression modeling. Our analysis revealed that while syllables per word significantly predict learner placement scores, content word frequency does not. The effect size, indicated by an \\(R^2\\) of 0.052, suggests that lexical sophistication, as measured by syllable count, accounts for a small but significant portion of the variance in learner proficiency. This study underscores the nuanced relationship between linguistic features and language learning outcomes, and highlights the potential of statistical modeling in educational research."
  },
  {
    "objectID": "recipes/recipe-10/index.html#check-your-understanding",
    "href": "recipes/recipe-10/index.html#check-your-understanding",
    "title": "10. Building inference models",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\nTRUEFALSE Simulation-based inference is a method that can be used to approximate traditional inferential statistics without the need for theoretical assumptions about the data.\nWhat is the primary goal of using {infer} for simulation-based inference? To visualize data distributionsTo perform regression analysisTo create resampling simulations for hypothesis testing and constructing confidence intervalsTo fit predictive models\nHow does simulation-based inference contribute to understanding population parameters? It allows us to make probabilistic statements about population parameters based on simulated sampling distributionsIt provides exact calculations of population parametersIt eliminates the need for sample dataIt guarantees more accurate results than traditional methods\nWhich of the following is not a typical step in simulation-based inference? Defining a model of interestGenerating resamples or permutationsCalculating summary statisticsTraining a machine learning classifier\nIn the context of Second Language Acquisition research, why might simulation-based inference be particularly useful? Because it can handle very large datasetsBecause it allows for the exploration of hypotheses under different assumptions and conditionsBecause it simplifies the data collection processBecause it directly measures the language ability of individuals\nTRUEFALSE The results from simulation-based inference are always deterministic and do not vary between simulations."
  },
  {
    "objectID": "recipes/recipe-10/index.html#lab-preparation",
    "href": "recipes/recipe-10/index.html#lab-preparation",
    "title": "10. Building inference models",
    "section": "Lab preparation",
    "text": "Lab preparation\nIn preparation for Lab 10, ensure that you are comfortable with the following key concepts related to simulation-based inference:\n\nUnderstanding the principles of statistical inference and how simulations can be used to approximate traditional inferential statistics.\nUtilizing {infer} in R to create resampling simulations for hypothesis testing and constructing confidence intervals.\nInterpreting the results of simulation-based inference focusing on what the simulated distributions imply about the population parameters.\n\nIn this lab, you will be challenged to apply these core ideas to a new dataset of your choosing. Reflect on the nature of the data and the hypotheses you might test using simulation-based methods. Consider how you would design your simulation study to address a particular hypothesis. You will be expected to submit your code along with a concise reflection on your methodology and the insights gained from your analysis."
  },
  {
    "objectID": "guides/guide-02/index.html",
    "href": "guides/guide-02/index.html",
    "title": "02. Installing and managing R packages",
    "section": "",
    "text": "Outcomes\n\nRecognize the difference between the R interpreter and interfaces to the R interpreter.\nInstall R packages using the RStudio IDE interface and the R console.\nManage R packages in R sessions and the R environment."
  },
  {
    "objectID": "guides/guide-02/index.html#r-ide",
    "href": "guides/guide-02/index.html#r-ide",
    "title": "02. Installing and managing R packages",
    "section": "R != IDE",
    "text": "R != IDE\nAs you begin your journey into R programming, it is key to understand an important distinction that can often be overlooked by many a clever student; the difference between R and RStudio (or any other integrated development environment (IDE) or editor).\nWhen you install R on your computing environment, what you are in fact installing is an R interpreter. That is, as R is a programming “language”, we need software to make sense of the R code we write and execute. The interpreter is the engine that we send commands to and from which the results are sent back. To send commands to the R interpreter, we can use many various interfaces ranging from black and white screens with a flashing cursor at the prompt to sophisticated graphical user interfaces (GUI), such as RStudio or Visual Studio Code.\nWhen you open up RStudio, you are opening up an IDE that is designed to make working with R easier. It provides a console to interact with the R interpreter, a script editor to write and run R code, and many other features to help you write, debug, and share your R code.\n\n\n\n\n\n\nFigure 1: RStudio on clean start\n\n\n\nRStudio is not R. It is a tool that helps you work with R. You can use R without RStudio, but you cannot use RStudio without R. Keep this distinction in mind as you continue your journey into R programming.\nRStudio has a number of keyboard shortcuts that can be used to speed up your workflow. You can find a list of them here.\nFor starters, here are the ones I use the most to work with Quarto and R:\n\nFor Quarto document elements\n\n\n\n\n\n\nDescription\nShortcut\n\n\n\n\nRender Quarto documents\n\n\n\nAdd a code block to a Quarto document\n\n\n\n\n\n\n\nSymbols used in keyboard shortcuts\n\n\n\n\n\n\nSymbol\nKey\n\n\n\n\nShiftShift\nShift key\n\n\nCtrlCtrl\nControl key\n\n\nCommandCommand\nCommand key\n(Mac only)\n\n\nAltAlt\nAlt key\n\n\nOptionOption\nOption key\n(Mac only)\n\n\nEscEsc\nEscape key\n\n\nTabTab\nTab key\n\n\nEnterEnter\nEnter key\n\n\n\n\nFor R code elements\n\n\n\n\n\n\nDescription\nShortcut\n\n\n\n\nTo invoke code completion when typing R code\n\n\n\nRun current line or selection from the Editor in the Console\n\n\n\nTo comment or uncomment a line or selection so that it is or is not run as R code\n\n\n\nTo insert the &lt;- operator to assign code output to a variable\n\n\n\nTo insert a |&gt; operator to pipe the output of one operation to the input of the next\n\n\n\nTo reformat R code so that indentation is more legible"
  },
  {
    "objectID": "guides/guide-02/index.html#install-packages",
    "href": "guides/guide-02/index.html#install-packages",
    "title": "02. Installing and managing R packages",
    "section": "Install packages",
    "text": "Install packages\nAnother key principle in programming is that there is often more than one way to get something done. For package installation, there are two primary methods: using a GUI, such as the windows and panes in the RStudio IDE interface or using the R console. We will cover both methods here as getting comfortable with both will make you a more versatile R programmer.\nIn RStudio, you can install packages using the Packages pane. This pane is located in the bottom right corner of interface. If you don’t see it, you can open it by clicking on the “Packages” tab in the bottom right corner of the RStudio interface. You can also open it by going to the “Tools” menu and selecting “Install Packages…”.\nFrom the “Install Packages” dialog, you can search for packages by name. As you type a package name, the list of available packages will be filtered. Click install to install the package. You can also install multiple packages at once by separating the package names with a space.\n\n\n\n\n\n\nFigure 2: RStudio Packages pane and Install Packages dialog\n\n\n\nFrom the R console, you can install packages using the install.packages() function. This function takes the name of the package you want to install as an argument. For example, to install {dplyr}, you would run:\ninstall.packages(\"dplyr\")\nYou can also install multiple packages at once by passing a vector of package names to the install.packages() function. For example, to install {dplyr} and {ggplot2} packages, you would run:\ninstall.packages(c(\"dplyr\", \"ggplot2\"))\nIn either case, you will need to select a CRAN mirror the first time you install a package in a new session. At this point, it does not matter which mirror you choose – 1 is usually a good choice.\n\n\n\n\n\n\n Dive deeper\nYou can set a default CRAN mirror in your .Rprofile file. This file is located in your home directory (~/.Rprofile). If the file does not exist, you can create it. Add the following line to set a default CRAN mirror:\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n\n\nBoth methods will install the package and its dependencies from CRAN. If you want to install a package from GitHub (and/ or CRAN), you can use {pak}. Once installed, {pak} provides the pak::pak() function, which can install packages from CRAN or GitHub. To install a package from CRAN, you can use the package name as an argument. To install a package from GitHub, you can use the user/repo format as an argument.\nFor example, let’s install the {stringr} package from CRAN and the {qtkit} package from GitHub:\npak::pak(\"stringr\")\npak::pak(\"qtalr/qtkit\")"
  },
  {
    "objectID": "guides/guide-02/index.html#using-packages",
    "href": "guides/guide-02/index.html#using-packages",
    "title": "02. Installing and managing R packages",
    "section": "Using packages",
    "text": "Using packages\n\nR Sessions\nTo understand how to use packages for programming with R, you need to understand how R sessions work. Every time you open an R session, you start a new R session. This session starts with a clean environment. No packages or variables are available at this point –other than the base R functions and variables.\nWe can see this by running the search() function in the R console.\n\n\n\n\n\n\nFigure 3: R session search path on clean start\n\n\n\n\nAttach packages\nTo make a package available for use in an R session, you need to “attach it” to the search path. This will persist until you close this R session, or “detach” the package manually. You can attach a package to the search path using the library() function. For example, to attach the dplyr package to the search path, you would run:\nlibrary(dplyr)\nYou can see what packages are currently attached by running search().\n\n\n\n\n\n\nFigure 4: R session search path with {dplyr} attached\n\n\n\nIn RStudio, you can see the checkboxes next to the packages in the Packages pane. This is a visual representation of packages attached to the search path.\nSome R session gotchas:\n\nIf you attach a package in one session, it will not be available in another session automatically. Each session is independent.\nRunning an R script or rendering a literate document (e.g., R Markdown, Quarto) will start a new R session, and close it when the script or document is done running.\nTherefore, packages you attach in the R console are not available in other scripts or documents, and vice versa.\nTo make a package available in a script or document, you need to attach it in that script or document. This is a good thing, as it makes your scripts and documents self-contained and reproducible.\n\n\n\nDetach packages\nIf you “quit” an R session (q()), the packages you attached will be detached automatically. You can also detach a package manually by running detach(\"package:package_name\"). For example, to detach {dplyr}, you would run:\ndetach(\"package:dplyr\")\n\n\n\nManaging packages\nIn addition to installing packages, you may need to manage them. This will include listing packages that have newer versions available, updating packages, and removing packages.\nIn RStudio, you can see which packages have newer versions available in the Packages pane. The “Updates” tab will list packages that have newer versions available. You can update packages by selecting the checkboxes or some or all packages and clicking the “Update” button.\nAt the R Console, the old.packages() function will list packages that have newer versions available. You can then decide which packages to update, or just update all packages by running:\nupdate.packages(ask = FALSE)\nThis will update all packages without asking for confirmation. It also is of note that this function updates packages installed by any method. That is, there is not a {pak} version of this function.\nTo remove a package in RSudio, you can click the  button next to the package in the Packages pane. In the R console, you can remove a package using the remove.packages() function. For example, to remove the {dplyr} package, you would run:\nremove.packages(\"dplyr\")\n\n\n\n\n\n\n Dive deeper\nIf you are working on a project and want to ensure that the package versions are consistent across all collaborators, you can use {renv}. {renv} is a package that helps you create and manage project-specific R environments. You can use {renv} to create a project-specific library of packages, and ensure that all collaborators are using the same versions of packages. To use {renv}, you will need to install it first:\ninstall.packages(\"renv\")\nThen, you can use the following functions to manage packages in your project:\n\nrenv::init(): Initialize a new project with {renv}.\nrenv::install(\"package_name\"): Install a package in the project library.\nrenv::snapshot(): Snapshot the project library.\nrenv::restore(): Restore the project library to a previous snapshot.\nrenv::status(): Show the status of the project library."
  },
  {
    "objectID": "guides/guide-02/index.html#summary",
    "href": "guides/guide-02/index.html#summary",
    "title": "02. Installing and managing R packages",
    "section": "Summary",
    "text": "Summary\nIn this guide, we covered how to install and manage R packages. We discussed two primary methods for installing packages: using the RStudio IDE interface and using the R console. We also covered how to attach and detach packages in an R session, and how to manage packages by listing, updating, and removing them. Finally, we introduced {renv} as a way to manage project-specific R environments."
  },
  {
    "objectID": "guides/guide-04/index.html",
    "href": "guides/guide-04/index.html",
    "title": "04. Setting up Git and GitHub",
    "section": "",
    "text": "Outcomes\n\nRecognize the purpose of Git and GitHub\nEstablish a working Git and GitHub environment\nRecognize the basic Git and GitHub workflow for managing a project"
  },
  {
    "objectID": "guides/guide-04/index.html#introduction",
    "href": "guides/guide-04/index.html#introduction",
    "title": "04. Setting up Git and GitHub",
    "section": "Introduction",
    "text": "Introduction\nThis textbook places a heavy emphasis on reproducible research. The most important thing you can do to ensure reproducibility is to use a version control system and a hosting service. For most people, the best option is Git and GitHub."
  },
  {
    "objectID": "guides/guide-04/index.html#what-is-git-github-and-why-should-i-use-it",
    "href": "guides/guide-04/index.html#what-is-git-github-and-why-should-i-use-it",
    "title": "04. Setting up Git and GitHub",
    "section": "What is Git, Github? And why should I use it?",
    "text": "What is Git, Github? And why should I use it?\nGit is a version control system. It allows you to track changes to files and folders over time. It also allows you to collaborate with others on projects. Think of it as MS Word’s “Track Changes” feature on steroids.\nGit is a command line tool, but there are also GUIs to interact with Git in a more user-friendly way. Git is a great tool for managing projects. It is especially useful for managing projects that involve multiple people.\nGitHub is a web-based hosting service for Git repositories. It allows you to store your Git repositories in the cloud. It also allows you to collaborate with others on projects by sharing projects. GitHub is a great place to store and share your code. It is also a great place to find code that others have shared.\nCombining Git with GitHub allows you to store your Git managed project repositories in the cloud. This means that you can access your repositories from anywhere. It also means that you can collaborate with others on projects. You can also use GitHub to share your code with others. This is especially useful for making your projects reproducible."
  },
  {
    "objectID": "guides/guide-04/index.html#how-do-i-set-up-git-and-github",
    "href": "guides/guide-04/index.html#how-do-i-set-up-git-and-github",
    "title": "04. Setting up Git and GitHub",
    "section": "How do I set up Git and GitHub?",
    "text": "How do I set up Git and GitHub?\n\nInstall and setup Git\nThe process for installation and setup will differ based on what operating system you are using. If you are using a Windows machine, you will likely need to install Git. If you are using a Mac or Linux machine, you will likely already have Git installed.\nWindows users can install Git by downloading the installer from https://git-scm.com/downloads. Once you have downloaded the installer, you will need to run it. You will need to follow the instructions in the installer to complete the installation.\n\n\nTerminal pane in RStudio\n\nMac and Linux users can verify that Git is installed by opening a terminal window and typing git --version. If Git is installed, you will see a version number. If Git is not installed, you will see an error message. If you need to install Git, you can do so following these instructions:\n\nMac users can install Xcode Command Line Tools by running xcode-select --install in the terminal. This will install Git along with other tools that may be useful for development.\nLinux users can install Git using the package manager for their distribution. For example, Ubuntu users can install Git by running sudo apt-get install git in the terminal.\n\nOnce you have Git installed, you will need to set up your Git configuration. Most of the defaults will be fine for now, but you will need to at least set your name and email address, this will be the same address you use to create your GitHub account, so choose accordingly.\nYou can do this by opening a terminal window and entering the following commands (changing the name and email address to your own):\ngit config --global user.name \"Your Name\"; \\\ngit config --global user.email \"your.email@email.edu\"\nAlternatively, you can use {usethis}(Wickham et al. 2024) in R to set up Git in your environment. Open RStudio, install and/ or load {usethis}, and run the following code in the console:\nuse_git_config(user.name = \"Jerid Francom\", user.email = \"francojc@wfu.edu\")\n\n\n\n\n\n\nFigure 1: Console pane in RStudio\n\n\n\n\n\nHow do I set up GitHub?\nTo set up GitHub, you will need to create an account. You can do this at [github.com(https://github.com). Be sure to use the email address you used in the Git setup!\nThe service is free and there are extra features available for students and educators. Once you have created an account, you will be able to create repositories. You can also create organizations and teams. You can use these to collaborate with others on projects."
  },
  {
    "objectID": "guides/guide-04/index.html#understanding-the-basic-git-and-github-workflow",
    "href": "guides/guide-04/index.html#understanding-the-basic-git-and-github-workflow",
    "title": "04. Setting up Git and GitHub",
    "section": "Understanding the basic Git and GitHub workflow",
    "text": "Understanding the basic Git and GitHub workflow\nOnce your Git installation and Github account are set up, a number of options for working with and managing your project are available to you. At this point, let’s focus on one typical scenario that you will encounter early on in this textbook.\n\n\n\n\n\n\nFigure 2: Visualizing a common scenario using Git and GitHub\n\n\n\nSome definitions are in order.\nFirst, let’s define some key nouns:\n\nRepository: This is a collection of folders (directories) and files in a project that are managed by Git.\nRemote Repository: This is the repository that is stored on GitHub. It can be your repository or a repository owned by someone else. These repositories can be public, accessible by anyone, or private, accessible only by those with permission.\nLocal Repository: This is the repository that is stored on your local machine. This is where you will make changes to your project. The local repository may be a copy of a remote repository or a new repository that you create that is not stored on GitHub (yet, or ever).\n\nNow, some key verbs:\n\nClone: This is the process of copying a remote repository to your local machine.\nEdit: This is the process of making changes to the files in your local repository.\n\nThere are many other actions that you can perform with Git and GitHub, but these are sufficient to get you started with this textbook.\n\n\n\n\n\n\n Tip\nBryan and Hester (2020) is an excellent reference resource for all things Git and GitHub for R users."
  },
  {
    "objectID": "guides/guide-04/index.html#how-do-i-manage-my-project-with-git-and-github",
    "href": "guides/guide-04/index.html#how-do-i-manage-my-project-with-git-and-github",
    "title": "04. Setting up Git and GitHub",
    "section": "How do I manage my project with Git and GitHub?",
    "text": "How do I manage my project with Git and GitHub?\nLet’s describe the step-wise process in Scenario A, visualized in the diagram above. This scenario involves cloning a remote repository to your local machine and then making changes to the files in the local repository. Conveniently, this is the process you will follow when working on a lab assignment in this textbook.\n\nNavigate to the repository on GitHub\nClick on the ‘Code’ button and copy the clone URL (https) to your clipboard.\nOpen RStudio\nFrom the menu, select File &gt; New Project &gt; Version Control &gt; Git\nPaste the URL of the repository into the ‘Repository URL’ field\nChoose the directory location for the project\nClick ‘Create Project’\n\nRStudio will then clone the repository to your local machine and open the project as a new RStudio project. You can now make changes to the files in the project."
  },
  {
    "objectID": "guides/guide-04/index.html#summary",
    "href": "guides/guide-04/index.html#summary",
    "title": "04. Setting up Git and GitHub",
    "section": "Summary",
    "text": "Summary\nIn this guide, we covered the basics of setting up Git and GitHub. We also covered the basics of using Git and GitHub to manage a project. We discussed the purpose of Git and GitHub, and why you should use them. We also discussed how to install and set up Git, how to set up GitHub, and the basic Git and GitHub workflow for managing a project.\nWe will return to Git and GitHub later in the textbook to cover more functionality and use cases."
  },
  {
    "objectID": "guides/guide-01/index.html",
    "href": "guides/guide-01/index.html",
    "title": "01. Setting up an R environment",
    "section": "",
    "text": "Choosing to work with R locally means that you will install R and an integrated development environment (IDE) on your local computer. This approach offers the following advantages:\n\nFast and responsive performance\nNo (inherent) reliance on internet connectivity\nIncreased flexibility to customize your environment\n\nThe main disadvantages of working locally are:\n\nyou will need to install R and an IDE on your local computer,\nmanage your own software environment, and\nmanage your own backups and version control for collaborative projects.\n\nRunning and managing R locally can sometimes be a challenge for new users, compared to some other methds. However, there are important advantages to this approach. Remember there are a number of resources available to help you get started and troubleshoot any issues you may encounter.\nTo get started, install R from CRAN. You can download the latest version of R for your operating system here. Once you have installed R, you will need to install an IDE or editor. For complete beginners, I recommend RStudio, a free and open-source IDE for R. RStudio provides a number of features that make it easier to work with R. If you have experience with programming and/ or are looking for a more customizable editor, you may prefer to use Visual Studio Code. Setting up VS Code for R can be found here.\n\n\n\nAn alternative to running R locally is to work with R in the cloud. This is known as a remote environment. There are a number of cloud-based options for working with R, including Posit Cloud, Google Colab, and Microsoft Azure. These options provide an R environment that you can access from any computer with an internet connection.\nRemote environments provide an environment where you can create, edit, and run R projects from anywhere with internet access. They offers several advantages:\n\nNo need to install R or an IDE/ editor locally\nAccess your projects from any device\nCollaborate with others in real-time\nEasily share your work\n\nSome of the drawbacks of working in the cloud include\n\nReliance on stable internet connection\nPotential latency and performance issues\nSomewhat limited customization options compared to a local setup\n\nTo get started with Posit Cloud, you will need to create an account. You can sign up for a free account here. Once you have created an account, you will see a list of spaces. By default you will have your personal workspace, but you can also join or be invited to other spaces. Instructors may create spaces for their courses which can provide pre-configured environments for students.\n\n\n\n\n\n\nFigure 1: Posit Cloud interface\n\n\n\nVisit the Guide documentation to learn more about the features of Posit Cloud.\n\n\n\nIf you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE/ editor on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nIn addtion to the advantages of working with R locally, using Docker with Rocker offers several benefits:\n\nSafe and isolated environment from the host system\nReproducible and portable environments\nSimplified dependency management using {pak}\n\nThe drawbacks to using Docker with Rocker include:\n\nLearning curve for setting up and managing Docker containers\nIncreased memory and resource requirements for the host machine\nIncreased complexity in managing Git/ GitHub credentials\n\nTo start using Docker with a Rocker image, follow these steps:\n\nInstall Docker on your local machine (pay special attention to the installation instructions for your operating system)\nPull the desired Rocker image from Docker Hub\n\n\n\n\n\n\n\nFigure 2: Pull rocker/rstudio image from Docker Hub\n\n\n\n\nRun a container using the pulled image\nNote: you will need to specify the following options before running the container.\n\n\ncontainer name: no spaces\nport mapping: 8787 on the host to 8787 on the container\nenvironment variables: PASSWORD to set the password for RStudio, ROOT to allow root access.\n\n\n\n\n\n\n\nFigure 3: Run a container using the rocker/rstudio image\n\n\n\nOptional: you can also mount a volume to share files between the host and container.\n\nAccess RStudio in your browser at http://localhost:8787 and log in with username rstudio and the password you set"
  },
  {
    "objectID": "guides/guide-01/index.html#environment-setups",
    "href": "guides/guide-01/index.html#environment-setups",
    "title": "01. Setting up an R environment",
    "section": "",
    "text": "Choosing to work with R locally means that you will install R and an integrated development environment (IDE) on your local computer. This approach offers the following advantages:\n\nFast and responsive performance\nNo (inherent) reliance on internet connectivity\nIncreased flexibility to customize your environment\n\nThe main disadvantages of working locally are:\n\nyou will need to install R and an IDE on your local computer,\nmanage your own software environment, and\nmanage your own backups and version control for collaborative projects.\n\nRunning and managing R locally can sometimes be a challenge for new users, compared to some other methds. However, there are important advantages to this approach. Remember there are a number of resources available to help you get started and troubleshoot any issues you may encounter.\nTo get started, install R from CRAN. You can download the latest version of R for your operating system here. Once you have installed R, you will need to install an IDE or editor. For complete beginners, I recommend RStudio, a free and open-source IDE for R. RStudio provides a number of features that make it easier to work with R. If you have experience with programming and/ or are looking for a more customizable editor, you may prefer to use Visual Studio Code. Setting up VS Code for R can be found here.\n\n\n\nAn alternative to running R locally is to work with R in the cloud. This is known as a remote environment. There are a number of cloud-based options for working with R, including Posit Cloud, Google Colab, and Microsoft Azure. These options provide an R environment that you can access from any computer with an internet connection.\nRemote environments provide an environment where you can create, edit, and run R projects from anywhere with internet access. They offers several advantages:\n\nNo need to install R or an IDE/ editor locally\nAccess your projects from any device\nCollaborate with others in real-time\nEasily share your work\n\nSome of the drawbacks of working in the cloud include\n\nReliance on stable internet connection\nPotential latency and performance issues\nSomewhat limited customization options compared to a local setup\n\nTo get started with Posit Cloud, you will need to create an account. You can sign up for a free account here. Once you have created an account, you will see a list of spaces. By default you will have your personal workspace, but you can also join or be invited to other spaces. Instructors may create spaces for their courses which can provide pre-configured environments for students.\n\n\n\n\n\n\nFigure 1: Posit Cloud interface\n\n\n\nVisit the Guide documentation to learn more about the features of Posit Cloud.\n\n\n\nIf you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE/ editor on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nIn addtion to the advantages of working with R locally, using Docker with Rocker offers several benefits:\n\nSafe and isolated environment from the host system\nReproducible and portable environments\nSimplified dependency management using {pak}\n\nThe drawbacks to using Docker with Rocker include:\n\nLearning curve for setting up and managing Docker containers\nIncreased memory and resource requirements for the host machine\nIncreased complexity in managing Git/ GitHub credentials\n\nTo start using Docker with a Rocker image, follow these steps:\n\nInstall Docker on your local machine (pay special attention to the installation instructions for your operating system)\nPull the desired Rocker image from Docker Hub\n\n\n\n\n\n\n\nFigure 2: Pull rocker/rstudio image from Docker Hub\n\n\n\n\nRun a container using the pulled image\nNote: you will need to specify the following options before running the container.\n\n\ncontainer name: no spaces\nport mapping: 8787 on the host to 8787 on the container\nenvironment variables: PASSWORD to set the password for RStudio, ROOT to allow root access.\n\n\n\n\n\n\n\nFigure 3: Run a container using the rocker/rstudio image\n\n\n\nOptional: you can also mount a volume to share files between the host and container.\n\nAccess RStudio in your browser at http://localhost:8787 and log in with username rstudio and the password you set"
  },
  {
    "objectID": "guides/guide-01/index.html#summary",
    "href": "guides/guide-01/index.html#summary",
    "title": "01. Setting up an R environment",
    "section": "Summary",
    "text": "Summary\nIn this guide, we have discussed strategies for working with R. All three options offer unique advantages. In Table 1, we summarize some of the characteristics, benefits, and drawbacks of each option.\n\n\n\nTable 1: Comparison of different environments for working with R and RStudio\n\n\n\n\n\n\n\n\n\n\n\nEnvironment\nCharacteristics\nBenefits\nDrawbacks\n\n\n\n\nLocal (Computer)\n- R/RStudio installed locally- Project files stored on local machine- Accessible without internet connection- Full control over software version and environment\n- Fast and responsive performance- No reliance on internet connectivity- Ability to work offline- Complete control over software version and environment\n- Limited collaboration options- Difficulty in sharing projects with others- Potential compatibility issues with different operating systems\n\n\nRemote (Cloud)\n- R/RStudio accessed via web browser- Project files stored on cloud server- Accessible from any device with internet connection- Easy collaboration with others- Automatic backups and version control\n- No need for local installation or setup- Easy access from anywhere- Seamless collaboration with teammates- Backup and version control provided by the cloud service\n- Reliance on stable internet connection- Potential latency and performance issues- Limited customization options compared to a local setup\n\n\nVirtual (Docker)\n- R/RStudio environment encapsulated in a Docker container- Project files stored locally or on the cloud- Consistent environment across different machines\n- Reproducible and portable environment- Easy setup and sharing of the container- Flexibility to run on different operating systems- Isolation from host system dependencies\n- Learning curve for setting up and managing Docker containers- Increased memory and resource requirements- Potential compatibility issues with certain packages or libraries\n\n\n\n\n\n\nGive them a try and see which one works best for your needs! Remember, you can always switch between different environments as your needs change."
  },
  {
    "objectID": "guides/guide-01/index.html#references",
    "href": "guides/guide-01/index.html#references",
    "title": "01. Setting up an R environment",
    "section": "References",
    "text": "References"
  }
]