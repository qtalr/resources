---
title: 07. Web scraping with R
subtitle: ''
description: >
  This guide will provide you with an overview of web scraping and how you can use R to scrape data from the web using packages such as {rvest}. Web scraping is a technique used to extract data from websites. It is a powerful tool that can be used to collect data for research, analysis, and visualization. In this guide, you will learn how to use R to scrape data from websites and save it in a format that can be used for further analysis.
categories:
  - guides
---

<!--
Goal: The goal of the guide is to provide readers of the textbook "An Introduction to Quantitative Text Analysis for Linguistics: Reproducible Research Using R" an overview of web scraping as an approach to gathering (language) data from the web for use in linguistic research. The guide needs to be accessible to readers who are new to web scraping and R. The guide should provide a basic introduction to web scraping, explain HTML (the language of the web) enough to understand how packages like {rvest} can use the Document Object Structure to target and extract information. Finally, the guide should provide a simple example of how to scrape data from a website using R that is relevant to the field of linguistics.
-->

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

- Understand the concept of web scraping and its applications.
- Learn how to use R to scrape data from websites.
- Save scraped data in a format that can be used for further analysis.
:::

## Introduction

Web scraping is a technique used to extract data from websites. It is a powerful tool that can be used to collect data from documents such as PDF or DOCX files, but is most often used to acquire the contents of the public-facing web.

The language of the web is HTML. A markup language, raw HTML contains a semi-structured document which is formed around the concept of tags. Tags are opened `<tag>` and closed `</tag>` in a hierarchical fashion. The tags come pre-defined in terms of how they are used and displayed when a browser parses the document. For example, `<ul></ul>` delimits an unordered list. Embedded inside will be a series of `<li></li>` tags for items in the unordered list. So for example, the HTML fragment in @lst-html-ul is displayed by a browser as in @lst-html-ul-browser.

:::{#lst-html-ul}
```{.html}
<ul>
  <li>First item</li>
  <li>Second item</li>
  <li>Third item</li>
</ul>
```
:::


:::{#lst-html-ul-browser}
```{=html}
<ul>
  <li>First item</li>
  <li>Second item</li>
  <li>Third item</li>
</ul>
```
:::

This structure is what makes it possible to target and extract content from websites, as we will soon see. However, in addition to tags we need to be aware of and understand `ids` and `classes`. Ids and classes are used in HTML to specify that particular tags should behave in a certain way.

OK, that isn't very insightful. Let me give you an example. So imagine that we have two lists much like in @lst-html-ul but one corresponds to the table of contents of our page and the other is used in the content area as a basic list. Say we want to make our table of contents appear in bold font and the other list to appear as normal text. One way to do this is to distinguish between these two lists using a class attribute, as in @lst-html-ul-class.

:::{#lst-html-ul-class}
```{.html}
<ul class="toc">             <# 1>
  <li>First item</li>
  <li>Second item</li>
  <li>Third item</li>
</ul>
```
:::

After doing this a web designer would then create a CSS expression to target tags with the `toc` class and make them bold. In our toy case, this only targets our unordered list with the `class="toc"`.

:::{#lst-css-class-toc}
```{.css}
.toc {
  font-weight: bold
}
```
:::

Now, our list from @lst-html-ul-class will appear as in @lst-html-ul-class-browser.

:::{#lst-html-ul-class-browser}
```{=html}
<style>
  .toc {
    font-weight: bold
  }
</style>

<ul class="toc">
  <li>First item</li>
  <li>Second item</li>
  <li>Third item</li>
</ul>
```
:::

Ids work in a similar way, but instead have the apt `id="..."` attribute.

All this is to say that the combination of the HTML tag structure and the use of id and class attributes tends to give the would-be web scraper various ways to target certain elements on a webpage and not others.

## Web Scraping

Where it has always been possible to navigate to a webpage, select/copy, and paste content into a document, web scraping makes this workflow automatic. This is particularly useful when you need to collect data from multiple pages or websites.

In R, {rvest} makes it straightforward to:

1. Download webpage content
2. Parse the HTML structure (using tags, attributes, etc.)
3. Extract text content
4. Save the extracted content

::: {.callout}
**{{< fa regular info-circle >}} Note**

- Check the website's robots.txt file and terms of service
- Be respectful with request frequency
- Consider using an API if one is available
:::

## A Text Analysis Example

Let's look at a simple example using a toy HTML document that contains some text excerpts we want to extract. Here's our sample HTML:

```r
# Create a sample HTML string
html_string <- '
<html>
  <body>
    <div class="article">
      <h1>Language Varieties</h1>
      
      <div class="excerpt" id="excerpt1">
        <h2>American English</h2>
        <p>American English is the variety of English spoken in the United States. 
        It has several distinctive features in pronunciation, vocabulary, and grammar.</p>
      </div>
      
      <div class="excerpt" id="excerpt2">
        <h2>British English</h2>
        <p>British English refers to the English language as spoken and written in 
        Great Britain. It differs from American English in spelling, vocabulary, 
        and some grammatical constructions.</p>
      </div>
    </div>
  </body>
</html>'

# Now let's use rvest to parse and extract content
library(rvest)
library(stringr)

# Read the HTML string
page <- read_html(html_string)

# Extract all excerpts
excerpts <- page %>%
  html_elements(".excerpt") %>%  # Target elements with class "excerpt"
  html_text2()                   # Extract text with preserved whitespace

# Extract just the titles (h2 elements)
titles <- page %>%
  html_elements(".excerpt h2") %>%
  html_text2()

# Extract just the paragraphs
paragraphs <- page %>%
  html_elements(".excerpt p") %>%
  html_text2()

# Create a simple data frame of our extracted content
text_data <- data.frame(
  title = titles,
  content = paragraphs
)

# View the results
print(text_data)
```

This example shows how to:
1. Create a simple HTML document as a string
2. Parse it using {rvest}
3. Extract specific elements using CSS selectors:
   - `.excerpt` targets elements with class "excerpt"
   - `.excerpt h2` targets h2 elements within excerpt class elements
   - `.excerpt p` targets paragraph elements within excerpt class elements
4. Organize the extracted text into a structured format

The CSS selectors we used (`.excerpt`, `h2`, `p`) demonstrate how we can target specific parts of an HTML document based on both tags and classes. This is the same principle we would use when scraping actual websites, just with more complex HTML structures.

## Best Practices for Text Collection

When scraping web text for linguistic research:

1. Document your source URL and date of collection
2. Save the raw scraped text before any processing
3. Include error handling in your code
4. Consider text encoding issues
5. Verify the quality of extracted text
6. Document any cleaning steps performed

These practices ensure your text collection is reproducible and suitable for linguistic analysis.

## Next Steps

To build on these basics:

- Explore different text sources and their HTML structures
- Learn to handle different text encodings
- Practice extracting text from more complex webpage layouts
- Consider using the {polite} package for ethical scraping
- Explore ways to collect metadata along with your text

The textbook provides more detailed examples of text collection and processing in Chapter X, where we explore building and analyzing corpora from web sources.

