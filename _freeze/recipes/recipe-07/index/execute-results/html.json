{
  "hash": "f09faf48022bf742a7b44e326c42eab8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"07. Transforming and documenting data\"\nsubtitle: \"Prepare and enrich datasets for analysis\"\ndescription: |\n  The curated dataset reflects a tidy version of the original data. This data is relatively project-neutral. A such, project-specific changes are often made to bring the data more in line with the research goals. This may include modifying the unit of observation and/ or adding additional attributes to the data. This process may generate one or more new datasets that are used for analysis. In this recipe, we will explore a practical example of transforming data.\ncategories: [preparation]\n---\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Skills**\n\n- Text normalization and tokenization\n- Creating new variables by splitting, merging, and recoding existing variables\n- Augmenting data with additional variables from other sources or resources\n:::\n\nIn this recipe, we will employ a variety of tools and techniques to accomplish these tasks. Let's load the packages we will need for this recipe. Let's load the packages we will need for this recipe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(qtalrkit)\n```\n:::\n\n\nIn Lab 7, we will apply what we have learned in this recipe to a new dataset.\n\n## Concepts and strategies\n\n### Orientation\n\n<!-- about curated data and goal of transformation -->\n\nCurated datasets are often project-neutral. That is, they are not necessarily designed to answer a specific research question. Rather, they are designed to be flexible enough to be used in a variety of projects. This is a good thing, but it also means that we will likely need to transform the data to bring it more in line with our research goals. This can include normalizing text, modifying the unit of observation and/ or adding additional attributes to the data.\n\n<!-- about the MASC dataset -->\n\nIn this recipe, we will explore a practical example of transforming data. We will start with a curated dataset and transform it to reflect a specific research goal. The dataset we will use is the MASC dataset [@Ide2008]. This dataset contains a collection of words from a variety of genres and modalities of American English.\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nThe MASC dataset is a curated version of the original data. This data is relatively project-neutral.\n\nIf you would like to acquire the original data and curate it for use in this recipe, you can do so by running the following code:\n\n```r\n# Acquire the original data\nqtalrkit::get_compressed_data(\n  url = \"..\",\n  target_dir = \"data/original/masc/\"\n)\n\n# Curate the data\n\n# ... write a function and add it to the package\n```\n\n:::\n\nAs a starting point, I will assume that the curated dataset is available in the *data/derived/masc/* directory, as seen below.\n\n```{.bash}\ndata/\n├── analysis/\n├── derived/\n│   ├── masc_curated_dd.csv\n│   ├── masc/\n│   │   ├── masc_curated.csv\n├── original/\n│   ├── masc_do.csv\n│   ├── masc/\n│   │   ├── ...\n```\n\nThe first step is to inspect the data dictionary file. This file contains information about the variables in the dataset. It is also a good idea to review the data origin file, which contains information about the original data source.\n\nLooking at the data dictionary, in @tbl-masc-data-dictionary.\n\n\n::: {#tbl-masc-data-dictionary .cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Data dictionary for the MASC dataset</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> variable </th>\n   <th style=\"text-align:left;\"> name </th>\n   <th style=\"text-align:left;\"> description </th>\n   <th style=\"text-align:left;\"> variable_type </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> file </td>\n   <td style=\"text-align:left;\"> File </td>\n   <td style=\"text-align:left;\"> ID number of the source file </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ref </td>\n   <td style=\"text-align:left;\"> Reference </td>\n   <td style=\"text-align:left;\"> Reference number within the source file </td>\n   <td style=\"text-align:left;\"> integer </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> base </td>\n   <td style=\"text-align:left;\"> Base </td>\n   <td style=\"text-align:left;\"> Base form of the word (lemma) </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> msd </td>\n   <td style=\"text-align:left;\"> MSD </td>\n   <td style=\"text-align:left;\"> Part-of-speech tag (PENN tagset) </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> string </td>\n   <td style=\"text-align:left;\"> String </td>\n   <td style=\"text-align:left;\"> Text content of the word </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> title </td>\n   <td style=\"text-align:left;\"> Title </td>\n   <td style=\"text-align:left;\"> Title of the source file </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> source </td>\n   <td style=\"text-align:left;\"> Source </td>\n   <td style=\"text-align:left;\"> Name of the source </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> date </td>\n   <td style=\"text-align:left;\"> Date </td>\n   <td style=\"text-align:left;\"> Date of the source file (if available) </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> class </td>\n   <td style=\"text-align:left;\"> Class </td>\n   <td style=\"text-align:left;\"> Classification of the source. Modality and genre </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> domain </td>\n   <td style=\"text-align:left;\"> Domain </td>\n   <td style=\"text-align:left;\"> Domain or topic of the source </td>\n   <td style=\"text-align:left;\"> character </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nLet's read in the data and take a glimpse at it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the data\nmasc_curated <- read_csv(\"data/derived/masc/masc_curated.csv\")\n\n# Preview\nglimpse(masc_curated)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 591,097\nColumns: 10\n$ file   <chr> \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"…\n$ ref    <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ base   <chr> \"december\", \"1998\", \"your\", \"contribution\", \"to\", \"goodwill\", \"…\n$ msd    <chr> \"NNP\", \"CD\", \"PRP$\", \"NN\", \"TO\", \"NNP\", \"MD\", \"VB\", \"JJR\", \"IN\"…\n$ string <chr> \"December\", \"1998\", \"Your\", \"contribution\", \"to\", \"Goodwill\", \"…\n$ title  <chr> \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"…\n$ source <chr> \"ICIC Corpus of Philanthropic Fundraising Discourse\", \"ICIC Cor…\n$ date   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ class  <chr> \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", …\n$ domain <chr> \"philanthropic fundraising discourse\", \"philanthropic fundraisi…\n```\n\n\n:::\n:::\n\n\nWe may also want to do a summary overview of the dataset with {skimr}. This will give us a sense of the data types and the number of missing values.\n\n```txt\n── Data Summary ───────────────────────\n                           Values\nName                       masc_curated\nNumber of rows             591097\nNumber of columns          10\n_______________________\nColumn type frequency:\n  character                9\n  numeric                  1\n________________________\nGroup variables            None\n\n── Variable type: character ───────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 file                  0         1       3  40     0      392          0\n2 base                  4         1.00    1  99     0    28010          0\n3 msd                   0         1       1   8     0       60          0\n4 string               25         1.00    1  99     0    39474          0\n5 title                 0         1       3 203     0      373          0\n6 source             5732         0.990   3 139     0      348          0\n7 date              94002         0.841   4  17     0       62          0\n8 class                 0         1       5   5     0       18          0\n9 domain            18165         0.969   4  35     0       21          0\n\n── Variable type: numeric ─────────────\n  skim_variable n_missing complete_rate  mean    sd p0 p25  p50  p75  p100 hist\n1 ref                   0             1 3854. 4633.  0 549 2033 5455 24519 ▇▂▁▁▁\n```\n\nIn summary, the dataset contains 591,097 observations and 10 variables. The unit of observation is the word. The variable names are somewhat opaque, but the data dictionary provides some context that will help us understand the data.\n\nNow we want to consider how we plan to use this data in our analysis. Let's assume that we want to use this data to explore lexical variation in the MASC dataset across modalities and genres. We will want to transform the data to reflect this goal.\n\nIn @tbl-masc-transformed-idealized, we see an idealized version of the dataset we would like to have.\n\n\n\n::: {#tbl-masc-transformed-idealized .cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Idealized version of the MASC dataset</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> variable </th>\n   <th style=\"text-align:left;\"> name </th>\n   <th style=\"text-align:left;\"> type </th>\n   <th style=\"text-align:left;\"> description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> doc_id </td>\n   <td style=\"text-align:left;\"> Document ID </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> A unique identifier for each document </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> modality </td>\n   <td style=\"text-align:left;\"> Modality </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The modality of the document (e.g., spoken, written) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> genre </td>\n   <td style=\"text-align:left;\"> Genre </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The genre of the document (e.g., blog, newspaper) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> term_num </td>\n   <td style=\"text-align:left;\"> Term number </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> The position of the term in the document </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> term </td>\n   <td style=\"text-align:left;\"> Term </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The word </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> lemma </td>\n   <td style=\"text-align:left;\"> Lemma </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The lemma of the word </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pos </td>\n   <td style=\"text-align:left;\"> Part-of-speech </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The part-of-speech tag of the word </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nOf note, in this recipe we will derive a single transformed dataset. In other projects, you may want to generate various datasets with different units of observations. It all depends on your research question and the research aim that you are adopting.\n\n### Transforming data\n\nTo get from the curated dataset to the idealized dataset, we will need to perform a number of transformations. Some of these transformations will be relatively straightforward, while others will require more work. Let's start with the easy ones.\n\n1. Let's drop the variables that we will not use and at the same time rename the variables to make them more intuitive.\n\nWe will use the `select()` function to drop or rename variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Drop and rename variables\nmasc_df <-\n  masc_curated |>\n  select(\n    doc_id = file,\n    term_num = ref,\n    term = string,\n    lemma = base,\n    pos = msd,\n    mod_gen = class\n  )\n\nmasc_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 591,097 × 6\n   doc_id    term_num term         lemma        pos   mod_gen\n   <chr>        <dbl> <chr>        <chr>        <chr> <chr>  \n 1 110CYL067        0 December     december     NNP   WR LT  \n 2 110CYL067        1 1998         1998         CD    WR LT  \n 3 110CYL067        2 Your         your         PRP$  WR LT  \n 4 110CYL067        3 contribution contribution NN    WR LT  \n 5 110CYL067        4 to           to           TO    WR LT  \n 6 110CYL067        5 Goodwill     goodwill     NNP   WR LT  \n 7 110CYL067        6 will         will         MD    WR LT  \n 8 110CYL067        7 mean         mean         VB    WR LT  \n 9 110CYL067        8 more         more         JJR   WR LT  \n10 110CYL067        9 than         than         IN    WR LT  \n# ℹ 591,087 more rows\n```\n\n\n:::\n:::\n\n\nThat's a good start on the structure.\n\n2. Next, we will split the `mod_gen` variable into two variables: `modality` and `genre`.\n\nWe have a variable `mod_gen` that contains two pieces of information: modality and genre (e.g., `WR LT`). The information appears to separated by a space. We can make sure this is the case by tabulating the values. The `count()` function will count the number of occurrences of each value in a variable, and as a side effect it will summarize the values of the variable so we can see if there are any unexpected values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tabulate mod_gen\nmasc_df |>\n  count(mod_gen) |>\n  arrange(-n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 2\n   mod_gen     n\n   <chr>   <int>\n 1 SP TR   71630\n 2 WR EM   62036\n 3 WR FC   38608\n 4 WR ES   34938\n 5 WR FT   34373\n 6 WR BL   33278\n 7 WR JO   33042\n 8 WR JK   32420\n 9 WR NP   31225\n10 SP MS   29879\n11 WR NF   29531\n12 WR TW   28128\n13 WR GV   27848\n14 WR TG   27624\n15 WR LT   26468\n16 SP FF   23871\n17 WR TC   19419\n18 SP TP    6779\n```\n\n\n:::\n:::\n\n\nLooks good, our values are separated by a space. We can use the `separate_wider_delim()` function from {tidyr} to split the variable into two variables. We will use the `delim` argument to specify the delimiter and the `names` argument to specify the names of the new variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split mod_gen into modality and genre\nmasc_df <-\n  masc_df |>\n  separate_wider_delim(\n    cols = mod_gen,\n    delim = \" \",\n    names = c(\"modality\", \"genre\")\n  )\n\nmasc_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 591,097 × 7\n   doc_id    term_num term         lemma        pos   modality genre\n   <chr>        <dbl> <chr>        <chr>        <chr> <chr>    <chr>\n 1 110CYL067        0 December     december     NNP   WR       LT   \n 2 110CYL067        1 1998         1998         CD    WR       LT   \n 3 110CYL067        2 Your         your         PRP$  WR       LT   \n 4 110CYL067        3 contribution contribution NN    WR       LT   \n 5 110CYL067        4 to           to           TO    WR       LT   \n 6 110CYL067        5 Goodwill     goodwill     NNP   WR       LT   \n 7 110CYL067        6 will         will         MD    WR       LT   \n 8 110CYL067        7 mean         mean         VB    WR       LT   \n 9 110CYL067        8 more         more         JJR   WR       LT   \n10 110CYL067        9 than         than         IN    WR       LT   \n# ℹ 591,087 more rows\n```\n\n\n:::\n:::\n\n\n3. Create a document id variable.\n\nNow that we have the variables we want, we can turn our attention to the values of the variables. Let's start with the `doc_id` variable. This may a good variable to use as the document id. If we take a look at the values, however, we can see that the values are not very informative.\n\nLet's use the `distinct()` function to only show the unique values of the variable. We will also chain a `slice_sample()` function to randomly select a sample of the values. This will give us a sense of the values in the variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview doc_id\nmasc_df |>\n  distinct(doc_id) |>\n  slice_sample(n = 10)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 1\n   doc_id             \n   <chr>              \n 1 JurassicParkIV-INT \n 2 111367             \n 3 NYTnewswire6       \n 4 sw2014-ms98-a-trans\n 5 52713              \n 6 new_clients        \n 7 cable_spool_fort   \n 8 jokes10            \n 9 wsj_2465           \n10 wsj_0158           \n```\n\n\n:::\n:::\n\n\nYou can run this code various times to get a different sample of values.\n\nSince the `doc_id` variable is not informative, let's replace the variable's values with numeric values. In the end, we want a digit for each unique document and we want the words in each document to be grouped together.\n\nTo do this we will need to group the data by `doc_id` and then generate a new number for each group. We can achieve this by passing the data grouped by `doc_id` (`group_by()`) to the `mutate()` function and then using the `cur_group_id()` function to generate a number for each group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recode doc_id\nmasc_df <-\n  masc_df |>\n  group_by(doc_id) |>\n  mutate(doc_id = cur_group_id()) |>\n  ungroup()\n\nmasc_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre\n    <int>    <dbl> <chr>        <chr>        <chr> <chr>    <chr>\n 1      1        0 December     december     NNP   WR       LT   \n 2      1        1 1998         1998         CD    WR       LT   \n 3      1        2 Your         your         PRP$  WR       LT   \n 4      1        3 contribution contribution NN    WR       LT   \n 5      1        4 to           to           TO    WR       LT   \n 6      1        5 Goodwill     goodwill     NNP   WR       LT   \n 7      1        6 will         will         MD    WR       LT   \n 8      1        7 mean         mean         VB    WR       LT   \n 9      1        8 more         more         JJR   WR       LT   \n10      1        9 than         than         IN    WR       LT   \n# ℹ 591,087 more rows\n```\n\n\n:::\n:::\n\n\nTo check, we can again apply the `count()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check\nmasc_df |>\n  count(doc_id) |>\n  arrange(-n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 392 × 2\n   doc_id     n\n    <int> <int>\n 1    158 24520\n 2    300 22261\n 3    112 18459\n 4    113 17986\n 5    215 17302\n 6    312 14752\n 7    311 13376\n 8    200 13138\n 9    217 11753\n10    186 10665\n# ℹ 382 more rows\n```\n\n\n:::\n:::\n\n\nWe have 392 unique documents in the dataset. We also can see that the word lengths vary quite a bit. That's something we will need to keep in mind as we move forward into the analysis.\n\n4. Check the values of the `pos` variable.\n\nThe `pos` variable contains the part-of-speech tags for each word. The PENN Treebank tagset is used. Let's take a look at the values to get familiar with them, and also to see if there are any unexpected values.\n\nLet's use the `slice_sample()` function to randomly select a sample of the values. This will give us a sense of the values in the variable.\n\n```{.r}\n# Preview pos\nmasc_df |>\n  slice_sample(n = 10)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 7\n   doc_id term_num term          lemma         pos   modality genre\n    <int>    <dbl> <chr>         <chr>         <chr> <chr>    <chr>\n 1    303     2511 proliferation proliferation NN    WR       TC   \n 2     76     5245 And           and           CC    WR       FT   \n 3    300    17170 DAVY          davy          NNP   SP       MS   \n 4     80     5341 ”             ”             NN    WR       FT   \n 5    171      900 .             .             .     WR       TG   \n 6    166     2588 out           out           RP    WR       BL   \n 7     67       58 organization  organization  NN    WR       LT   \n 8    216     2944 include       include       VB    WR       TG   \n 9    234     1304 donation      donation      NN    WR       LT   \n10    231     3539 say           say           VB    WR       NF   \n```\n\n\n:::\n:::\n\n\nAfter running this code a few times, we can see that the many of the values are as expected. There are, however, some unexpected values. In particular, some punctuation and symbols are tagged as nouns.\n\nWe can get a better appreciation for the unexpected values by filtering the data to only show non alpha-numeric values (`^\\\\W+$`) in the `term` column and then tabulating the values by `term` and `pos`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter and tabulate\nmasc_df |>\n  filter(str_detect(term, \"^\\\\W+$\")) |>\n  count(term, pos) |>\n  arrange(-n) |>\n  print(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 152 × 3\n   term  pos       n\n   <chr> <chr> <int>\n 1 \",\"   ,     27112\n 2 \".\"   .     26256\n 3 \"\\\"\"  ''     5495\n 4 \":\"   :      4938\n 5 \"?\"   .      3002\n 6 \")\"   )      2447\n 7 \"(\"   (      2363\n 8 \"-\"   :      1778\n 9 \"!\"   .      1747\n10 \"/\"   NN     1494\n11 \"’\"   NN     1319\n12 \"-\"   -      1213\n13 \"”\"   NN     1076\n14 \"“\"   NN     1061\n15 \"]\"   NN     1003\n16 \"[\"   NN     1001\n17 \";\"   :       991\n18 \"--\"  :       772\n19 \">\"   NN      752\n20 \"...\" ...     716\n# ℹ 132 more rows\n```\n\n\n:::\n:::\n\n\nAs we can see from the sample above and from the PENN tagset documentation, most punctuation is tagged as the punctuation itself. For example, the period is tagged as `.` and the comma is tagged as `,`. Let's edit the data to reflect this.\n\nLet's look at the code, and then we will discuss it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recode\nmasc_df <-\n  masc_df |>\n  mutate(pos = case_when(\n    str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),\n    TRUE ~ pos\n  ))\n\n# Check\nmasc_df |>\n  filter(str_detect(term, \"^\\\\W+$\")) |> # preview\n  count(term, pos) |>\n  arrange(-n) |>\n  print(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 127 × 3\n   term  pos       n\n   <chr> <chr> <int>\n 1 \",\"   \",\"   27113\n 2 \".\"   \".\"   26257\n 3 \"\\\"\"  \"\\\"\"   5502\n 4 \":\"   \":\"    4939\n 5 \"?\"   \"?\"    3002\n 6 \"-\"   \"-\"    2994\n 7 \")\"   \")\"    2447\n 8 \"(\"   \"(\"    2363\n 9 \"!\"   \"!\"    1747\n10 \"/\"   \"/\"    1495\n11 \"’\"   \"’\"    1325\n12 \"”\"   \"”\"    1092\n13 \"“\"   \"“\"    1078\n14 \"]\"   \"]\"    1003\n15 \"[\"   \"[\"    1001\n16 \";\"   \";\"     993\n17 \"--\"  \"-\"     772\n18 \">\"   \">\"     753\n19 \"...\" \".\"     747\n20 \"'\"   \"'\"     741\n# ℹ 107 more rows\n```\n\n\n:::\n:::\n\n\nThe `case_when()` function allows us to specify a series of conditions and values. The first condition is that the `term` variable contains only non alpha-numeric characters. If it does, then we want to replace the value of the `pos` variable with the first character of the `term` variable, `str_sub(term, start = 1, end = 1)`. If the condition is not met, then we want to keep the original value of the `pos` variable, `TRUE ~ pos`.\n\nWe can see that our code worked by filtering the data to only show non alpha-numeric values (`^\\\\W+$`) in the `term` column and then tabulating the values by `term` and `pos`.\n\nFor completeness, I will also recode the `lemma` values for these values as well as the lemma can some times be multiple punctuation marks (*e.g.* `!!!!!`, `---`, *etc.*) for these terms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recode\nmasc_df <-\n  masc_df |>\n  mutate(lemma = case_when(\n    str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),\n    TRUE ~ lemma\n  ))\n\n# Check\nmasc_df |>\n  filter(str_detect(term, \"^\\\\W+$\")) |> # preview\n  count(term, lemma) |>\n  arrange(-n) |>\n  print(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 127 × 3\n   term  lemma     n\n   <chr> <chr> <int>\n 1 \",\"   \",\"   27113\n 2 \".\"   \".\"   26257\n 3 \"\\\"\"  \"\\\"\"   5502\n 4 \":\"   \":\"    4939\n 5 \"?\"   \"?\"    3002\n 6 \"-\"   \"-\"    2994\n 7 \")\"   \")\"    2447\n 8 \"(\"   \"(\"    2363\n 9 \"!\"   \"!\"    1747\n10 \"/\"   \"/\"    1495\n11 \"’\"   \"’\"    1325\n12 \"”\"   \"”\"    1092\n13 \"“\"   \"“\"    1078\n14 \"]\"   \"]\"    1003\n15 \"[\"   \"[\"    1001\n16 \";\"   \";\"     993\n17 \"--\"  \"-\"     772\n18 \">\"   \">\"     753\n19 \"...\" \".\"     747\n20 \"'\"   \"'\"     741\n# ℹ 107 more rows\n```\n\n\n:::\n:::\n\n\n5. Check the values of the `modality` variable.\n\nThe `modality` variable contains the modality tags for each document. Let's take a look at the values.\n\nLet's tabulate the values with `count()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tabulate modality\nmasc_df |>\n  count(modality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  modality      n\n  <chr>     <int>\n1 SP       132159\n2 WR       458938\n```\n\n\n:::\n:::\n\n\nWe see that the values are `SP` and `WR`, which stand for spoken and written, respectively. To make this a bit more transparent, we can recode these values to `Spoken` and `Written`. We will use the `case_when()` function to do this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recode modality\nmasc_df <-\n  masc_df |>\n  mutate(\n    modality = case_when(\n      modality == \"SP\" ~ \"Spoken\",\n      modality == \"WR\" ~ \"Written\"\n    )\n  )\n\nmasc_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre\n    <int>    <dbl> <chr>        <chr>        <chr> <chr>    <chr>\n 1      1        0 December     december     NNP   Written  LT   \n 2      1        1 1998         1998         CD    Written  LT   \n 3      1        2 Your         your         PRP$  Written  LT   \n 4      1        3 contribution contribution NN    Written  LT   \n 5      1        4 to           to           TO    Written  LT   \n 6      1        5 Goodwill     goodwill     NNP   Written  LT   \n 7      1        6 will         will         MD    Written  LT   \n 8      1        7 mean         mean         VB    Written  LT   \n 9      1        8 more         more         JJR   Written  LT   \n10      1        9 than         than         IN    Written  LT   \n# ℹ 591,087 more rows\n```\n\n\n:::\n:::\n\n\n6. Check the values of the `genre` variable.\n\nLet's look at the values of the `genre` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tabulate genre\nmasc_df |>\n  count(genre) |>\n  print(n = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 2\n   genre     n\n   <chr> <int>\n 1 BL    33278\n 2 EM    62036\n 3 ES    34938\n 4 FC    38608\n 5 FF    23871\n 6 FT    34373\n 7 GV    27848\n 8 JK    32420\n 9 JO    33042\n10 LT    26468\n11 MS    29879\n12 NF    29531\n13 NP    31225\n14 TC    19419\n15 TG    27624\n16 TP     6779\n17 TR    71630\n18 TW    28128\n```\n\n\n:::\n:::\n\n\nThese genre labels are definitely cryptic. The data dictionary does not list these labels and their more verbose descriptions. However, looking at the original data's README, we can find the file (`resource-headers.xml`) that lists these genre labels.\n\n```txt\n1. 'BL' for blog\n2. 'NP' is newspaper\n3. 'EM' is email\n4. 'ES' is essay\n5. 'FT' is fictlets\n6. 'FC' is fiction\n7. 'GV' is government\n8. 'JK' is jokes\n9. 'JO' is journal\n10. 'LT' is letters\n11. 'MS' is movie script\n12. 'NF' is non-fiction\n13. 'FF' is face-to-face\n14. 'TC' is technical\n15. 'TG' is travel guide\n16. 'TP' is telephone\n17. 'TR' is transcript\n18. 'TW' is twitter\n```\n\nNow we can again use the `case_when()` function. This time we will see if `genre` is equal to one of the genre labels and if it is, then we will replace the value with the more verbose description.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recode genre\nmasc_df <-\n  masc_df |>\n  mutate(\n    genre = case_when(\n      genre == \"BL\" ~ \"Blog\",\n      genre == \"NP\" ~ \"Newspaper\",\n      genre == \"EM\" ~ \"Email\",\n      genre == \"ES\" ~ \"Essay\",\n      genre == \"FT\" ~ \"Fictlets\",\n      genre == \"FC\" ~ \"Fiction\",\n      genre == \"GV\" ~ \"Government\",\n      genre == \"JK\" ~ \"Jokes\",\n      genre == \"JO\" ~ \"Journal\",\n      genre == \"LT\" ~ \"Letters\",\n      genre == \"MS\" ~ \"Movie script\",\n      genre == \"NF\" ~ \"Non-fiction\",\n      genre == \"FF\" ~ \"Face-to-face\",\n      genre == \"TC\" ~ \"Technical\",\n      genre == \"TG\" ~ \"Travel guide\",\n      genre == \"TP\" ~ \"Telephone\",\n      genre == \"TR\" ~ \"Transcript\",\n      genre == \"TW\" ~ \"Twitter\"\n    )\n  )\n\nmasc_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 591,097 × 7\n   doc_id term_num term         lemma        pos   modality genre  \n    <int>    <dbl> <chr>        <chr>        <chr> <chr>    <chr>  \n 1      1        0 December     december     NNP   Written  Letters\n 2      1        1 1998         1998         CD    Written  Letters\n 3      1        2 Your         your         PRP$  Written  Letters\n 4      1        3 contribution contribution NN    Written  Letters\n 5      1        4 to           to           TO    Written  Letters\n 6      1        5 Goodwill     goodwill     NNP   Written  Letters\n 7      1        6 will         will         MD    Written  Letters\n 8      1        7 mean         mean         VB    Written  Letters\n 9      1        8 more         more         JJR   Written  Letters\n10      1        9 than         than         IN    Written  Letters\n# ℹ 591,087 more rows\n```\n\n\n:::\n:::\n\n\nDuring the process of transformation and afterwards, it is a good idea to tabulate and/ or visualize the dataset. This provides us an opportunity to get to know the dataset better and also may help us identify inconsistencies that we would like to address in the transformation, or at least be aware of as we move towards analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many documents are in each modality?\nmasc_df |>\n  distinct(doc_id, modality) |>\n  count(modality) |>\n  arrange(-n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  modality     n\n  <chr>    <int>\n1 Written    371\n2 Spoken      21\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many documents are in each genre?\nmasc_df |>\n  distinct(doc_id, genre) |>\n  count(genre) |>\n  arrange(-n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 2\n   genre            n\n   <chr>        <int>\n 1 Email          174\n 2 Newspaper       54\n 3 Letters         49\n 4 Blog            21\n 5 Jokes           16\n 6 Journal         12\n 7 Essay            8\n 8 Fiction          7\n 9 Travel guide     7\n10 Face-to-face     6\n11 Movie script     6\n12 Technical        6\n13 Fictlets         5\n14 Government       5\n15 Non-fiction      5\n16 Telephone        5\n17 Transcript       4\n18 Twitter          2\n```\n\n\n:::\n\n```{.r .cell-code}\n# What is the averge length of documents (in words)?\nmasc_df |>\n  group_by(doc_id) |>\n  summarize(n = n()) |>\n  summarize(\n    mean = mean(n),\n    median = median(n),\n    min = min(n),\n    max = max(n)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n   mean median   min   max\n  <dbl>  <dbl> <int> <int>\n1 1508.   418.    45 24520\n```\n\n\n:::\n\n```{.r .cell-code}\nmasc_df |>\n  group_by(doc_id) |>\n  summarize(n = n()) |>\n  ggplot(aes(x = n)) +\n  geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/masc-transformed-evaluation-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# What is the distribution of the length of documents by modality?\nmasc_df |>\n  group_by(doc_id, modality) |>\n  summarize(n = n()) |>\n  ggplot(aes(x = n, fill = modality)) +\n  geom_density(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/masc-transformed-evaluation-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# What is the distribution of the length of documents by genre?\nmasc_df |>\n  group_by(doc_id, modality, genre) |>\n  summarize(n = n()) |>\n  ggplot(aes(x = genre, y = n)) +\n  geom_boxplot() +\n  facet_wrap(~ modality, scales = \"free_x\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/masc-transformed-evaluation-3.png){width=672}\n:::\n:::\n\n\nOnce we are satisfied with the structure and values of the dataset, we can save it to a file. We will use the `write_csv()` function from {readr} to do this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the data\nwrite_csv(masc_df, \"data/derived/masc/masc_transformed.csv\")\n```\n:::\n\n\nThe structure of the *data/* directory in our project should now look like this:\n\n```{.bash}\ndata/\n├── analysis/\n├── derived/\n│   ├── masc_curated_dd.csv\n│   ├── masc/\n│   │   ├── masc_curated.csv\n│   │   ├── masc_transformed.csv\n├── original/\n```\n\n### Documenting data\n\nThe last step is to document the process and the resulting dataset(s). In this particular case we only derived one transformed dataset. The documentation steps are the same as in the curation step. We will organize and document the process file (often a `.qmd` file) and then create a data dictionary for each of the transformed datasets. The `create_data_dictionary()` function can come in handy for scaffolding the data dictionary file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data dictionary\ncreate_data_dictionary(\n  data = masc_df,\n  file_path = \"data/derived/masc/masc_transformed_dd.csv\"\n)\n```\n:::\n\n\n## Summary\n\nIn this recipe, we have looked at an example of transforming a curated dataset. This recipe included operations such as:\n\n- Text normalization\n- Variable recoding\n- Splitting variables\n\nIn other projects, the transformation steps will inevitably differ, but these strategies are commonly necessary in almost any project.\n\nJust as with other steps in the data preparation process, it is important to document the transformation steps. This will help you and others understand the process and the resulting dataset(s).\n\n## Check your understanding\n\n1. Which function would you use to remove duplicate rows in a dataset? <select class='webex-select'><option value='blank'></option><option value=''>group_by()</option><option value=''>mutate()</option><option value='answer'>distinct()</option><option value=''>filter()</option></select>\n2. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> The `str_c()` function from {stringr} is used to separate strings rather than combine them.\n3. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select> The `count()` function from {dplyr} is used to tabulate the values of a variable.\n4. If you want to recode the age of learners into categories such as \"child\", \"teen\", and \"adult\" based on their age, which function should you use? <select class='webex-select'><option value='blank'></option><option value=''>mutate()</option><option value='answer'>case_when()</option><option value=''>unite()</option><option value=''>separate_wider_delim()</option></select>\n5. To normalize text by removing leading and trailing whitespace, you use the <input class='webex-solveme nospaces' size='8' data-answer='[\"str_trim\"]'/>`()` function from {stringr}.\n6. To normalize text by converting all characters to lowercase, you use the <input class='webex-solveme nospaces' size='12' data-answer='[\"str_to_lower\"]'/>`()` function from {stringr}.\n\n## Lab preparation\n\n<!-- Lab 07 description and checklist -->\n\nIn preparation for [Lab 7](https://github.com/qtalr/lab-07), review and ensure you are comfortable with the following:\n\n- Vector, data frame, and list data structures\n- Subsetting and filtering data structures with and without regular expressions\n- Reshaping datasets by rows and columns\n\nIn this lab, we will practice these skills and expand our knowledge of data preparation by transforming and documenting data with Tidyverse packages such as {dplyr}, {tidyr}, and {stringr}.\n\nYou will have a choice of a dataset to transform. Before you start the lab, you should consider which dataset you would like to use, what the idealized structure the transformed dataset will take, and what strategies you will likely employ to transform the dataset. You should also consider the information you need to document the data transformation process.\n\n## References\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}