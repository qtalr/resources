{
  "hash": "b23ad42aa640381dd51b5475b9ac7df7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"09. Building predictive models\"\nsubtitle: \"Supervised machine learning\"\ndescription: |\n  This recipe will cover the process of building a predictive model to classify text into one of three Spanish dialects: Argentinian, Mexican, or Spanish. We will take a step-by-step approach that includes data preparation, model training and evaluation, and result interpretation. We will see practical examples of how to apply the {tidymodels} framework to build and evaluate a predictive model.\ncategories: [analysis]\n---\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Skills**\n\n- How to identify variables of interest\n- How to inspect datasets\n- How to interrogate datasets and iteratively develop a model to improve performance\n- How to interpret results of predictive models\n:::\n\n\nThe workflow for building a predictive model is shown in @tbl-pda-workflow. Note that Step 6 includes an optional step to iterate on the model to improve performance. This is optional because it is not always necessary to iterate on the model. However, it is often the case that the first model you build is not the best model. So it is good to be prepared to iterate on the model.\n\n<!-- Workflow: steps -->\n\n| Step | Name | Description |\n|------|-------------|-------------|\n| 1 | Identify | Consider the research question and aim and identify relevant variables |\n| 2 |  | Split the data into representative training and testing sets |\n| 3 |  | Apply variable selection and engineering procedures |\n| 4 | Inspect | Inspect the data to ensure that it is in the correct format and that the training and testing sets are representative of the data |\n| 5 | Interrogate | Train and evaluate the model on the training set, adjusting models or hyperparameters as needed, to produce a final model |\n| 6 | (Optional) Iterate | Repeat steps 3-5 to selecting new variables, models, hyperparameters |\n| 7 | Interpret | Interpret the results of the final model in light of the research question or hypothesis |\n\n: The predictive modeling workflow {#tbl-pda-workflow tbl-colwidths=\"[10, 10, 80]\"}\n\n<!-- Approach -->\n\nLet's get started by loading some of the key packages we will use in this recipe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels) # for modeling\nlibrary(textrecipes) # for text preprocessing\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation\nlibrary(stringr) # for string manipulation\nlibrary(tidytext) # for text manipulation\nlibrary(ggplot2) # for visualization\nlibrary(janitor) # for tabyl()\n\ntidymodels_prefer() # avoid function name conflicts\n```\n:::\n\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nNote that loading {tidymodels} will load 22 other packages commonly used in modeling workflows. You can see the list of packages by running `tidymodels::tidyverse_packages()`.\n:::\n\n## Concepts and strategies\n\n### Orientation\n\n<!-- About the dataset and the aim of the analysis -->\n\nWe will use the ACTIV-ES corpus to build a predictive model that can classify text as one of three dialects of Spanish: Argentinian, Mexican, or Spanish. We will frame this as a supervised learning problem, where we have a set of texts that have been labeled with the dialect of Spanish that they are written in. In contrast to the classification task in the chapter, which was binary, this is a multiclass classification task, where we are trying to classify each document as one of three classes.\n\n<!-- Prep the dataset -->\n\n\n::: {.cell}\n\n:::\n\n\n\n\nLet's preview the structure of the ACTIVES dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naes_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# # A tibble: 430 × 3\n#    doc_id variety   text                                                        \n#     <dbl> <fct>     <chr>                                                       \n#  1 199500 Argentina No está , señora . Aquí tampoco . No aparece , señora . ¿ D…\n#  2 184782 Argentina ALGUIEN AL TELÉFONO . LA ANGUSTIA . Ah , no , no , no mi hi…\n#  3  47823 Argentina Habrá que cumplir su última voluntad , ¿ el medallón ? Lo v…\n#  4 282622 Argentina Sucedió en Hualfin Esta es la historia de tres generaciones…\n#  5  62433 Argentina 10 secuestros en 10 días ! Y no hay el menor índice . Bueno…\n#  6  70250 Argentina Y preguntada que fue sí reconocen el cadáver exhumado ... y…\n#  7  71897 Argentina ¡ Jeremías ! ¡ Jeremías ! ¡ No dejés parir a tu mujer ! Sei…\n#  8 333883 Argentina Usted . Usted que frecuenta el éxito como una costumbre más…\n#  9 333954 Argentina Miles de campanas nos traen , a través de los siglos , el t…\n# 10 175243 Argentina Y ? Enseguida viene , fue al baño . Bueno , pero la mesa la…\n# # ℹ 420 more rows\n```\n\n\n:::\n:::\n\n\nThis dataset contains 430 documents, each of which is labeled with the variety of Spanish that it is written in and the text of the document. A document ID is also included, which we will be able to use to index the documents. The `variety` vector is a factor. As this will be the outcome variable in our predictive model, this is good as most predictive models require classification variables to be factors.\n\nLet's get a sense of the distribution of the `variety` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naes_df |>\n  tabyl(variety) |>\n  adorn_pct_formatting(digits = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   variety   n percent\n Argentina 128   29.8%\n    Mexico 119   27.7%\n     Spain 183   42.6%\n```\n\n\n:::\n:::\n\n\nWe can see that the dataset is somewhat balanced, with Peninsular Spanish comprising the larger portion of the texts.\n\n### Analysis\n\nAt this point we can start to approach building a predictive model that can distinguish between the Spanish varieties using the text. We will first start by applying steps 1 and 2 of the workflow. We will then apply steps 3-5 iteratively to build, evaluate, and improve the model, as necessary, before applying it to the test data to assess and interpret the results.\n\nLet's go ahead and perform steps 1 and 2. To split the data into training and testing sets, we will use {rsample}. The `initial_split()` function, sets up the splits and we use `variety` as the stratification variable to ensure that the training and testing sets are representative of the distribution of the outcome variable. As this process is random, we will set the seed for reproducibility. Finally, we call the `training()` and `testing()` functions to extract the training and testing sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed for reproducibility\nset.seed(1234)\n\n# Split the data into training and testing sets\naes_split <-\n  initial_split(\n    data = aes_df,\n    prop = 0.8,\n    strata = variety\n  )\n\naes_train <- training(aes_split) # extract the training set\naes_test <- testing(aes_split) # extract the testing set\n```\n:::\n\n\nWe will then set the base recipe which formally identifies the relationship between the predictor and outcome variables. The `recipe()` function from {recipes} is used to create a recipe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the base recipe\naes_base_rec <-\n  recipe(\n    formula = variety ~ text,\n    data = aes_train\n  )\n```\n:::\n\n\nWe now have steps 1 and 2 of the workflow completed. We have identified the variables of interest and split the data into training and testing sets.\n\nOne more thing we will do here is to set up the cross-validation folds. Every time we fit a model to the training data, we will want to evaluate the model's performance on the training data. However, we don't want to do this in a way that is biased --testing the model on the same data that it was trained on! For this reason, we will use cross-validation to split the training data into multiple training and validation sets which represent different splits of the training data.\n\nWe will use the `vfold_cv()` function from {rsample} to set up the cross-validation folds. We will use 10 folds, which is a common number of folds to use. We will also use the `strata` argument to ensure that the folds are representative of the distribution of the outcome variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(1234)\n\n# Set up the cross-validation folds\ncv_folds <-\n  vfold_cv(\n    data = aes_train,\n    v = 10,\n    strata = variety\n  )\n```\n:::\n\n\nWith these common steps completed, we can now apply and reapply steps 3-5 of the workflow to build, evaluate, and improve the model.\n\n#### Approach 1\n\nIn our first approach, let's start simple by using words as features and apply a logistic regression model. We won't be completely naive, however, as we are familiar with the undo influence of the most frequent words. To address this, we will apply a term frequency-inverse document frequency (TF-IDF) transformation to the text in order to downweight the influence of the most frequent words and promote words that are more indicative of each class. Furthermore, we know that we will want to use a regularized regression model to avoid overfitting to particular words.\n\nTo get started, we will use {textrecipes} to add steps to our `aes_base_rec` recipe to preprocess the text. We will use the `step_tokenize()` function to tokenize the text. This tokenization process will likely result in a very large number of terms, most of which will not be informative and will add computational overhead. We will want to restrict the number of terms with the `step_tokenfilter()` function. However, it is not clear how many terms we should restrict the tokens to. For now, we will start with 1,000 tokens, but we will likely want to revisit this later. We will also use the `step_tfidf()` function to apply a TF-IDF transformation to the text setting `smooth_idf` to `FALSE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add preprocessing steps to the recipe\naes_rec <-\n  aes_base_rec |>\n  step_tokenize(text) |>\n  step_tokenfilter(text, max_tokens = 1000) |>\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Preview the recipe\naes_rec\n```\n:::\n\n\nTo implement the recipe and to preview the text preprocessing steps we apply the `prep()` and `bake()` functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naes_bake <-\n  aes_rec |>\n  prep() |>\n  bake(new_data = NULL)\n\n# Preview\ndim(aes_bake)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  343 1001\n```\n\n\n:::\n\n```{.r .cell-code}\naes_bake[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 5\n  variety   tfidf_text_1 tfidf_text_10 tfidf_text_15 tfidf_text_2\n  <fct>            <dbl>         <dbl>         <dbl>        <dbl>\n1 Argentina     0.000206     0.000599       0.000402      0      \n2 Argentina     0            0              0             0      \n3 Argentina     0            0.0000887      0             0      \n4 Argentina     0.00437      0              0.00142       0.00381\n5 Argentina     0            0.00155        0             0.00124\n```\n\n\n:::\n:::\n\n\nWe now have a recipe that will tokenize the text, restrict the tokens to the most common 1,000 tokens, and create a TF-IDF matrix.\n\nIt is not a bad idea to inspect the features at this point to make sure that the preprocessing steps have been applied correctly and to gauge what this feature selection looks like so that when it comes time to interpret the model, we have a sense of what the model is doing.\n\nAs TF-IDF is going to be the main feature in our model, let's visualize the top 20 terms by class. To do this, we will use {dplyr} to get the median TF-IDF score for each word by class, convert the data from wide to long format using the `pivot_longer()` function, and then use {ggplot2} to visualize the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sum the term frequencies by class\nclass_freq_wide <-\n  aes_bake |>\n  group_by(variety) |>\n  summarize(\n    across(\n      starts_with(\"tfidf_\"),\n      median\n    )\n  ) |>\n  ungroup()\n\n# Convert the data from wide to long format\nclass_freq_long <-\n  class_freq_wide |>\n  pivot_longer(\n    cols = starts_with(\"tfidf_\"),\n    names_to = \"term\",\n    values_to = \"tfidf\"\n  ) |>\n  mutate(term = str_remove(term, \"tfidf_text_\"))\n\n# Visualize the top 20 terms by class\nclass_freq_long |>\n  slice_max(n = 20, order_by = tfidf, by = variety) |>\n  mutate(term = reorder_within(term, tfidf, variety)) |>\n  ggplot(aes(x = term, y = tfidf)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~variety, scales = \"free_y\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![Top 20 terms by class](index_files/figure-html/fig-tfidf-viz-1.png){#fig-tfidf-viz width=672}\n:::\n:::\n\n\nIn @fig-tfidf-viz, we see the words that are most indicative of each language variety. If you are familiar with Spanish, you can probably detect some variety-specific terms. For example, \"vos\" is a pronoun used in Argentinian Spanish and \"os\" is a pronoun used in Peninsular Spanish. There is also some overlap between the varieties, such as \"tienes\" and \"te\".\n\nAnother point to note is the difference in magnitude of the TF-IDF scores between the Argentinian and other varieties. This suggests that the Argentinian variety is more distinct from the other varieties than the other varieties are from each other. Among the most distinctive terms are verbal forms that are specific to Argentinian Spanish, such as \"tenés\" and \"sos\".\n\nNow let's create a model specification. We will use the `multinom_reg()` function from {parsnip} to create a multinomial logistic regression model, as we have multiple classes in our prediction task. We will use the \"glmnet\" engine, which will allow us to apply regularization to the model, arbitrarily set to 0.01. We will use the `set_engine()` function to set the engine and the `set_mode()` function to set the mode to \"classification\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a model specification\naes_spec <-\n  multinom_reg(\n    penalty = 0.01,\n    mixture = 1\n  ) |>\n  set_engine(\"glmnet\") |>\n  set_mode(\"classification\")\n```\n:::\n\n\nTo combine the recipe and the model specification, we will use {workflows}. We will use the `workflow()` function and pass `add_recipe(aes_rec)` and `add_model(aes_spec)` as arguments to add the recipe and the model specification to the workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a workflow\naes_wf <-\n  workflow() |>\n  add_recipe(aes_rec) |>\n  add_model(aes_spec)\n```\n:::\n\n\nWe can now use the cross-validation folds that we set up earlier. We will use the `fit_resamples()` function to fit the model to the training data using the cross-validation folds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model to the training data\naes_train_fit <-\n  aes_wf |>\n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n\nWe can now evaluate the model's performance on the training data. We will use the `collect_metrics()` function to collect the metrics from the cross-validation folds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Evaluate the model's performance on the training data\naes_train_fit |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    multiclass 0.804    10  0.0210 Preprocessor1_Model1\n2 brier_class multiclass 0.147    10  0.0130 Preprocessor1_Model1\n3 roc_auc     hand_till  0.923    10  0.0109 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n\nWe can see that the model has a mean accuracy of 80.4% and ROC-AUC of 14.7%. That pretty good for a first pass. To get a sense of how good (or bad) it is, let's compare it to a baseline model.\n\nA baseline model is the simplest model that we can use to compare the performance of our model to. A common baseline model is a model that always predicts the most frequent class. In our case, this is Peninsular Spanish, which accounts for 42.6% of the data. So it is clear that our model is doing much better than a baseline model which will have an accuracy score of 42.6%.\n\nWe can visualize the correct and incorrect predictions using a confusion matrix. We will use the `conf_mat_resampled()` function from {yardstick} to create the confusion matrix and the `autoplot()` function from {ggfortify} to visualize it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naes_train_fit |>\n  conf_mat_resampled(tidy = FALSE) |>\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![Confusion matrix for the model in Approach 1](index_files/figure-html/fig-aes-a1-conf-mat-1.png){#fig-aes-a1-conf-mat width=672}\n:::\n:::\n\n\nThe left-downward diagonal of the confusion matrix represents the average number of documents correctly predicted for the aggregated model across the cross-validation folds. Other cells represent the average number of documents incorrectly predicted for each class-class combination. You can read these by using the row label to identify the predicted class and the column label to identify the actual class. So, for example, the model predicted Mexico $n$ times when the actual class was Argentina. And so on.\n\n#### Approach 2\n\nIn our first approach we applied a TF-IDF transformation to the text and used a regularized multinomial logistic regression model. We also restricted the tokens to the 1,000 most frequent tokens and arbitrarily set the regularization parameter to 0.01. This resulted in an aggregate accuracy score of 80.4% on the training data. This is a good start, but see if we can do better.\n\nIn this second approach, let's try to improve the model by applying a more principled approach to feature and hyperparameter selection.\n\nTo do this we will 'tune' the `max_tokens` and `penalty` hyperparameters in our recipe and model specifications, respectively. We need to update our recipe and model specification to include placeholders for these parameters replacing the previous values with `tune()`. We will also need to update our workflow to include the updated recipe and model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Update the recipe\naes_rec <-\n  aes_base_rec |>\n  step_tokenize(text) |>\n  step_tokenfilter(text, max_tokens = tune()) |> # adds placeholder\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Update the model specification\naes_spec <-\n  multinom_reg(\n    penalty = tune(), # adds placeholder\n    mixture = 1\n  ) |>\n  set_engine(\"glmnet\") |>\n  set_mode(\"classification\")\n```\n:::\n\n\nWe can now create a workflow that includes the recipe and the model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a workflow\naes_wf <-\n  workflow() |>\n  add_recipe(aes_rec) |>\n  add_model(aes_spec)\n```\n:::\n\n\nNow we set up the range of values for both the `max_tokens` and `penalty` hyperparameters. The `grid_regular()` function from {dials} will allow us to specify a grid of values for each hyperparameter.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the hyperparameter grid\naes_grid <-\n  grid_regular(\n    max_tokens(range = c(250, 2000)),\n    penalty(range = c(-3, -1)),\n    levels = c(max_tokens = 5, penalty = 10)\n  )\n\naes_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 2\n   max_tokens penalty\n        <int>   <dbl>\n 1        250 0.001  \n 2        687 0.001  \n 3       1125 0.001  \n 4       1562 0.001  \n 5       2000 0.001  \n 6        250 0.00167\n 7        687 0.00167\n 8       1125 0.00167\n 9       1562 0.00167\n10       2000 0.00167\n# ℹ 40 more rows\n```\n\n\n:::\n:::\n\n\nThe `range = ` argument specifies the range of values to include in the grid. For `max_tokens`, this is straightforward. For `penalty`, we are specifying the range of values on the log scale. So the range of values is 0.001 to 0.1. The `levels` argument specifies the number of values to include in the grid. In this case, we will include 5 values for `max_tokens` and 10 values for `penalty`. This will result in 50 combinations of hyperparameter values.\n\nWe will then pass our `aes_wf` workflow to the `tune_grid()` function with the grid values we specified to tune the hyperparameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tune the hyperparameters\naes_tune <-\n  aes_wf |>\n  tune_grid(\n    resamples = cv_folds,\n    grid = aes_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n\nThe `aes_grid` object is a tibble which contains the grid all the combinations of hyperparameter values. In this case, there are 50 combinations. That means we are going to fit 50 models to the training data! This is a lot of models, but it is worth it to get a more robust estimate of the model's performance.\n\nWe can use the `collect_metrics()` function to collect the metrics from the cross-validation folds for each of our tuning parameters, but this will result in a lot of output. Instead, we can use the `autoplot()` function to visualize the metrics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the collected metrics\n\naes_tune |> autoplot()\n```\n\n::: {.cell-output-display}\n![Metrics for model tuning in Approach 2](index_files/figure-html/fig-aes-tune-metrics-1.png){#fig-aes-tune-metrics width=672}\n:::\n:::\n\n\nWe see some variation across the folds in the accuracy and ROC-AUC scores. This will help us make a more informed decision about which hyperparameters to use.\n\nThe metric to use to select the best model is something to consider.Accuracy is an important measure, but does not tell the whole story. In particular, accuracy does not tell us how well the model is doing for each class --only the overall correct and incorrect predictions. To get a better sense of how the model is doing across the classes, we can pay attention to the ROC-AUC score. The ROC-AUC score is a measure of the area under the receiver operating characteristic (ROC) curve. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) for each class at different probability thresholds. This measure is useful because it is not affected by class imbalance.\n\nLet's select the best model based on the ROC-AUC score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the best model\naes_tune_best <-\n  aes_tune |>\n  select_best(metric = \"roc_auc\")\n\naes_tune_best\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  penalty max_tokens .config              \n    <dbl>      <int> <chr>                \n1  0.0215       1562 Preprocessor4_Model07\n```\n\n\n:::\n:::\n\n\nWe can now update our workflow with the best hyperparameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Update the workflow\naes_wf <-\n  aes_wf |>\n  finalize_workflow(aes_tune_best)\n\naes_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.0215443469003188\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\nWe can now see that the updated workflow will replace the `tune()` placeholders with the best hyperparameters we selected.\n\nLet's again perform a resampled fit on the training data using our new tuned model and then compare our results with the previous, abritrarily tuned model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model to the training data\naes_train_fit <-\n  aes_wf |>\n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Evaluate the model's performance on the training data\naes_train_fit |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    multiclass 0.819    10  0.0135 Preprocessor1_Model1\n2 brier_class multiclass 0.130    10  0.0101 Preprocessor1_Model1\n3 roc_auc     hand_till  0.938    10  0.0104 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n\nThe accuracy score has improved just a bit, from an aggregate score of 80.4% to 81.9%.\n\nIn all likelihood, we would want to continue to iterate on this model, applying different feature selection and engineering procedures, different models, and different hyperparameters --I will consider some suggestions in the next section. However, for the sake of time, we will stop here and train our final model on the training data and then apply the model to the test data to assess and interpret the results.\n\n##### Interpreting the model\n\nAt this stage we are ready to interpret the model. We first fit the model to the training data, then apply the model to the test data, and evaluate the model's performance on the test data. Finally, we will dig into the model to interpret the importance of the features to help us understand what the model is doing and what it can tell us about words that are indicative, or not, of each variety.\n\nLet's fit our final model to the training data and evaluate it on the testing data using the `last_fit()` function which takes our updated workflow and the original split we created earlier which is stored in `aes_split`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the final model\naes_final_fit <- last_fit(aes_wf, aes_split)\n```\n:::\n\n\nWe can now collect the performance metrics from the testing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the performance metrics\naes_final_fit |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    multiclass     0.736 Preprocessor1_Model1\n2 roc_auc     hand_till      0.872 Preprocessor1_Model1\n3 brier_class multiclass     0.180 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n\nThe accuracy of this model on the test data is 73.6%. This is lower than the accuracy on the training data. Should we be surprised? Not really. The model was trained on the training data, so it is not surprising that it would perform better on the training data than the test data, despite the fact that we used cross-validation to evaluate the model on the training data. This is a good reminder that the model is not perfect and that we should not expect it to be.\n\nWhat does the 'kap' metric mean? The Kappa statistic is a measure of agreement between the predicted and actual classes. It is a measure of agreement that is corrected for the possibility that some correct prediction may have occurred by chance. The kappa statistic ranges from 0 to 1, with 0 indicating no agreement above chance and 1 indicating perfect agreement. In this case, the kappa statistic is 87.2%, which indicates that there is a moderate amount of agreement between the predicted and actual classes.\n\nLet's explore if there is a difference in performance across the classes. To do this, we will use the `conf_mat()` function from {yardstick} to create the confusion matrix and the `autoplot()` function from {ggfortify} to visualize it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naes_final_fit |>\n  collect_predictions() |>\n  conf_mat(truth = variety, estimate = .pred_class) |>\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![Confusion matrix for the model in Approach 2](index_files/figure-html/fig-aes-a2-conf-mat-1.png){#fig-aes-a2-conf-mat width=672}\n:::\n:::\n\n\nWe can see that the model is doing a good job of predicting Peninsular Spanish, but is not doing as well with the other varieties. This is not surprising given that Peninsular Spanish is the most frequent class in the data. This is a good reminder that accuracy is not the only metric to consider when evaluating a model. We can get a better sense of how the model is doing across the classes by looking at the ROC-AUC score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the ROC-AUC score\naes_final_fit |>\n  collect_predictions() |>\n  roc_curve(truth = variety, .pred_Argentina:.pred_Spain) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![ROC plot for the final fitted model](index_files/figure-html/fig-final-fit-roc-curve-1.png){#fig-final-fit-roc-curve width=672}\n:::\n:::\n\n\nTaken together, we have a decent model that can predict the variety of Spanish that a text is written in. We can also see that although prediction accuracy appears higher for Peninsular Spanish, the ROC-AUC curves suggest that the model is doing a better job of predicting the other varieties based on the features.\n\nThere is still room for improvement --as we recognized earlier. However, it is important that we do not start to use the testing data to improve the model. The testing data should only be used to evaluate the model. If we start to use the testing data to improve the model, we will no longer have an unbiased estimate of the model's performance.\n\nLet's now dig into our model's features to explore what words are driving the model's predictions. The approach to do this will depend on the model. In this case, we used a multinomial logistic regression model, which is a linear model. This means that we can interpret the model's coefficients to understand the importance of the features. Coefficients that are positive indicate that the feature is associated with the reference class and coefficients that are negative indicate that the feature is associated with the non-reference class. For classification tasks with two classes, this is straightforward to interpret.\n\nThe issue here, however, is that we have more than two classes (i.e., Argentina, Mexico, and Spain). In these cases, the coefficients estimates for each class need to be extracted and standardized to be compared across classes.\n\nWe can do this using the `extract_fit_parsnip()` function from {parsnip}. This will extract the model object from the workflow object. The `tidy()` function from {broom} will then organize the coefficients (log-odds) for each predictor terms for each outcome class. We can then use the `filter()` function from {dplyr} to remove the intercept term and the `mutate()` function from {dplyr} to remove the \"tfidf_text_\" prefix from the term names so that they are more legible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the coefficients\naes_coefs <-\n  aes_final_fit |>\n  extract_fit_parsnip() |>\n  tidy() |>\n  filter(term != \"(Intercept)\") |>\n  mutate(term = str_remove(term, \"tfidf_text_\"))\n\nslice_sample(aes_coefs, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   class     term      estimate penalty\n   <chr>     <chr>        <dbl>   <dbl>\n 1 Mexico    vengó            0  0.0215\n 2 Mexico    tú               0  0.0215\n 3 Mexico    tin              0  0.0215\n 4 Mexico    papel            0  0.0215\n 5 Spain     están            0  0.0215\n 6 Argentina perdido          0  0.0215\n 7 Argentina caballero        0  0.0215\n 8 Spain     señora           0  0.0215\n 9 Argentina casar            0  0.0215\n10 Spain     serás            0  0.0215\n```\n\n\n:::\n:::\n\n\nNow to standardize the log-odds coefficients so that they are comparable across the classes, we will use the `scale()` function from base R to transform the coeffients such that each class has a mean of 0 and a standard deviation of 1. `scale()` returns a matrix, so we will use the `as.vector()` function to convert the matrix to a vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naes_coefs_z <-\n  aes_coefs |>\n  group_by(class) |>\n  mutate(z_score = as.vector(scale(estimate))) |>\n  ungroup()\n\nslice_sample(aes_coefs_z, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 5\n   class     term     estimate penalty z_score\n   <chr>     <chr>       <dbl>   <dbl>   <dbl>\n 1 Argentina iba             0  0.0215 -0.0857\n 2 Spain     pudo            0  0.0215 -0.0320\n 3 Spain     pará            0  0.0215 -0.0320\n 4 Argentina dicho           0  0.0215 -0.0857\n 5 Argentina sirve           0  0.0215 -0.0857\n 6 Mexico    cuesta          0  0.0215 -0.0787\n 7 Spain     demonios        0  0.0215 -0.0320\n 8 Argentina punto           0  0.0215 -0.0857\n 9 Mexico    también         0  0.0215 -0.0787\n10 Spain     cabeza          0  0.0215 -0.0320\n```\n\n\n:::\n:::\n\n\nFinally, let's visualize the top 25 terms by class. Note that we are using the `reorder_within()` and `scale_x_reordered()` functions from {tidytext} to reorder the terms in such a way that our facets allow for distinct terms on the x-axis for each class. Then the `coord_flip()` function from {ggplot2} is used to flip the axes for easier reading.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naes_coefs_z |>\n  mutate(term = reorder_within(term, z_score, class)) |>\n  slice_max(n = 25, order_by = z_score, by = class) |>\n  ggplot(aes(x = term, y = z_score)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~class, scales = \"free_y\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![Top 25 terms by class](index_files/figure-html/fig-aes-a2-coefs-viz-1.png){#fig-aes-a2-coefs-viz width=672}\n:::\n:::\n\n\nWe can assess the distinct features for each class and also gauge the magnitude of the estimates. We should be cautious, however, as these terms are derived from our model that only performs moderately well.\n\n\n#### Other approaches\n\nAs we have seen, there are many decisions to make when building a predictive model. We have only scratched the surface of the options available. In this section, I will briefly consider some other approaches that may be of interest.\n\nFeatures:\n\nWe used words in this recipe and in the chapter classification task. This is merely in order to keep the focus on the process of building a predictive model. There are many other features that could be used. For example, we could use n-grams, character n-grams, or word embeddings. {textrecipes} provides many options for text preprocessing and feature engineering.\n\nLet's look at how we can derive other linguistic units using {textrecipes}. First, let's set up a simple dataset and base recipe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tibble(\n  outcome = factor(c(\"a\", \"a\", \"b\", \"b\")),\n  date = as.Date(c(\"2020-01-01\", \"2021-06-14\", \"2020-11-05\", \"2023-12-25\")),\n  text = c(\n    \"This is a fantastic sentence.\",\n    \"This is another great sentence.\",\n    \"This is a third, boring sentence.\",\n    \"This is a fourth and final sentence.\"\n  )\n)\n\nbase_rec <- recipe(outcome ~ text, data = df)\n```\n:::\n\n\nNow, say instead of words, we were interested in deriving word $n$-grams as our terms. We again use the `step_tokenize()` function in our recipe. This time, however, we add a value to the `token = ` argument. In this case, we will use \"ngrams\". {textrecipes} uses the tokenization engine from {tokenizers}, so the types of tokenization available are the same as those available (see `help(tokenizers)` for more information).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_rec |>\n  step_tokenize(\n    text,\n    token = \"ngrams\", # word n-grams\n  ) |>\n  show_tokens(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] \"this is a\"            \"is a fantastic\"       \"a fantastic sentence\"\n\n[[2]]\n[1] \"this is another\"        \"is another great\"       \"another great sentence\"\n\n[[3]]\n[1] \"this is a\"             \"is a third\"            \"a third boring\"       \n[4] \"third boring sentence\"\n\n[[4]]\n[1] \"this is a\"          \"is a fourth\"        \"a fourth and\"      \n[4] \"fourth and final\"   \"and final sentence\"\n```\n\n\n:::\n:::\n\n\nBy default `tokens = \"ngrams\"` produces trigrams.\n\nAnother option is to use character n-grams. This is useful when we want to capture information about the morphology of the words. For character n-grams, we can use \"character_shingle\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_rec |>\n  step_tokenize(\n    text,\n    token = \"character_shingle\" # character n-grams\n  ) |>\n  show_tokens(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afa\" \"fan\" \"ant\" \"nta\" \"tas\" \"ast\"\n[13] \"sti\" \"tic\" \"ics\" \"cse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[2]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"san\" \"ano\" \"not\" \"oth\" \"the\" \"her\" \"erg\"\n[13] \"rgr\" \"gre\" \"rea\" \"eat\" \"ats\" \"tse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[3]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"sat\" \"ath\" \"thi\" \"hir\" \"ird\" \"rdb\" \"dbo\"\n[13] \"bor\" \"ori\" \"rin\" \"ing\" \"ngs\" \"gse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\"\n\n[[4]]\n [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afo\" \"fou\" \"our\" \"urt\" \"rth\" \"tha\"\n[13] \"han\" \"and\" \"ndf\" \"dfi\" \"fin\" \"ina\" \"nal\" \"als\" \"lse\" \"sen\" \"ent\" \"nte\"\n[25] \"ten\" \"enc\" \"nce\"\n```\n\n\n:::\n:::\n\n\nBy default `tokens = \"character_shingle\"` also produces trigrams.\n\nNow, say we want to change the number of words in each n-gram or character n-gram. We can do this using the `options = ` argument. This is where we pass tokenizer-specific options. For example, to change the number of words in each n-gram, we can use the `n = ` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_rec |>\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2) # word bigrams\n  ) |>\n  show_tokens(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] \"this is\"            \"is a\"               \"a fantastic\"       \n[4] \"fantastic sentence\"\n\n[[2]]\n[1] \"this is\"        \"is another\"     \"another great\"  \"great sentence\"\n\n[[3]]\n[1] \"this is\"         \"is a\"            \"a third\"         \"third boring\"   \n[5] \"boring sentence\"\n\n[[4]]\n[1] \"this is\"        \"is a\"           \"a fourth\"       \"fourth and\"    \n[5] \"and final\"      \"final sentence\"\n```\n\n\n:::\n:::\n\n\nIf you would like to calculate multiple $n$-gram windows, you can pass the `n_min = ` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_rec |>\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2, n_min = 1) # word unigrams and bigrams\n  ) |>\n  show_tokens(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] \"this\"               \"this is\"            \"is\"                \n[4] \"is a\"               \"a\"                  \"a fantastic\"       \n[7] \"fantastic\"          \"fantastic sentence\" \"sentence\"          \n\n[[2]]\n[1] \"this\"           \"this is\"        \"is\"             \"is another\"    \n[5] \"another\"        \"another great\"  \"great\"          \"great sentence\"\n[9] \"sentence\"      \n\n[[3]]\n [1] \"this\"            \"this is\"         \"is\"              \"is a\"           \n [5] \"a\"               \"a third\"         \"third\"           \"third boring\"   \n [9] \"boring\"          \"boring sentence\" \"sentence\"       \n\n[[4]]\n [1] \"this\"           \"this is\"        \"is\"             \"is a\"          \n [5] \"a\"              \"a fourth\"       \"fourth\"         \"fourth and\"    \n [9] \"and\"            \"and final\"      \"final\"          \"final sentence\"\n[13] \"sentence\"      \n```\n\n\n:::\n:::\n\n\nNames and values of the arguments that `options = ` will take will depend on the type of tokenization specified.\n\nWe could also use metadata, such as the year the text was written, the author, the genre, *etc*. In these cases, will will update our base recipe to include the metadata as predictors and then we can use the necessary preprocessing steps to prepare the metadata for modeling using functions from the `recipes()` package (*i.e.*, `step_normalize(), step_dummy(), *etc.*).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_rec <- recipe(outcome ~ date + text, data = df) # add date\n\nbase_rec |>\n  step_tokenize(text) |>\n  step_date(date, features = c(\"year\")) |> # extract the year\n  prep() |>\n  juice()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  date             text outcome date_year\n  <date>      <tknlist> <fct>       <int>\n1 2020-01-01 [5 tokens] a            2020\n2 2021-06-14 [5 tokens] a            2021\n3 2020-11-05 [6 tokens] b            2020\n4 2023-12-25 [7 tokens] b            2023\n```\n\n\n:::\n:::\n\n\nWe could also use other features derived from the text, such as word length, syntactic complexity, sentiment, readability, etc. A number of stylistic features are available using the `step_textfeature()` function, some 26 (see `?count_functions`). However, it is also possible to derive your own features working with the original dataset and then adding the features\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <-\n  df |>\n  left_join(\n    # Calculate word count and average word length\n    df |>\n      unnest_tokens(word, text, drop = FALSE) |>\n      group_by(text) |>\n      summarize(\n        word_count = n(),\n        avg_word_length = mean(nchar(word))\n      )\n  )\n\nrecipe(\n  outcome ~ ., # use all variables\n  data = df\n) |>\n  step_tokenize(text) |>\n  step_tf(text) |>\n  prep() |>\n  bake(new_data = NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 16\n  date       word_count avg_word_length outcome tf_text_a tf_text_and\n  <date>          <int>           <dbl> <fct>       <int>       <int>\n1 2020-01-01          5            4.8  a               1           0\n2 2021-06-14          5            5.2  a               0           0\n3 2020-11-05          6            4.33 b               1           0\n4 2023-12-25          7            4.14 b               1           1\n# ℹ 10 more variables: tf_text_another <int>, tf_text_boring <int>,\n#   tf_text_fantastic <int>, tf_text_final <int>, tf_text_fourth <int>,\n#   tf_text_great <int>, tf_text_is <int>, tf_text_sentence <int>,\n#   tf_text_third <int>, tf_text_this <int>\n```\n\n\n:::\n:::\n\n\nModels:\n\nA big advantage to using the {tidymodels} approach to modeling is that it allows us to easily try different models. We have used a multinomial logistic regression model in this recipe, but we could also try other models, such as a random forest model, a support vector machine, or a neural network. We can do this by simply changing the model specification in our workflow.\n\nFor example, we could use a random forest model. We would first need to update our model specification to use the `rand_forest()` function from {parsnip} to create a random forest model. We would also need to update the engine to use {ranger}, which is a fast implementation of random forest models. Finally, we would need to update the mode to \"classification\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a model specification\naes_spec <-\n  # Random Forest\n  rand_forest(\n    mtry = 10,\n    trees = 1000\n  ) |>\n  set_engine(\"ranger\") |> # use the ranger engine\n  set_mode(\"classification\")\n```\n:::\n\n\nIt is important to understand that different models have different hyperparameters. As we say with the `logistic_reg()` and `multinom_reg()` models, we can tune the `penalty` hyperparameter. However, this is not the case for all models. For example, the `rand_forest()` model does not have a `penalty` hyperparameter. Instead, it has a `mtry` hyperparameter, which is the number of variables to consider at each split. We can tune this hyperparameter in the same way that we tuned the `penalty` hyperparameter using `tune()`, `grid_regular()`, and `tune_grid()`.\n\nOther models to consider for text classification include Naive Bayes, Support Vector Machines, and Neural Networks. The {tidymodels} framework supports all of these models.\n\nA last point to consider is whether we will want to be able to interpret the features that drive the model's performance. If so, we will want to use a model that allows us to interpret the features. For example, we could use a linear model, such as a logistic regression model, or a tree-based model, such as a random forest model. However, we would not be able to interpret the features of a neural network model.\n\nFurthermore, the methods we use to interpret the features will depend on the model. For example, we can interpret the features of a linear model by looking at the coefficients. However, we cannot interpret the features of a random forest model in the same way. Instead, we can use the `vip()` function from {vip} to visualize the importance of the features.\n\n## Summary\n\nIn this recipe, we've covered the foundational skills needed to construct a predictive (classification) model using the tidymodels framework. We examined the key steps in predictive modeling: identifying data, dividing it into training and test sets, preprocessing, iterative model training, and result interpretation.\n\nWe used a dataset of Spanish texts from three different varieties to demonstrate the process iterating over two approaches. In the first approach, we used a multinomial logistic regression model with TF-IDF features. In the second approach, we tuned the hyperparameters of the model and the preprocessing steps to improve the model's performance. We also touched upon alternative methods, like incorporating other features such as n-grams and experimenting with other models such as random forests, which may prove useful in text classification tasks.\n\nWith the matierals in this chapter you should now have an understanding of how to build and understand a text classification model in R, equipped with insights to further develop your predictive analysis projects.\n\n## Check your understanding\n\n1. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select> There are two basic types of prediction models: regression and classification.\n2. What is the purpose of splitting data into training and testing sets? <select class='webex-select'><option value='blank'></option><option value=''>To make computation faster</option><option value='answer'>To avoid overfitting the model</option><option value=''>To decrease the size of the dataset</option><option value=''>To make the model simpler</option></select>\n3. What is the purpose of cross-validation? <select class='webex-select'><option value='blank'></option><option value=''>To make computation faster</option><option value=''>To avoid overfitting the model</option><option value='answer'>To evaluate the model's performance</option><option value=''>To make the model simpler</option></select>\n4. Which of the following models would not be appropriate for a classification task? <select class='webex-select'><option value='blank'></option><option value=''>Logistic regression</option><option value=''>Random forest</option><option value=''>Support vector machine</option><option value='answer'>Linear regression</option></select>\n5. Iterative improvement in modeling involves: <select class='webex-select'><option value='blank'></option><option value=''>Changing the model</option><option value=''>Changing the hyperparameters</option><option value=''>Changing the preprocessing steps</option><option value='answer'>All of the above</option></select>\n6. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> Feature importance measures are uniform across models.\n\n## Lab preparation\n\nIn preparation for [Lab 9](https://github.com/qtalr/lab-09), review and ensure that you are familiar with the following concepts:\n\n- Building feature engineering pipelines with {recipes}\n- Building model specifications with {parsnip}\n- Iterative model training, evaluation, and improvement with {workflows}, {tune}, and {yardstick}\n\nIn this lab, you will have an opportunity to apply these concepts to a new dataset and classification task. You should consider the dataset and the task in be performed in the lab and think about how you might approach the task from a feature engineering and model selection perspective. You will be asked to submit you code and a brief reflection on your approach and the results.\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}