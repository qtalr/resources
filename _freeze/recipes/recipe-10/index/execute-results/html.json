{
  "hash": "7be9f4881e6829de69ae5b1fb4534f3b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"10. Building inference models\"\nsubtitle: \"Simulation-based Null Hypothesis Testing\"\ndescription: |\n  In this recipe, we will explore statistical modeling and data analysis with using a practical research hypothesis in the area of Second Language Acquisition and Teaching. We'll use {infer} to understand inference-based models. You'll learn how to work with key variables, examine data distributions, and employ statistical methods to test hypotheses about their relationships. Our discussion will also involve improving our computing skills through practical exercises in data manipulation, visualization, and statistical analysis. This will provide you with the necessary tools to prepare, conduct, and interpret complex datasets and analyses.\ncategories: [analysis]\n---\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Skills**\n\n- identify and map the hypothesis statement to the appropriate response and explanatory variables\n- employ simulation-based methods for statistical inference\n- interpret and evaluate the results of inferential models\n:::\n\nStatistical inference is the most structured approach to data analysis. It is the process of using data to draw conclusions about a population, therefore the underlying data and the process to conduct the analysis must be rigorous and exploration is limited and iteration is avoided. The workflow for building inference-based models can be seen in @tbl-ida-workflow.\n\n| Step | Name | Description |\n|:-----|:-----|:------------|\n| 1 | Identify | Identify and map the hypothesis statement to the appropriate response and explanatory variables |\n| 2 | Inspect | Assess the distribution of the variable(s) with the appropriate descriptive statistics and visualizations. |\n| 3 | Interrogate | Apply the appropriate statistical procedure to the dataset. |\n| 4 | Interpret | Review the statistical results and interpret them in the context of the hypothesis. |\n\n: The inference-based modeling workflow {#tbl-ida-workflow tbl-colwidths=\"[10, 10, 80]\"}\n\nBefore we begin, let's load the packages we will use in this recipe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) # for reading data\nlibrary(kableExtra) # for table formatting\nlibrary(dplyr) # for data wrangling\nlibrary(skimr) # for data summaries\nlibrary(janitor) # for data cleaning/ tablulations\nlibrary(ggplot2) # for data visualizations\nlibrary(infer) # for statistical inference\n```\n:::\n\n\n## Concepts and strategies\n\n### Orientation\n\nIn the area of Second Language Acquisition and Teaching, the relationship between learner proficiency and particular linguistic variables has been a topic of interest for decades. In this section, we will explore the relationship between placement scores and lexical features of learner sample writing. The goal is to determine the extent to which a simplified set of lexical features can be employed as a diagnostic tool for assessing overall learner proficiency.\n\nThe background here comes from a study by @Crossley2010, who investigated the relationship between lexical features and learner proficiency. The authors used a corpus of learner writing samples which were assessed and scored by human raters. Then a set of lexical features were extracted from the writing samples and subjected to a series of statistical analyses. The results suggested three key variables which were highly correlated with the human ratings of learner proficiency: lexical diversity, word hypernymy values, and content word frequency.\n\nIn this sample study, I will suggest that the variables lexical diversity and content word frequency actually represent a single underlying construct, which I will call \"lexical sophistication\". Lexical diversity aims to gauge the range of vocabulary used by the learner. In the context of L2 writing, it is less likely that demonstrated range of unique vocabulary is what constitutes proficiency --as a lexically diverse text in which the writer uses primarily spoken register vocabulary is unlikely to be considered in academic writing contexts. Instead, rather it is more likely that the ability to use more sophisticated vocabulary is what is being measured. On the other hand, content word frequency purports to gauge the degree to which more infrequent words are used in L2 writing. However, I would argue that this is also a measure of lexical sophistication. In other words, the ability to use more sophisticated vocabulary inherently taps into the use of more infrequent words.\n\nThe goal of this study is to determine the extent to which this construct can be used as a proxy for lexical diversity and content word frequency, and thus as a diagnostic tool for assessing learner proficiency. The research statement is as follows:\n\n> The lexical sophistication of learner writing is positively correlated with learner proficiency.\n\nOperationalizing this statement requires a few steps. First, we need to identify the variables which will be used to represent the construct of lexical sophistication.\n\nIn addition, ideally the variables used to represent lexical sophistication should be easy to extract from learner writing samples, if this is to be used as a diagnostic tool. Plausible linguistic variables to consider in this study are the following:\n\n- Number of syllables per word\n- Number of morphemes per word\n\nIn addtion, to these variables, we will also consider word frequency estimates to maintain consistency with the original study. I will also consider the number of characters per word, although not strictly linguistic in nature, this is a variable which is easy to extract from learner writing samples and is likely to be correlated with the number of syllables and/ or morphemes per word.\n\nSecond, we need to identify the variables which will be used to represent learner proficiency. In this study, we will use the placement scores of the learners as a proxy for proficiency. The placement scores are based on the results of a placement test which was administered to the learners prior to the writing samples being collected.\n\nThe hypothesis statement is as follows:\n\n> Learner proficiency as measured by placement scores is positively correlated with lexical sophistication as measured by the number of syllables per word, number of morphemes per word, and word frequency estimates.\n\n### Analysis\n\nThe dataset used in this study is is transformed version of the Pittsburgh English Language Institute Corpus (PELIC). Writing samples and placement scores were extracted from the corpus for learners in the English for Academic Purposes (EAP) program. The tokenized writing samples were filtered for content words (i.e. nouns, verbs, adjectives). Subsequently, lexical features were joined from the English Lexicon Project (ELP) database by word form.\n\nThe data dictionary for the dataset is as follows:\n\n\n::: {#tbl-data-dictionary .cell tbl-cap='Data dictionary for the transformed PELIC/ ELP dataset'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:left;\"> Name </th>\n   <th style=\"text-align:left;\"> Type </th>\n   <th style=\"text-align:left;\"> Description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> id </td>\n   <td style=\"text-align:left;\"> ID </td>\n   <td style=\"text-align:left;\"> categorical </td>\n   <td style=\"text-align:left;\"> Unique identifier for each learner </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> placement </td>\n   <td style=\"text-align:left;\"> Placement </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> Numerical value indicating score on the placement test for each learner (0-100) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> chars </td>\n   <td style=\"text-align:left;\"> Characters </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> Mean number of characters per word in the text sample </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sylls </td>\n   <td style=\"text-align:left;\"> Syllables </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> Mean number of syllables per word in the text sample </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> morphs </td>\n   <td style=\"text-align:left;\"> Morphemes </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> Mean number of morphemes per word in the text sample </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> freq </td>\n   <td style=\"text-align:left;\"> Frequency </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> Mean frequency of occurrence per word in the text sample </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n#### Identify\n\nIn @tbl-data-dictionary, we can see the variables which will be used to represent lexical sophistication and learner proficiency. The explanatory variables which will be used to represent lexical sophistication are `chars`, `sylls`, `morphs`, and `freq`. The response variable which will be used to represent learner proficiency is `placement`.\n\nLet's read in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npelic <- read_csv(\"data/derived/pelic/pelic_transformed.csv\")\n\npelic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 276 × 6\n   id    placement chars sylls morphs     freq\n   <chr>     <dbl> <dbl> <dbl>  <dbl>    <dbl>\n 1 aa0          80  6.01  1.90   1.66  498076.\n 2 aa1          66  5.96  1.85   1.53  657678.\n 3 ab8          40  5.81  1.81   1.52  375383.\n 4 ac3          65  5.98  1.81   1.52  419318.\n 5 ad4          72  6.26  1.93   1.69  345658.\n 6 ad9          56  5.82  1.72   1.57  375747.\n 7 ae0          61  4.99  1.53   1.35 1087512.\n 8 ae2          81  6.32  1.98   1.69  403618.\n 9 ae4          69  5.23  1.68   1.45  788536.\n10 ae9          71  5.64  1.75   1.47  586251.\n# ℹ 266 more rows\n```\n\n\n:::\n:::\n\n\nThe dataset contains 276 observations for our variables of interest. Let's now map the hypothesis statement to the appropriate response and explanatory variables.\n\n```r\nplacement ~ chars + sylls + morphs + freq\n```\n\nWe will specify the relationship between the response and explanatory variables using the formula notation in the interrogation phase. The explanatory variables will be used in an additive model using multiple linear regression.\n\n#### Inspect\n\nFirst step is to get a statistical overview of the dataset. We can use the `skim()` function from {skimr} to get a statistical summary of the dataset.\n\n```r\npelic |>\n  skim()\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n── Data Summary ────────────────────────\n                           Values\nName                       pelic \nNumber of rows             276   \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  character                1     \n  numeric                  5     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 id                    0             1   3   3     0      276          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate      mean         sd        p0       p25\n1 placement             0             1     59.2      12.7       32        48   \n2 chars                 0             1      5.79      0.473      3.75      5.50\n3 sylls                 0             1      1.79      0.169      1.25      1.68\n4 morphs                0             1      1.53      0.127      1.12      1.44\n5 freq                  0             1 556751.   160046.    165138.   453424.  \n        p50       p75       p100 hist \n1     59        69         88    ▃▆▇▆▃\n2      5.79      6.08       8.07 ▁▂▇▁▁\n3      1.78      1.88       2.66 ▁▇▆▁▁\n4      1.53      1.62       1.97 ▁▅▇▃▁\n5 544026.   631784.   1379696.   ▂▇▂▁▁\n```\n\n\n:::\n:::\n\n\nWe are looking for any missing values or other anomalies in the dataset. We can see that there are no missing values in the dataset.\n\nLet's now inspect the variables of interest to get a sense of the distributions of the variables. There are really three things we are looking to find out:\n\n- What is the distribution of the variables individually?\n- What is the relationship between the explanatory variables and the response variable?\n- What is the relationship (if any) between the variables?\n\nSince we are working with numeric variables, we would create a histogram or density plot for each individual variable. Then a scatterplot for the relationship between the explanatory variables and the response variable. Finally, a correlation matrix for the relationship between the variables.\n\nI would like to introduce a shortcut to create all three types of visualizations in one plot. The `ggpairs()` function from {GGally} creates a matrix of plots for all combinations of variables in the dataset. We can specify the type of plot to create for each combination of variables. Let's create a matrix of plots for the variables of interest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the GGally package\nlibrary(GGally)\n\n# Create the matrix of plots\npelic |>\n  select(placement, chars, sylls, morphs, freq) |>\n  ggpairs()\n```\n\n::: {.cell-output-display}\n![Matrix of plots for the variables of interest](index_files/figure-html/fig-ggpairs-1.png){#fig-ggpairs width=576}\n:::\n:::\n\n\nThe plot in @fig-ggpairs shows a lot of information. Let's break it down.\n\nThe diagonal plots show the distribution of each variable. In these plots we are looking for any outliers or skewness in the distributions. We can see that on the whole the distributions are fairly normal. Our simulation-based inference methods do not require the data to be normally distributed, but highly skewed distributions can compress the range of the data and thus affect the results.\n\nThe lower triangle plots show the scatterplots for the relationship between the explanatory variables and the response variable and in the upper triangle we see the correlation coefficients for the relationship between the variables.\n\nLet's focus on the upper triangle, specifically the first row of statistics reported. The first row shows the correlation between the response variable and the explanatory variables. All variables show a positive correlation with the response variable, except for frequency, which is negative. This is what we predicted in the hypothesis statement. The strengths of these correlations are fairly weak, especially for frequency. We will let our model determine the strength of the relationship and whether it is statistically significant, but it's worth noting.\n\nNow let's focus on the upper triangle for the second, third, and fourth row of correlation measures. These show the correlation between the explanatory variables themselves. The variables `chars`, `sylls`, and `morphs` are highly intercorrelated. This is not surprising since the number of characters in a word is related to the number of syllables and morphemes. Yet if we consider all three in our model we run the risk of multicollinearity. So we need to decide which of these variables to include in our model.\n\nOne way to do this is assess the theoretical importance of each variable. In this case, we might consider the number of syllables and morphemes to be more important than the number of characters. Another perspective is to see which of the remaining two variables is least correlated with `freq` with the hopes of capturing non-overlapping variance in the response variable. In this case, `sylls` is less correlated with `freq` than `morphs`. So we will include `sylls` and `freq` in our model.\n\nIf we were to use the response variable `placement` as our reason for selecting the explanatory variables, we would be committing the logical fallacy of circular reasoning --in essence, tailoring the model to the data.\n\nSo lets select `sylls` and `freq` as our explanatory variables for our final model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npelic <-\n  pelic |>\n  select(placement, sylls, freq)\n```\n:::\n\n\nA last thing to consider before we enter into the model building phase, is to address the fact that the `sylls` and `freq` variables are on very distinct scales. In regression modeling, this can cause problems both for fitting the model and for interpreting the model.\n\nWe can address this by normalizing the variables. This will transform the variables to have a mean of zero and a standard deviation of one. This is known as a z-score. Z-score normalization does not change the distribution nor the relationship between the variables, but it does make the variables more comparable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to get z-score\nget_z_score <- function(x) {\n  (x - mean(x)) / sd(x)\n}\n\n# Normalize the variables\npelic <-\n  pelic |>\n  mutate(\n    sylls_z = get_z_score(sylls),\n    freq_z = get_z_score(freq)\n  )\n```\n:::\n\n\n#### Interrogate\n\nNow we will analyze the data using {infer}. {infer} is a framework for conducting statistical inference using simulation-based methods. The steps for using {infer} are as follows:\n\n1. Specify the model relationships\n2. Calculate the model statistics (fit)\n3. Simulate the null distribution\n4. Calculate the $p$-value\n5. Simulate model statistics (fit)\n6. Calculate the confidence interval\n7. Calculate the effect size\n\nStep 1. We will use the `specify()` function to add the formula notation to specify the relationship between the response and explanatory variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify the model\npelic_spec <-\n  pelic |>\n  specify(placement ~ sylls_z + freq_z)\n\npelic_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse: placement (numeric)\nExplanatory: sylls_z (numeric), freq_z (numeric)\n# A tibble: 276 × 3\n   placement sylls_z freq_z\n       <dbl>   <dbl>  <dbl>\n 1        80   0.691 -0.367\n 2        66   0.381  0.631\n 3        40   0.157 -1.13 \n 4        65   0.123 -0.859\n 5        72   0.822 -1.32 \n 6        56  -0.378 -1.13 \n 7        61  -1.49   3.32 \n 8        81   1.15  -0.957\n 9        69  -0.624  1.45 \n10        71  -0.189  0.184\n# ℹ 266 more rows\n```\n\n\n:::\n:::\n\n\nStep 2. We will use the `fit()` function to calculate the model statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the model statistics\npelic_obs_fit <-\n  pelic_spec |>\n  fit()\n\npelic_obs_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept   59.2  \n2 sylls_z      2.91 \n3 freq_z       0.617\n```\n\n\n:::\n:::\n\n\nWe now have the calculated model statistics. The model statistics in a linear regression model are the intercept and the slopes for the explanatory variables. The intercept is the predicted value of the response variable when all explanatory variables are zero. The slopes are the predicted change in the response variable for a one unit change in the explanatory variable.\n\nStep 3. At this point we need to use `hypothesize()` and to use 'independence' as our null hypothesis. The `hypothesize()` function takes the model object `pelic_spec` and the type of null hypothesis we are assuming, in this case 'independence'. Then we will pass this to `generate()` to create a null distribution. The `generate()` function takes the model object, the number of simulations, and the type of simulation as arguments. The type of simulation for models with multiple independent variables is `permute`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the null distribution\npelic_null_fit <-\n  pelic_spec |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 10000, type = \"permute\") |>\n  fit()\n```\n:::\n\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nThe larger the number of `reps` the more reliable the results will be. Note, that the `generate()` function will take a while to run for large reps and depending on your computer's memory, it may crash. If this happens, try reducing the number of reps.\n:::\n\nStep 4. The `get_p_value()` function takes the model object and the null distribution object as arguments. We choose a \"two-sided\" test because we are interested in whether the explanatory variables are positively or negatively correlated with the response variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the p-value\npelic_p_value <-\n  get_pvalue(\n    x = pelic_null_fit,\n    obs_stat = pelic_obs_fit,\n    direction = \"two-sided\"\n  )\n\npelic_p_value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  term      p_value\n  <chr>       <dbl>\n1 freq_z     0.445 \n2 intercept  1     \n3 sylls_z    0.0012\n```\n\n\n:::\n:::\n\n\nOK. So results suggest that syllables is a significant predictor of placement scores, but frequency is not. Let's now move to the interpretation phase.\n\n#### Interpret\n\nOur signficant $p$-value suggests that the explanatory variable `sylls_z` is a significant predictor of the response variable `placement`. However, a $p$-value is an arbitrary threshold. We need to consider the likelihood of the observed test statistic is different from zero. We can do this by calculating the confidence interval.\n\nStep 5. We will need to simulate the model statistics again, but this time we will use the `generate()` function with the type \"bootstrap\" to simulate the model statistics with replacement giving us a distribution of model statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the bootstrap distribution\npelic_bootstrap_fit <-\n  pelic_spec |>\n  generate(reps = 10000, type = \"bootstrap\") |>\n  fit()\n```\n:::\n\n\nStep 6. Calculate the confidence interval.\n\nWe get the confidence interval by using the `get_ci()` function. The `get_ci()` function takes the bootstrapped model object and the observed statistics as the estimates to calculate the confidence interval. We will use the default confidence level of 95%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the confidence interval\npelic_obs_ci <-\n  pelic_bootstrap_fit |>\n  get_ci(point_estimate = pelic_obs_fit)\n\npelic_obs_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 freq_z      -0.906     2.21\n2 intercept   57.7      60.7 \n3 sylls_z      1.26      4.81\n```\n\n\n:::\n:::\n\n\nThe confidence interval underscores the results of the $p$-value. The confidence interval does not include zero for syllables, but does for frequency. Comparing the confidence interval to the actual observed statistics, we can see how close the observed statistics are to the confidence interval margins.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npelic_obs_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept   59.2  \n2 sylls_z      2.91 \n3 freq_z       0.617\n```\n\n\n:::\n:::\n\n\nThe observed statistic for `sylls_z` is nicely within the confidence interval.\n\nStep 7. Calculate effect size\n\nWhether a explanatory variable is significant or not is one thing, but gauging the magnitude of the relationship is another. Looking at the observed fit of the model, we can see that the coefficient for syllables is 2.91. But what does this mean? It means the the model predicts that for an increase of our syllable measure by one unit, the placement score will increase by 2.91 units.\n\nNow the placement score is still in the original scale, so this means that we are dealing with 2.91 score points. For the syllables, however, we have standardized the variable, so we are dealing with standard deviations --less straightforward to interpret. The good news is we can back-transform the standardized variable to the original scale by multiplying by the standard deviation and adding the mean from the original variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Back-transform the standardized variable\n(2.91 * sd(pelic$sylls)) + mean(pelic$sylls)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.28\n```\n\n\n:::\n:::\n\n\nTherefore a mean increase of 1 syllable is associated with a 2.28 point increase in placement score.\n\nAnother helpful way to interpret the results is to consider how much of the variance in the dependent variable is explained by the independent variable. $R^2$ is a typical measure of effect size. To calculate $R^2$ we need the correlation coefficient ($r$), which can be calculated by dividing the coefficient by the standard deviation of the response variable. Then we square the correlation coefficient to get $R^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation coefficient and R^2\npelic_obs_fit |>\n  mutate(\n    r = estimate / sd(pelic_spec$placement),\n    r2 = r^2\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  term      estimate      r       r2\n  <chr>        <dbl>  <dbl>    <dbl>\n1 intercept   59.2   4.64   21.6    \n2 sylls_z      2.91  0.229   0.0524 \n3 freq_z       0.617 0.0485  0.00235\n```\n\n\n:::\n:::\n\n\nWe can see her that the $R^2$ is 0.052, which means that the number of syllables explains 5.2% of the variance in placement scores. This means that syllables don't explain all that much in the variation in placement scores. But it is a significant relationship.\n\n## Summary\n\nIn summary, this programming tutorial has provided a comprehensive guide to building inference-based models using the infer package in R. We have explored the concept of lexical sophistication as a diagnostic tool for assessing learner proficiency in Second Language Acquisition and Teaching. Through a detailed workflow involving identifying variables, inspecting data distributions, interrogating the dataset with statistical procedures, and interpreting results, we have demonstrated the importance of careful variable selection and normalization in regression modeling. Our analysis revealed that while syllables per word significantly predict learner placement scores, content word frequency does not. The effect size, indicated by an $R^2$ of 0.052, suggests that lexical sophistication, as measured by syllable count, accounts for a small but significant portion of the variance in learner proficiency. This study underscores the nuanced relationship between linguistic features and language learning outcomes, and highlights the potential of statistical modeling in educational research.\n\n## Check your understanding\n\n<!-- [ ] return to edit these comprehension questions -->\n\n1. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select> Simulation-based inference is a method that can be used to approximate traditional inferential statistics without the need for theoretical assumptions about the data.\n2. What is the primary goal of using {infer} for simulation-based inference? <select class='webex-select'><option value='blank'></option><option value=''>To visualize data distributions</option><option value=''>To perform regression analysis</option><option value='answer'>To create resampling simulations for hypothesis testing and constructing confidence intervals</option><option value=''>To fit predictive models</option></select>\n3. How does simulation-based inference contribute to understanding population parameters? <select class='webex-select'><option value='blank'></option><option value='answer'>It allows us to make probabilistic statements about population parameters based on simulated sampling distributions</option><option value=''>It provides exact calculations of population parameters</option><option value=''>It eliminates the need for sample data</option><option value=''>It guarantees more accurate results than traditional methods</option></select>\n4. Which of the following is not a typical step in simulation-based inference? <select class='webex-select'><option value='blank'></option><option value=''>Defining a model of interest</option><option value=''>Generating resamples or permutations</option><option value=''>Calculating summary statistics</option><option value='answer'>Training a machine learning classifier</option></select>\n5. In the context of Second Language Acquisition research, why might simulation-based inference be particularly useful? <select class='webex-select'><option value='blank'></option><option value=''>Because it can handle very large datasets</option><option value='answer'>Because it allows for the exploration of hypotheses under different assumptions and conditions</option><option value=''>Because it simplifies the data collection process</option><option value=''>Because it directly measures the language ability of individuals</option></select>\n6. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> The results from simulation-based inference are always deterministic and do not vary between simulations.\n\n## Lab preparation\n\nIn preparation for [Lab 10](https://github.com/qtalr/lab-09), ensure that you are comfortable with the following key concepts related to simulation-based inference:\n\n- Understanding the principles of statistical inference and how simulations can be used to approximate traditional inferential statistics.\n\n- Utilizing {infer} in R to create resampling simulations for hypothesis testing and constructing confidence intervals.\n\n- Interpreting the results of simulation-based inference focusing on what the simulated distributions imply about the population parameters.\n\nIn this lab, you will be challenged to apply these core ideas to a new dataset of your choosing. Reflect on the nature of the data and the hypotheses you might test using simulation-based methods. Consider how you would design your simulation study to address a particular hypothesis. You will be expected to submit your code along with a concise reflection on your methodology and the insights gained from your analysis.\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}