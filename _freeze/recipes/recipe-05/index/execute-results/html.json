{
  "hash": "00948b0124f74b43644263e552dc6368",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"05. Collecting and documenting data\"\nsubtitle: \"Acquiring datasets with the Project Gutenberg API\"\ndescription: |\n  At this point, we now have a strong undertanding of the foundations of programming in R and the data science workflow. Previous lessons, recipes, and labs focused on developing these skills while the chapters aimed to provide a conceptual framework for understanding the steps in the data science workflow. We now turn to applying our conceptual knowledge and our technical skills to accomplish the tasks of the data science workflow, starting with data acquisition.\ncategories: [preparation]\n---\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Skills**\n\n- Finding data sources\n- Data collection strategies\n- Data documentation\n:::\n\n## Concepts and strategies\n\n### Finding data sources\n\nTo find data sources, it is best to have a research question in mind. This will help to narrow the search for data sources. However, finding data sources can also be a good way to generate research questions. In either case, it takes some sleuthing to find data sources that will work for your research question. In addition, to the data source itself, you will also need to consider the permissions and licensing of the data source. It is best to consider these early in the process to avoid surprises later. Finally, you will also need to consider the data format and how it will be used in the analysis. It can be the case that a data source seems ideal, but the data format is not conducive to the analysis you would like to do.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nConsult the [Identifying data and data sources](/guides/guide-04/) guide for some ideas on where to find data sources.\n:::\n\nIn this recipe, we will consider some hypothetical reseach aimed at exploring potential similarities and differences in the lexical, syntactic, and/ or stylistic features between American and English literature during the mid 19th century.\n\n::: {.callout .halfsize}\n**{{< fa medal >}} Dive deeper**\n\nIf you are interested in understanding a literary analysis perspective to text analysis, I highly recommend [Matthew Jockers' book *Text Analysis with R for Students of Literature*](https://link.springer.com/book/10.1007/978-3-319-03164-4) [@Jockers2014]. This book is a great resource for understanding how to apply text analysis to literary analysis.\n:::\n\nProject Gutenberg is a great source of data for this research question. Project Gutenberg is a volunteer effort to digitize and archive cultural works. The great majority of the works in the Project Gutenberg database are in the public domain in the United States. This means that the works can be freely used and shared.\n\nFurthermore, {gutenbergr} provides an API for accessing the Project Gutenberg database. This means that we can use R to access the Project Gutenberg database and download the text and metadata for the works we are interested in. {gutenbergr} also provides a number of data frames that can help us to identify the works we are interested in.\n\n### Data collection strategy\n\nLet's now turn to the data collection strategy. There are a number of data collection strategies that can be used to acquire data for a text analysis project. In the chapter, we covered manual and programmatic downloads and APIs. Here we will use an R package which will provide an API for accessing the data source.\n\n::: {.callout .halfsize}\n**`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M4.1 38.2C1.4 34.2 0 29.4 0 24.6C0 11 11 0 24.6 0H133.9c11.2 0 21.7 5.9 27.4 15.5l68.5 114.1c-48.2 6.1-91.3 28.6-123.4 61.9L4.1 38.2zm503.7 0L405.6 191.5c-32.1-33.3-75.2-55.8-123.4-61.9L350.7 15.5C356.5 5.9 366.9 0 378.1 0H487.4C501 0 512 11 512 24.6c0 4.8-1.4 9.6-4.1 13.6zM80 336a176 176 0 1 1 352 0A176 176 0 1 1 80 336zm184.4-94.9c-3.4-7-13.3-7-16.8 0l-22.4 45.4c-1.4 2.8-4 4.7-7 5.1L168 298.9c-7.7 1.1-10.7 10.5-5.2 16l36.3 35.4c2.2 2.2 3.2 5.2 2.7 8.3l-8.6 49.9c-1.3 7.6 6.7 13.5 13.6 9.9l44.8-23.6c2.7-1.4 6-1.4 8.7 0l44.8 23.6c6.9 3.6 14.9-2.2 13.6-9.9l-8.6-49.9c-.5-3 .5-6.1 2.7-8.3l36.3-35.4c5.6-5.4 2.5-14.8-5.2-16l-50.1-7.3c-3-.4-5.7-2.4-7-5.1l-22.4-45.4z\"/></svg>`{=html} Dive deeper**\n\nIf you are interested in learning about another data collection strategy, web scraping, I suggest you look at the [Web scraping with R](guide-5.html) guide.\n:::\n\nWe will load {dplyr}, {readr}, and {gutenbergr} to prepare for the data collection process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(gutenbergr)\n```\n:::\n\n\nThe main workhorse of {gutenbergr} is the `gutenberg_download()`. It's only required argument is the id(s) used by Project Gutenberg to index all of the works in their database. This function will then download the text of the work(s) and return a data frame with the gutenberg id and the text of the work(s).\n\nSo how do we find the gutenberg ids? The manual method is to go to the Project Gutenberg website and search for the work you are interested in. For example, let's say we are interested in the work \"A Tale of Two Cities\" by Charles Dickens. We can search for this work on the Project Gutenberg website and then click on the link to the work. The url for this work is: <https://www.gutenberg.org/ebooks/98>. The gutenberg id is the number at the end of the url, in this case 98.\n\nThis will work for individual works, but why wouldn't we just download the text from the Project Gutenberg website? For the works on Project Gutenberg this would be perfectly fine. We can share the text with others as the license for the works on Project Gutenberg are in the public domain.\n\nHowever, what if are interested in downloading multiple works? As the number of works increases, the time it takes to manually download each work increases. Furthermore, {gutenbergr} provides a number of additional attributes that can be downloaded and organized along side the text. Finally, the results of the `gutenberg_download()` function are returned as a data frame which can be easily manipulated and analyzed in R.\n\nIn our data acquisition plan, we want to collect works from a number of authors. So it will be best to leverge {gutenbergr} to download the works we are interested in. To do this we need to know the gutenberg ids for the works we are interested in.\n\nConvienently, {gutenbergr} also includes a number of data frames that contain meta data for the works in the Project Gutenberg database. These data frames include meta data for works in the Project Gutenberg database (`gutenberg_metadata`), authors (`gutenberg_authors`), and subjects (`gutenberg_subjects`).\n\nLet's take a look at the structure of these data frames.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(gutenberg_metadata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 72,569\nColumns: 8\n$ gutenberg_id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ title               <chr> \"The Declaration of Independence of the United Sta…\n$ author              <chr> \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id <int> 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, …\n$ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf <chr> \"Politics/American Revolutionary War/United States…\n$ rights              <chr> \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(gutenberg_authors)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 23,980\nColumns: 7\n$ gutenberg_author_id <int> 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              <chr> \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               <chr> \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           <int> NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           <int> NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           <chr> \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             <chr> \"U.S.A.\", \"United States President (1861-1865)/Lin…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(gutenberg_subjects)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 231,741\nColumns: 3\n$ gutenberg_id <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type <chr> \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      <chr> \"United States -- History -- Revolution, 1775-1783 -- Sou…\n```\n\n\n:::\n:::\n\n\nFrom this overvew, we can see that there are 72,569 works in the Project Gutenberg database. We can also see that there are 23,980 authors and 231,741 subjects.\n\nAs we dicussed, each work in the Project Gutenberg database has a gutenberg id. The `gutenberg_id` appears in the `gutenberg_metadata` and also in the `gutenberg_subjects` data frame. This common attribute means that a work with a particular gutenberg id can be linked to the subject(s) associated with that work. Another important attribute is is the `gutenberg_author_id` which links the work to the author(s) of that work. Yes, the author name is in the `gutenberg_metadata` data frame, but the `gutenberg_author_id` can be used to link the work to the `gutenberg_authors` data frame which contains additional information about authors.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\n{gutenbergr} is periodically updated. To check to see when each data frame was last updated run:\n\n```r\nattr(gutenberg_metadata, \"date_updated\")\n```\n:::\n\nLet's now describe a few more attributes that will be useful for our data acquisition plan. In the `gutenberg_subjects` data frame, we have `subject_type` and `subject`. The `subject_type` is the type of subject classification system used to classify the work. If you tabulate this column, you will see that there are two types of subject classification systems used: Library of Congress Classification (`lcc`) and Library of Congress Subject Headings (`lcsh`). The `subject` column contains the subject code for the work. For `lsch` the subject code is a descriptive character string and for `lcc` the subject code is an id as a character string that is a combination of letters (and numbers) that the [Library of Congress uses to classify works](https://www.loc.gov/catdir/cpso/lcco/).\n\nFor our data acquistion plan, we will use the `lcc` subject classification system to select works from the Library of Congress Classification for English Literature (PR) and American Literature (PS).\n\nIn the `gutenberg_authors` data frame, we have the `birthdate` and `deathdate` attributes. These attributes will be useful for filtering the authors that lived during the mid 19th century.\n\nWith this overview of {gutenbergr} and the data frames that it contains, we can now begin to develop our data acquisition plan.\n\n1. Select the authors that lived during the mid 19th century from the `gutenberg_authors` data frame.\n2. Select the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the `gutenberg_subjects` data frame.\n3. Select works from `gutenberg_metadata` that are associated with the authors and subjects selected in steps 1 and 2.\n4. Download the text and metadata for the works selected in step 3 using the `gutenberg_download()` function.\n5. Write the data to disk in an appropriate format.\n\n### Data collection\n\nLet's take each of these steps in turn. First, we need to select the authors that lived during the mid 19th century from the `gutenberg_authors` data frame. To do this we will use the `filter()` function. We will pass the `gutenberg_authors` data frame to the `filter()` function and then use the `birthdate` column to select the authors that were born after 1800 and died before 1880 --this year is chosen as the mid 19th century is generally considered to be the period from 1830 to 1870. We will then assign the result to the variable name `authors`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nauthors <-\n  gutenberg_authors |>\n  filter(\n    birthdate > 1800,\n    deathdate < 1880\n  )\n```\n:::\n\n\nThat's it! We now have a data frame with the authors that lived during the mid 19th century, some 787 authors in total. This will span all subjects and languages, so this isn't the final number of authors we will be working with.\n\nThe next step is to select the works from the Library of Congress Classification for English Literature (PR) and American Literature (PS) from the `gutenberg_subjects` data frame. To do this we will use the `filter()` function again. We will pass the `gutenberg_subjects` data frame to the `filter()` function and then use the `subject_type` and `subject` columns to select the works that are associated with the Library of Congress Classification for English Literature (PR) and American Literature (PS). We will then assign the result to the variable name `subjects`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubjects <-\n  gutenberg_subjects |>\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n```\n:::\n\n\nNow, we have a data frame with the subjects that we are interested in. Let's inspect this data frame to see how many works we have for each subject.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubjects |>\n  count(subject)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  subject     n\n  <chr>   <int>\n1 PR       9926\n2 PS      10953\n```\n\n\n:::\n:::\n\n\nThe next step is to subset the `gutenberg_metadata` data frame to select works from the authors and subjects selected in the previous steps. Again, we will use `filter()` to do this. We will pass the `gutenberg_metadata` data frame to the `filter()` function and then use the `gutenberg_author_id` and `gutenberg_id` columns to select the works that are associated with the authors and subjects selected in the previous steps. We will then assign the result to the variable name `works`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nworks <-\n  gutenberg_metadata |>\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n\nworks\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,014 × 8\n   gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n          <int> <chr>    <chr>                <int> <chr>    <chr>              \n 1           33 The Sca… Hawth…                  28 en       \"Harvard Classics/…\n 2           46 A Chris… Dicke…                  37 en       \"Children's Litera…\n 3           71 On the … Thore…                  54 en       \"\"                 \n 4           77 The Hou… Hawth…                  28 en       \"Best Books Ever L…\n 5           98 A Tale … Dicke…                  37 en       \"Historical Fictio…\n 6          205 Walden,… Thore…                  54 en       \"\"                 \n 7          258 Poems b… Gordo…                 145 en       \"\"                 \n 8          271 Black B… Sewel…                 154 en       \"Best Books Ever L…\n 9          292 Beauty … Taylo…                 167 en       \"\"                 \n10          394 Cranford Gaske…                 220 en       \"\"                 \n# ℹ 1,004 more rows\n# ℹ 2 more variables: rights <chr>, has_text <lgl>\n```\n\n\n:::\n:::\n\n\nFiltering the `gutenberg_metadata` data frame by the authors and subjects selected in the previous steps, we now have a data frame with 1,014 works. This is the final number of works we will be working with so we can now download the text and metadata for these works using the `gutenberg_download()` function.\n\nA few things to note about the `gutenberg_download()` function. First, it is vectorized, that is, it can take a single value or multiple values for the argument `gutenberg_id`. This is good as we will be passing a vector of gutenberg ids to the function. A small fraction of the works on Project Gutenberg are not in the public domain and therefore cannot be downloaded, this is documented in the `rights` column. Furthermore, not all of the works have text available, as seen in the `has_text` column. Finally, the `gutenberg_download()` function returns a data frame with the gutenberg id and the text of the work(s) --but we can also select additional attributes to be returned by passing a character vector of the attribute names to the argument `meta_fields`. The column names of the `gutenberg_metadata` data frame contains the available attributes.\n\nWith this in mind, let's do a quick test before we download all of the works. Let's select the first 5 works from the `works` data frame that fit our criteria and then download the text and metadata for these works using the `gutenberg_download()` function. We will then assign the result to the variable name `works_sample`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nworks_sample <-\n  works |>\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |>\n  slice_head(n = 5) |>\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  )\n\nworks_sample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 34,385 × 6\n   gutenberg_id text        title author gutenberg_author_id gutenberg_bookshelf\n          <int> <chr>       <chr> <chr>                <int> <chr>              \n 1           33 \"The Scarl… The … Hawth…                  28 Harvard Classics/M…\n 2           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 3           33 \"by Nathan… The … Hawth…                  28 Harvard Classics/M…\n 4           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 5           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 6           33 \"Contents\"  The … Hawth…                  28 Harvard Classics/M…\n 7           33 \"\"          The … Hawth…                  28 Harvard Classics/M…\n 8           33 \" THE CUST… The … Hawth…                  28 Harvard Classics/M…\n 9           33 \" THE SCAR… The … Hawth…                  28 Harvard Classics/M…\n10           33 \" I. THE P… The … Hawth…                  28 Harvard Classics/M…\n# ℹ 34,375 more rows\n```\n\n\n:::\n:::\n\n\nLet's inspect the `works_sample` data frame. First, from the output we can see that all of our meta data attributes were returned. Second, we can see that the `text` column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded, even blank lines. To make sure that we have the correct number of works, we can use the `count()` function to count the number of works by `gutenberg_id`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nworks_sample |>\n  count(gutenberg_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  gutenberg_id     n\n         <int> <int>\n1           33  8212\n2          258 11050\n3          271  5997\n4          292  9126\n```\n\n\n:::\n:::\n\n\nYes, we have 5 works and we can see how many lines are in each of these works.\n\nWe could now run this code on the entire `works` data frame and then write the data to disk like so:\n\n```{.r}\nworks |>\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |>\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |>\n  write_csv(file = \"data/original/gutenberg/works.csv\")\n```\n\nThis would accomplish the primary goal of our data acquisition plan.\n\nHowever, there is some key functionality that we are missing if we would like to make this code more reproducible-friendly. First, we are not checking to see if the data already exists on disk. If we already have run this code in our script, we likely do not want to run it again. Second, we may want to use this code again with different parameters, for example, we may want to retrieve different subject codes, or different time periods, or other languages.\n\nAll three of these additional features can be accomplished with writing a custom function. Let's take a look at the code we have written so far and see how we can turn this into a custom function.\n\n```{.r}\n# Get authors within years\nauthors <-\n  gutenberg_authors |>\n  filter(\n    birthdate > 1800,\n    deathdate < 1880\n  )\n# Get LCC subjects\nsubjects <-\n  gutenberg_subjects |>\n  filter(\n    subject_type == \"lcc\",\n    subject %in% c(\"PR\", \"PS\")\n  )\n# Get works based on authors and subjects\nworks <-\n  gutenberg_metadata |>\n  filter(\n    gutenberg_author_id %in% authors$gutenberg_author_id,\n    gutenberg_id %in% subjects$gutenberg_id\n  )\n# Download works\nworks |>\n  filter(\n    rights == \"Public domain in the USA.\",\n    has_text == TRUE\n  ) |>\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n  ) |>\n  write_csv(file = \"data/original/gutenberg/works.csv\")\n```\n\n#### Build the custom function\n\n::: {.panel-tabset}\n\n##### Name\n\nLet's start to create our function by creating a name and calling the `function()` function. We will name our function `get_gutenberg_works()`.\n\n```{.r}\nget_gutenberg_works <- function() {\n\n}\n```\n\n##### Arguments\n\nNow, we need to think of the arguments that we would like to pass to our function so they can be used to customize the data acquisition process. First, we want to check to see if the data already exists on disk. To do this we will need to pass the path to the data file to our function. We will name this argument `target_file`.\n\n```{.r}\nget_gutenberg_works <- function(target_file) {\n\n}\n```\n\nNext, we want to pass the subject code that the works should be associated with. We will name this argument `lcc_subject`.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject) {\n\n}\n```\n\nFinally, we want to pass the birth year and death year that the authors should be associated with. We will name these arguments `birth_year` and `death_year`.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n\n}\n```\n\n##### Code: comments\n\nWe now turn to the code. I like to start by creating comments to describe the steps inside the function before adding code.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n```\n\n##### Code: packages\n\nWe have some packages we want to make sure are installed and loaded. We will use the {pacman} package to do this. We will use the `p_load()` function to install and load the packages. We will pass the character vector of package names to the `p_load()` function.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n\n  # Check to see if the data already exists\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n```\n\n##### Code: data check\n\nWe need to create the code to check if the data exists. We will use an `if` statement to do this. If the data does exist, we will print a message to the console that the data already exists and stop the function. If the data does not exist, we will create the directory structure and continue with the data acquisition process. I will use {fs} [@R-fs] in this code so I will load the library at the top of the function.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir <- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n```\n\n##### Code: authors\n\nLet's now add the code to get the authors within the years. We will now use the `birth_year` and `death_year` arguments to filter the `gutenberg_authors` data frame.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir <- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors <-\n    gutenberg_authors |>\n    filter(\n      birthdate > birth_year,\n      deathdate < death_year\n    )\n\n  # Get LCC subjects\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n```\n\n##### Code: subject\n\nUsing the `lcc_subject` argument, we will now filter the `gutenberg_subjects` data frame.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir <- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors <-\n    gutenberg_authors |>\n    filter(\n      birthdate > birth_year,\n      deathdate < death_year\n    )\n\n  # Get LCC subjects\n  subjects <-\n    gutenberg_subjects |>\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n\n  # Download works\n\n  # Write works to disk\n}\n```\n\n##### Code: works\n\nWe will use the `authors` and `subjects` data frames to filter the `gutenberg_metadata` data frame as before.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir <- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors <-\n    gutenberg_authors |>\n    filter(\n      birthdate > birth_year,\n      deathdate < death_year\n    )\n\n  # Get LCC subjects\n  subjects <-\n    gutenberg_subjects |>\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works <-\n    gutenberg_metadata |>\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n\n  # Write works to disk\n}\n```\n\n##### Code: download\n\nWe will now use the `works` data frame to download the text and metadata for the works using the `gutenberg_download()` function and assign it to `results`.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir <- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors <-\n    gutenberg_authors |>\n    filter(\n      birthdate > birth_year,\n      deathdate < death_year\n    )\n\n  # Get LCC subjects\n  subjects <-\n    gutenberg_subjects |>\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works <-\n    gutenberg_metadata |>\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results <-\n    works |>\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |>\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n}\n```\n\n##### Code: write\n\nFinally, we will write the `results` data frame to disk using the `write_csv()` function and the `target_file` argument.\n\n```{.r}\nget_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {\n  # Load packages\n  library(dplyr)\n  library(gutenbergr)\n  library(readr)\n  library(fs)\n\n  # Check to see if the data already exists\n  if (file_exists(target_file)) {\n    message(\"Data already exists \\n\")\n    return()\n  } else {\n    target_dir <- dirname(target_file)\n    dir_create(path = target_dir, recurse = TRUE)\n  }\n\n  # Get authors within years\n  authors <-\n    gutenberg_authors |>\n    filter(\n      birthdate > birth_year,\n      deathdate < death_year\n    )\n\n  # Get LCC subjects\n  subjects <-\n    gutenberg_subjects |>\n    filter(\n      subject_type == \"lcc\",\n      subject %in% lcc_subject\n    )\n\n  # Get works based on authors and subjects\n  works <-\n    gutenberg_metadata |>\n    filter(\n      gutenberg_author_id %in% authors$gutenberg_author_id,\n      gutenberg_id %in% subjects$gutenberg_id\n    )\n\n  # Download works\n  results <-\n    works |>\n    filter(\n      rights == \"Public domain in the USA.\",\n      has_text == TRUE\n    ) |>\n    gutenberg_download(\n      meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")\n    )\n\n  # Write works to disk\n  write_csv(results, file = target_file)\n}\n```\n\n:::\n\n#### Using the custom function\n\nWe now have a function, `get_gutenberg_works()`, that we can use to acquire works from Project Gutenberg for a given LCC code for authors that lived during a given time period. We now have a flexible function that we can use to acquire data.\n\nWe can add this function to the script in which we use it, or we can add it to a separate script and source it into any script in which we want to use it.\n\n```{.r}\n# Source function\nsource(\"get_gutenberg_works.R\")\n\n# Get works for PR and PS for authors born between 1800 and 1880\nget_gutenberg_works(\n  target_file = \"data/original/gutenberg/works.csv\",\n  lcc_subject = c(\"PR\", \"PS\"),\n  birth_year = 1800,\n  death_year = 1880\n)\n```\n\nAnother option is to add this function to your own package. This is a great option if you plan to use this function in multiple projects or share it with others. Since I have already created a package for this book, {qtkit}, I've added this function, with some additional functionality, to the package.\n\n<!-- [ ] change to {qtkit} -->\n\n```{.r}\n# Load package\nlibrary(qtalrkit)\n\n# Get works for fiction for authors born between 1870 and 1920\nget_gutenberg_works(\n  target_dir = \"data/original/gutenberg/\",\n  lcc_subject = \"PZ\",\n  birth_year = 1870,\n  death_year = 1920\n)\n```\n\nThis modified function will create a directory structure for the data file if it does not already exist. It will also create a file name for the data file based on the arguments passed to the function.\n\n### Data documentation\n\nFinding data sources and collecting data are important steps in the acquisition process. However, it is also important to document the data collection process. This is important so that you, and others, can reproduce the data collection process.\n\nIn data acquisition, the documentation is includes the code, code comments, and prose in the process file used to acquire the data and also a data origin file. The data origin file is a text file that describes the data source and the data collection process.\n\nThe {qtkit} package includes a function, `create_data_origin()`, that can be used to scaffold a data origin file. This simply takes a file path and creates a data origin file in CSV format.\n\n```{.xml}\nattribute,description\nResource name,The name of the resource.\nData source,\"URL, DOI, etc.\"\nData sampling frame,\"Language, language variety, modality, genre, etc.\"\nData collection date(s),The dates the data was collected.\nData format,\".txt, .csv, .xml, .html, etc.\"\nData schema,\"Relationships between data elements: files, folders, etc.\"\nLicense,\"CC BY, CC BY-SA, etc.\"\nAttribution,Citation information.\n```\n\nThe you edit this file and ensure that it contains all of the information needed to document the data. Make sure that this file is near the data file so that it is easy to find.\n\n```{.bash}\ndata\n  ├── analysis/\n  ├── derived/\n  └── original/\n      ├── works_do.csv\n      └── gutenberg/\n          ├── works_pr.csv\n          └── works_ps.csv\n```\n\n## Summary\n\nIn this recipe, we have covered acquiring data for a text analysis project. We used the {gutenbergr} [@R-gutenbergr] to acquire works from Project Gutenberg. After exploring the resources available, we established an acquisition plan. We then used R to implement our plan. To make our code more reproducible-friendly, we wrote a custom function to acquire the data. Finally, we discussed the importance of documenting the data collection process and introduced the data origin file.\n\n## Check your understanding\n\n1. In the chapter and in this recipe, strategies for acquiring data were discussed. Which of the following was not discussed as a strategy for acquiring data? <select class='webex-select'><option value='blank'></option><option value=''>Direct download</option><option value=''>Programmatic download</option><option value=''>APIs</option><option value='answer'>Web scraping</option></select>\n2. In this recipe, we used {gutenbergr} to acquire works from Project Gutenberg. What is the name of the function that we used to acquire the actual text? <select class='webex-select'><option value='blank'></option><option value=''>gutenberg_metata</option><option value=''>gutenberg_get()</option><option value=''>gutenberg_search()</option><option value='answer'>gutenberg_download()</option></select>\n3. <select class='webex-select'><option value='blank'></option><option value=''>True</option><option value='answer'>False</option></select> A custom function is only really necessary if you are writting an R package.\n4. When writing a custom function, what is the first step? <select class='webex-select'><option value='blank'></option><option value=''>Write the code</option><option value=''>Write the comments</option><option value=''>Load the packages</option><option value='answer'>Create the function arguments</option></select>\n5. What does it mean when we say that a function is 'vectorized' in R? <select class='webex-select'><option value='blank'></option><option value=''>The function returns a vector</option><option value=''>The function can take a vector as an argument</option><option value='answer'>The function can take a vector and operates on each element of the vector</option></select>\n6. Which Tidyverse package allows us to apply non-vectorized functions to vectors? <select class='webex-select'><option value='blank'></option><option value=''>dplyr</option><option value=''>stringr</option><option value=''>readr</option><option value='answer'>purrr</option></select>\n\n## Lab preparation\n\nBefore beginning [Lab 5](https://github.com/qtalr/lab-05), make sure you are comfortable with the following:\n\n- Reading and subsetting data in R\n- Writing data in R\n- The project structure of reproducible projects\n\nThe additional skills covered in this lab are:\n\n- Identifying data sources\n- Acquiring data through manual and programmatic downloads and APIs\n- Creating a data acquisition plan\n- Documenting the data collection process\n- Writing a custom function\n- Documenting the data source with a data origin file\n\nYou will have a choice of data source to acquire data from. Before you start the lab, you should consider which data source you would like to use, what strategy you will use to acquire the data, and what data you will acquire. You should also consider the information you need to document the data collection process.\n\nConsult the [Identifying data and data sources](guide-4.html) guide for some ideas on where to find data sources.\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}