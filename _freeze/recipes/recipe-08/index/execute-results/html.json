{
  "hash": "877c3734065ef27d13d3c9782f8bd906",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"08. Employing exploratory methods\"\nsubtitle: \"Prepare and enrich datasets for analysis\"\ndescription: |\n  Exploratory analysis is a wide-ranging term that encompasses many different methods. In this recipe, we will focus on the methods that are most commonly used in the analysis of textual data. These include frequency and distributional analysis, clustering, and word embedding models.\ncategories: [analysis]\n---\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Skills**\n\n- Tokenize and prepare features for analysis\n- Conduct frequency and dispersion analysis including measures and visualizations\n- Train word embeddings and create a Term-Document Matrix\n- Conduct a word embedding analysis\n:::\n\nThe approach to exploratory analysis is rarely linear, but rather an interative cycle of the steps in @tbl-eda-workflow. This cycle is repeated until the research question(s) have been addressed.\n\n| Step | Name | Description |\n| --- | --- | --- |\n| 1 | Identify | Consider the research question and identify variables of potential interest to provide insight into our question. |\n| 2 | Inspect | Check for missing data, outliers, *etc*. and check data distributions and transform if necessary. |\n| 3 | Interrogate | Submit the selected variables to descriptive or unsupervised learning methods to provide quantitative measures to evaluate. |\n| 4 | Interpret | Evaluate the results and determine if they are valid and meaningful to respond to the research question. |\n\n: The exploratory analysis workflow {#tbl-eda-workflow tbl-colwidths=\"[10,10, 80]\"}\n\nWe will model how to explore iteratively using the output of one method to inform the next and ultimately to address the research question. For this reason, the subsequent sections of this recipe are grouped by research question rather than by approach step or method.\n\nLet's get started by loading some of the packages we will likely use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)      # for data manipulation\nlibrary(stringr)    # for string manipulation\nlibrary(tidyr)      # for data tidying\nlibrary(tidytext)   # for text analysis\nlibrary(ggplot2)    # for data visualization\n```\n:::\n\n\n## Concepts and strategies\n\n### Orientation\n\nWe will use the SOTU corpus to demonstrate the different methods. We will select a subset of the corpus (post-1945) and explore the question:\n\n- How has the language of the SOTU changed over time?\n\nThis will include methods such as frequency and distributional analysis, dimensionality reduction, and word embedding models.\n\n\n::: {.cell}\n\n:::\n\n\nLet's look at the first few rows of the data to get a sense of what we have.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# # A tibble: 73 × 4\n#    president   year party      address                                          \n#    <chr>      <dbl> <chr>      <chr>                                            \n#  1 Truman      1947 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  2 Truman      1948 Democratic \"Mr. President, Mr. Speaker, and Members of the …\n#  3 Truman      1949 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  4 Truman      1950 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  5 Truman      1951 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  6 Truman      1952 Democratic \"Mr. President, Mr. Speaker, Members of the Cong…\n#  7 Eisenhower  1953 Republican \"Mr. President, Mr. Speaker, Members of the Eigh…\n#  8 Eisenhower  1954 Republican \"Mr. President, Mr. Speaker, Members of the Eigh…\n#  9 Eisenhower  1955 Republican \"Mr. President, Mr. Speaker, Members of the Cong…\n# 10 Eisenhower  1956 Republican \"My Fellow Citizens: This morning I sent to the …\n# # ℹ 63 more rows\n```\n\n\n:::\n:::\n\n\nWe can see that the dataset contains the president, year, party, and address for each SOTU address.\n\nNow let's view a statistical overview summary of the data by using the `skim()` function.\n\n```r\nskimr::skim(sotu_df)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# ── Data Summary ────────────────────────\n#                            Values \n# Name                       sotu_df\n# Number of rows             73     \n# Number of columns          4      \n# _______________________           \n# Column type frequency:            \n#   character                3      \n#   numeric                  1      \n# ________________________          \n# Group variables            None   \n# \n# ── Variable type: character ────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate  min   max empty n_unique whitespace\n# 1 president             0             1    4    10     0       12          0\n# 2 party                 0             1   10    10     0        2          0\n# 3 address               0             1 6160 51947     0       73          0\n# \n# ── Variable type: numeric ──────────────────────────────────────────────────────\n#   skim_variable n_missing complete_rate  mean   sd   p0  p25  p50  p75 p100\n# 1 year                  0             1 1984. 21.6 1947 1965 1984 2002 2020\n#   hist \n# 1 ▇▇▇▇▇\n```\n\n\n:::\n:::\n\n\nWe can see that the dataset contains 73 rows --corresponding to the number of SOTU addresses. Looking at missing values, we can see that there are no missing values for any of the variables. Taking a closer look at each variable, the `president` variable is a character vector with 12 unique presidents. There are two unique parties and 73 unique addresses. The `year` variable is numeric with a minimum value of 1947 and a maximum value of 2020, therefore the addresses include 73 years.\n\n### Identify\n\nWith a general sense of the data, we can now move on to the first step in the exploratory analysis workflow: identifying variables of interest.\n\n- What linguistic variables might be of interest to address this question?\n\nWords might be the most obvious variable, but we might also be interested in other linguistic variables such as parts of speech, syntactic structures, or some combination of these.\n\nLet's start with words. If we look at words, we might be interested in the frequency of words, the distribution of words, or the meaning of words. We might also be interested in the relationship between words. For example, we might be interested in the co-occurrence of words or the similarity of words. These, and more, are all possible approaches that we might consider.\n\nAnother consideration is if we want to do comparisons across time, across presidents, across parties, *etc.*. In our research question, we have already identified that we want to compare across time so that will be our focus. However, what we mean by \"time\" is not clear. Do we mean across years, across decades, across presidencies, *etc.*? We will need to make a decision about how we want to define time, but we can fold this into our exploratory analysis, as we will see below.\n\nLet's posit the following sub-questions:\n\n1. What are the most frequent words across time periods?\n2. How does the distribution of words change across time periods?\n3. How does the meaning of words change across time periods?\n\nWe will use these sub-questions to guide our exploratory analysis.\n\n### Inspect\n\nThe next step is to inspect the data. We will transform the data as necessary to prepare it for analysis and then do some diagnostic checks to make sure that the data is ready for analysis.\n\nSince we will be working with words, let's tokenize the `addresses` variable to extract the words and maintain a tidy dataset. We will use the `unnest_tokens()` function from {tidytext} to do this. Let's apply the function and assign the result to a new variable called `sotu_words_df`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tokenize the words by year (with numbers removed)\nsotu_words_df <-\n  sotu_df |>\n  unnest_tokens(word, address, strip_numeric = TRUE)\n```\n:::\n\n\nLet's continue by looking at whether there is a relationship between the number of words and years. We can do this by using the `count()` function on the `year` variable. This will group and count the number of observations (words) per year as `n`.\n\nWe can then visualize this with a line plot where the x-axis is the year and the y-axis is the number of words `n`. I'll add the plot to a variable called `p` so that we can add layers to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the number of words per year --------------------------------\np <-\n  sotu_words_df |>\n  count(year) |>\n  ggplot(aes(x = year, y = n)) +\n  geom_line()\n\n# View\np\n```\n\n::: {.cell-output-display}\n![Number of words per year](index_files/figure-html/fig-sotu-words-over-time-1.png){#fig-sotu-words-over-time width=672}\n:::\n:::\n\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nThe `count()` function is a wrapper for the `summarize()` function. It is a convenient way to count the number of observations in a dataset.\n\n```r\n\n# the output of\ndf |>\n  count(var1)\n\n# is equivalent to\ndf |>\n  group_by(var1) |>\n  summarize(n = n()) |>\n  ungroup()\n```\n\nThe difference is that `count()` grouping is added and removed automatically. In other cases, we can mimic this behavior for other operations inside a `summarize()` or `mutate()` function by using the `.by` argument. For example:\n\n```r\ndf |>\n  summarize(n = n(), .by = var1)\n```\n\n:::\n\nWe can see from @fig-sotu-words-over-time-trend-lm that the number of words per year varies, sometimes quite a bit. To get a sense of the relationship between the number of words and the year, we can add a linear trend line to the plot. We can do this by adding the `geom_smooth()` function to the plot. We will set the `method` argument to `\"lm\"` to use a linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-sotu-words-over-time-trend-lm-1.png){#fig-sotu-words-over-time-trend-lm width=672}\n:::\n:::\n\n\nThis plot shows that there is a positive relationship between the number of words and the year --that is the number of words increases over time. But the line is not a great fit and furthermore the angle of the line is more horizontal than vertical. This suggests that the relationship is not a strong one. We can confirm this by calculating the correlation between the number of words and the year. We can do this by using the `cor.test()` function on the `year` and `n` variables inside a `summarize()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the correlation between the number of words and the year ----\nsotu_words_df |>\n  count(year) |>\n  summarize(cor = cor.test(year, n)$estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n    cor\n  <dbl>\n1 0.342\n```\n\n\n:::\n:::\n\n\nSo, even though we are working to inspect our data, we already have a finding. The number of words increases over time, despite the fact that the relationship is not a strong one.\n\nNow, let's turn our attention to the frequency of individual words. Let's start by looking at the most frequent words for the entire corpus. We can do this by grouping by the `word` variable and then summarizing the number of times each word occurs. We will then arrange the data in descending order by the number of times each word occurs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the most frequent words ------------------------------------\n\nsotu_words_df |>\n  count(word, sort = TRUE) |>\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   word      n\n   <chr> <int>\n 1 the   21565\n 2 and   14433\n 3 to    13679\n 4 of    13087\n 5 in     8116\n 6 we     7327\n 7 a      7137\n 8 our    6822\n 9 that   5391\n10 for    4513\n```\n\n\n:::\n:::\n\n\nThe usual suspects are at the top of the list. This is not surprising, and will likely be the case for most corpora, and sizeable subcorpora --in our case the time periods. Let's address this.\n\nWe could use a stopwords list to just eliminate many of these common words, but that might be a bit too agressive and we will likely lose some words that we want to keep. Considering a more nuanced approach, we will use the $tf$-$idf$ transformation to attenuate the effect of words that are common across all time periods, and on the flip side, to promote the effect of words that are more distinctive to each time period.\n\nIn order to do this, we will need to calculate the $tf$-$idf$ for each word in each time period. To keep things simple, we will calculate the $tf$-$idf$ for each word in each decade. We will do this by creating a new variable called `decade` that is the year rounded down to the nearest decade. Then we can group by this `decade` variable and then count the number of times each word occurs. We will then calculate the $tf$-$idf$ for each word in each decade using the `bind_tf_idf()` function from {tidytext}. We will then arrange the data in descending order by the $tf$-$idf$ value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the tf-idf of words by decade ------------------------------\n\nsotu_words_tfidf_df <-\n  sotu_words_df |>\n  mutate(decade = floor(year / 10) * 10) |>\n  count(decade, word) |>\n  bind_tf_idf(word, decade, n) |>\n  arrange(decade, desc(tf_idf))\n\n# Preview\nsotu_words_tfidf_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 40,073 × 6\n   decade word               n       tf   idf   tf_idf\n    <dbl> <chr>          <int>    <dbl> <dbl>    <dbl>\n 1   1940 boycotts           5 0.000344 2.20  0.000756\n 2   1940 jurisdictional     7 0.000482 1.50  0.000725\n 3   1940 interpretation     4 0.000275 2.20  0.000605\n 4   1940 unjustified        4 0.000275 2.20  0.000605\n 5   1940 insecurity         5 0.000344 1.50  0.000518\n 6   1940 arbitration        3 0.000207 2.20  0.000454\n 7   1940 output             6 0.000413 1.10  0.000454\n 8   1940 unjustifiable      3 0.000207 2.20  0.000454\n 9   1940 management        25 0.00172  0.251 0.000433\n10   1940 rental             4 0.000275 1.50  0.000414\n# ℹ 40,063 more rows\n```\n\n\n:::\n:::\n\n\nOK. Even the preview shows that we are getting a more interesting list of words.\n\nLet's look at the top 10 words for each decade. We group by `decade` and then slice the top 10 words by $tf$-$idf$ value with `slice_max()`. Then we will use the `reorder_within()` function from {tidytext} to reorder the words within each facet by the $tf$-$idf$ value.\n\nWe will visualize this with a bar chart where word is on the x-axis and the height of the bar is the $tf$-$idf$ value. We will also facet the plot by decade. I've flipped the coordinates so that the words are on the y-axis and the bars are horizontal. This is a personal preference, but I find it easier to read the words this way.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_words_tfidf_df |>\n  group_by(decade) |>\n  slice_max(n = 10, tf_idf) |>\n  ungroup() |>\n  mutate(decade = as.character(decade)) |>\n  mutate(word = reorder_within(word, desc(tf_idf), decade)) |>\n  ggplot(aes(word, tf_idf, fill = decade)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~decade, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![Visualize the top 10 words by decade](index_files/figure-html/fig-sotu-freq-words-tfidf-top-10-1.png){#fig-sotu-freq-words-tfidf-top-10 width=672}\n:::\n:::\n\n\nScanning the top words for the decades we can see some words that signal contemporary issues. This is a hint that we are picking up on some of the changes in the language of the SOTU over time.\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nThe plot above makes use of the `reorder_within()` and `scale_x_reordered()` functions from {tidytext}. These functions allow us to reorder the words within each facet by the $tf$-$idf$ value. This is a nice way to visualize the most distinctive words for each decade. These are more advanced functions. If you are interested in learning more about them, you can read more about them in the help documentation `?tidytext::reorder_within`\n:::\n\nTo my eye, however, the 1940s and the 2020s don't seem to jump out at me in the same way. Let's take a closer look at the 1940s and the 2020s in our original dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_df |>\n  filter(year < 1950 | year >= 2020)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  president  year party      address                                            \n  <chr>     <dbl> <chr>      <chr>                                              \n1 Truman     1947 Democratic \"Mr. President, Mr. Speaker, Members of the Congre…\n2 Truman     1948 Democratic \"Mr. President, Mr. Speaker, and Members of the 80…\n3 Truman     1949 Democratic \"Mr. President, Mr. Speaker, Members of the Congre…\n4 Trump      2020 Republican \"Madam Speaker, Mr. Vice President, Members of Con…\n```\n\n\n:::\n:::\n\n\nWell, that explains it. There are only 3 addresses in the 1940s and 1 address in the 2020s. This is not enough data to get a good sense of the language of the SOTU for these decades. Let's remove these decades from our original dataset as not being representative of the language of the SOTU.\n\nAnother consideration that catches my eye in looking at the top words by decade is that our words like \"communist\" and \"communists\" are being counted separately. That is fine, but what if we want to count these as the same word? We can do this by lemmatizing the words --that is reducing the words to their root form. We can do this using the `lemmatize_words()` function from {textstem}.\n\nSo consider this example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lemmatize the words --------------------------------------------\nwords_chr <- c(\"freedom\", \"free\", \"frees\", \"freeing\", \"freed\")\ntextstem::lemmatize_words(words_chr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"freedom\" \"free\"    \"free\"    \"free\"    \"free\"   \n```\n\n\n:::\n:::\n\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nBy default, the `lemmatize_words()` function uses a lookup table for English to lemmatize words. This is a simple approach that works well for many cases. However, it is not perfect. For example, it will not lemmatize words that are not in the lookup table.\n\nIf you want to lemmatize words that are not in the lookup table, or you want to lemmatize words in another language, you can create or add to a lookup table. You can read more about this in the help documentation `?textstem::lemmatize_words()`.\n\nA resource for lemma lookup tables can be found here <https://github.com/michmech/lemmatization-lists>.\n:::\n\nWith these considerations in mind, let's update our `sotu_df` dataset to remove the 1940s and 2020s, tokenize and lemmatize the words, and add a decade variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Update the dataset ----------------------------------------------\nsotu_terms_df <-\n  sotu_df |>\n  filter(year >= 1950 & year < 2020) |> # Remove the 1940s and 2020s\n  unnest_tokens(word, address, strip_numeric = TRUE) |> # Tokenize the words\n  mutate(lemma = textstem::lemmatize_words(word)) |> # Lemmatize the words\n  mutate(decade = floor(year / 10) * 10) |> # Add a decade variable\n  select(president, decade, year, party, word, lemma) #\n\n# Preview\nsotu_terms_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 368,586 × 6\n   president decade  year party      word      lemma    \n   <chr>      <dbl> <dbl> <chr>      <chr>     <chr>    \n 1 Truman      1950  1950 Democratic mr        mr       \n 2 Truman      1950  1950 Democratic president president\n 3 Truman      1950  1950 Democratic mr        mr       \n 4 Truman      1950  1950 Democratic speaker   speaker  \n 5 Truman      1950  1950 Democratic members   member   \n 6 Truman      1950  1950 Democratic of        of       \n 7 Truman      1950  1950 Democratic the       the      \n 8 Truman      1950  1950 Democratic congress  congress \n 9 Truman      1950  1950 Democratic a         a        \n10 Truman      1950  1950 Democratic year      year     \n# ℹ 368,576 more rows\n```\n\n\n:::\n:::\n\n\nThis inspection process could go on for a while. We could continue to inspect the data and make changes to the dataset but it is often the case that in the process of analysis we will often run into issues that require us to go back and make changes to the dataset. So we will move on to the next step in the exploratory analysis workflow.\n\n### Interrogate\n\nNow that we have our data in a tidy format, we can move on to the next step in the exploratory analysis workflow: interrogating the data. We will submit the selected variables to descriptive or unsupervised learning methods to provide quantitative measures to evaluate.\n\n#### Frequency\n\n1. What are the most frequent words across time periods?\n\nWe have already made some progress on this question in the inspection phase, but now we can do it again with the updated dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the most frequent lemmas by decade --------------------------\n\nsotu_lemmas_tfidf_df <-\n  sotu_terms_df |>\n  count(decade, lemma) |> # Count the lemmas by decade\n  bind_tf_idf(lemma, decade, n) |> # Calculate the tf-idf\n  arrange(decade, desc(tf_idf))\n\n# Preview\nsotu_lemmas_tfidf_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26,435 × 6\n   decade lemma            n       tf   idf   tf_idf\n    <dbl> <chr>        <int>    <dbl> <dbl>    <dbl>\n 1   1950 armament        16 0.000321 1.95  0.000625\n 2   1950 imperialism      9 0.000181 1.95  0.000351\n 3   1950 shall          107 0.00215  0.154 0.000331\n 4   1950 disarmament     13 0.000261 1.25  0.000327\n 5   1950 mineral          8 0.000160 1.95  0.000312\n 6   1950 mobilization    12 0.000241 1.25  0.000302\n 7   1950 survivor        12 0.000241 1.25  0.000302\n 8   1950 expenditure     42 0.000842 0.336 0.000283\n 9   1950 adequate        25 0.000501 0.560 0.000281\n10   1950 constantly      11 0.000221 1.25  0.000276\n# ℹ 26,425 more rows\n```\n\n\n:::\n:::\n\n\nNow we can visualize the top 10 lemmas for each decade, as we did above, but for the lemmas instead of the words and for seven full decades.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_lemmas_tfidf_df |>\n  group_by(decade) |>\n  slice_max(n = 10, tf_idf) |>\n  ungroup() |>\n  mutate(decade = as.character(decade)) |>\n  mutate(lemma = reorder_within(lemma, desc(tf_idf), decade)) |>\n  ggplot(aes(lemma, tf_idf, fill = decade)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(~decade, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![Visualize the top 10 lemmas by decade](index_files/figure-html/fig-sotu-freq-lemmas-tfidf-top-10-1.png){#fig-sotu-freq-lemmas-tfidf-top-10 width=672}\n:::\n:::\n\n\n#### Distribution\n\n2. How does the distribution of words change across time periods?\n\nOK. Now let's focus on word frequency distributions over time. We will return to the `sotu_terms_df`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_terms_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 368,586 × 6\n   president decade  year party      word      lemma    \n   <chr>      <dbl> <dbl> <chr>      <chr>     <chr>    \n 1 Truman      1950  1950 Democratic mr        mr       \n 2 Truman      1950  1950 Democratic president president\n 3 Truman      1950  1950 Democratic mr        mr       \n 4 Truman      1950  1950 Democratic speaker   speaker  \n 5 Truman      1950  1950 Democratic members   member   \n 6 Truman      1950  1950 Democratic of        of       \n 7 Truman      1950  1950 Democratic the       the      \n 8 Truman      1950  1950 Democratic congress  congress \n 9 Truman      1950  1950 Democratic a         a        \n10 Truman      1950  1950 Democratic year      year     \n# ℹ 368,576 more rows\n```\n\n\n:::\n:::\n\n\nWe want to get a sense of how the word distributions change over time. We need to calculate the frequency of words for each year, first off. So we need to go back to the `sotu_terms_df` dataset and group by `year` and `word` and then count the number of times each word occurs.\n\nSince we will lose the `lemma` variable in this process, we will add it back after the $tf$-$idf$ transformation by using the `mutate()` function and the `textstem::lemmatize_words()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_terms_tfidf_df <-\n  sotu_terms_df |>\n  count(year, word) |> # Count the words by year\n  bind_tf_idf(word, year, n) |> # Calculate the tf-idf\n  mutate(lemma = textstem::lemmatize_words(word)) |> # Lemmatize the words\n  arrange(year, desc(tf_idf))\n\n# Preview\nsotu_terms_tfidf_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 97,213 × 7\n    year word               n       tf   idf  tf_idf lemma         \n   <dbl> <chr>          <int>    <dbl> <dbl>   <dbl> <chr>         \n 1  1950 enjoyment          3 0.000585 3.54  0.00207 enjoyment     \n 2  1950 rent               3 0.000585 2.85  0.00167 rend          \n 3  1950 widespread         3 0.000585 2.85  0.00167 widespread    \n 4  1950 underdeveloped     2 0.000390 4.23  0.00165 underdeveloped\n 5  1950 ideals             7 0.00136  1.14  0.00156 ideal         \n 6  1950 transmit           3 0.000585 2.62  0.00153 transmit      \n 7  1950 peoples            8 0.00156  0.976 0.00152 people        \n 8  1950 democratic        12 0.00234  0.623 0.00146 democratic    \n 9  1950 expenditures       7 0.00136  1.06  0.00144 expenditure   \n10  1950 businessmen        3 0.000585 2.44  0.00143 businessman   \n# ℹ 97,203 more rows\n```\n\n\n:::\n:::\n\n\nWith this format, we can visualize the distinctiveness of words over time. All we need to do is to filter the data to the lemmas we are interested first.\n\nLet's just start with some random poltical-oriented words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_terms <- c(\"crime\", \"law\", \"free\", \"terror\", \"family\", \"government\")\n\nsotu_terms_tfidf_df |>\n  filter(lemma %in% plot_terms) |>\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.25) +\n  facet_wrap(~lemma, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![Distictiveness of political words over time](index_files/figure-html/fig-sotu-terms-tfidf-df-political-1.png){#fig-sotu-terms-tfidf-df-political width=672}\n:::\n:::\n\n\nWe can see in @fig-sotu-terms-tfidf-df-political that the distinctiveness of these lemmas varies over time. Now, in this plot I've used a small span value for the `geom_smooth()` function to get a sense of the more fine-grained changes over time. However, this is may be too fine-grained. We can adjust this by increasing the `span` value. Let's try a `span` value of 0.5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_terms_tfidf_df |>\n  filter(lemma %in% plot_terms) |>\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  facet_wrap(~lemma, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![Distinctiveness of political words over time](index_files/figure-html/fig-sotu-terms-tfidf-df-political-smooth-detail-1.png){#fig-sotu-terms-tfidf-df-political-smooth-detail width=672}\n:::\n:::\n\n\nFigure \\@ref(fig:sotu-terms-tfidf-df-political-smooth-detail) seems to be picking up on some of the more general word usage trends over time.\n\nAnother thing to note about the way we plotted the data is that we used the `facet_wrap()` function to create a separate plot for each word but we used the `scales = \"free_y\"` allowing the y-axis to vary for each plot. This means we are not comparing the y-axis values across plots and thus can not say anything about the differing magnitudes from a visual inspection.\n\nTo address this, we can remove the `scales = \"free_y\"` argument and use the default which will fix the x- and y-axis scales across plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_terms_tfidf_df |>\n  filter(lemma %in% plot_terms) |>\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  facet_wrap(~lemma)\n```\n\n::: {.cell-output-display}\n![Distinctiveness of political words over time](index_files/figure-html/fig-sotu-terms-tfidf-df-political-smooth-detailed-fixed-1.png){#fig-sotu-terms-tfidf-df-political-smooth-detailed-fixed width=672}\n:::\n:::\n\n\nIn @fig-sotu-terms-tfidf-df-political-smooth-detailed-fixed, we can clearly see that the magnitude of the words \"crime\" and \"terror\" are much higher than the other words we happend to select. Furthermore, these words have interesting patterns. In particular, \"terror\" has two peaks on around 1980 and another around the turn of the century. \"Crime\" also has two distinctive peaks on in the 1970s and one in the 1990s.\n\nThe terms that I selected before were somewhat arbitrary. How can I identify the words that have changed the most drastically over these years from the data itself? We can do this by creating a term-document matrix with and then calculating the standard deviation of the $tf$-$idf$ values for each word. This will give us a sense of the words that have changed the most over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create TDM with words-year and tf-idf values\nsotu_word_tfidf_mat <-\n  sotu_terms_tfidf_df |>\n  cast_tdm(word, year, tf_idf) |>\n  as.matrix()\n\n# Preview\ndim(sotu_word_tfidf_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13109    69\n```\n\n\n:::\n\n```{.r .cell-code}\nsotu_word_tfidf_mat[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Docs\nTerms               1950    1951     1952 1953 1954\n  enjoyment      0.00207 0.00000 0.000661    0    0\n  rent           0.00167 0.00000 0.000000    0    0\n  widespread     0.00167 0.00000 0.000000    0    0\n  underdeveloped 0.00165 0.00000 0.000000    0    0\n  ideals         0.00156 0.00171 0.000853    0    0\n```\n\n\n:::\n:::\n\n\nTo calculate the standard deviation of the $tf$-$idf$ values for each word, we can use the `apply()` function to iterate over each row of the matrix and calculate the standard deviation of the values in each row. You can think of the `apply()` function as a cousin of the `map()` function. The `apply()` function iterates over the rows or columns of a matrix or data frame and applies a function to each row or column. We choose whether the function is applied to the rows or columns with the `MARGIN` argument. We can set `MARGIN = 1` to apply the function to the rows and `MARGIN = 2` to apply the function to the columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the standard deviation of the tf-idf values for each word\nsotu_words_sd <-\n  apply(sotu_word_tfidf_mat, MARGIN = 1, FUN = sd, na.rm = TRUE)\n\n# Preview seed words\nsotu_words_sd |>\n  sort(decreasing = TRUE) |>\n  head(100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     vietnam      hussein       saddam         salt         iraq        shall \n    0.001188     0.001138     0.001121     0.001067     0.000976     0.000887 \n         oil        iraqi       that's   inspectors        qaida       terror \n    0.000798     0.000786     0.000733     0.000715     0.000702     0.000662 \n  terrorists         it's        crude       soviet    terrorist     activity \n    0.000651     0.000647     0.000635     0.000633     0.000632     0.000630 \n    covenant arrangements      session           al  disarmament       steven \n    0.000611     0.000609     0.000596     0.000596     0.000589     0.000588 \n     wartime      barrels         isil    mentioned        ought        elvin \n    0.000573     0.000569     0.000567     0.000559     0.000558     0.000556 \n      iraqis         ryan        we're         kids        let's          gun \n    0.000551     0.000539     0.000539     0.000535     0.000533     0.000532 \n     rebekah           cj        camps      empower         cory        we've \n    0.000531     0.000527     0.000524     0.000523     0.000522     0.000518 \n         92d  afghanistan    childcare        100th        music         gulf \n    0.000516     0.000514     0.000513     0.000507     0.000500     0.000494 \n        21st   extremists   foundation      picture     beguiled  contemplate \n    0.000491     0.000489     0.000487     0.000486     0.000473     0.000473 \n   enumerate    hurriedly   hysterical  ingredients        smile       smiles \n    0.000473     0.000473     0.000473     0.000473     0.000473     0.000473 \n    wagehour     josefina        shi'a        alice     alliance    seventies \n    0.000473     0.000471     0.000468     0.000468     0.000467     0.000464 \n      herman       joshua      matthew        julie         11th       border \n    0.000463     0.000463     0.000463     0.000462     0.000461     0.000459 \n     persian    recommend    communist        mayor    nicaragua       planes \n    0.000456     0.000453     0.000447     0.000443     0.000442     0.000442 \n      trevor        corey       kenton      preston        seong     property \n    0.000440     0.000439     0.000439     0.000439     0.000439     0.000439 \n     regimes       disarm      mention    peacetime   localities    objective \n    0.000437     0.000436     0.000436     0.000433     0.000429     0.000428 \n         i'm       surtax        banks       pounds       foster          she \n    0.000424     0.000423     0.000423     0.000422     0.000421     0.000418 \n        isis  sandinistas  discharging         gold \n    0.000417     0.000416     0.000416     0.000415 \n```\n\n\n:::\n:::\n\n\nNow we can choose from the top words that have changed the most over time. Here's another selection of words based on the standard deviation of the $tf$-$idf$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_terms <- c(\"equality\", \"right\", \"community\", \"child\", \"woman\", \"man\")\n\nsotu_terms_tfidf_df |>\n  filter(lemma %in% plot_terms) |>\n  ggplot(aes(year, tf_idf)) +\n  geom_smooth(se = FALSE, span = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~lemma)\n```\n\n::: {.cell-output-display}\n![Distinctiveness of words over time](index_files/figure-html/fig-sotu-terms-tfidf-sd-based-1.png){#fig-sotu-terms-tfidf-sd-based width=672}\n:::\n:::\n\n\nWe can see that the words I selected based on the standard deviation, in @fig-sotu-terms-tfidf-sd-based, can either increase, decrease, or fluctuate over time.\n\n#### Meaning\n\n3. How does the meaning of words change across time periods?\n\nFor this approach we need to turn to word embeddings. Word embeddings have been show to capture distributional semantics --that is the meaning of words based on their distribution in a corpus [@Hamilton2016].\n\nSince our research question is aimed a change over time we are performing a diachronic analysis. This means that we will need to create word embeddings for each time period, identify the common vocabulary across time periods, and then align the word embeddings to a common space before we can compare them.\n\nLet's load some packages that we will need.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fs) # for file system functions\nlibrary(PsychWordVec) # for working with word embeddings\nlibrary(purrr) # for iterating over lists\n```\n:::\n\n\nLet's first start by creating sub-corpora for each decade. We will write these to disk so that we can use them again if necessary. Note that data or datasets that are generated in the process of analysis are often stored in an `analysis/` folder inside of the main data folder to keep them separate from the other data acquired or derived in the project.\n\nWe will use the `str_c()` function to summarize the words for each address into a single string by decade. We will then use the `write_lines()` function to write the string to a text file. The `pwalk()` function from {purrr} is a convenient way to iterate over multiple arguments (`decade` and `address` in this case) in a function without returning a value to the console. The `p` in `pwalk()` stands for \"parallel\" and indicates that the function will iterate over the arguments in parallel.\n\n```{.r}\n# Create sub-corpora for each decade and write to disk ------------\nsotu_terms_df |>\n  summarize(address = str_c(lemma, collapse = \" \"), .by = decade) |>\n  select(decade, address) |>\n  pwalk(\\(decade, address) {\n    file_name <- str_c(\"../data/analysis/sotu/\", decade, \"s.txt\")\n    write_lines(address, file_name)\n  })\n```\n\n\n::: {.cell}\n\n:::\n\n\nNow I have written a function to read the text files, train the word embeddings for each, write the word embeddings to disk, and then load them back as VectorSpaceModel objects in a list.\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nA note on the training of the word embeddings. There are two main approaches to training word embeddings: Continuous Bag of Words (CBOW) and Skip-gram. CBOW is better for more common words and larger datasets. Skip-gram is better for less common words and smaller datasets. Given the varying sizes of the sub-corpora, Skip-gram might be more suitable as it may capture more nuances in less frequent words. Furthermore, the number of dimensions is a hyperparameter that needs to be tuned. The default is 50, but I have chosen 100 as we are attempting to capture more nuanced changes in the language of the SOTU.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_embeddings <- function(dir_path, dims = 100) {\n  # Get the text file paths\n  txt_files <- dir_ls(dir_path, regexp = \"\\\\.txt$\")\n  # Train the word embeddings\n  models <-\n    txt_files |>\n    map(\\(file) {\n      train_wordvec(\n        text = file,\n        dims = 100,\n        normalize = TRUE\n      )\n    })\n  # Modify the list names\n  names(models) <-\n    names(models) |>\n    basename() |>\n    str_remove(\"\\\\.txt\")\n\n  # Convert to embed matrices\n  models <- map(models, as_embed)\n\n  return(models)\n}\n```\n:::\n\n\nWe can now apply the `create_embeddings()` custom function to the decade sub-corpora text files in `../analysis/sotu/`.\n\n```{.r}\nsotu_vec_mods <- create_embeddings(\"../analysis/sotu/\")\n```\n\n\n\n\n\nEach word embedding model is a matrix with the words as the rows and the dimensions as the columns as an element of the `sotu_vec_mods` list. Each of these elements have the name of the decade. We can extract an element from a list using the `pluck()` function from {purrr}.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract an element from a list\nsotu_vec_mods |> pluck(\"1950s\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        dim1 ...     dim100\n   1: the             0.2910 ... <100 dims>\n   2: of              0.2229 ... <100 dims>\n   3: and             0.2818 ... <100 dims>\n   4: be              0.1370 ... <100 dims>\n   5: to              0.2336 ... <100 dims>\n-----                                      \n1164: appear          0.2471 ... <100 dims>\n1165: allegiance      0.2689 ... <100 dims>\n1166: accumulate      0.2158 ... <100 dims>\n1167: accomplishment  0.2241 ... <100 dims>\n1168: accept          0.2440 ... <100 dims>\n```\n\n\n:::\n:::\n\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nSince we have our models in a list, we will be using {purrr} functions quite a bit. Here's a quick summary of some of the {purrr} functions we will be using.\n\n- `map()` iterates over a list and applies a function to each element of the list. It returns a list.\n- `walk()` iterates over a list and applies a function to each element of the list. It does not return a value.\n\nEach of these has a parallel version, `pmap()` and `pwalk()`, which iterate over multiple lists in parallel, and a version that iterates over named lists, `imap()` and `iwalk()`. The `p` in `pmap()` and `pwalk()` stands for \"parallel\" and indicates that the function will iterate over the arguments in parallel. The `i` in `imap()` and `iwalk()` stands for \"indexed\" and indicates that the function will iterate over the arguments in parallel and return the index of the list element.\n:::\n\nThese embeddings can be explored in a number of ways. For example, we can get the words closest to a given word in the vector space for each decade. {PsychWordVec} has a function, `most_similar()`, that will do this for us. We can use the `map()` function to iterate over each model in the list and get the words with the most similar vectors. We will then use the `str_c()` function to summarize the words into a single string for each decade.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the closest words\nsotu_vec_mods |>\n  map(\\(mod) {\n    most_similar(mod, \"freedom\", verbose = FALSE) # get words similar to \"freedom\"\n  }) |>\n  map(\\(res) {\n    str_c(res$word, collapse = \", \") # summarize the words into a single string\n  })\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1950s`\n[1] \"justice, peace, man, not, spirit, ideal, peaceful, fight, us, preserve\"\n\n$`1960s`\n[1] \"free, unity, berlin, asia, independence, course, communist, nation, europe, both\"\n\n$`1970s`\n[1] \"defend, close, destroy, found, shape, side, deter, honor, root, international\"\n\n$`1980s`\n[1] \"democracy, peace, struggle, defend, everywhere, democratic, secure, country, fighter, free\"\n\n$`1990s`\n[1] \"liberty, define, all, lead, perfect, share, force, communism, shape, latin\"\n\n$`2000s`\n[1] \"east, democracy, friend, liberty, woman, determine, military, middle, region, remain\"\n\n$`2010s`\n[1] \"heart, vision, remind, hero, hopeful, dignity, capitol, celebrate, journey, honor\"\n```\n\n\n:::\n:::\n\n\nThe previous example shows that the closest words to \"freedom\" in each decade in a synchronic manner. We can inspect these synchronic changes and draw conclusions from them. However, we are interested in diachronic changes. To do this, we will need to align the word embeddings to a common space.\n\nNow we will identify the common words across the decades and subset the word embeddings to the common vocabulary. The `map()` function iterates over each model in the list and returns the rownames with `rownames()` (words) for each model. The `reduce()` function then iterates over the list of words and returns the intersection of the words across the models with `intersect()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the common vocabulary -----------------------------------\ncommon_vocab <-\n  sotu_vec_mods |>\n  map(rownames) |>\n  reduce(intersect)\n\nlength(common_vocab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 534\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(common_vocab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"the\" \"of\"  \"and\" \"be\"  \"to\"  \"in\" \n```\n\n\n:::\n:::\n\n\nThere are 534 words in the common vocabulary. Now we can subset the word embeddings to the common vocabulary. We will use the `map()` function to iterate over each model in the list and subset the model to the common vocabulary using the `common_vocab` variable as part of the bracket notation subset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Subset the models to the common vocabulary ----------------------\nsotu_vec_common_mods <-\n  sotu_vec_mods |>\n  map(\\(mod) {\n    mod[common_vocab, ] # subset each model to the common vocabulary\n  })\n\nsotu_vec_common_mods |> pluck(\"1950s\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   dim1 ...     dim100\n  1: the         0.2910 ... <100 dims>\n  2: of          0.2229 ... <100 dims>\n  3: and         0.2818 ... <100 dims>\n  4: be          0.1370 ... <100 dims>\n  5: to          0.2336 ... <100 dims>\n----                                  \n530: happen      0.2186 ... <100 dims>\n531: everything  0.2229 ... <100 dims>\n532: different   0.2373 ... <100 dims>\n533: debate      0.2244 ... <100 dims>\n534: city        0.2320 ... <100 dims>\n```\n\n\n:::\n\n```{.r .cell-code}\nsotu_vec_common_mods |> pluck(\"2010s\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   dim1 ...     dim100\n  1: the        -0.1464 ... <100 dims>\n  2: of         -0.1187 ... <100 dims>\n  3: and        -0.1090 ... <100 dims>\n  4: be         -0.1162 ... <100 dims>\n  5: to         -0.0842 ... <100 dims>\n----                                  \n530: happen     -0.1037 ... <100 dims>\n531: everything -0.0982 ... <100 dims>\n532: different  -0.1032 ... <100 dims>\n533: debate     -0.1111 ... <100 dims>\n534: city       -0.1179 ... <100 dims>\n```\n\n\n:::\n:::\n\n\nSo now each of the models in the list has the same vocabulary and the same number of dimensions, 100.\n\nNow we can align the models to a common space. The reason that the models need to be aligned is that the word embeddings are trained on different corpora. This means that the words will be represented in different spaces. We will use the Orthogonal Procrustes solution to align the models to a common coordinate space. {PsychWordVec} [@R-PsychWordVec] has a function, `orth_procrustes()`, that will do this for us. In the process of aligning the models, the models are converted to plain matrices so we will need to convert them back to `embed` matrix objects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Align the models to a common space\nsotu_aligned_mods <-\n  sotu_vec_common_mods |>\n  map(\\(mod) {\n    orth_procrustes(sotu_vec_common_mods[[1]], mod) # align to the first model\n  }) |>\n  map(\\(mod) {\n    emb <- as_embed(mod) # convert to a embed matrix object\n    emb\n  })\n```\n:::\n\n\nHaving a model for each decade which is aligned in vocabulary and space, we can now use these models to compare words across time. There are a number of ways we can compare words across time.\n\nIn our first approach, let's consider the semantic displacement of words over time in the vector space. We will do this by calculating the cosine difference between the word embeddings for a word in each decade. Once we have the cosine difference, we can visualize the change over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the cosine difference between the models --------------\nword <- \"freedom\"\n\nword_vectors <-\n  sotu_aligned_mods |>\n  map(\\(mod) {\n    mod[word, ]\n  })\n\ndifferences <-\n  word_vectors |>\n  map(\\(vec) {\n    cos_dist(vec, word_vectors[[1]])\n  })\n\ndifferences\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1950s`\n[1] 0\n\n$`1960s`\n[1] 0.0236\n\n$`1970s`\n[1] 0.0437\n\n$`1980s`\n[1] 0.0354\n\n$`1990s`\n[1] 0.0203\n\n$`2000s`\n[1] 0.0207\n\n$`2010s`\n[1] 0.0333\n```\n\n\n:::\n:::\n\n\nWe now have a list with the cosine difference for each decade compared to the first decade (\"1950s\"). We can visualize this with a line plot where the x-axis is the decade and the y-axis is the cosine difference. We will add a linear trend line to the plot to get a sense of the overall trend.\n\nI'll write a function to get the differences for a word and then return a data frame with the decade and the difference. This will make it easier to visualize the differences for multiple words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to get the cosine difference over time\n\nget_cosine_diff <- function(word, models) {\n  word_vectors <- map(models, \\(mod) {\n    mod[word, ]\n  })\n\n  differences <- map(word_vectors, \\(vec) {\n    cos_dist(vec, word_vectors[[1]])\n  })\n\n  tibble(word, decade = basename(names(differences)), difference = unlist(differences))\n}\n\nget_cosine_diff(word = \"freedom\", models = sotu_aligned_mods)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 3\n  word    decade difference\n  <chr>   <chr>       <dbl>\n1 freedom 1950s      0     \n2 freedom 1960s      0.0236\n3 freedom 1970s      0.0437\n4 freedom 1980s      0.0354\n5 freedom 1990s      0.0203\n6 freedom 2000s      0.0207\n7 freedom 2010s      0.0333\n```\n\n\n:::\n:::\n\n\nLet's create a function which performs this for us and can plot multiple words at the same time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_words <-\n  c(\"freedom\", \"nation\", \"country\", \"america\")\n\nplot_words |>\n  map(get_cosine_diff, models = sotu_aligned_mods) |>\n  bind_rows() |>\n  arrange(decade) |>\n  ggplot(aes(decade, difference, group = word, color = word)) +\n  geom_smooth(se = FALSE, span = 1) +\n  labs(title = word)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-plot-words-cosine-diff-1.png){#fig-plot-words-cosine-diff width=672}\n:::\n:::\n\n\n\nWe can then focus in on a particular word and the nearest words to that word in each decade. We will use the `map()` function to iterate over each model in the list and get the closest words to a word. We will then use the `str_c()` function to summarize the words into a single string for each decade.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to get the closest words to a word ---------------------\nword <- \"america\"\n\nsotu_vec_common_mods |>\n  map(\\(mod) {\n    most_similar(mod, word, verbose = FALSE)\n  }) |>\n  map(\\(mod) {\n    str_c(mod$word, collapse = \", \")\n  }) |>\n  enframe(name = \"decade\", value = \"words\") |>\n  unnest(words)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 2\n  decade words                                                                  \n  <chr>  <chr>                                                                  \n1 1950s  people, upon, life, democracy, standard, american, fail, within, deep,…\n2 1960s  leadership, agency, whole, serious, basic, historic, respect, responsi…\n3 1970s  where, liberty, determine, future, faith, happen, always, achieve, tru…\n4 1980s  never, great, let, ready, nation, faith, future, we, hope, struggle    \n5 1990s  liberty, freedom, important, duty, moment, arm, ahead, great, always, …\n6 2000s  remember, common, moment, first, rest, sense, beyond, share, reach, ch…\n7 2010s  strong, state, once, may, believe, union, your, among, citizen, forward\n```\n\n\n:::\n:::\n\n\nAnother approach is to visualize the vector space that words occupy over time. To do this we will collapse the word embeddings for each decade into a single matrix. We will append the decade to each word as not to lose the decade information. The we will extract the first two principal components of the matrix, so that we can visualize the data in two dimensions. We will then plot the data with a scatter plot where the x-axis is the first principal component and the y-axis is the second principal component. We will label the points with the words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the vector space of words over time -------------------\nsotu_joined_mods <-\n  sotu_aligned_mods |>\n  imap(\\(mod, index) {\n    rownames(mod) <- str_c(rownames(mod), \"_\", index)\n    mod\n  }) |>\n  reduce(rbind) |>\n  as_embed()\n```\n:::\n\n\nPCA on the word embeddings, yes! With the aligned models the results are much more sensible and interesting. The individual words are grouped closer together across time, in general, but there are exceptions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsotu_joined_pca <-\n  sotu_joined_mods |>\n  scale() |>\n  prcomp()\n\nsotu_pca_df <-\n  as_tibble(sotu_joined_pca$x[, 1:2]) |>\n  mutate(word = names(sotu_joined_pca$x[, 1]))\n\nsotu_pca_df |>\n  filter(str_detect(word, \"^(nation|country|america)_\")) |>\n  ggplot(aes(x = PC1, y = PC2, label = word)) +\n  geom_point() +\n  ggrepel::geom_text_repel()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-sotu-word-embed-pca-summary-1.png){#fig-sotu-word-embed-pca-summary width=672}\n:::\n:::\n\n\nThese kinds of visualizations can be very useful for exploring the data and drawing conclusions.\n\n## Summary\n\nIn this recipe, we have explored the State of the Union addresses from 1950 to 2019. We have used a number of tools and techniques to explore the data and draw conclusions. We have used {tidytext} to tokenize and lemmatize the words in the addresses. We have used {word2vec} to train word embeddings for each decade. We have used {PsychWordVec} to align the word embeddings to a common space. We have used {wordVectors} to explore the word embeddings. We have used {ggplot2} to visualize the data.\n\nThese strategies, and others, can be used to explore these questions or other questions in more depth. Exploratory analysis is where your creativity and curiosity can shine.\n\n## Check your understanding\n\n<!-- Comprehension questions -->\n\n1. In text analysis, <input class='webex-solveme nospaces' size='6' data-answer='[\"tf-idf\",\"tfidf\"]'/> is used to transform the effect of words that are common across all time periods and promote the effect of words that are more distinctive to each time period.\n2. What is the correct method to use when adding a linear trend line to a plot in ggplot2? <select class='webex-select'><option value='blank'></option><option value='answer'>geom_smooth(method = 'lm')</option><option value=''>geom_line()</option><option value=''>geom_bar()</option><option value=''>geom_point()</option></select>\n3. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select> When working with lists, `walk()` is like `map()` but does not return a value.\n4. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> When creating word embeddings, the CBOW model is better suited for less common words and smaller datasets compared to Skip-gram.\n6. The process of reducing the number of features in a dataset while retaining as much information as possible is known as <input class='webex-solveme nospaces' size='14' data-answer='[\"dimensionality\",\"dimension\"]'/> reduction.\n\n## Lab preparation\n\n<!-- Lab 08 description -->\n\nIn preparation for [Lab 8](https://github.com/qtalr/lab-08), review and and ensure that you are familiar with the following concepts:\n\n- Tokenizing text\n- Generating frequency and dispersion measures\n- Creating Term-Document Matrices\n- Using {purrr} to iterate over lists\n- Visualizations with {ggplot2}\n\nIn this lab, you will have the opportunity to apply the concepts from the materials in this chapter to a new dataset. You should consider the dataset and the questions that you want to ask of the data. You should also consider the tools and techniques that you will use to explore the data and draw conclusions. You will be asked to submit your code and a brief report of your findings.\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}