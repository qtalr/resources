{
  "hash": "8f3337240f981676d3fd8130b41aa9b4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"06. Organizing and documenting data\"\nsubtitle: \"Curating semi-structured data\"\ndescription: |\n  After acquiring data, the next step in process is to organize data that is not tabular into a curated dataset. A curated dataset is a tidy dataset that reflects the data without major modifications. This dataset serves as a more general starting point for further data transformation. In this recipe, we will focus on curating data from a semi-structured format.\ncategories: [preparation]\n---\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Skills**\n\n- Reading and parsing semi-structured data\n- Creating a custom function and iterating over a collection of files\n- Combining the results into a single dataset\n- Documenting the data curation process and resulting dataset\n:::\n\nIn this recipe, we will make use of {readr}, {dplyr}, {stringr}, and {purrr}, employ regular expressions to parse the semi-structured data, and use {qtalrkit} to document the dataset. Let's load those packages now.\n\n<!-- [ ] replace with {qtkit} -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(fs)\nlibrary(qtalrkit)\n```\n:::\n\n\nIn Lab 6, we will apply what we learn in this recipe to curate and document acquired data.\n\n## Concepts and strategies\n\n### Assessing the data\n\nAcquired data can be in a variety of formats. This will range from unstructured data such as running text to structured data such as tabular data. Semi-structured data is somewhere in between. It has some structure, but it is not as well defined as structured data and requires some work to organize it into a tidy dataset.\n\nAs a semi-structured example we will work with the The Switchboard Dialog Act Corpus (SWDA) [@SWDA2008] which extends the [Switchboard Corpus](https://catalog.ldc.upenn.edu/LDC97S62) with speech act annotation.\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nIf you would like to download and decompress the data yourself, you can do so by running the following code:\n\n<!-- [ ] get_archive_data() {qtkit} -->\n\n```r\nqtalrkit::get_compressed_data(\n  url = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz\",\n  target_dir = \"data/original/swda/\"\n)\n```\n:::\n\nAs a starting point, let's assume you have acquired the SWDA corpus and decompressed it into your project's *data/original/swda/* directory, as seen below.\n\n```{.bash}\ndata/\n├── analysis/\n├── derived/\n└── original/\n    └── swda/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\n```\n\nThe first step is to inspect the data directory and file structure (and of course any documentation files).\n\nThe *README* file contains basic information about the resource, the *doc/* directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with *sw...* contain individual conversation files.\n\nTaking a closer look at the first conversation file directory, *sw00utt/* we can see that it contains files with the *.utt* extension.\n\n```{.bash}\n├── sw00utt\n│   ├── sw_0001_4325.utt\n│   ├── sw_0002_4330.utt\n│   ├── sw_0003_4103.utt\n│   ├── sw_0004_4327.utt\n│   ├── sw_0005_4646.utt\n```\n\nLet's take a look inside a conversation file (*sw_0001_4325.utt*) to see how it is structured internally. You can do this by opening the file in a text editor or by using the `read_lines()` function from the {readr} package.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:\t4325_1632_1519\nTOPIC#:\t\t323\nDATE:\t\t920323\nTRANSCRIBER:\tglp\nUTT_CODER:\ttc\nDIFFICULTY:\t1\nTOPICALITY:\t3\nNATURALNESS:\t2\nECHO_FROM_B:\t1\nECHO_FROM_A:\t4\nSTATIC_ON_A:\t1\nSTATIC_ON_B:\t1\nBACKGROUND_A:\t1\nBACKGROUND_B:\t2\nREMARKS:        None.\n\n=========================================================================\n  \n\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }   \n\nqy^d          B.2 utt1: [ [ I guess, +   \n\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /  \n\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /  \n\nqy          A.5 utt1: Does it say something? /  \n\nsd          B.6 utt1: I think it usually does.  /\n```\n\n\n:::\n:::\n\n\nThere are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of `=` characters. Second, the header contains meta-information of various types. Third, the conversation text is interleaved with an annotation scheme.\n\nSome of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let's take a look at the *README* file.\n\nIn this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 'DAMSL' dialog act labels. The *README* file refers us to the *doc/manual.august1.html* file for more information on this scheme.\n\nAt this point we open the the *doc/manual.august1.html* file in a browser and do some investigation. We find out that 'DAMSL' stands for 'Discourse Annotation and Markup System of Labeling' and that the first characters of each line of the conversation text  correspond to one or a combination of labels for each utterance. So for our first utterances we have:\n\n```txt\no = \"Other\"\nqw = \"Wh-Question\"\nqy^d = \"Declarative Yes-No-Question\"\n+ = \"Segment (multi-utterance)\"\n```\n\nEach utterance is also labeled for speaker ('A' or 'B'), speaker turn ('1', '2', '3', etc.), and each utterance within that turn ('utt1', 'utt2', etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.\n\nNow let's turn to the meta-data in the header. We see here that there is information about the creation of the file: 'FILENAME', 'TOPIC', 'DATE', etc. The *doc/manual.august1.html* file doesn't have much to say about this information so I returned to the [LDC Documentation](https://catalog.ldc.upenn.edu/docs/LDC97S62/) and found more information in the [Online Documentation](https://catalog.ldc.upenn.edu/docs/LDC97S62/) section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the *caller_tab.csv* file. This tabular file does not contain column names, but the *caller_doc.txt* does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the 'FILENAME' information contained three pieces of useful information delimited by underscores `_`.\n\n```txt\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:\t4325_1632_1519\nTOPIC#:\t\t323\nDATE:\t\t920323\nTRANSCRIBER:\tglp\n```\n\nThe first information is the document id (`4325`), the second and third correspond to the speaker number: the first being speaker A (`1632`) and the second speaker B (`1519`).\n\nIn sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of `=` characters. The header section contains a 'FILENAME' line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let's set out to create a tidy dataset with the column structure as in @tbl-swda-idealized-dataset.\n\n\n::: {#tbl-swda-idealized-dataset .cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Idealized curated dataset</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> variable </th>\n   <th style=\"text-align:left;\"> name </th>\n   <th style=\"text-align:left;\"> type </th>\n   <th style=\"text-align:left;\"> description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> doc_id </td>\n   <td style=\"text-align:left;\"> Document ID </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The unique identifier for the conversation </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> damsl_tag </td>\n   <td style=\"text-align:left;\"> DAMSL Tag </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The DAMSL tag for the utterance </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> speaker </td>\n   <td style=\"text-align:left;\"> Speaker </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The speaker of the utterance </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> turn_num </td>\n   <td style=\"text-align:left;\"> Turn Number </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The turn number of the utterance </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> utterance_num </td>\n   <td style=\"text-align:left;\"> Utterance Number </td>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> The utterance number of the utterance </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> utterance_text </td>\n   <td style=\"text-align:left;\"> Utterance Text </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The text of the utterance </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> speaker_id </td>\n   <td style=\"text-align:left;\"> Speaker ID </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> The unique identifier for the speaker </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### Tidy the data\n\nThere are many ways to approach the task of tidying the data in general, and this semi-structured data in particular. In this recipe, we will take a step-by-step approach to parsing the semi-structured data in one file and then apply this process to all of the files in the corpus using a custom function.\n\nLet's begin by reading one of the conversation files into R as a character vector using the `read_lines()` function from {readr}.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read a single file as character vector\ndoc_chr <-\n  read_lines(file = \"data/original/swda/sw00utt/sw_0001_4325.utt\")\n```\n:::\n\n\nTo isolate the vector element that contains the document and speaker ids, we use `str_subset()` from {stringr}. This function takes two arguments, a string and a pattern, and returns any vector element that matches the pattern.\n\nIn this case we are looking for a pattern that matches three groups of digits separated by underscores. To test out a pattern, we can use the `str_view()` function. We will use the regular expression character class `\\\\d` for digits and the `+` operator to match 1 or more contiguous digits. We then separate three groups of `\\\\d+` with underscores `_`. The result is `\\\\d+_\\\\d+_\\\\d+`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test out a pattern\ndoc_chr |>\n  str_view(pattern = \"\\\\d+_\\\\d+_\\\\d+\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[15] │ FILENAME:{\\t}<4325_1632_1519>\n```\n\n\n:::\n:::\n\n\nWe can see that this pattern matches the line we are looking for. Now we can use this pattern with `str_subset()` to return the vector element that contains this pattern.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Isolate the vector element that contains the document and speaker ids\nstr_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"FILENAME:\\t4325_1632_1519\"\n```\n\n\n:::\n:::\n\n\n::: {.callout}\n**{{< fa regular hand-point-up >}} Tip**\n\nRegular Expressions are a powerful pattern matching syntax. They are used extensively in text manipulation and we will see them again and again.\n\nTo develop regular expressions, it is helpful to have a tool that allows you to interactively test your pattern matching. {stringr} has a handy function `str_view()` which allows for interactive pattern matching. A good website to practice Regular Expressions is [RegEx101](https://regex101.com/). You can also install {regexplain} [@R-regexplain] in R to get access to a useful [RStudio Addin](https://rstudio.github.io/rstudioaddins/).\n:::\n\nThe next step is to extract the three digit sequences that correspond to the `doc_id`, `speaker_a_id`, and `speaker_b_id`. First we extract the pattern that we have identified with `str_extract()` and then we can break up the single character vector into multiple parts based on the underscore `_`. The `str_split()` function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |> # isolate vector element\n  str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |> # extract the pattern\n  str_split(\"_\") # split the character vector by underscore\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] \"4325\" \"1632\" \"1519\"\n```\n\n\n:::\n:::\n\n\nA list is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same --hence the tabular format).\n\nIn this case we have a list of length 1, whose sole element is a character vector of length 3 --one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our `str_split()` function we don't want the results to be conflated as a single character vector blurring the distinction between the individual character vectors.\n\nIn this case, however, we want to extract the three elements of the character vector and assign them to meaningful variable names. To do this we will use the `unlist()` function which will convert the list into a single character vector. We will assign this result to `speaker_info_chr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspeaker_info_chr <-\n  str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |>\n  str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |>\n  str_split(\"_\") |>\n  unlist() # convert the list to a character vector\n\n# Preview\nspeaker_info_chr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"4325\" \"1632\" \"1519\"\n```\n\n\n:::\n:::\n\n\n`speaker_info_chr` is now a character vector of length three. Let's subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoc_id <- speaker_info_chr[1] # extract by index\nspeaker_a_id <- speaker_info_chr[2] # extract by index\nspeaker_b_id <- speaker_info_chr[3] # extract by index\n```\n:::\n\n\nThe next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of `=` separates the header section from the text section. What we need to do is to index the point in our character vector `doc_chr` where that line occurs and then subset the `doc_chr` from that point until the end of the character vector.\n\nLet's first find the point where the `=` sequence occurs. We will again use the `str_view()` to test out a pattern that matches a contiguous sequence of `=`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_view(doc_chr, \"=+\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[31] │ <=========================================================================>\n```\n\n\n:::\n:::\n\n\nSo for this file we see there is one element that matches and that element's index is `31`.\n\nNow it is important to keep in mind that we are working with a single file from the `swda/` data. Since our plan is to use this code to apply to other files, we need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the `=+` pattern will match `=`, or `==`, or `===`, etc. it is not implausible to believe that there might be a `=` character on some other line in one of the other files.\n\nLet's update our regular expression to avoid this potential scenario by only matching sequences of three or more `=`. In this case we will make use of the curly bracket operators `{}`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_view(doc_chr, \"={3,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[31] │ <=========================================================================>\n```\n\n\n:::\n:::\n\n\nWe will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for `===`, `====`, etc.\n\nTo extract just the index of the match, we can use the `str_which()` function with the same pattern. This will return the index of the vector element that matches the pattern. However, consider what we are doing. We actually are using this index to subset the vector, so we need to increment the index by 1 to get the next vector element. Let's do this and then assign the result to `text_start_index`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find where text starts\ntext_start_index <- str_which(doc_chr, \"={3,}\") + 1\n```\n:::\n\n\nThe index for the end of the text is simply the length of the `doc_chr` vector. We can use the `length()` function to get this index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find where text ends\ntext_end_index <- length(doc_chr)\n```\n:::\n\n\nWe now have the bookends, so to speak, for our text section. To extract the text we subset the `doc_chr` vector by these indices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract text between indices\ntext <- doc_chr[text_start_index:text_end_index]\n\n# Preview\nhead(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"  \"                                       \n[2] \"\"                                         \n[3] \"o          A.1 utt1: Okay.  /\"            \n[4] \"qw          A.1 utt2: {D So, }   \"        \n[5] \"\"                                         \n[6] \"qy^d          B.2 utt1: [ [ I guess, +   \"\n```\n\n\n:::\n:::\n\n\nThe text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the `str_trim()` function which by default will remove leading and trailing whitespace from each line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove leading and trailing whitespace\ntext <- str_trim(text)\n\n# Preview\nhead(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"                                      \n[2] \"\"                                      \n[3] \"o          A.1 utt1: Okay.  /\"         \n[4] \"qw          A.1 utt2: {D So, }\"        \n[5] \"\"                                      \n[6] \"qy^d          B.2 utt1: [ [ I guess, +\"\n```\n\n\n:::\n:::\n\n\nTo remove blank lines we will use `str_subset()` to subset the `text` vector. The `.+` pattern will match elements that are not blank. We will assign the result to `text` overwriting the original `text` vector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove blank lines\ntext <- str_subset(text, \".+\")\n\n# Preview\nhead(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"o          A.1 utt1: Okay.  /\"                                                                  \n[2] \"qw          A.1 utt2: {D So, }\"                                                                 \n[3] \"qy^d          B.2 utt1: [ [ I guess, +\"                                                         \n[4] \"+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\"\n[5] \"+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\"                        \n[6] \"qy          A.5 utt1: Does it say something? /\"                                                 \n```\n\n\n:::\n:::\n\n\nOur first step towards a tidy dataset is to now combine the `doc_id` and each element of `text` in a data frame, leaving aside our speaker ids. We will use the `tibble()` function and pass the variables as named arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine info and text into a data frame\nswda_df <- tibble(doc_id, text)\n\n# Preview\nslice_head(swda_df, n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n  doc_id text                                                                   \n  <chr>  <chr>                                                                  \n1 4325   o          A.1 utt1: Okay.  /                                          \n2 4325   qw          A.1 utt2: {D So, }                                         \n3 4325   qy^d          B.2 utt1: [ [ I guess, +                                 \n4 4325   +          A.3 utt1: What kind of experience [ do you, + do you ] have…\n5 4325   +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n```\n\n\n:::\n:::\n\n\nWith our data now in a data frame, it's time to parse the `text` column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns.\n\nTo do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row.\n\nThe best way to learn regular expressions is to use them. To this end I've included a link to the interactive regular expression practice website [regex101](https://regex101.com).\n\nOpen this site and copy the text below into the 'TEST STRING' field.\n\n```txt\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }\nqy^d          B.2 utt1: [ [ I guess, +\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\nqy          A.5 utt1: Does it say something? /\nsd          B.6 utt1: I think it usually does.  /\nad          B.6 utt2: You might try, {F uh, }  /\nh          B.6 utt3: I don't know,  /\nad          B.6 utt4: hold it down a little longer,  /\n```\n\n\n::: {.cell}\n::: {.cell-output-display}\n![RegEx101](images/regex-101.png){width=959}\n:::\n:::\n\n\nNow manually type the following regular expressions into the 'REGULAR EXPRESSION' field one-by-one (each is on a separate line). Notice what is matched as you type and when you've finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.\n\n```txt\n^.+?\\s\n[AB]\\.\\d+\nutt\\d+\n:.+$\n```\n\nAs you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text.\n\nTo apply these expressions to our data and extract this information into separate columns we will make use of the `mutate()` and `str_extract()` functions. `mutate()` will take our data frame and create new columns with values we match and extract from each row in the data frame with `str_extract()`.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nNotice that `str_extract()` is different than `str_extract_all()`. When we work with `mutate()` each row will be evaluated in turn, therefore we only need to make one match per row.\n:::\n\nI've chained each of these steps in the code below, dropping the original `text` column with `select(-text)`, and overwriting `swda_df` with the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract column information from `text`\nswda_df <-\n  swda_df |> # current dataset\n  mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |> # damsl tags\n  mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |> # speaker_turn pairs\n  mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |> # utterance number\n  mutate(utterance_text = str_extract(text, \":.+$\")) |> # utterance text\n  select(-text) # drop the `text` column\n\n# Preview\nglimpse(swda_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 159\nColumns: 5\n$ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      <chr> \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n$ speaker_turn   <chr> \"A.1\", \"A.1\", \"B.2\", \"A.3\", \"B.4\", \"A.5\", \"B.6\", \"B.6\",…\n$ utterance_num  <chr> \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n$ utterance_text <chr> \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n```\n\n\n:::\n:::\n\n\n::: {.callout .halfsize}\n**{{< fa exclamation-triangle >}} Warning**\n\nOne twist you will notice is that regular expressions in R require double backslashes (`\\\\`) where other programming environments use a single backslash (`\\`).\n:::\n\nThere are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the `speaker_turn` column into `speaker` and `turn_num` columns and second we need to remove unwanted characters from the `damsl_tag`, `utterance_num`, and `utterance_text` columns.\n\nTo separate the values of a column into two columns we use the `separate_wider_delim()` function. It takes a column to separate, a delimiter to use to separate the values, and a character vector of the names of the new columns to create.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Separate speaker_turn into distinct columns\nswda_df <-\n  swda_df |>\n  separate_wider_delim(\n    cols = speaker_turn,\n    delim = \".\",\n    names = c(\"speaker\", \"turn_num\")\n  )\n\n# Preview\nglimpse(swda_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 159\nColumns: 6\n$ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      <chr> \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n$ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  <chr> \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n$ utterance_text <chr> \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n```\n\n\n:::\n:::\n\n\nTo remove unwanted leading or trailing whitespace we apply the `str_trim()` function. For removing other characters we matching the character(s) and replace them with an empty string (`\"\"`) with the `str_replace()` function. Again, I've chained these functions together and overwritten `data` with the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean up column information\nswda_df <-\n  swda_df |> # current dataset\n  mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace\n  mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |> # remove 'utt'\n  mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |> # remove ': '\n  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\n# Preview\nglimpse(swda_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 159\nColumns: 6\n$ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n```\n\n\n:::\n:::\n\n\nTo round out our tidy dataset for this single conversation file we will connect the `speaker_a_id` and `speaker_b_id` with speaker A and B in our current dataset adding a new column `speaker_id`. The `case_when()` function does exactly this: allows us to map rows of `speaker` with the value \"A\" to `speaker_a_id` and rows with value \"B\" to `speaker_b_id`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Link speaker with speaker_id\nswda_df <-\n  swda_df |> # current dataset\n  mutate(speaker_id = case_when( # create speaker_id\n    speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n    speaker == \"B\" ~ speaker_b_id, # speaker_b_id value when B\n    TRUE ~ NA_character_ # NA otherwise\n  ))\n\n# Preview\nglimpse(swda_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 159\nColumns: 7\n$ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n```\n\n\n:::\n:::\n\n\nWe now have the tidy dataset we set out to create. But this dataset only includes one conversation file! We want to apply this code to all 1,155 conversation files in the `swda/` corpus.\n\nThe approach will be to create a custom function which groups the code we've done for this single file and then iteratively send each file from the corpus through this function and combine the results into one data frame.\n\nHere's the custom function with some extra code to print a progress message for each file when it runs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# [ ] add to {qtalrkit}, note the convention of `extract_` prefix for curation functions. In combination with `get_compressed_data()` this corpus can be curated with few steps.\n\nextract_swda_data <- function(file) {\n  # Progress message\n  file_basename <- basename(file) # file name\n  message(\"Processing \", file_basename, \"\\n\")\n\n  # Read `file` by lines\n  doc_chr <- read_lines(file)\n\n  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`\n  speaker_info_chr <-\n    str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |>\n    str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |>\n    str_split(\"_\") |>\n    unlist()\n\n  doc_id <- speaker_info_chr[1]\n  speaker_a_id <- speaker_info_chr[2]\n  speaker_b_id <- speaker_info_chr[3]\n\n  # Extract `text`\n  text_start_index <- str_which(doc_chr, \"={3,}\") + 1\n  text_end_index <- length(doc_chr)\n\n  text <-\n    doc_chr[text_start_index:text_end_index] |>\n    str_trim() |>\n    str_subset(\".+\")\n\n  swda_df <- tibble(doc_id, text) # tidy format `doc_id` and `text`\n\n  # Extract column information from `text`\n  swda_df <-\n    swda_df |> # current dataset\n    mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |> # damsl tags\n    mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |> # speaker_turn pairs\n    mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |> # utterance number\n    mutate(utterance_text = str_extract(text, \":.+$\")) |> # utterance text\n    select(-text) # drop the `text` column\n\n  # Separate speaker_turn into distinct columns\n  swda_df <-\n    swda_df |> # current dataset\n    separate_wider_delim(\n      cols = speaker_turn,\n      delim = \".\",\n      names = c(\"speaker\", \"turn_num\")\n    )\n\n  # Clean up column information\n  swda_df <-\n    swda_df |> # current dataset\n    mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace\n    mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |> # remove 'utt'\n    mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |> # remove ': '\n    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\n  # Link speaker with speaker_id\n  swda_df <-\n    swda_df |> # current dataset\n    mutate(speaker_id = case_when( # create speaker_id\n      speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n      speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B\n    ))\n\n  message(\"Processed \", file_basename, \"\\n\")\n  return(swda_df)\n}\n```\n:::\n\n\nAs a sanity check we will run the `extract_swda_data()` function on a the conversation file we were just working on to make sure it works as expected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Process a single file (test)\nextract_swda_data(\n  file = \"../data/original/swda/sw00utt/sw_0001_4325.utt\"\n) |>\n  glimpse()\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 159\nColumns: 7\n$ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n```\n\n\n:::\n:::\n\n\nLooks good!\n\nSo now it's time to create a vector with the paths to all of the conversation files. The `ls_dif()` function from {fs} interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (`regexp = \\\\.utt$`) so we don't accidentally include other files in the corpus. `recurse` set to `TRUE` means we will get the full path to each file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# List all conversation files\nswda_files_chr <-\n  dir_ls(\n    path = \"../data/original/swda/\", # source directory\n    recurse = TRUE, # traverse all sub-directories\n    type = \"file\", # only return files\n    regexp = \"\\\\.utt$\"\n  ) # only return files ending in .utt\n\nhead(swda_files_chr) # preview file paths\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\ndata/original/swda/sw00utt/sw_0001_4325.utt\ndata/original/swda/sw00utt/sw_0002_4330.utt\ndata/original/swda/sw00utt/sw_0003_4103.utt\ndata/original/swda/sw00utt/sw_0004_4327.utt\ndata/original/swda/sw00utt/sw_0005_4646.utt\ndata/original/swda/sw00utt/sw_0006_4108.utt\n```\n\n\n:::\n:::\n\n\nTo pass each conversation file in the vector of paths to our conversation files iteratively to the `extract_swda_data()` function we use `map_dfr()`. This will apply the function to each conversation file and return a data frame for each and then combine the results into a single data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Process all conversation files\nswda_df <-\n  swda_files_chr |> # pass file names\n  map_dfr(extract_swda_data) # read and tidy iteratively\n\n# Preview\nglimpse(swda_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 223,606\nColumns: 7\n$ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n$ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n$ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n$ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n$ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n$ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n```\n\n\n:::\n:::\n\n\nWe now see that we have 223, 606 observations (individual utterances in this dataset). The structure of the data frame matches our idealized dataset in Table \\@ref(tab:swda-idealized-dataset).\n\nIt also is a good idea to inspect the data frame to ensure that the data is as expected. One is to check for missing values. We can use the `skim()` function from {skimr} to get a quick summary of the data frame. Another is to spot check the data frame to see if the values are as expected. As we are working with a fairly large dataset, we can use the `slice_sample()` function from {dplyr} to randomly sample a subset of rows from the data frame.\n\n### Documentation\n\nWe now have a tidy dataset, but we need to document the data curation process and the resulting dataset. The script used to curate the data should be cleaned up and well documented in prose and code comments.\n\nWe then need to write the dataset to disk and create a data dictionary. We will make sure to add the curated dataset to the `derived/` directory and the data dictionary close to the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write to disk\ndir_create(path = \"data/derived/swda/\") # create swda subdirectory\n\nwrite_csv(swda_df,\n  file = \"data/derived/swda/swda_curated.csv\"\n)\n```\n:::\n\n\nThe directory structure now looks like this:\n\n```{.bash}\ndata/\n├── analysis/\n├── derived/\n│   └── swda/\n│       └── swda_curated.csv\n└── original/\n    └── swda/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\n```\n\nThe data dictionary file will contain information about the dataset variables and their values. This file can be created manually and edited with a text editor or spreadsheet software. Or alternatively, the scaffolding for a CSV file can be generated with the `create_data_dictionary()` function from {qtalrkit}.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data dictionary\ncreate_data_dictionary(\n  data = swda,\n  file_path = \"data/derived/swda/swda_dd.csv\"\n)\n```\n:::\n\n\n## Summary\n\nIn this recipe, we learned how to read and parse semi-structured data, create a custom function and iterate over a collection of files, combine the results into a single dataset, and document the data curation process and resulting dataset.\n\nThe skills we used in this recipe include regular expressions, the {readr}, {dplyr}, {stringr}, and {purrr}, and {qtalrkit} for documenting the dataset.\n\n## Check your understanding\n\n1. The first thing that should be done in the data curation process is to <select class='webex-select'><option value='blank'></option><option value=''>know what packages you are going to use</option><option value='answer'>explore the data documentation and understand the resource</option><option value=''>read the data into R</option><option value=''>parse the data into a tidy dataset</option></select>.\n2. The `read_lines()` function from {readr} will read a file into R as a <select class='webex-select'><option value='blank'></option><option value='answer'>character vector</option><option value=''>data frame</option><option value=''>list</option><option value=''>matrix</option></select>.\n3. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select> The `separate_wider_delim()` function from {tidyr} will separate a column into two or more columns based on a delimiter (*e.g.* `-`, `.`, *etc.*).\n4. Which of the following functions from {stringr} will return vector elements which contain a match for a pattern? <select class='webex-select'><option value='blank'></option><option value='answer'>str_subset()</option><option value=''>str_extract()</option><option value=''>str_replace()</option><option value=''>str_trim()</option></select>\n5. The `map_dfr()` function from {purrr} will apply a function to each element of a vector and return a <select class='webex-select'><option value='blank'></option><option value=''>list</option><option value=''>nested data frame</option><option value='answer'>data frame with rows combined</option><option value=''>data frame with columns combined</option></select>.\n6. A data dictionary is a document that describes the <select class='webex-select'><option value='blank'></option><option value=''>data curation process</option><option value=''>data analysis process</option><option value='answer'>dataset variables and their values</option><option value=''>data visualization process</option></select>.\n\n## Lab preparation\n\nBefore beginning [Lab 6](https://github.com/qtalr/lab-06), review and ensure that you are familiar with the following:\n\n- Vector, data frame, and list data structures\n- Subsetting and indexing vectors, data frames, and lists\n- Basic regular expressions such as character classes, quantifiers, and anchors\n- Reading, writing, and manipulating files\n- Creating and employing custom functions\n\nIn this lab, we will practice these skills and expand our use of the {readr}, {dplyr}, {stringr}, and {purrr} to curate and document a dataset.\n\nYou will have a choice of data to curate. Before you start the lab, you should consider which data source you would like to use, what the idealized structure the curated dataset will take, and what strategies you will likely employ to curate the dataset. You should also consider the information you need to document the data curation process.\n\n## References\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}